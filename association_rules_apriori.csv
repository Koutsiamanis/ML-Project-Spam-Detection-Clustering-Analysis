antecedents,consequents,antecedent support,consequent support,support,confidence,lift,representativity,leverage,conviction,zhangs_metric,jaccard,certainty,kulczynski
"frozenset({'math_cat_υψηλό', 'gender_female'})","frozenset({'writing_cat_υψηλό', 'reading_cat_υψηλό', 'lunch_standard'})",0.068,0.144,0.059,0.8676470588235293,6.02532679738562,1.0,0.049207999999999995,6.467555555555551,0.8948861569797046,0.3856209150326797,0.8453820780648708,0.6386846405228758
"frozenset({'writing_cat_υψηλό', 'reading_cat_υψηλό', 'lunch_standard'})","frozenset({'math_cat_υψηλό', 'gender_female'})",0.144,0.068,0.059,0.4097222222222222,6.02532679738562,1.0,0.049207999999999995,1.5789176470588233,0.9743386662442579,0.3856209150326797,0.366654744873629,0.6386846405228758
"frozenset({'writing_cat_υψηλό', 'lunch_standard'})","frozenset({'math_cat_υψηλό', 'gender_female', 'reading_cat_υψηλό'})",0.162,0.066,0.059,0.3641975308641975,5.518144407033295,1.0,0.048308,1.469009708737864,0.9770640346264311,0.3491124260355029,0.3192693049937875,0.6290684624017957
"frozenset({'math_cat_υψηλό', 'gender_female', 'reading_cat_υψηλό'})","frozenset({'writing_cat_υψηλό', 'lunch_standard'})",0.066,0.162,0.059,0.8939393939393938,5.518144407033295,1.0,0.048308,7.9011428571428475,0.8766377526948064,0.3491124260355029,0.8734360309539305,0.6290684624017957
"frozenset({'writing_cat_υψηλό', 'lunch_standard'})","frozenset({'math_cat_υψηλό', 'gender_female'})",0.162,0.068,0.059,0.3641975308641975,5.355846042120551,1.0,0.047984,1.4658640776699028,0.9705109016625543,0.34502923976608185,0.31780850951094153,0.6159222948438634
"frozenset({'math_cat_υψηλό', 'gender_female'})","frozenset({'writing_cat_υψηλό', 'lunch_standard'})",0.068,0.162,0.059,0.8676470588235293,5.355846042120551,1.0,0.047984,6.331555555555552,0.8726267549283481,0.34502923976608185,0.8420609293836866,0.6159222948438634
frozenset({'math_cat_υψηλό'}),"frozenset({'gender_male', 'reading_cat_υψηλό', 'lunch_standard'})",0.176,0.062,0.058,0.3295454545454546,5.315249266862171,1.0,0.047088000000000005,1.399050847457627,0.9852695011717442,0.32222222222222224,0.28522969567745693,0.6325146627565983
"frozenset({'gender_male', 'reading_cat_υψηλό', 'lunch_standard'})",frozenset({'math_cat_υψηλό'}),0.062,0.176,0.058,0.935483870967742,5.315249266862171,1.0,0.047088000000000005,12.772000000000013,0.8655245937798692,0.32222222222222224,0.9217037269025995,0.6325146627565983
"frozenset({'math_cat_υψηλό', 'gender_female'})","frozenset({'writing_cat_υψηλό', 'reading_cat_υψηλό'})",0.068,0.185,0.065,0.9558823529411764,5.166931637519872,1.0,0.05242,18.473333333333304,0.8653020798943547,0.34574468085106386,0.9458679177192348,0.6536168521462639
"frozenset({'writing_cat_υψηλό', 'reading_cat_υψηλό'})","frozenset({'math_cat_υψηλό', 'gender_female'})",0.185,0.068,0.065,0.35135135135135137,5.166931637519872,1.0,0.05242,1.4368333333333332,0.9895233600755073,0.34574468085106386,0.30402505509801653,0.6536168521462639
"frozenset({'math_cat_υψηλό', 'gender_female', 'lunch_standard'})","frozenset({'writing_cat_υψηλό', 'reading_cat_υψηλό'})",0.062,0.185,0.059,0.9516129032258064,5.143853530950305,1.0,0.047529999999999996,16.84333333333331,0.8588413862888945,0.31382978723404253,0.940629329111419,0.6352659110723626
"frozenset({'writing_cat_υψηλό', 'reading_cat_υψηλό'})","frozenset({'math_cat_υψηλό', 'gender_female', 'lunch_standard'})",0.185,0.062,0.059,0.3189189189189189,5.143853530950304,1.0,0.047529999999999996,1.3772222222222221,0.9884579390662369,0.31382978723404253,0.27390076643807987,0.6352659110723626
"frozenset({'math_cat_υψηλό', 'writing_cat_υψηλό', 'gender_female'})","frozenset({'reading_cat_υψηλό', 'lunch_standard'})",0.065,0.182,0.059,0.9076923076923076,4.987320371935756,1.0,0.04717,8.861666666666656,0.8550711501858062,0.31382978723404253,0.8871544103817941,0.6159340659340659
"frozenset({'reading_cat_υψηλό', 'lunch_standard'})","frozenset({'math_cat_υψηλό', 'writing_cat_υψηλό', 'gender_female'})",0.182,0.065,0.059,0.3241758241758242,4.987320371935756,1.0,0.04717,1.3834959349593496,0.9773735029629936,0.31382978723404253,0.2771933948404537,0.6159340659340659
"frozenset({'gender_male', 'reading_cat_υψηλό'})","frozenset({'math_cat_υψηλό', 'lunch_standard'})",0.076,0.156,0.058,0.7631578947368421,4.892037786774629,1.0,0.046144000000000004,3.5635555555555563,0.8610240334378265,0.33333333333333337,0.7193813918682964,0.5674763832658569
"frozenset({'math_cat_υψηλό', 'lunch_standard'})","frozenset({'gender_male', 'reading_cat_υψηλό'})",0.156,0.076,0.058,0.3717948717948718,4.892037786774629,1.0,0.046144000000000004,1.4708571428571429,0.9426376858963883,0.33333333333333337,0.3201243201243201,0.5674763832658569
frozenset({'math_cat_υψηλό'}),"frozenset({'gender_male', 'reading_cat_υψηλό'})",0.176,0.076,0.065,0.3693181818181819,4.859449760765552,1.0,0.051624,1.4650810810810813,0.9638536221060493,0.34759358288770054,0.31744391971664704,0.6122906698564594
"frozenset({'gender_male', 'reading_cat_υψηλό'})",frozenset({'math_cat_υψηλό'}),0.076,0.176,0.065,0.855263157894737,4.859449760765551,1.0,0.051624,5.693090909090913,0.8595404595404595,0.34759358288770054,0.8243484925907002,0.6122906698564594
"frozenset({'writing_cat_υψηλό', 'test preparation course_none'})","frozenset({'gender_female', 'reading_cat_υψηλό', 'lunch_standard'})",0.091,0.12,0.053,0.5824175824175825,4.853479853479854,1.0,0.04208,2.107368421052632,0.8734458351495528,0.33544303797468356,0.5254745254745256,0.5120421245421245
"frozenset({'gender_female', 'reading_cat_υψηλό', 'lunch_standard'})","frozenset({'writing_cat_υψηλό', 'test preparation course_none'})",0.12,0.091,0.053,0.44166666666666665,4.853479853479853,1.0,0.04208,1.6280597014925373,0.902229845626072,0.33544303797468356,0.38577191052438575,0.5120421245421245
"frozenset({'reading_cat_υψηλό', 'lunch_standard'})","frozenset({'math_cat_υψηλό', 'gender_female'})",0.182,0.068,0.06,0.32967032967032966,4.848093083387201,1.0,0.047624,1.3903606557377046,0.9703341483292584,0.3157894736842105,0.280762156298637,0.6060116354234001
"frozenset({'math_cat_υψηλό', 'gender_female'})","frozenset({'reading_cat_υψηλό', 'lunch_standard'})",0.068,0.182,0.06,0.8823529411764705,4.848093083387201,1.0,0.047624,6.952999999999992,0.8516452074391989,0.3157894736842105,0.8561771897022865,0.6060116354234001
"frozenset({'test preparation course_completed', 'reading_cat_υψηλό', 'lunch_standard'})","frozenset({'math_cat_υψηλό', 'writing_cat_υψηλό'})",0.096,0.114,0.053,0.5520833333333333,4.842836257309941,1.0,0.042055999999999996,1.9780465116279067,0.8777759225246283,0.33757961783439483,0.49445071482317526,0.5084978070175438
"frozenset({'math_cat_υψηλό', 'writing_cat_υψηλό'})","frozenset({'test preparation course_completed', 'reading_cat_υψηλό', 'lunch_standard'})",0.114,0.096,0.053,0.46491228070175433,4.842836257309941,1.0,0.042055999999999996,1.6894426229508195,0.8956088419438647,0.33757961783439483,0.4080888060860114,0.5084978070175438
"frozenset({'math_cat_υψηλό', 'writing_cat_υψηλό'})","frozenset({'reading_cat_υψηλό', 'lunch_standard'})",0.114,0.182,0.099,0.868421052631579,4.771544245228456,1.0,0.078252,6.216800000000002,0.8921266844517409,0.5025380710659899,0.8391455411143998,0.7061885482938115
"frozenset({'reading_cat_υψηλό', 'lunch_standard'})","frozenset({'math_cat_υψηλό', 'writing_cat_υψηλό'})",0.182,0.114,0.099,0.543956043956044,4.771544245228456,1.0,0.078252,1.9427951807228918,0.9662888049196117,0.5025380710659899,0.485277701981991,0.7061885482938115
"frozenset({'writing_cat_υψηλό', 'math_cat_μέτριο'})","frozenset({'gender_female', 'reading_cat_υψηλό'})",0.094,0.159,0.071,0.7553191489361701,4.7504348989696235,1.0,0.05605399999999999,3.4371304347826075,0.8714050306252525,0.3901098901098901,0.7090596301262427,0.6009300147196573
"frozenset({'gender_female', 'reading_cat_υψηλό'})","frozenset({'writing_cat_υψηλό', 'math_cat_μέτριο'})",0.159,0.094,0.071,0.4465408805031446,4.7504348989696235,1.0,0.05605399999999999,1.6369772727272724,0.9387550032657299,0.3901098901098901,0.38911796965026996,0.6009300147196573
"frozenset({'math_cat_υψηλό', 'gender_female', 'reading_cat_υψηλό'})",frozenset({'writing_cat_υψηλό'}),0.066,0.208,0.065,0.9848484848484849,4.734848484848485,1.0,0.051272,52.27200000000005,0.8445396145610278,0.3110047846889952,0.9808692990511173,0.6486742424242424
frozenset({'writing_cat_υψηλό'}),"frozenset({'math_cat_υψηλό', 'gender_female', 'reading_cat_υψηλό'})",0.208,0.066,0.065,0.3125,4.734848484848484,1.0,0.051272,1.3585454545454545,0.9959595959595958,0.3110047846889952,0.2639186295503212,0.6486742424242424
frozenset({'writing_cat_υψηλό'}),"frozenset({'math_cat_υψηλό', 'gender_female', 'reading_cat_υψηλό', 'lunch_standard'})",0.208,0.06,0.059,0.28365384615384615,4.727564102564102,1.0,0.04652,1.312214765100671,0.9955487074131142,0.2822966507177033,0.23792962356792144,0.6334935897435897
"frozenset({'math_cat_υψηλό', 'gender_female', 'reading_cat_υψηλό', 'lunch_standard'})",frozenset({'writing_cat_υψηλό'}),0.06,0.208,0.059,0.9833333333333333,4.727564102564102,1.0,0.04652,47.519999999999854,0.8388027407140282,0.2822966507177033,0.9789562289562289,0.6334935897435897
"frozenset({'reading_cat_υψηλό', 'lunch_standard'})","frozenset({'test preparation course_completed', 'math_cat_υψηλό', 'writing_cat_υψηλό'})",0.182,0.062,0.053,0.29120879120879123,4.696915987238568,1.0,0.041716,1.3233798449612404,0.9622180190985837,0.2774869109947644,0.2443590524613979,0.5730237504431053
"frozenset({'test preparation course_completed', 'math_cat_υψηλό', 'writing_cat_υψηλό'})","frozenset({'reading_cat_υψηλό', 'lunch_standard'})",0.062,0.182,0.053,0.8548387096774194,4.696915987238568,1.0,0.041716,5.6351111111111125,0.8391197650561212,0.2774869109947644,0.8225412098745957,0.5730237504431053
"frozenset({'writing_cat_υψηλό', 'lunch_standard'})","frozenset({'math_cat_υψηλό', 'reading_cat_υψηλό'})",0.162,0.131,0.099,0.6111111111111112,4.664970313825276,1.0,0.077778,2.234571428571429,0.937513560425255,0.5103092783505154,0.5524868942590462,0.6834181509754029
"frozenset({'math_cat_υψηλό', 'reading_cat_υψηλό'})","frozenset({'writing_cat_υψηλό', 'lunch_standard'})",0.131,0.162,0.099,0.7557251908396947,4.664970313825275,1.0,0.077778,3.4305625,0.9040694633329845,0.5103092783505154,0.7085026143671774,0.6834181509754029
"frozenset({'math_cat_υψηλό', 'reading_cat_υψηλό'})","frozenset({'test preparation course_completed', 'writing_cat_υψηλό', 'lunch_standard'})",0.131,0.087,0.053,0.4045801526717557,4.650346582433974,1.0,0.041603,1.5333717948717946,0.9032937447076449,0.32121212121212117,0.3478424454236097,0.5068877774853031
"frozenset({'test preparation course_completed', 'writing_cat_υψηλό', 'lunch_standard'})","frozenset({'math_cat_υψηλό', 'reading_cat_υψηλό'})",0.087,0.131,0.053,0.6091954022988506,4.650346582433974,1.0,0.041603,2.223617647058824,0.8597615160470354,0.32121212121212117,0.550282396201209,0.5068877774853031
"frozenset({'math_cat_υψηλό', 'gender_female'})",frozenset({'writing_cat_υψηλό'}),0.068,0.208,0.065,0.9558823529411764,4.595588235294118,1.0,0.050856,17.951999999999973,0.8394849785407725,0.30805687203791465,0.944295900178253,0.6341911764705882
frozenset({'writing_cat_υψηλό'}),"frozenset({'math_cat_υψηλό', 'gender_female'})",0.208,0.068,0.065,0.3125,4.595588235294118,1.0,0.050856,1.3556363636363635,0.9878787878787878,0.30805687203791465,0.2623390557939914,0.6341911764705882
"frozenset({'math_cat_υψηλό', 'gender_female', 'lunch_standard'})",frozenset({'writing_cat_υψηλό'}),0.062,0.208,0.059,0.9516129032258064,4.575062034739454,1.0,0.046104,16.367999999999977,0.8330743377543277,0.27962085308056867,0.9389051808406647,0.6176333746898263
frozenset({'writing_cat_υψηλό'}),"frozenset({'math_cat_υψηλό', 'gender_female', 'lunch_standard'})",0.208,0.062,0.059,0.28365384615384615,4.575062034739454,1.0,0.046104,1.309422818791946,0.9866461222393426,0.27962085308056867,0.23630474003608332,0.6176333746898263
"frozenset({'writing_cat_υψηλό', 'test preparation course_none'})","frozenset({'gender_female', 'reading_cat_υψηλό'})",0.091,0.159,0.066,0.7252747252747254,4.561476259589468,1.0,0.05153100000000001,3.061240000000001,0.858935893589359,0.3586956521739131,0.6733349884360587,0.5701845324486834
"frozenset({'gender_female', 'reading_cat_υψηλό'})","frozenset({'writing_cat_υψηλό', 'test preparation course_none'})",0.159,0.091,0.066,0.41509433962264153,4.561476259589467,1.0,0.05153100000000001,1.5540967741935485,0.9283861204194143,0.3586956521739131,0.3565394275276585,0.5701845324486834
"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced', 'reading_cat_χαμηλό'})","frozenset({'writing_cat_χαμηλό', 'gender_female'})",0.123,0.1,0.056,0.4552845528455285,4.5528455284552845,1.0,0.0437,1.6522388059701494,0.8898028994950318,0.33532934131736525,0.39476061427280945,0.5076422764227642
"frozenset({'writing_cat_χαμηλό', 'gender_female'})","frozenset({'math_cat_χαμηλό', 'lunch_free/reduced', 'reading_cat_χαμηλό'})",0.1,0.123,0.056,0.5599999999999999,4.5528455284552845,1.0,0.0437,1.993181818181818,0.8670634920634921,0.33532934131736525,0.49828962371721774,0.5076422764227642
"frozenset({'test preparation course_completed', 'math_cat_υψηλό', 'reading_cat_υψηλό'})","frozenset({'writing_cat_υψηλό', 'lunch_standard'})",0.072,0.162,0.053,0.7361111111111112,4.543895747599452,1.0,0.041336,3.1755789473684217,0.8404359141184125,0.292817679558011,0.6850967913020419,0.5316358024691358
"frozenset({'writing_cat_υψηλό', 'lunch_standard'})","frozenset({'test preparation course_completed', 'math_cat_υψηλό', 'reading_cat_υψηλό'})",0.162,0.072,0.053,0.32716049382716045,4.543895747599451,1.0,0.041336,1.379229357798165,0.9306975278065476,0.292817679558011,0.2749574286930608,0.5316358024691358
"frozenset({'writing_cat_υψηλό', 'gender_female'})","frozenset({'math_cat_μέτριο', 'reading_cat_υψηλό'})",0.154,0.103,0.071,0.461038961038961,4.476106417853991,1.0,0.05513799999999999,1.6643132530120481,0.9179569140611993,0.3817204301075269,0.3991515730646165,0.5751796746942378
"frozenset({'math_cat_μέτριο', 'reading_cat_υψηλό'})","frozenset({'writing_cat_υψηλό', 'gender_female'})",0.103,0.154,0.071,0.6893203883495146,4.476106417853991,1.0,0.05513799999999999,2.7230625,0.865765383830295,0.3817204301075269,0.632766416488788,0.5751796746942378
"frozenset({'writing_cat_υψηλό', 'test preparation course_none', 'lunch_standard'})","frozenset({'gender_female', 'reading_cat_υψηλό'})",0.075,0.159,0.053,0.7066666666666667,4.444444444444445,1.0,0.041075,2.8670454545454542,0.8378378378378378,0.292817679558011,0.651208878319461,0.52
"frozenset({'gender_female', 'reading_cat_υψηλό'})","frozenset({'writing_cat_υψηλό', 'test preparation course_none', 'lunch_standard'})",0.159,0.075,0.053,0.3333333333333333,4.444444444444445,1.0,0.041075,1.3875,0.9215219976218788,0.292817679558011,0.27927927927927926,0.52
"frozenset({'test preparation course_completed', 'math_cat_υψηλό'})","frozenset({'writing_cat_υψηλό', 'reading_cat_υψηλό', 'lunch_standard'})",0.084,0.144,0.053,0.6309523809523809,4.381613756613757,1.0,0.040903999999999996,2.319483870967742,0.8425475817747383,0.3028571428571429,0.5688696039163329,0.49950396825396826
"frozenset({'writing_cat_υψηλό', 'reading_cat_υψηλό', 'lunch_standard'})","frozenset({'test preparation course_completed', 'math_cat_υψηλό'})",0.144,0.084,0.053,0.3680555555555556,4.381613756613756,1.0,0.040903999999999996,1.4494945054945056,0.9016046552636219,0.3028571428571429,0.31010431829209123,0.49950396825396826
frozenset({'writing_cat_υψηλό'}),"frozenset({'test preparation course_completed', 'gender_female', 'reading_cat_υψηλό'})",0.208,0.077,0.07,0.33653846153846156,4.370629370629371,1.0,0.053984000000000004,1.3911884057971016,0.9737373737373737,0.3255813953488373,0.28119009917493126,0.6228146853146854
"frozenset({'test preparation course_completed', 'gender_female', 'reading_cat_υψηλό'})",frozenset({'writing_cat_υψηλό'}),0.077,0.208,0.07,0.9090909090909092,4.370629370629371,1.0,0.053984000000000004,8.712000000000009,0.8355362946912241,0.3255813953488373,0.8852157943067035,0.6228146853146854
"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced', 'writing_cat_χαμηλό'})","frozenset({'gender_female', 'reading_cat_χαμηλό'})",0.132,0.098,0.056,0.42424242424242425,4.329004329004329,1.0,0.043064,1.5666315789473686,0.8859447004608295,0.3218390804597701,0.3616878317543506,0.49783549783549785
"frozenset({'gender_female', 'reading_cat_χαμηλό'})","frozenset({'math_cat_χαμηλό', 'lunch_free/reduced', 'writing_cat_χαμηλό'})",0.098,0.132,0.056,0.5714285714285714,4.329004329004329,1.0,0.043064,2.025333333333333,0.852549889135255,0.3218390804597701,0.5062541145490453,0.49783549783549785
"frozenset({'math_cat_υψηλό', 'writing_cat_υψηλό'})","frozenset({'gender_female', 'reading_cat_υψηλό', 'lunch_standard'})",0.114,0.12,0.059,0.5175438596491228,4.312865497076023,1.0,0.04532,1.8239999999999998,0.8669701955082832,0.33714285714285713,0.45175438596491224,0.5046052631578947
"frozenset({'gender_female', 'reading_cat_υψηλό', 'lunch_standard'})","frozenset({'math_cat_υψηλό', 'writing_cat_υψηλό'})",0.12,0.114,0.059,0.49166666666666664,4.312865497076023,1.0,0.04532,1.7429508196721313,0.8728813559322034,0.33714285714285713,0.42626034612490593,0.5046052631578947
"frozenset({'test preparation course_completed', 'reading_cat_υψηλό'})","frozenset({'math_cat_υψηλό', 'writing_cat_υψηλό'})",0.125,0.114,0.061,0.488,4.280701754385965,1.0,0.04675,1.73046875,0.8758782201405152,0.34269662921348315,0.4221218961625282,0.5115438596491229
"frozenset({'math_cat_υψηλό', 'writing_cat_υψηλό'})","frozenset({'test preparation course_completed', 'reading_cat_υψηλό'})",0.114,0.125,0.061,0.5350877192982456,4.280701754385965,1.0,0.04675,1.8820754716981132,0.8650038855789514,0.34269662921348315,0.46867167919799496,0.5115438596491229
frozenset({'reading_cat_υψηλό'}),"frozenset({'test preparation course_completed', 'math_cat_υψηλό', 'writing_cat_υψηλό', 'lunch_standard'})",0.235,0.053,0.053,0.22553191489361704,4.255319148936171,1.0,0.040545,1.2227747252747252,1.0,0.22553191489361704,0.1821878721157519,0.6127659574468085
"frozenset({'test preparation course_completed', 'math_cat_υψηλό', 'writing_cat_υψηλό', 'lunch_standard'})",frozenset({'reading_cat_υψηλό'}),0.053,0.235,0.053,1.0,4.25531914893617,1.0,0.040545,inf,0.8078141499472017,0.22553191489361704,1.0,0.6127659574468085
frozenset({'reading_cat_υψηλό'}),"frozenset({'math_cat_υψηλό', 'writing_cat_υψηλό', 'gender_female'})",0.235,0.065,0.065,0.2765957446808511,4.25531914893617,1.0,0.049725000000000005,1.2925000000000002,1.0,0.2765957446808511,0.2263056092843327,0.6382978723404256
frozenset({'reading_cat_υψηλό'}),"frozenset({'math_cat_υψηλό', 'writing_cat_υψηλό', 'gender_female', 'lunch_standard'})",0.235,0.059,0.059,0.251063829787234,4.25531914893617,1.0,0.045134999999999995,1.2564488636363638,0.9999999999999999,0.251063829787234,0.20410608904063124,0.625531914893617
"frozenset({'math_cat_υψηλό', 'writing_cat_υψηλό', 'gender_female'})",frozenset({'reading_cat_υψηλό'}),0.065,0.235,0.065,1.0,4.25531914893617,1.0,0.049725000000000005,inf,0.8181818181818182,0.2765957446808511,1.0,0.6382978723404256
"frozenset({'math_cat_υψηλό', 'writing_cat_υψηλό', 'gender_female', 'lunch_standard'})",frozenset({'reading_cat_υψηλό'}),0.059,0.235,0.059,1.0,4.25531914893617,1.0,0.045134999999999995,inf,0.8129649309245482,0.251063829787234,1.0,0.625531914893617
"frozenset({'test preparation course_completed', 'math_cat_υψηλό', 'writing_cat_υψηλό'})",frozenset({'reading_cat_υψηλό'}),0.062,0.235,0.061,0.9838709677419355,4.186684969114619,1.0,0.04643,47.43000000000004,0.8114579328183439,0.2584745762711865,0.9789162977018765,0.621722717913521
frozenset({'reading_cat_υψηλό'}),"frozenset({'test preparation course_completed', 'math_cat_υψηλό', 'writing_cat_υψηλό'})",0.235,0.062,0.061,0.25957446808510637,4.186684969114619,1.0,0.04643,1.26683908045977,0.9949641058609237,0.2584745762711865,0.2106337612847616,0.621722717913521
"frozenset({'race/ethnicity_group C', 'reading_cat_υψηλό'})",frozenset({'writing_cat_υψηλό'}),0.067,0.208,0.058,0.8656716417910447,4.161882893226177,1.0,0.044064000000000006,5.895999999999998,0.8142809624126844,0.2672811059907834,0.8303934871099049,0.5722588978185993
frozenset({'writing_cat_υψηλό'}),"frozenset({'race/ethnicity_group C', 'reading_cat_υψηλό'})",0.208,0.067,0.058,0.27884615384615385,4.161882893226177,1.0,0.044064000000000006,1.29376,0.9592476489028213,0.2672811059907834,0.22705911451892158,0.5722588978185993
"frozenset({'math_cat_υψηλό', 'writing_cat_υψηλό', 'lunch_standard'})","frozenset({'test preparation course_completed', 'reading_cat_υψηλό'})",0.102,0.125,0.053,0.5196078431372549,4.1568627450980395,1.0,0.04025,1.8214285714285716,0.8456948354834644,0.3045977011494253,0.4509803921568628,0.4718039215686275
"frozenset({'test preparation course_completed', 'reading_cat_υψηλό'})","frozenset({'math_cat_υψηλό', 'writing_cat_υψηλό', 'lunch_standard'})",0.125,0.102,0.053,0.424,4.1568627450980395,1.0,0.04025,1.5590277777777777,0.8679245283018868,0.3045977011494253,0.3585746102449889,0.4718039215686275
"frozenset({'writing_cat_υψηλό', 'gender_female', 'lunch_standard'})","frozenset({'test preparation course_none', 'reading_cat_υψηλό'})",0.116,0.11,0.053,0.4568965517241379,4.153605015673981,1.0,0.04024,1.6387301587301586,0.8588747545462306,0.30635838150289013,0.3897714064316156,0.46935736677115986
"frozenset({'test preparation course_none', 'reading_cat_υψηλό'})","frozenset({'writing_cat_υψηλό', 'gender_female', 'lunch_standard'})",0.11,0.116,0.053,0.4818181818181818,4.153605015673981,1.0,0.04024,1.7059649122807017,0.8530845876616494,0.30635838150289013,0.4138214726450021,0.46935736677115986
"frozenset({'math_cat_χαμηλό', 'test preparation course_none', 'reading_cat_χαμηλό'})","frozenset({'writing_cat_χαμηλό', 'gender_female'})",0.169,0.1,0.07,0.41420118343195267,4.1420118343195265,1.0,0.05310000000000001,1.5363636363636364,0.9128416709644148,0.35175879396984927,0.34911242603550297,0.5571005917159764
"frozenset({'writing_cat_χαμηλό', 'gender_female'})","frozenset({'math_cat_χαμηλό', 'test preparation course_none', 'reading_cat_χαμηλό'})",0.1,0.169,0.07,0.7000000000000001,4.1420118343195265,1.0,0.05310000000000001,2.7700000000000005,0.8428571428571427,0.35175879396984927,0.6389891696750903,0.5571005917159764
frozenset({'reading_cat_υψηλό'}),"frozenset({'math_cat_υψηλό', 'writing_cat_υψηλό', 'lunch_standard'})",0.235,0.102,0.099,0.4212765957446809,4.1301627033792245,1.0,0.07503000000000001,1.5516911764705885,0.9906912259853436,0.4159663865546219,0.3555418660853908,0.6959324155193993
"frozenset({'math_cat_υψηλό', 'gender_female'})",frozenset({'reading_cat_υψηλό'}),0.068,0.235,0.066,0.9705882352941176,4.1301627033792245,1.0,0.05002,26.009999999999994,0.8131746651059957,0.2784810126582279,0.9615532487504805,0.6257196495619525
"frozenset({'math_cat_υψηλό', 'writing_cat_υψηλό', 'lunch_standard'})",frozenset({'reading_cat_υψηλό'}),0.102,0.235,0.099,0.9705882352941178,4.1301627033792245,1.0,0.07503000000000001,26.010000000000094,0.8439630154552205,0.4159663865546219,0.9615532487504808,0.6959324155193993
frozenset({'reading_cat_υψηλό'}),"frozenset({'math_cat_υψηλό', 'gender_female'})",0.235,0.068,0.066,0.28085106382978725,4.130162703379224,1.0,0.05002,1.2959763313609467,0.9906912259853436,0.2784810126582279,0.22838096977444985,0.6257196495619525
"frozenset({'test preparation course_completed', 'math_cat_υψηλό'})","frozenset({'reading_cat_υψηλό', 'lunch_standard'})",0.084,0.182,0.063,0.75,4.120879120879121,1.0,0.047712,3.2720000000000002,0.826783114992722,0.31034482758620685,0.6943765281173594,0.5480769230769231
"frozenset({'reading_cat_υψηλό', 'lunch_standard'})","frozenset({'test preparation course_completed', 'math_cat_υψηλό'})",0.182,0.084,0.063,0.34615384615384615,4.1208791208791204,1.0,0.047712,1.4009411764705884,0.9258353708231458,0.31034482758620685,0.28619415518978836,0.5480769230769231
"frozenset({'math_cat_υψηλό', 'gender_female', 'lunch_standard'})",frozenset({'reading_cat_υψηλό'}),0.062,0.235,0.06,0.9677419354838709,4.118050789293068,1.0,0.04543,23.71499999999994,0.8072139303482587,0.25316455696202533,0.9578325954037528,0.6115305422100206
frozenset({'reading_cat_υψηλό'}),"frozenset({'math_cat_υψηλό', 'gender_female', 'lunch_standard'})",0.235,0.062,0.06,0.2553191489361702,4.118050789293068,1.0,0.04543,1.2595999999999998,0.9897603485838781,0.25316455696202533,0.20609717370593839,0.6115305422100206
frozenset({'writing_cat_υψηλό'}),"frozenset({'gender_female', 'reading_cat_υψηλό'})",0.208,0.159,0.136,0.653846153846154,4.112239961296566,1.0,0.10292800000000002,2.4295555555555564,0.9555852644087939,0.5887445887445888,0.588402085429434,0.7545960328979198
"frozenset({'gender_female', 'reading_cat_υψηλό'})",frozenset({'writing_cat_υψηλό'}),0.159,0.208,0.136,0.8553459119496856,4.112239961296566,1.0,0.10292800000000002,5.47513043478261,0.8999090718332519,0.5887445887445888,0.8173559494314212,0.7545960328979198
frozenset({'reading_cat_υψηλό'}),"frozenset({'math_cat_υψηλό', 'writing_cat_υψηλό'})",0.235,0.114,0.11,0.46808510638297873,4.106009705113848,1.0,0.08321,1.66568,0.9888294711824124,0.46025104602510464,0.39964458959704147,0.7164986935423665
"frozenset({'math_cat_υψηλό', 'writing_cat_υψηλό'})",frozenset({'reading_cat_υψηλό'}),0.114,0.235,0.11,0.9649122807017544,4.106009705113848,1.0,0.08321,21.802500000000002,0.8537861686845886,0.46025104602510464,0.9541337002637312,0.7164986935423665
"frozenset({'lunch_free/reduced', 'reading_cat_χαμηλό'})","frozenset({'writing_cat_χαμηλό', 'gender_female'})",0.139,0.1,0.057,0.41007194244604317,4.100719424460431,1.0,0.0431,1.525609756097561,0.8782117896366933,0.3131868131868132,0.3445243804956035,0.49003597122302156
"frozenset({'writing_cat_χαμηλό', 'gender_female'})","frozenset({'lunch_free/reduced', 'reading_cat_χαμηλό'})",0.1,0.139,0.057,0.57,4.100719424460431,1.0,0.0431,2.0023255813953487,0.8401559454191032,0.3131868131868132,0.5005807200929151,0.49003597122302156
"frozenset({'gender_female', 'reading_cat_υψηλό', 'lunch_standard'})",frozenset({'writing_cat_υψηλό'}),0.12,0.208,0.102,0.85,4.086538461538462,1.0,0.07704,5.279999999999999,0.8582887700534759,0.4513274336283186,0.8106060606060606,0.6701923076923076
frozenset({'writing_cat_υψηλό'}),"frozenset({'gender_female', 'reading_cat_υψηλό', 'lunch_standard'})",0.208,0.12,0.102,0.49038461538461536,4.086538461538462,1.0,0.07704,1.7267924528301888,0.9536541889483067,0.4513274336283186,0.4208916083916084,0.6701923076923076
"frozenset({'race/ethnicity_group D', 'reading_cat_υψηλό'})",frozenset({'writing_cat_υψηλό'}),0.06,0.208,0.051,0.85,4.086538461538462,1.0,0.03852,5.279999999999999,0.8035043804755945,0.23502304147465433,0.8106060606060606,0.5475961538461538
frozenset({'writing_cat_υψηλό'}),"frozenset({'race/ethnicity_group D', 'reading_cat_υψηλό'})",0.208,0.06,0.051,0.24519230769230768,4.086538461538462,1.0,0.03852,1.2453503184713375,0.9536541889483067,0.23502304147465433,0.19701309328968902,0.5475961538461538
frozenset({'writing_cat_υψηλό'}),"frozenset({'test preparation course_completed', 'math_cat_υψηλό', 'reading_cat_υψηλό'})",0.208,0.072,0.061,0.2932692307692308,4.0731837606837615,1.0,0.046024,1.3130884353741499,0.9526411657559198,0.27853881278538817,0.23843667108753314,0.5702457264957266
"frozenset({'test preparation course_completed', 'math_cat_υψηλό', 'reading_cat_υψηλό'})",frozenset({'writing_cat_υψηλό'}),0.072,0.208,0.061,0.8472222222222223,4.0731837606837615,1.0,0.046024,5.184000000000004,0.8130299604296213,0.27853881278538817,0.8070987654320989,0.5702457264957266
"frozenset({'lunch_free/reduced', 'reading_cat_χαμηλό'})","frozenset({'math_cat_χαμηλό', 'gender_female', 'writing_cat_χαμηλό'})",0.139,0.099,0.056,0.4028776978417266,4.069471695370976,1.0,0.042239,1.5089036144578312,0.87603700016592,0.30769230769230765,0.3372671452183425,0.48426713174914615
"frozenset({'math_cat_χαμηλό', 'gender_female', 'writing_cat_χαμηλό'})","frozenset({'lunch_free/reduced', 'reading_cat_χαμηλό'})",0.099,0.139,0.056,0.5656565656565656,4.069471695370975,1.0,0.042239,1.9823023255813952,0.8371452354526716,0.30769230769230765,0.4955360809019345,0.48426713174914615
"frozenset({'writing_cat_χαμηλό', 'gender_female'})","frozenset({'math_cat_χαμηλό', 'lunch_free/reduced', 'test preparation course_none'})",0.1,0.126,0.051,0.5099999999999999,4.047619047619047,1.0,0.0384,1.7836734693877547,0.8366013071895425,0.29142857142857137,0.43935926773455364,0.45738095238095233
"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced', 'test preparation course_none'})","frozenset({'writing_cat_χαμηλό', 'gender_female'})",0.126,0.1,0.051,0.4047619047619047,4.047619047619047,1.0,0.0384,1.5119999999999998,0.8614887602638309,0.29142857142857137,0.33862433862433855,0.45738095238095233
frozenset({'writing_cat_υψηλό'}),"frozenset({'test preparation course_completed', 'math_cat_υψηλό', 'reading_cat_υψηλό', 'lunch_standard'})",0.208,0.063,0.053,0.2548076923076923,4.044566544566544,1.0,0.039896,1.2573935483870968,0.9504478749761769,0.24311926605504583,0.20470404728675803,0.5480387667887667
"frozenset({'test preparation course_completed', 'math_cat_υψηλό', 'reading_cat_υψηλό', 'lunch_standard'})",frozenset({'writing_cat_υψηλό'}),0.063,0.208,0.053,0.8412698412698413,4.044566544566544,1.0,0.039896,4.9896,0.8033668270876543,0.24311926605504583,0.7995831329164663,0.5480387667887667
"frozenset({'math_cat_υψηλό', 'reading_cat_υψηλό'})",frozenset({'writing_cat_υψηλό'}),0.131,0.208,0.11,0.8396946564885496,4.036993540810335,1.0,0.08275199999999999,4.9405714285714275,0.8656972486661785,0.480349344978166,0.7975942632431181,0.6842704051673517
frozenset({'writing_cat_υψηλό'}),"frozenset({'math_cat_υψηλό', 'reading_cat_υψηλό'})",0.208,0.131,0.11,0.5288461538461539,4.036993540810335,1.0,0.08275199999999999,1.8444081632653062,0.9498622589531679,0.480349344978166,0.45782066035230595,0.6842704051673517
"frozenset({'test preparation course_completed', 'math_cat_υψηλό', 'lunch_standard'})","frozenset({'writing_cat_υψηλό', 'reading_cat_υψηλό'})",0.071,0.185,0.053,0.7464788732394366,4.035020936429388,1.0,0.039865,3.214722222222222,0.8096553404959684,0.2610837438423645,0.6889311328091248,0.5164826798629616
"frozenset({'writing_cat_υψηλό', 'reading_cat_υψηλό'})","frozenset({'test preparation course_completed', 'math_cat_υψηλό', 'lunch_standard'})",0.185,0.071,0.053,0.2864864864864865,4.035020936429388,1.0,0.039865,1.3020075757575758,0.9229077439518463,0.2610837438423645,0.2319553137637099,0.5164826798629616
"frozenset({'math_cat_υψηλό', 'reading_cat_υψηλό', 'lunch_standard'})",frozenset({'writing_cat_υψηλό'}),0.118,0.208,0.099,0.8389830508474577,4.033572359843547,1.0,0.07445600000000001,4.918736842105266,0.8526993288898052,0.43612334801762126,0.7966957712720426,0.6574722946544981
frozenset({'writing_cat_υψηλό'}),"frozenset({'math_cat_υψηλό', 'reading_cat_υψηλό', 'lunch_standard'})",0.208,0.118,0.099,0.4759615384615385,4.0335723598435465,1.0,0.07445600000000001,1.6830825688073396,0.9495969799000102,0.43612334801762126,0.40585208442351306,0.6574722946544981
"frozenset({'math_cat_χαμηλό', 'reading_cat_χαμηλό'})","frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced', 'gender_female'})",0.218,0.064,0.056,0.25688073394495414,4.013761467889909,1.0,0.042048,1.2595555555555555,0.9601753744976251,0.247787610619469,0.2060691601976006,0.5659403669724771
"frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced', 'gender_female'})","frozenset({'math_cat_χαμηλό', 'reading_cat_χαμηλό'})",0.064,0.218,0.056,0.875,4.013761467889908,1.0,0.042048,6.256,0.8021978021978022,0.247787610619469,0.840153452685422,0.5659403669724771
"frozenset({'writing_cat_υψηλό', 'gender_female'})","frozenset({'test preparation course_none', 'reading_cat_υψηλό', 'lunch_standard'})",0.154,0.086,0.053,0.34415584415584416,4.001812141347026,1.0,0.039756,1.3936237623762375,0.8866586377626121,0.2834224598930481,0.28244621898888855,0.480217456961643
"frozenset({'test preparation course_none', 'reading_cat_υψηλό', 'lunch_standard'})","frozenset({'writing_cat_υψηλό', 'gender_female'})",0.086,0.154,0.053,0.6162790697674418,4.001812141347025,1.0,0.039756,2.2047272727272724,0.8206927872507328,0.2834224598930481,0.5464291604816097,0.480217456961643
frozenset({'writing_cat_υψηλό'}),"frozenset({'test preparation course_completed', 'reading_cat_υψηλό'})",0.208,0.125,0.104,0.5,4.0,1.0,0.078,1.75,0.946969696969697,0.45414847161572053,0.42857142857142855,0.6659999999999999
"frozenset({'test preparation course_completed', 'reading_cat_υψηλό'})",frozenset({'writing_cat_υψηλό'}),0.125,0.208,0.104,0.832,4.0,1.0,0.078,4.7142857142857135,0.8571428571428572,0.45414847161572053,0.7878787878787878,0.6659999999999999
"frozenset({'math_cat_χαμηλό', 'writing_cat_χαμηλό'})","frozenset({'lunch_free/reduced', 'gender_female', 'reading_cat_χαμηλό'})",0.23,0.061,0.056,0.2434782608695652,3.991446899501069,1.0,0.04197,1.241206896551724,0.9733302411873841,0.2382978723404255,0.19433254618697038,0.5807555238774056
"frozenset({'lunch_free/reduced', 'gender_female', 'reading_cat_χαμηλό'})","frozenset({'math_cat_χαμηλό', 'writing_cat_χαμηλό'})",0.061,0.23,0.056,0.9180327868852459,3.991446899501069,1.0,0.04197,9.394000000000002,0.7981515289821999,0.2382978723404255,0.8935490738769427,0.5807555238774056
"frozenset({'writing_cat_υψηλό', 'lunch_standard'})","frozenset({'gender_female', 'test preparation course_none', 'reading_cat_υψηλό'})",0.162,0.082,0.053,0.32716049382716045,3.98976211984342,1.0,0.039716,1.3643669724770642,0.8942225424415726,0.2774869109947644,0.26705936146749504,0.4867509786208972
"frozenset({'gender_female', 'test preparation course_none', 'reading_cat_υψηλό'})","frozenset({'writing_cat_υψηλό', 'lunch_standard'})",0.082,0.162,0.053,0.646341463414634,3.98976211984342,1.0,0.039716,2.3695172413793095,0.8162946520327208,0.2774869109947644,0.5779731067000407,0.4867509786208972
"frozenset({'reading_cat_υψηλό', 'lunch_standard'})","frozenset({'writing_cat_υψηλό', 'gender_female', 'test preparation course_none'})",0.182,0.073,0.053,0.29120879120879123,3.9891615234080993,1.0,0.039714,1.3078604651162793,0.9160400424412971,0.26237623762376233,0.23539243927593442,0.5086180942345326
"frozenset({'writing_cat_υψηλό', 'gender_female', 'test preparation course_none'})","frozenset({'reading_cat_υψηλό', 'lunch_standard'})",0.073,0.182,0.053,0.726027397260274,3.989161523408099,1.0,0.039714,2.9857000000000005,0.8083287537400012,0.26237623762376233,0.665070167799846,0.5086180942345326
"frozenset({'writing_cat_υψηλό', 'test preparation course_none'})","frozenset({'reading_cat_υψηλό', 'lunch_standard'})",0.091,0.182,0.066,0.7252747252747254,3.9850259630479417,1.0,0.049438,2.977520000000001,0.824049071573824,0.3188405797101449,0.6641500308981972,0.543956043956044
"frozenset({'reading_cat_υψηλό', 'lunch_standard'})","frozenset({'writing_cat_υψηλό', 'test preparation course_none'})",0.182,0.091,0.066,0.3626373626373627,3.9850259630479417,1.0,0.049438,1.426189655172414,0.9157220122990293,0.3188405797101449,0.2988309819992989,0.543956043956044
"frozenset({'gender_female', 'test preparation course_none', 'reading_cat_υψηλό', 'lunch_standard'})",frozenset({'writing_cat_υψηλό'}),0.064,0.208,0.053,0.828125,3.9813701923076925,1.0,0.039688,4.6080000000000005,0.8000322528624417,0.24200913242009128,0.7829861111111112,0.5414663461538461
frozenset({'writing_cat_υψηλό'}),"frozenset({'gender_female', 'test preparation course_none', 'reading_cat_υψηλό', 'lunch_standard'})",0.208,0.064,0.053,0.2548076923076923,3.981370192307692,1.0,0.039688,1.2560516129032258,0.9454926624737946,0.24200913242009128,0.20385437212360288,0.5414663461538461
"frozenset({'math_cat_υψηλό', 'reading_cat_υψηλό'})","frozenset({'test preparation course_completed', 'writing_cat_υψηλό'})",0.131,0.117,0.061,0.46564885496183206,3.9799047432635217,1.0,0.045673,1.6524714285714286,0.8616084061197155,0.3262032085561497,0.3948458153588132,0.4935081881646767
"frozenset({'test preparation course_completed', 'writing_cat_υψηλό'})","frozenset({'math_cat_υψηλό', 'reading_cat_υψηλό'})",0.117,0.131,0.061,0.5213675213675213,3.9799047432635213,1.0,0.045673,1.8155892857142855,0.8479475706885988,0.3262032085561497,0.4492146390880567,0.4935081881646767
"frozenset({'gender_female', 'reading_cat_υψηλό'})","frozenset({'writing_cat_υψηλό', 'lunch_standard'})",0.159,0.162,0.102,0.6415094339622641,3.95993477754484,1.0,0.07624199999999999,2.3375789473684208,0.8887878575925019,0.46575342465753417,0.5722069617688116,0.6355695317959469
"frozenset({'writing_cat_υψηλό', 'lunch_standard'})","frozenset({'gender_female', 'reading_cat_υψηλό'})",0.162,0.159,0.102,0.6296296296296295,3.9599347775448397,1.0,0.07624199999999999,2.2706999999999993,0.8919696756984415,0.46575342465753417,0.5596071695952789,0.6355695317959469
"frozenset({'test preparation course_completed', 'math_cat_υψηλό'})","frozenset({'writing_cat_υψηλό', 'reading_cat_υψηλό'})",0.084,0.185,0.061,0.7261904761904762,3.9253539253539254,1.0,0.04546,2.976521739130434,0.8135872288639129,0.2932692307692307,0.6640373940987438,0.527960102960103
"frozenset({'writing_cat_υψηλό', 'reading_cat_υψηλό'})","frozenset({'test preparation course_completed', 'math_cat_υψηλό'})",0.185,0.084,0.061,0.32972972972972975,3.9253539253539254,1.0,0.04546,1.3666129032258065,0.9144121492507292,0.2932692307692307,0.26826389708485776,0.527960102960103
frozenset({'math_cat_υψηλό'}),"frozenset({'writing_cat_υψηλό', 'reading_cat_υψηλό', 'lunch_standard'})",0.176,0.144,0.099,0.5625000000000001,3.906250000000001,1.0,0.07365600000000001,1.956571428571429,0.9029126213592233,0.4479638009049775,0.4889018691588786,0.6250000000000001
"frozenset({'writing_cat_υψηλό', 'reading_cat_υψηλό', 'lunch_standard'})",frozenset({'math_cat_υψηλό'}),0.144,0.176,0.099,0.6875000000000001,3.906250000000001,1.0,0.07365600000000001,2.6368000000000014,0.8691588785046731,0.4479638009049775,0.6207524271844661,0.6250000000000001
"frozenset({'test preparation course_completed', 'reading_cat_υψηλό', 'lunch_standard'})",frozenset({'writing_cat_υψηλό'}),0.096,0.208,0.078,0.8125,3.90625,1.0,0.058032,4.224,0.8230088495575221,0.3451327433628319,0.7632575757575758,0.59375
frozenset({'writing_cat_υψηλό'}),"frozenset({'test preparation course_completed', 'reading_cat_υψηλό', 'lunch_standard'})",0.208,0.096,0.078,0.375,3.90625,1.0,0.058032,1.4464000000000001,0.9393939393939393,0.3451327433628319,0.3086283185840708,0.59375
"frozenset({'gender_female', 'reading_cat_χαμηλό'})","frozenset({'math_cat_χαμηλό', 'test preparation course_none', 'writing_cat_χαμηλό'})",0.098,0.183,0.07,0.7142857142857143,3.9032006245121003,1.0,0.052066,2.8595,0.8246119733924612,0.33175355450236965,0.6502885119776186,0.5483996877439501
"frozenset({'math_cat_χαμηλό', 'test preparation course_none', 'writing_cat_χαμηλό'})","frozenset({'gender_female', 'reading_cat_χαμηλό'})",0.183,0.098,0.07,0.38251366120218583,3.9032006245121003,1.0,0.052066,1.4607610619469027,0.9104039167686658,0.33175355450236965,0.3154253450135097,0.5483996877439501
"frozenset({'writing_cat_υψηλό', 'gender_female'})","frozenset({'test preparation course_none', 'reading_cat_υψηλό'})",0.154,0.11,0.066,0.4285714285714286,3.8961038961038965,1.0,0.049060000000000006,1.5575,0.8786446020488574,0.3333333333333333,0.35794542536115576,0.5142857142857142
"frozenset({'test preparation course_none', 'reading_cat_υψηλό'})","frozenset({'writing_cat_υψηλό', 'gender_female'})",0.11,0.154,0.066,0.6,3.896103896103896,1.0,0.049060000000000006,2.1149999999999998,0.8352059925093634,0.3333333333333333,0.5271867612293144,0.5142857142857142
"frozenset({'test preparation course_completed', 'math_cat_υψηλό'})","frozenset({'writing_cat_υψηλό', 'lunch_standard'})",0.084,0.162,0.053,0.6309523809523809,3.8947677836566723,1.0,0.039391999999999996,2.2707096774193545,0.8114031474005108,0.27461139896373055,0.5596090464825548,0.47905643738977066
"frozenset({'writing_cat_υψηλό', 'lunch_standard'})","frozenset({'test preparation course_completed', 'math_cat_υψηλό'})",0.162,0.084,0.053,0.32716049382716045,3.894767783656672,1.0,0.039391999999999996,1.3613944954128439,0.8869275453685775,0.27461139896373055,0.26545905439646333,0.47905643738977066
"frozenset({'writing_cat_υψηλό', 'gender_female', 'lunch_standard'})","frozenset({'math_cat_υψηλό', 'reading_cat_υψηλό'})",0.116,0.131,0.059,0.5086206896551724,3.8826006843906287,1.0,0.043803999999999996,1.7684912280701752,0.8398650203236444,0.31382978723404253,0.43454624816475534,0.47950118452224266
"frozenset({'math_cat_υψηλό', 'reading_cat_υψηλό'})","frozenset({'writing_cat_υψηλό', 'gender_female', 'lunch_standard'})",0.131,0.116,0.059,0.45038167938931295,3.8826006843906287,1.0,0.043803999999999996,1.6083888888888889,0.8543621150357902,0.31382978723404253,0.3782598183137024,0.47950118452224266
"frozenset({'gender_female', 'test preparation course_none', 'reading_cat_υψηλό'})",frozenset({'writing_cat_υψηλό'}),0.082,0.208,0.066,0.8048780487804879,3.8696060037523456,1.0,0.048944,4.059000000000001,0.8078167293853569,0.2946428571428572,0.7536338999753635,0.5610928705440901
frozenset({'writing_cat_υψηλό'}),"frozenset({'gender_female', 'test preparation course_none', 'reading_cat_υψηλό'})",0.208,0.082,0.066,0.31730769230769235,3.8696060037523456,1.0,0.048944,1.3446760563380282,0.9363330272421181,0.2946428571428572,0.25632646220881516,0.5610928705440901
"frozenset({'test preparation course_completed', 'writing_cat_υψηλό', 'reading_cat_υψηλό', 'lunch_standard'})",frozenset({'math_cat_υψηλό'}),0.078,0.176,0.053,0.6794871794871795,3.860722610722611,1.0,0.039272,2.5708800000000003,0.8036671714484509,0.26368159203980096,0.6110281304456062,0.4903117715617716
frozenset({'math_cat_υψηλό'}),"frozenset({'test preparation course_completed', 'writing_cat_υψηλό', 'reading_cat_υψηλό', 'lunch_standard'})",0.176,0.078,0.053,0.30113636363636365,3.8607226107226107,1.0,0.039272,1.3192845528455286,0.8992489466935336,0.26368159203980096,0.2420134095839085,0.4903117715617716
"frozenset({'gender_female', 'test preparation course_none', 'reading_cat_χαμηλό'})","frozenset({'math_cat_χαμηλό', 'writing_cat_χαμηλό'})",0.079,0.23,0.07,0.8860759493670887,3.852504127682994,1.0,0.05183,6.758888888888893,0.8039398169691327,0.29288702928870297,0.8520466874897256,0.5952118877270226
"frozenset({'math_cat_χαμηλό', 'writing_cat_χαμηλό'})","frozenset({'gender_female', 'test preparation course_none', 'reading_cat_χαμηλό'})",0.23,0.079,0.07,0.30434782608695654,3.852504127682994,1.0,0.05183,1.3239375,0.961595547309833,0.29288702928870297,0.24467733559930133,0.5952118877270226
"frozenset({'writing_cat_υψηλό', 'lunch_standard'})","frozenset({'test preparation course_completed', 'reading_cat_υψηλό'})",0.162,0.125,0.078,0.48148148148148145,3.8518518518518516,1.0,0.057749999999999996,1.6874999999999998,0.8835138608408298,0.3732057416267942,0.4074074074074074,0.5527407407407408
"frozenset({'test preparation course_completed', 'reading_cat_υψηλό'})","frozenset({'writing_cat_υψηλό', 'lunch_standard'})",0.125,0.162,0.078,0.624,3.8518518518518516,1.0,0.057749999999999996,2.228723404255319,0.846153846153846,0.3732057416267942,0.5513126491646778,0.5527407407407408
"frozenset({'writing_cat_υψηλό', 'gender_female', 'test preparation course_none'})",frozenset({'reading_cat_υψηλό'}),0.073,0.235,0.066,0.904109589041096,3.847274846983387,1.0,0.048845000000000006,7.977857142857148,0.7983557255401915,0.27272727272727276,0.8746530575700601,0.5924803264354416
frozenset({'reading_cat_υψηλό'}),"frozenset({'writing_cat_υψηλό', 'gender_female', 'test preparation course_none'})",0.235,0.073,0.066,0.28085106382978725,3.847274846983387,1.0,0.048845000000000006,1.2890236686390533,0.9674192909487028,0.27272727272727276,0.22421905483256444,0.5924803264354416
"frozenset({'math_cat_υψηλό', 'reading_cat_υψηλό', 'lunch_standard'})","frozenset({'test preparation course_completed', 'writing_cat_υψηλό'})",0.118,0.117,0.053,0.4491525423728814,3.8389106185716355,1.0,0.039194,1.6029846153846155,0.8384460702520002,0.29120879120879123,0.37616369464652477,0.45107199768216716
"frozenset({'test preparation course_completed', 'writing_cat_υψηλό'})","frozenset({'math_cat_υψηλό', 'reading_cat_υψηλό', 'lunch_standard'})",0.117,0.118,0.053,0.45299145299145294,3.838910618571635,1.0,0.039194,1.6124062499999998,0.8374965277035834,0.29120879120879123,0.3798089036184274,0.45107199768216716
frozenset({'reading_cat_υψηλό'}),"frozenset({'writing_cat_υψηλό', 'gender_female', 'test preparation course_none', 'lunch_standard'})",0.235,0.059,0.053,0.22553191489361704,3.8225748287053736,1.0,0.039135,1.2150274725274726,0.9652238253792084,0.21991701244813278,0.17697334207610738,0.5619184998196899
"frozenset({'writing_cat_υψηλό', 'gender_female', 'test preparation course_none', 'lunch_standard'})",frozenset({'reading_cat_υψηλό'}),0.059,0.235,0.053,0.8983050847457628,3.8225748287053736,1.0,0.039135,7.5225000000000035,0.7846931205261365,0.21991701244813278,0.8670654702558991,0.5619184998196899
frozenset({'reading_cat_υψηλό'}),"frozenset({'test preparation course_completed', 'writing_cat_υψηλό', 'lunch_standard'})",0.235,0.087,0.078,0.3319148936170213,3.8151137197358773,1.0,0.057555,1.3665923566878981,0.9645550527903469,0.31967213114754106,0.26825289552795323,0.6142333088774762
"frozenset({'test preparation course_completed', 'writing_cat_υψηλό', 'lunch_standard'})",frozenset({'reading_cat_υψηλό'}),0.087,0.235,0.078,0.896551724137931,3.815113719735877,1.0,0.057555,7.395000000000001,0.8081978262701155,0.31967213114754106,0.8647734956051386,0.6142333088774762
"frozenset({'writing_cat_χαμηλό', 'gender_female'})","frozenset({'math_cat_χαμηλό', 'reading_cat_χαμηλό'})",0.1,0.218,0.083,0.83,3.8073394495412844,1.0,0.061200000000000004,4.599999999999999,0.819277108433735,0.3531914893617022,0.7826086956521738,0.6053669724770642
"frozenset({'math_cat_χαμηλό', 'reading_cat_χαμηλό'})","frozenset({'writing_cat_χαμηλό', 'gender_female'})",0.218,0.1,0.083,0.38073394495412843,3.807339449541284,1.0,0.061200000000000004,1.4533333333333336,0.9429020429544264,0.3531914893617022,0.3119266055045871,0.6053669724770642
"frozenset({'reading_cat_υψηλό', 'lunch_standard'})",frozenset({'writing_cat_υψηλό'}),0.182,0.208,0.144,0.7912087912087912,3.8038884192730347,1.0,0.10614399999999999,3.7932631578947364,0.9011138277641945,0.5853658536585364,0.7363747363747364,0.7417582417582418
frozenset({'writing_cat_υψηλό'}),"frozenset({'reading_cat_υψηλό', 'lunch_standard'})",0.208,0.182,0.144,0.6923076923076923,3.8038884192730347,1.0,0.10614399999999999,2.6585,0.930695847362514,0.5853658536585364,0.6238480346059808,0.7417582417582418
"frozenset({'writing_cat_υψηλό', 'test preparation course_none'})",frozenset({'reading_cat_υψηλό'}),0.091,0.235,0.081,0.8901098901098902,3.787701660042086,1.0,0.059615,6.961500000000004,0.8096673864917356,0.33061224489795926,0.8563527975292682,0.61739537058686
frozenset({'reading_cat_υψηλό'}),"frozenset({'writing_cat_υψηλό', 'test preparation course_none'})",0.235,0.091,0.081,0.34468085106382984,3.787701660042086,1.0,0.059615,1.3871103896103898,0.962075365125474,0.33061224489795926,0.2790768438545983,0.61739537058686
frozenset({'writing_cat_υψηλό'}),frozenset({'reading_cat_υψηλό'}),0.208,0.235,0.185,0.889423076923077,3.784779050736498,1.0,0.13612000000000002,6.918260869565222,0.929019929019929,0.7170542635658916,0.8554550025138261,0.8383285597381342
frozenset({'reading_cat_υψηλό'}),frozenset({'writing_cat_υψηλό'}),0.235,0.208,0.185,0.7872340425531915,3.784779050736498,1.0,0.13612000000000002,3.722400000000001,0.9618088676912206,0.7170542635658916,0.7313561143348378,0.8383285597381342
frozenset({'reading_cat_υψηλό'}),"frozenset({'test preparation course_completed', 'writing_cat_υψηλό'})",0.235,0.117,0.104,0.44255319148936173,3.7825059101654848,1.0,0.07650499999999999,1.5840076335877862,0.9616013071895424,0.4193548387096774,0.36868991108647986,0.6657210401891253
"frozenset({'writing_cat_υψηλό', 'lunch_standard'})",frozenset({'reading_cat_υψηλό'}),0.162,0.235,0.144,0.8888888888888888,3.7825059101654848,1.0,0.10593,6.884999999999997,0.8778341288782817,0.5691699604743082,0.8547567175018155,0.7508274231678487
"frozenset({'test preparation course_completed', 'writing_cat_υψηλό'})",frozenset({'reading_cat_υψηλό'}),0.117,0.235,0.104,0.8888888888888888,3.7825059101654848,1.0,0.07650499999999999,6.884999999999997,0.833097395243488,0.4193548387096774,0.8547567175018155,0.6657210401891253
frozenset({'reading_cat_υψηλό'}),"frozenset({'writing_cat_υψηλό', 'lunch_standard'})",0.235,0.162,0.144,0.6127659574468085,3.7825059101654843,1.0,0.10593,2.164065934065934,0.9616013071895425,0.5691699604743082,0.5379068704615854,0.7508274231678487
"frozenset({'writing_cat_χαμηλό', 'lunch_standard'})","frozenset({'gender_male', 'test preparation course_none', 'reading_cat_χαμηλό'})",0.145,0.135,0.074,0.5103448275862069,3.780332056194125,1.0,0.054425,1.766549295774648,0.8602023075707287,0.35922330097087374,0.4339246561690253,0.5292464878671774
"frozenset({'gender_male', 'test preparation course_none', 'reading_cat_χαμηλό'})","frozenset({'writing_cat_χαμηλό', 'lunch_standard'})",0.135,0.145,0.074,0.548148148148148,3.780332056194125,1.0,0.054425,1.892213114754098,0.8502577722230902,0.35922330097087374,0.47151830192765853,0.5292464878671774
"frozenset({'math_cat_χαμηλό', 'gender_female', 'reading_cat_χαμηλό'})","frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced'})",0.095,0.156,0.056,0.5894736842105264,3.77867746288799,1.0,0.04118,2.0558974358974362,0.8125493291239148,0.28717948717948716,0.5135944125717137,0.4742240215924427
"frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced'})","frozenset({'math_cat_χαμηλό', 'gender_female', 'reading_cat_χαμηλό'})",0.156,0.095,0.056,0.358974358974359,3.7786774628879893,1.0,0.04118,1.4118000000000002,0.8712762356127285,0.28717948717948716,0.2916843745573027,0.4742240215924427
"frozenset({'math_cat_χαμηλό', 'gender_male', 'lunch_standard'})","frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό'})",0.062,0.252,0.059,0.9516129032258064,3.7762416794674856,1.0,0.043376,15.458666666666645,0.7837808536012432,0.23137254901960783,0.935311367948939,0.5928699436763952
"frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό'})","frozenset({'math_cat_χαμηλό', 'gender_male', 'lunch_standard'})",0.252,0.062,0.059,0.2341269841269841,3.7762416794674856,1.0,0.043376,1.2247461139896372,0.9828695730988851,0.23137254901960783,0.1835042474701323,0.5928699436763952
frozenset({'reading_cat_υψηλό'}),"frozenset({'test preparation course_completed', 'math_cat_υψηλό', 'lunch_standard'})",0.235,0.071,0.063,0.2680851063829787,3.7758465687743485,1.0,0.046315,1.2692732558139537,0.960991804129059,0.25925925925925924,0.21214758491171012,0.5777045250224753
"frozenset({'test preparation course_completed', 'math_cat_υψηλό', 'lunch_standard'})",frozenset({'reading_cat_υψηλό'}),0.071,0.235,0.063,0.8873239436619719,3.7758465687743485,1.0,0.046315,6.789375000000002,0.791344165940506,0.25925925925925924,0.8527110374666299,0.5777045250224753
"frozenset({'gender_female', 'reading_cat_υψηλό'})","frozenset({'test preparation course_completed', 'writing_cat_υψηλό'})",0.159,0.117,0.07,0.44025157232704404,3.7628339515131968,1.0,0.051397000000000005,1.577494382022472,0.8730592831662987,0.33980582524271846,0.36608332086867956,0.5192710853088212
"frozenset({'test preparation course_completed', 'writing_cat_υψηλό'})","frozenset({'gender_female', 'reading_cat_υψηλό'})",0.117,0.159,0.07,0.5982905982905983,3.7628339515131968,1.0,0.051397000000000005,2.0935531914893617,0.8315321145445721,0.33980582524271846,0.5223431608687257,0.5192710853088212
frozenset({'reading_cat_υψηλό'}),"frozenset({'writing_cat_υψηλό', 'gender_female'})",0.235,0.154,0.136,0.5787234042553192,3.757944183476099,1.0,0.09981000000000001,2.0081818181818183,0.9593425605536332,0.5375494071146245,0.5020371208691716,0.7309201436861013
"frozenset({'writing_cat_υψηλό', 'gender_female'})",frozenset({'reading_cat_υψηλό'}),0.154,0.235,0.136,0.8831168831168832,3.757944183476099,1.0,0.09981000000000001,6.545000000000004,0.8674906132665833,0.5375494071146245,0.8472116119174944,0.7309201436861013
"frozenset({'writing_cat_χαμηλό', 'math_cat_χαμηλό'})","frozenset({'lunch_free/reduced', 'reading_cat_χαμηλό'})",0.23,0.139,0.12,0.5217391304347826,3.7535189239912414,1.0,0.08803,1.8002727272727272,0.9527056277056277,0.4819277108433735,0.444528606776751,0.692524241476384
"frozenset({'lunch_free/reduced', 'reading_cat_χαμηλό'})","frozenset({'writing_cat_χαμηλό', 'math_cat_χαμηλό'})",0.139,0.23,0.12,0.8633093525179855,3.753518923991241,1.0,0.08803,5.633157894736836,0.8520131629887727,0.4819277108433735,0.8224796785947863,0.692524241476384
frozenset({'reading_cat_υψηλό'}),"frozenset({'writing_cat_υψηλό', 'test preparation course_none', 'lunch_standard'})",0.235,0.075,0.066,0.28085106382978725,3.7446808510638303,1.0,0.048375,1.2862426035502958,0.9581105169340464,0.27049180327868855,0.22254169062679702,0.5804255319148937
"frozenset({'writing_cat_υψηλό', 'test preparation course_none', 'lunch_standard'})",frozenset({'reading_cat_υψηλό'}),0.075,0.235,0.066,0.8800000000000001,3.7446808510638303,1.0,0.048375,6.375000000000006,0.7923832923832923,0.27049180327868855,0.8431372549019609,0.5804255319148937
frozenset({'reading_cat_υψηλό'}),"frozenset({'writing_cat_υψηλό', 'gender_female', 'lunch_standard'})",0.235,0.116,0.102,0.4340425531914894,3.7417461482024943,1.0,0.07474,1.561954887218045,0.957836729463027,0.40963855421686746,0.3597766438817753,0.6566764490095377
"frozenset({'writing_cat_υψηλό', 'gender_female', 'lunch_standard'})",frozenset({'reading_cat_υψηλό'}),0.116,0.235,0.102,0.8793103448275861,3.7417461482024943,1.0,0.07474,6.338571428571422,0.8288971697276196,0.40963855421686746,0.8422357448726615,0.6566764490095377
"frozenset({'math_cat_χαμηλό', 'reading_cat_χαμηλό'})","frozenset({'writing_cat_χαμηλό', 'gender_female', 'test preparation course_none'})",0.218,0.086,0.07,0.3211009174311927,3.7337315980371244,1.0,0.051252000000000006,1.3462972972972975,0.9362805991962002,0.2991452991452992,0.25722201031859154,0.5675272029016429
"frozenset({'writing_cat_χαμηλό', 'gender_female', 'test preparation course_none'})","frozenset({'math_cat_χαμηλό', 'reading_cat_χαμηλό'})",0.086,0.218,0.07,0.8139534883720931,3.7337315980371244,1.0,0.051252000000000006,4.203250000000002,0.8010628321350421,0.2991452991452992,0.7620888598108608,0.5675272029016429
"frozenset({'test preparation course_completed', 'reading_cat_υψηλό', 'lunch_standard'})",frozenset({'math_cat_υψηλό'}),0.096,0.176,0.063,0.65625,3.728693181818182,1.0,0.046104000000000006,2.397090909090909,0.8095238095238096,0.30143540669856456,0.5828276699029126,0.5071022727272727
frozenset({'math_cat_υψηλό'}),"frozenset({'test preparation course_completed', 'reading_cat_υψηλό', 'lunch_standard'})",0.176,0.096,0.063,0.35795454545454547,3.728693181818182,1.0,0.046104000000000006,1.408,0.8881183541377716,0.30143540669856456,0.28977272727272724,0.5071022727272727
"frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced'})","frozenset({'gender_female', 'reading_cat_χαμηλό'})",0.156,0.098,0.057,0.3653846153846154,3.7284144427001573,1.0,0.041712,1.4213333333333333,0.8670491394362684,0.2893401015228426,0.2964352720450282,0.47350863422291994
"frozenset({'gender_female', 'reading_cat_χαμηλό'})","frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced'})",0.098,0.156,0.057,0.5816326530612245,3.728414442700157,1.0,0.041712,2.0173658536585366,0.8112965340179717,0.2893401015228426,0.5043040913047683,0.47350863422291994
"frozenset({'math_cat_μέτριο', 'gender_female', 'reading_cat_υψηλό'})",frozenset({'writing_cat_υψηλό'}),0.092,0.208,0.071,0.7717391304347826,3.710284280936455,1.0,0.05186399999999999,3.4697142857142858,0.8044921511447539,0.31004366812227074,0.7117918313570487,0.5565426421404682
frozenset({'writing_cat_υψηλό'}),"frozenset({'math_cat_μέτριο', 'gender_female', 'reading_cat_υψηλό'})",0.208,0.092,0.071,0.34134615384615385,3.710284280936455,1.0,0.05186399999999999,1.3785693430656936,0.9223218096457533,0.31004366812227074,0.27461030159268046,0.5565426421404682
"frozenset({'writing_cat_υψηλό', 'lunch_standard'})","frozenset({'test preparation course_none', 'reading_cat_υψηλό'})",0.162,0.11,0.066,0.40740740740740744,3.703703703703704,1.0,0.04818,1.501875,0.8711217183770883,0.3203883495145631,0.3341656263004578,0.5037037037037038
"frozenset({'test preparation course_none', 'reading_cat_υψηλό'})","frozenset({'writing_cat_υψηλό', 'lunch_standard'})",0.11,0.162,0.066,0.6,3.7037037037037033,1.0,0.04818,2.0949999999999998,0.8202247191011236,0.3203883495145631,0.522673031026253,0.5037037037037038
"frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό', 'lunch_standard'})","frozenset({'math_cat_χαμηλό', 'gender_male'})",0.119,0.134,0.059,0.4957983193277311,3.6999874576696348,1.0,0.043053999999999995,1.7175666666666667,0.8282960426326016,0.30412371134020616,0.4177809692006133,0.4680484133952088
"frozenset({'math_cat_χαμηλό', 'gender_male'})","frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό', 'lunch_standard'})",0.134,0.119,0.059,0.4402985074626865,3.6999874576696348,1.0,0.043053999999999995,1.574053333333333,0.8426429717775081,0.30412371134020616,0.36469751130838424,0.4680484133952088
"frozenset({'test preparation course_none', 'reading_cat_υψηλό', 'lunch_standard'})",frozenset({'writing_cat_υψηλό'}),0.086,0.208,0.066,0.7674418604651164,3.689624329159214,1.0,0.048112,3.4056000000000024,0.7975598435117035,0.2894736842105263,0.706365985435753,0.5423747763864044
frozenset({'writing_cat_υψηλό'}),"frozenset({'test preparation course_none', 'reading_cat_υψηλό', 'lunch_standard'})",0.208,0.086,0.066,0.31730769230769235,3.6896243291592135,1.0,0.048112,1.3388169014084508,0.9204162840526476,0.2894736842105263,0.2530718734219829,0.5423747763864044
"frozenset({'math_cat_χαμηλό', 'gender_male', 'test preparation course_none'})","frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό'})",0.099,0.252,0.092,0.9292929292929293,3.687670354337021,1.0,0.067052,10.57885714285714,0.8089079766443081,0.35521235521235517,0.9054718306055204,0.6471861471861472
"frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό'})","frozenset({'math_cat_χαμηλό', 'gender_male', 'test preparation course_none'})",0.252,0.099,0.092,0.36507936507936506,3.687670354337021,1.0,0.067052,1.419075,0.9743664264124622,0.35521235521235517,0.29531561052093785,0.6471861471861472
"frozenset({'lunch_free/reduced', 'test preparation course_none', 'reading_cat_χαμηλό'})","frozenset({'math_cat_χαμηλό', 'writing_cat_χαμηλό'})",0.105,0.23,0.089,0.8476190476190476,3.6853002070393375,1.0,0.06484999999999999,5.053125,0.8141359613332495,0.36178861788617883,0.8021026592455164,0.617287784679089
"frozenset({'math_cat_χαμηλό', 'writing_cat_χαμηλό'})","frozenset({'lunch_free/reduced', 'test preparation course_none', 'reading_cat_χαμηλό'})",0.23,0.105,0.089,0.3869565217391304,3.6853002070393375,1.0,0.06484999999999999,1.4599290780141845,0.9463008901211148,0.36178861788617883,0.31503521982025745,0.617287784679089
"frozenset({'reading_cat_υψηλό', 'lunch_standard'})",frozenset({'math_cat_υψηλό'}),0.182,0.176,0.118,0.6483516483516484,3.6838161838161843,1.0,0.08596799999999999,2.3432500000000003,0.8906385976544693,0.49166666666666664,0.5732422916888936,0.6594030969030968
frozenset({'math_cat_υψηλό'}),"frozenset({'reading_cat_υψηλό', 'lunch_standard'})",0.176,0.182,0.118,0.6704545454545454,3.683816183816184,1.0,0.08596799999999999,2.482206896551724,0.8841533651472765,0.49166666666666664,0.597132696154701,0.6594030969030968
"frozenset({'race/ethnicity_group C', 'writing_cat_υψηλό'})",frozenset({'reading_cat_υψηλό'}),0.067,0.235,0.058,0.8656716417910447,3.683709114004446,1.0,0.042255,5.694999999999998,0.7808515356469675,0.2377049180327869,0.8244073748902545,0.5562400762146713
frozenset({'reading_cat_υψηλό'}),"frozenset({'race/ethnicity_group C', 'writing_cat_υψηλό'})",0.235,0.067,0.058,0.2468085106382979,3.683709114004446,1.0,0.042255,1.2387288135593222,0.9523326572008114,0.2377049180327869,0.19272080454265583,0.5562400762146713
"frozenset({'gender_female', 'reading_cat_χαμηλό'})","frozenset({'writing_cat_χαμηλό', 'math_cat_χαμηλό'})",0.098,0.23,0.083,0.8469387755102041,3.6823425022182787,1.0,0.06046,5.030666666666669,0.8075762028156973,0.33877551020408164,0.8012191889742911,0.6039041703637977
"frozenset({'writing_cat_χαμηλό', 'math_cat_χαμηλό'})","frozenset({'gender_female', 'reading_cat_χαμηλό'})",0.23,0.098,0.083,0.36086956521739133,3.6823425022182787,1.0,0.06046,1.411292517006803,0.9460178375841025,0.33877551020408164,0.29142967319001256,0.6039041703637977
"frozenset({'writing_cat_υψηλό', 'race/ethnicity_group D'})",frozenset({'reading_cat_υψηλό'}),0.059,0.235,0.051,0.864406779661017,3.6783267219617746,1.0,0.037135,5.6418750000000015,0.7737909191306703,0.20987654320987653,0.8227539603411986,0.5407140281283809
frozenset({'reading_cat_υψηλό'}),"frozenset({'writing_cat_υψηλό', 'race/ethnicity_group D'})",0.235,0.059,0.051,0.2170212765957447,3.6783267219617746,1.0,0.037135,1.201820652173913,0.9518134051006023,0.20987654320987653,0.16792909308793272,0.5407140281283809
"frozenset({'test preparation course_completed', 'writing_cat_υψηλό', 'gender_female'})",frozenset({'reading_cat_υψηλό'}),0.081,0.235,0.07,0.8641975308641976,3.6774363015497773,1.0,0.05096500000000001,5.6331818181818205,0.7922431214052542,0.28455284552845533,0.822480432502219,0.5810349356448647
frozenset({'reading_cat_υψηλό'}),"frozenset({'test preparation course_completed', 'writing_cat_υψηλό', 'gender_female'})",0.235,0.081,0.07,0.29787234042553196,3.6774363015497773,1.0,0.05096500000000001,1.308878787878788,0.9517273576097106,0.28455284552845533,0.23598731275901189,0.5810349356448647
"frozenset({'writing_cat_χαμηλό', 'gender_female', 'reading_cat_χαμηλό'})","frozenset({'math_cat_χαμηλό', 'lunch_free/reduced'})",0.084,0.182,0.056,0.6666666666666666,3.663003663003663,1.0,0.040712,2.4539999999999997,0.7936681222707423,0.26666666666666666,0.5925020374898124,0.48717948717948717
"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced'})","frozenset({'writing_cat_χαμηλό', 'gender_female', 'reading_cat_χαμηλό'})",0.182,0.084,0.056,0.3076923076923077,3.663003663003663,1.0,0.040712,1.3231111111111111,0.8887530562347188,0.26666666666666666,0.24420557608330534,0.48717948717948717
"frozenset({'reading_cat_υψηλό', 'lunch_standard'})","frozenset({'test preparation course_completed', 'writing_cat_υψηλό'})",0.182,0.117,0.078,0.4285714285714286,3.663003663003663,1.0,0.056706,1.54525,0.8887530562347188,0.35294117647058826,0.3528555249959554,0.5476190476190477
"frozenset({'test preparation course_completed', 'writing_cat_υψηλό'})","frozenset({'reading_cat_υψηλό', 'lunch_standard'})",0.117,0.182,0.078,0.6666666666666666,3.663003663003663,1.0,0.056706,2.4539999999999997,0.8233295583238958,0.35294117647058826,0.5925020374898124,0.5476190476190477
"frozenset({'test preparation course_none', 'reading_cat_χαμηλό'})","frozenset({'writing_cat_χαμηλό', 'race/ethnicity_group D'})",0.214,0.064,0.05,0.23364485981308414,3.6507009345794397,1.0,0.036304,1.2213658536585366,0.9237659033078881,0.21929824561403508,0.18124450834731212,0.5074474299065421
"frozenset({'writing_cat_χαμηλό', 'race/ethnicity_group D'})","frozenset({'test preparation course_none', 'reading_cat_χαμηλό'})",0.064,0.214,0.05,0.78125,3.6507009345794392,1.0,0.036304,3.5931428571428574,0.7757264957264958,0.21929824561403508,0.7216921119592875,0.5074474299065421
"frozenset({'test preparation course_completed', 'math_cat_υψηλό'})",frozenset({'reading_cat_υψηλό'}),0.084,0.235,0.072,0.857142857142857,3.64741641337386,1.0,0.052259999999999994,5.354999999999994,0.7923944687045124,0.291497975708502,0.8132586367880483,0.5817629179331306
frozenset({'reading_cat_υψηλό'}),"frozenset({'test preparation course_completed', 'math_cat_υψηλό'})",0.235,0.084,0.072,0.30638297872340425,3.64741641337386,1.0,0.052259999999999994,1.3206134969325154,0.9488017429193899,0.291497975708502,0.2427761776456378,0.5817629179331306
"frozenset({'math_cat_χαμηλό', 'gender_male'})","frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό'})",0.134,0.252,0.123,0.917910447761194,3.642501776830135,1.0,0.089232,9.111999999999998,0.8377175688616009,0.46768060836501896,0.8902546093064091,0.703002842928216
"frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό'})","frozenset({'math_cat_χαμηλό', 'gender_male'})",0.252,0.134,0.123,0.4880952380952381,3.6425017768301347,1.0,0.089232,1.6917209302325582,0.9698708751793401,0.46768060836501896,0.40888595623006707,0.703002842928216
"frozenset({'reading_cat_υψηλό', 'lunch_standard'})","frozenset({'writing_cat_υψηλό', 'gender_female'})",0.182,0.154,0.102,0.5604395604395604,3.6392179249322107,1.0,0.073972,1.92465,0.8865717436118702,0.4358974358974359,0.480425012339906,0.6113886113886113
"frozenset({'writing_cat_υψηλό', 'gender_female'})","frozenset({'reading_cat_υψηλό', 'lunch_standard'})",0.154,0.182,0.102,0.6623376623376623,3.6392179249322107,1.0,0.073972,2.4225384615384615,0.8572289435868911,0.4358974358974359,0.5872098561585114,0.6113886113886113
"frozenset({'math_cat_υψηλό', 'writing_cat_υψηλό', 'lunch_standard'})","frozenset({'gender_female', 'reading_cat_υψηλό'})",0.102,0.159,0.059,0.5784313725490197,3.6379331606856584,1.0,0.042782,1.9949302325581397,0.807481786267034,0.29207920792079206,0.49872933715697937,0.4747502774694784
"frozenset({'gender_female', 'reading_cat_υψηλό'})","frozenset({'math_cat_υψηλό', 'writing_cat_υψηλό', 'lunch_standard'})",0.159,0.102,0.059,0.3710691823899371,3.637933160685658,1.0,0.042782,1.4278199999999999,0.8622100405086762,0.29207920792079206,0.2996316062248743,0.4747502774694784
"frozenset({'writing_cat_υψηλό', 'gender_female'})","frozenset({'test preparation course_completed', 'reading_cat_υψηλό'})",0.154,0.125,0.07,0.4545454545454546,3.6363636363636367,1.0,0.05075,1.6041666666666667,0.8569739952718677,0.3349282296650718,0.3766233766233767,0.5072727272727273
"frozenset({'test preparation course_completed', 'reading_cat_υψηλό'})","frozenset({'writing_cat_υψηλό', 'gender_female'})",0.125,0.154,0.07,0.56,3.6363636363636367,1.0,0.05075,1.9227272727272728,0.8285714285714285,0.3349282296650718,0.4799054373522459,0.5072727272727273
"frozenset({'test preparation course_none', 'reading_cat_υψηλό', 'lunch_standard'})",frozenset({'math_cat_υψηλό'}),0.086,0.176,0.055,0.6395348837209303,3.6337209302325584,1.0,0.039864000000000004,2.285935483870968,0.7929978118161926,0.2657004830917874,0.5625423346127794,0.47601744186046513
frozenset({'math_cat_υψηλό'}),"frozenset({'test preparation course_none', 'reading_cat_υψηλό', 'lunch_standard'})",0.176,0.086,0.055,0.3125,3.6337209302325584,1.0,0.039864000000000004,1.3294545454545454,0.8796116504854369,0.2657004830917874,0.24781181619256018,0.47601744186046513
"frozenset({'math_cat_χαμηλό', 'gender_male'})","frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced', 'reading_cat_χαμηλό'})",0.134,0.133,0.064,0.47761194029850745,3.5910672202895295,1.0,0.046178,1.6596857142857144,0.8331769630484988,0.31527093596059114,0.39747628638812854,0.47940747390865224
"frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced', 'reading_cat_χαμηλό'})","frozenset({'math_cat_χαμηλό', 'gender_male'})",0.133,0.134,0.064,0.48120300751879697,3.5910672202895295,1.0,0.046178,1.6692463768115942,0.8322159746251441,0.31527093596059114,0.4009272604143152,0.47940747390865224
"frozenset({'test preparation course_completed', 'math_cat_υψηλό', 'lunch_standard'})",frozenset({'writing_cat_υψηλό'}),0.071,0.208,0.053,0.7464788732394366,3.588840736728061,1.0,0.038232,3.124,0.7764892255823872,0.2345132743362832,0.679897567221511,0.5006432827735645
frozenset({'writing_cat_υψηλό'}),"frozenset({'test preparation course_completed', 'math_cat_υψηλό', 'lunch_standard'})",0.208,0.071,0.053,0.2548076923076923,3.5888407367280606,1.0,0.038232,1.2466580645161291,0.9108061749571185,0.2345132743362832,0.19785542767243516,0.5006432827735645
"frozenset({'gender_female', 'reading_cat_υψηλό'})","frozenset({'math_cat_υψηλό', 'writing_cat_υψηλό'})",0.159,0.114,0.065,0.4088050314465409,3.5860090477766744,1.0,0.046874,1.498659574468085,0.8574773621146986,0.3125,0.3327370558087369,0.48949023502151606
"frozenset({'math_cat_υψηλό', 'writing_cat_υψηλό'})","frozenset({'gender_female', 'reading_cat_υψηλό'})",0.114,0.159,0.065,0.5701754385964912,3.5860090477766744,1.0,0.046874,1.956612244897959,0.8139260288244486,0.3125,0.48891253103031057,0.48949023502151606
"frozenset({'writing_cat_χαμηλό', 'lunch_standard'})","frozenset({'gender_male', 'reading_cat_χαμηλό'})",0.145,0.177,0.092,0.6344827586206897,3.5846483537892078,1.0,0.066335,2.2516037735849057,0.843312992626494,0.4000000000000001,0.555872124690996,0.5771283849600624
"frozenset({'gender_male', 'reading_cat_χαμηλό'})","frozenset({'writing_cat_χαμηλό', 'lunch_standard'})",0.177,0.145,0.092,0.519774011299435,3.5846483537892073,1.0,0.066335,1.7804117647058824,0.8761028052195047,0.4000000000000001,0.4383321769584035,0.5771283849600624
frozenset({'math_cat_υψηλό'}),"frozenset({'writing_cat_υψηλό', 'lunch_standard'})",0.176,0.162,0.102,0.5795454545454546,3.5774410774410774,1.0,0.073488,1.9930810810810813,0.874357509994289,0.4322033898305085,0.49826426556736825,0.6045875420875421
"frozenset({'writing_cat_υψηλό', 'lunch_standard'})",frozenset({'math_cat_υψηλό'}),0.162,0.176,0.102,0.6296296296296295,3.577441077441077,1.0,0.073488,2.2247999999999997,0.8597501052927138,0.4322033898305085,0.5505213951815893,0.6045875420875421
"frozenset({'math_cat_χαμηλό', 'writing_cat_χαμηλό'})","frozenset({'lunch_free/reduced', 'gender_male', 'reading_cat_χαμηλό'})",0.23,0.078,0.064,0.2782608695652174,3.5674470457079153,1.0,0.046060000000000004,1.2774698795180723,0.9346590909090909,0.26229508196721313,0.21720267848722058,0.5493868450390189
"frozenset({'lunch_free/reduced', 'gender_male', 'reading_cat_χαμηλό'})","frozenset({'math_cat_χαμηλό', 'writing_cat_χαμηλό'})",0.078,0.23,0.064,0.8205128205128205,3.567447045707915,1.0,0.046060000000000004,4.289999999999999,0.780572125813449,0.26229508196721313,0.7668997668997669,0.5493868450390189
"frozenset({'math_cat_χαμηλό', 'gender_male', 'test preparation course_none'})","frozenset({'writing_cat_χαμηλό', 'lunch_standard'})",0.099,0.145,0.051,0.5151515151515151,3.5527690700104495,1.0,0.036645,1.7634375,0.7974799242671541,0.26424870466321243,0.4329257487152224,0.43343782654127483
"frozenset({'writing_cat_χαμηλό', 'lunch_standard'})","frozenset({'math_cat_χαμηλό', 'gender_male', 'test preparation course_none'})",0.145,0.099,0.051,0.35172413793103446,3.552769070010449,1.0,0.036645,1.389840425531915,0.8403852769177846,0.26424870466321243,0.2804929388801714,0.43343782654127483
"frozenset({'test preparation course_completed', 'math_cat_υψηλό'})",frozenset({'writing_cat_υψηλό'}),0.084,0.208,0.062,0.738095238095238,3.5485347985347984,1.0,0.044528,3.023999999999999,0.7840540921256515,0.26956521739130435,0.6693121693121692,0.5180860805860805
frozenset({'writing_cat_υψηλό'}),"frozenset({'test preparation course_completed', 'math_cat_υψηλό'})",0.208,0.084,0.062,0.2980769230769231,3.5485347985347984,1.0,0.044528,1.3049863013698633,0.9068100358422938,0.26956521739130435,0.23370843130668453,0.5180860805860805
frozenset({'writing_cat_υψηλό'}),"frozenset({'test preparation course_none', 'reading_cat_υψηλό'})",0.208,0.11,0.081,0.3894230769230769,3.5402097902097904,1.0,0.058120000000000005,1.4576377952755903,0.9059733133807208,0.3417721518987342,0.31395851339671565,0.5628933566433566
"frozenset({'test preparation course_none', 'reading_cat_υψηλό'})",frozenset({'writing_cat_υψηλό'}),0.11,0.208,0.081,0.7363636363636363,3.5402097902097904,1.0,0.058120000000000005,3.0041379310344825,0.8062144541545291,0.3417721518987342,0.6671258034894398,0.5628933566433566
"frozenset({'math_cat_χαμηλό', 'reading_cat_χαμηλό'})","frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced'})",0.218,0.156,0.12,0.5504587155963303,3.5285815102328866,1.0,0.085992,1.877469387755102,0.9163682864450127,0.47244094488188976,0.46736814644114966,0.6598447424135497
"frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced'})","frozenset({'math_cat_χαμηλό', 'reading_cat_χαμηλό'})",0.156,0.218,0.12,0.7692307692307692,3.528581510232886,1.0,0.085992,3.3886666666666656,0.8490521327014219,0.47244094488188976,0.704898681880779,0.6598447424135497
"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced', 'gender_male'})","frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό'})",0.072,0.252,0.064,0.888888888888889,3.5273368606701943,1.0,0.045856,6.732000000000004,0.7720905172413792,0.24615384615384614,0.8514557338086751,0.5714285714285714
"frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό'})","frozenset({'math_cat_χαμηλό', 'lunch_free/reduced', 'gender_male'})",0.252,0.072,0.064,0.25396825396825395,3.5273368606701943,1.0,0.045856,1.2439148936170212,0.9578877005347595,0.24615384615384614,0.19608648056923916,0.5714285714285714
"frozenset({'math_cat_χαμηλό', 'gender_male', 'writing_cat_χαμηλό'})","frozenset({'lunch_free/reduced', 'reading_cat_χαμηλό'})",0.131,0.139,0.064,0.48854961832061067,3.5147454555439612,1.0,0.045791,1.6834477611940295,0.8233422036823935,0.3106796116504854,0.40598097365924585,0.4744906364984348
"frozenset({'lunch_free/reduced', 'reading_cat_χαμηλό'})","frozenset({'math_cat_χαμηλό', 'gender_male', 'writing_cat_χαμηλό'})",0.139,0.131,0.064,0.46043165467625896,3.5147454555439612,1.0,0.045791,1.6105466666666668,0.8309923054587688,0.3106796116504854,0.3790928132062819,0.4744906364984348
frozenset({'writing_cat_υψηλό'}),"frozenset({'math_cat_μέτριο', 'reading_cat_υψηλό'})",0.208,0.103,0.075,0.3605769230769231,3.5007468259895447,1.0,0.053576,1.402827067669173,0.901952861952862,0.3177966101694915,0.2871537603979076,0.5443661314413741
"frozenset({'math_cat_μέτριο', 'reading_cat_υψηλό'})",frozenset({'writing_cat_υψηλό'}),0.103,0.208,0.075,0.7281553398058253,3.5007468259895447,1.0,0.053576,2.9134285714285717,0.7963730955035303,0.3177966101694915,0.6567617926841228,0.5443661314413741
"frozenset({'math_cat_χαμηλό', 'test preparation course_none', 'writing_cat_χαμηλό'})","frozenset({'lunch_free/reduced', 'reading_cat_χαμηλό'})",0.183,0.139,0.089,0.48633879781420764,3.4988402720446588,1.0,0.063563,1.6762021276595744,0.8741628044503735,0.3819742489270386,0.4034132378794514,0.5633132837991901
"frozenset({'lunch_free/reduced', 'reading_cat_χαμηλό'})","frozenset({'math_cat_χαμηλό', 'test preparation course_none', 'writing_cat_χαμηλό'})",0.139,0.183,0.089,0.6402877697841726,3.4988402720446588,1.0,0.063563,2.2712599999999994,0.8294901408083102,0.3819742489270386,0.5597157524898073,0.5633132837991901
"frozenset({'math_cat_χαμηλό', 'writing_cat_χαμηλό', 'lunch_standard'})","frozenset({'test preparation course_none', 'reading_cat_χαμηλό'})",0.098,0.214,0.073,0.7448979591836734,3.4808315849704368,1.0,0.05202799999999999,3.0811199999999994,0.7901467059502474,0.3054393305439331,0.6754426961624344,0.5430097272553881
"frozenset({'test preparation course_none', 'reading_cat_χαμηλό'})","frozenset({'math_cat_χαμηλό', 'writing_cat_χαμηλό', 'lunch_standard'})",0.214,0.098,0.073,0.3411214953271028,3.4808315849704363,1.0,0.05202799999999999,1.3689929078014185,0.9067586879988845,0.3054393305439331,0.269536025861533,0.5430097272553881
"frozenset({'reading_cat_χαμηλό', 'lunch_standard'})","frozenset({'writing_cat_χαμηλό', 'gender_male', 'test preparation course_none'})",0.136,0.157,0.074,0.5441176470588235,3.4657174971899587,1.0,0.05264799999999999,1.8491612903225805,0.8234484484484484,0.33789954337899536,0.45921429069848574,0.5077276133383289
"frozenset({'writing_cat_χαμηλό', 'gender_male', 'test preparation course_none'})","frozenset({'reading_cat_χαμηλό', 'lunch_standard'})",0.157,0.136,0.074,0.47133757961783435,3.4657174971899583,1.0,0.05264799999999999,1.6343132530120479,0.8439613991215414,0.33789954337899536,0.38812219863175274,0.5077276133383289
"frozenset({'writing_cat_χαμηλό', 'gender_female'})","frozenset({'math_cat_χαμηλό', 'lunch_free/reduced'})",0.1,0.182,0.063,0.63,3.4615384615384617,1.0,0.0448,2.210810810810811,0.7901234567901234,0.2876712328767123,0.5476772616136919,0.4880769230769231
"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced'})","frozenset({'writing_cat_χαμηλό', 'gender_female'})",0.182,0.1,0.063,0.34615384615384615,3.4615384615384612,1.0,0.0448,1.3764705882352941,0.8693289866883999,0.2876712328767123,0.2735042735042735,0.4880769230769231
frozenset({'math_cat_υψηλό'}),"frozenset({'test preparation course_completed', 'writing_cat_υψηλό', 'lunch_standard'})",0.176,0.087,0.053,0.30113636363636365,3.4613375130616513,1.0,0.037688,1.3064065040650408,0.8629785675032057,0.2523809523809524,0.23454147167181122,0.4551658829676071
"frozenset({'test preparation course_completed', 'writing_cat_υψηλό', 'lunch_standard'})",frozenset({'math_cat_υψηλό'}),0.087,0.176,0.053,0.6091954022988506,3.4613375130616513,1.0,0.037688,2.1084705882352943,0.7788546983818636,0.2523809523809524,0.5257225756054011,0.4551658829676071
frozenset({'reading_cat_χαμηλό'}),"frozenset({'math_cat_χαμηλό', 'gender_male', 'lunch_standard'})",0.275,0.062,0.059,0.2145454545454545,3.4604105571847503,1.0,0.041949999999999994,1.1942129629629628,0.9807130333138515,0.2122302158273381,0.1626284163597596,0.5830791788856304
frozenset({'reading_cat_χαμηλό'}),"frozenset({'math_cat_χαμηλό', 'gender_male', 'writing_cat_χαμηλό', 'lunch_standard'})",0.275,0.062,0.059,0.2145454545454545,3.4604105571847503,1.0,0.041949999999999994,1.1942129629629628,0.9807130333138515,0.2122302158273381,0.1626284163597596,0.5830791788856304
"frozenset({'math_cat_χαμηλό', 'gender_male', 'writing_cat_χαμηλό', 'lunch_standard'})",frozenset({'reading_cat_χαμηλό'}),0.062,0.275,0.059,0.9516129032258064,3.4604105571847503,1.0,0.041949999999999994,14.983333333333311,0.7580138050666763,0.2122302158273381,0.9332591768631812,0.5830791788856304
"frozenset({'math_cat_χαμηλό', 'gender_male', 'lunch_standard'})",frozenset({'reading_cat_χαμηλό'}),0.062,0.275,0.059,0.9516129032258064,3.4604105571847503,1.0,0.041949999999999994,14.983333333333311,0.7580138050666763,0.2122302158273381,0.9332591768631812,0.5830791788856304
"frozenset({'math_cat_χαμηλό', 'test preparation course_none'})","frozenset({'writing_cat_χαμηλό', 'gender_female'})",0.246,0.1,0.085,0.3455284552845529,3.4552845528455287,1.0,0.06040000000000001,1.3751552795031057,0.9424247152441879,0.32567049808429127,0.27280939476061433,0.5977642276422764
"frozenset({'writing_cat_χαμηλό', 'gender_female'})","frozenset({'math_cat_χαμηλό', 'test preparation course_none'})",0.1,0.246,0.085,0.85,3.4552845528455283,1.0,0.06040000000000001,5.0266666666666655,0.7895424836601307,0.32567049808429127,0.8010610079575596,0.5977642276422764
"frozenset({'lunch_free/reduced', 'reading_cat_χαμηλό'})","frozenset({'math_cat_χαμηλό', 'gender_male'})",0.139,0.134,0.064,0.46043165467625896,3.4360571244496936,1.0,0.045374,1.6049866666666668,0.8234247967479674,0.3062200956937799,0.376941864522239,0.4690217974873832
"frozenset({'math_cat_χαμηλό', 'gender_male'})","frozenset({'lunch_free/reduced', 'reading_cat_χαμηλό'})",0.134,0.139,0.064,0.47761194029850745,3.4360571244496936,1.0,0.045374,1.6482,0.8186706120092379,0.3062200956937799,0.39327751486470086,0.4690217974873832
"frozenset({'math_cat_υψηλό', 'lunch_standard'})","frozenset({'writing_cat_υψηλό', 'reading_cat_υψηλό'})",0.156,0.185,0.099,0.6346153846153847,3.430353430353431,1.0,0.07014000000000001,2.230526315789474,0.8394370242711475,0.40909090909090917,0.5516753185464843,0.58487525987526
"frozenset({'writing_cat_υψηλό', 'reading_cat_υψηλό'})","frozenset({'math_cat_υψηλό', 'lunch_standard'})",0.185,0.156,0.099,0.5351351351351352,3.430353430353431,1.0,0.07014000000000001,1.8155813953488376,0.8693065625580965,0.40909090909090917,0.4492122454207763,0.58487525987526
"frozenset({'writing_cat_χαμηλό', 'test preparation course_none', 'lunch_standard'})","frozenset({'gender_male', 'reading_cat_χαμηλό'})",0.122,0.177,0.074,0.6065573770491803,3.4268778364360473,1.0,0.052405999999999994,2.0917916666666665,0.806593609554885,0.3288888888888889,0.5219409198653467,0.512318236547189
"frozenset({'gender_male', 'reading_cat_χαμηλό'})","frozenset({'writing_cat_χαμηλό', 'test preparation course_none', 'lunch_standard'})",0.177,0.122,0.074,0.4180790960451977,3.426877836436047,1.0,0.052405999999999994,1.5087961165048542,0.8604971922104365,0.3288888888888889,0.33721992715853955,0.512318236547189
"frozenset({'math_cat_χαμηλό', 'gender_male'})","frozenset({'writing_cat_χαμηλό', 'test preparation course_none', 'reading_cat_χαμηλό'})",0.134,0.201,0.092,0.6865671641791045,3.4157570357169376,1.0,0.065066,2.549190476190476,0.8166733607791947,0.37860082304526743,0.6077186034782284,0.572139303482587
"frozenset({'writing_cat_χαμηλό', 'test preparation course_none', 'reading_cat_χαμηλό'})","frozenset({'math_cat_χαμηλό', 'gender_male'})",0.201,0.134,0.092,0.4577114427860696,3.415757035716937,1.0,0.065066,1.5969357798165136,0.8851553572400284,0.37860082304526743,0.37380074224719356,0.572139303482587
frozenset({'reading_cat_χαμηλό'}),"frozenset({'math_cat_χαμηλό', 'gender_male', 'writing_cat_χαμηλό'})",0.275,0.131,0.123,0.4472727272727272,3.4142956280360854,1.0,0.086975,1.572203947368421,0.9753294084664985,0.4346289752650176,0.36395020399623385,0.6931020124913254
"frozenset({'math_cat_χαμηλό', 'gender_male', 'writing_cat_χαμηλό'})",frozenset({'reading_cat_χαμηλό'}),0.131,0.275,0.123,0.9389312977099236,3.4142956280360854,1.0,0.086975,11.87187499999998,0.8137098056826368,0.4346289752650176,0.9157673071861014,0.6931020124913254
"frozenset({'math_cat_χαμηλό', 'gender_male', 'test preparation course_none', 'writing_cat_χαμηλό'})",frozenset({'reading_cat_χαμηλό'}),0.098,0.275,0.092,0.9387755102040816,3.413729128014842,1.0,0.06505,11.841666666666653,0.7838860503229538,0.32740213523131667,0.9155524278676987,0.636660482374768
frozenset({'reading_cat_χαμηλό'}),"frozenset({'math_cat_χαμηλό', 'gender_male', 'test preparation course_none', 'writing_cat_χαμηλό'})",0.275,0.098,0.092,0.33454545454545453,3.413729128014842,1.0,0.06505,1.3554644808743168,0.9752623688155923,0.32740213523131667,0.2622455150171336,0.636660482374768
"frozenset({'gender_male', 'reading_cat_χαμηλό'})","frozenset({'math_cat_χαμηλό', 'writing_cat_χαμηλό', 'lunch_standard'})",0.177,0.098,0.059,0.3333333333333333,3.401360544217687,1.0,0.041654,1.353,0.8578371810449574,0.2731481481481481,0.26090169992609014,0.4676870748299319
"frozenset({'math_cat_χαμηλό', 'writing_cat_χαμηλό', 'lunch_standard'})","frozenset({'gender_male', 'reading_cat_χαμηλό'})",0.098,0.177,0.059,0.6020408163265305,3.401360544217687,1.0,0.041654,2.0680512820512815,0.7827050997782704,0.2731481481481481,0.516452996751556,0.4676870748299319
"frozenset({'writing_cat_υψηλό', 'math_cat_μέτριο'})",frozenset({'reading_cat_υψηλό'}),0.094,0.235,0.075,0.7978723404255319,3.395201448619285,1.0,0.05291,3.784736842105263,0.7786607799852833,0.29527559055118113,0.735780837157558,0.5585106382978724
frozenset({'reading_cat_υψηλό'}),"frozenset({'writing_cat_υψηλό', 'math_cat_μέτριο'})",0.235,0.094,0.075,0.3191489361702128,3.395201448619285,1.0,0.05291,1.3306875,0.9221786492374728,0.29527559055118113,0.24850875956977128,0.5585106382978724
"frozenset({'parental level of education_high school', 'reading_cat_χαμηλό'})","frozenset({'math_cat_χαμηλό', 'writing_cat_χαμηλό'})",0.073,0.23,0.057,0.7808219178082193,3.3948779035139967,1.0,0.04021,3.5131250000000023,0.7609909347262439,0.23170731707317074,0.7153531400106744,0.5143240023823705
"frozenset({'math_cat_χαμηλό', 'writing_cat_χαμηλό'})","frozenset({'parental level of education_high school', 'reading_cat_χαμηλό'})",0.23,0.073,0.057,0.24782608695652172,3.394877903513996,1.0,0.04021,1.23242774566474,0.9161540214171793,0.23170731707317074,0.18859340556259085,0.5143240023823705
frozenset({'reading_cat_υψηλό'}),"frozenset({'writing_cat_υψηλό', 'math_cat_μέτριο', 'gender_female'})",0.235,0.089,0.071,0.3021276595744681,3.394692804207507,1.0,0.050085,1.3053963414634147,0.9221209610604806,0.2806324110671937,0.23394913235397155,0.549940234281616
"frozenset({'writing_cat_υψηλό', 'math_cat_μέτριο', 'gender_female'})",frozenset({'reading_cat_υψηλό'}),0.089,0.235,0.071,0.797752808988764,3.3946928042075064,1.0,0.050085,3.7824999999999993,0.774338677509624,0.2806324110671937,0.735624586913417,0.549940234281616
"frozenset({'math_cat_χαμηλό', 'test preparation course_none'})","frozenset({'writing_cat_χαμηλό', 'gender_female', 'reading_cat_χαμηλό'})",0.246,0.084,0.07,0.28455284552845533,3.387533875338754,1.0,0.049336000000000005,1.2803181818181821,0.9347480106100795,0.2692307692307693,0.21894415450704727,0.5589430894308943
"frozenset({'writing_cat_χαμηλό', 'gender_female', 'reading_cat_χαμηλό'})","frozenset({'math_cat_χαμηλό', 'test preparation course_none'})",0.084,0.246,0.07,0.8333333333333334,3.3875338753387534,1.0,0.049336000000000005,4.524000000000001,0.7694323144104803,0.2692307692307693,0.7789566755083996,0.5589430894308943
"frozenset({'math_cat_χαμηλό', 'gender_male', 'lunch_standard'})","frozenset({'writing_cat_χαμηλό', 'test preparation course_none'})",0.062,0.243,0.051,0.8225806451612903,3.38510553564317,1.0,0.035933999999999994,4.266727272727271,0.7511601655587609,0.20078740157480313,0.7656283291430519,0.5162285941855834
"frozenset({'writing_cat_χαμηλό', 'test preparation course_none'})","frozenset({'math_cat_χαμηλό', 'gender_male', 'lunch_standard'})",0.243,0.062,0.051,0.20987654320987653,3.38510553564317,1.0,0.035933999999999994,1.18715625,0.9307638511150828,0.20078740157480313,0.15765089894443127,0.5162285941855834
"frozenset({'math_cat_χαμηλό', 'gender_male', 'test preparation course_none'})",frozenset({'reading_cat_χαμηλό'}),0.099,0.275,0.092,0.9292929292929293,3.3792470156106518,1.0,0.064775,10.253571428571426,0.7814384982869275,0.326241134751773,0.9024730059212818,0.6319191919191919
frozenset({'reading_cat_χαμηλό'}),"frozenset({'math_cat_χαμηλό', 'gender_male', 'test preparation course_none'})",0.275,0.099,0.092,0.33454545454545453,3.3792470156106518,1.0,0.064775,1.3539617486338797,0.9711394302848576,0.326241134751773,0.26142669760871756,0.6319191919191919
"frozenset({'writing_cat_υψηλό', 'reading_cat_υψηλό'})",frozenset({'math_cat_υψηλό'}),0.185,0.176,0.11,0.5945945945945946,3.378378378378379,1.0,0.07744000000000001,2.0325333333333337,0.8638036809815952,0.43824701195219123,0.5080031487798479,0.6097972972972974
frozenset({'math_cat_υψηλό'}),"frozenset({'writing_cat_υψηλό', 'reading_cat_υψηλό'})",0.176,0.185,0.11,0.625,3.3783783783783785,1.0,0.07744000000000001,2.1733333333333333,0.8543689320388349,0.43824701195219123,0.5398773006134969,0.6097972972972974
"frozenset({'test preparation course_none', 'reading_cat_χαμηλό', 'lunch_standard'})","frozenset({'writing_cat_χαμηλό', 'gender_male'})",0.109,0.201,0.074,0.6788990825688073,3.377607375964215,1.0,0.052091,2.488314285714285,0.7900476233809568,0.3135593220338983,0.5981215050923746,0.5235291432744533
"frozenset({'writing_cat_χαμηλό', 'gender_male'})","frozenset({'test preparation course_none', 'reading_cat_χαμηλό', 'lunch_standard'})",0.201,0.109,0.074,0.36815920398009944,3.377607375964215,1.0,0.052091,1.4101653543307084,0.8810168115549843,0.3135593220338983,0.2908633041303024,0.5235291432744533
"frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced'})","frozenset({'math_cat_χαμηλό', 'test preparation course_none', 'reading_cat_χαμηλό'})",0.156,0.169,0.089,0.5705128205128205,3.3758155059930206,1.0,0.062636,1.934865671641791,0.8338569678896641,0.37711864406779655,0.48316825573143257,0.5485700197238659
"frozenset({'math_cat_χαμηλό', 'test preparation course_none', 'reading_cat_χαμηλό'})","frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced'})",0.169,0.156,0.089,0.5266272189349112,3.37581550599302,1.0,0.062636,1.7829499999999996,0.8469016617314998,0.37711864406779655,0.4391317759892312,0.5485700197238659
"frozenset({'math_cat_χαμηλό', 'reading_cat_χαμηλό'})","frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced', 'test preparation course_none'})",0.218,0.121,0.089,0.40825688073394495,3.3740238077185536,1.0,0.062622,1.4854418604651163,0.8997672346906521,0.35600000000000004,0.3267996367849203,0.5718970354082948
"frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced', 'test preparation course_none'})","frozenset({'math_cat_χαμηλό', 'reading_cat_χαμηλό'})",0.121,0.218,0.089,0.7355371900826446,3.374023807718553,1.0,0.062622,2.9569375,0.8004755148214903,0.35600000000000004,0.6618122635327937,0.5718970354082948
frozenset({'reading_cat_χαμηλό'}),"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced', 'gender_male', 'writing_cat_χαμηλό'})",0.275,0.069,0.064,0.23272727272727273,3.372859025032938,1.0,0.045024999999999996,1.2133886255924171,0.9703663793103448,0.22857142857142856,0.17586173225270968,0.5801317523056653
"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced', 'gender_male', 'writing_cat_χαμηλό'})",frozenset({'reading_cat_χαμηλό'}),0.069,0.275,0.064,0.9275362318840579,3.3728590250329376,1.0,0.045024999999999996,10.004999999999987,0.7556558807733619,0.22857142857142856,0.9000499750124936,0.5801317523056653
"frozenset({'writing_cat_χαμηλό', 'gender_male'})","frozenset({'reading_cat_χαμηλό', 'lunch_standard'})",0.201,0.136,0.092,0.4577114427860696,3.365525314603453,1.0,0.064664,1.593247706422018,0.8796865647276488,0.3755102040816326,0.3723512069283213,0.5670910155106819
"frozenset({'reading_cat_χαμηλό', 'lunch_standard'})","frozenset({'writing_cat_χαμηλό', 'gender_male'})",0.136,0.201,0.092,0.676470588235294,3.365525314603453,1.0,0.064664,2.4696363636363627,0.8135064412238325,0.3755102040816326,0.5950820879039976,0.5670910155106819
"frozenset({'race/ethnicity_group C', 'reading_cat_χαμηλό'})","frozenset({'writing_cat_χαμηλό', 'math_cat_χαμηλό'})",0.083,0.23,0.064,0.7710843373493975,3.3525405971712936,1.0,0.04491,3.3636842105263147,0.765233097055616,0.25702811244979923,0.7027069316225942,0.5246726034573075
"frozenset({'writing_cat_χαμηλό', 'math_cat_χαμηλό'})","frozenset({'race/ethnicity_group C', 'reading_cat_χαμηλό'})",0.23,0.083,0.064,0.2782608695652174,3.3525405971712936,1.0,0.04491,1.2705421686746987,0.9113230519480519,0.25702811244979923,0.2129344270067801,0.5246726034573075
"frozenset({'reading_cat_μέτριο', 'gender_female'})","frozenset({'math_cat_χαμηλό', 'writing_cat_μέτριο', 'test preparation course_none'})",0.261,0.063,0.055,0.210727969348659,3.3448884023596666,1.0,0.038557,1.1871699029126215,0.9486283675728872,0.20446096654275092,0.15766058628458804,0.541871921182266
"frozenset({'math_cat_χαμηλό', 'writing_cat_μέτριο', 'test preparation course_none'})","frozenset({'reading_cat_μέτριο', 'gender_female'})",0.063,0.261,0.055,0.873015873015873,3.3448884023596666,1.0,0.038557,5.819625,0.7481711458232269,0.20446096654275092,0.8281676224842666,0.541871921182266
frozenset({'reading_cat_χαμηλό'}),"frozenset({'math_cat_χαμηλό', 'gender_male'})",0.275,0.134,0.123,0.4472727272727272,3.3378561736770687,1.0,0.08614999999999999,1.5667763157894736,0.9660779366414354,0.43006993006993,0.36174679823640554,0.6825915875169606
"frozenset({'math_cat_χαμηλό', 'gender_male'})",frozenset({'reading_cat_χαμηλό'}),0.134,0.275,0.123,0.917910447761194,3.3378561736770687,1.0,0.08614999999999999,8.83181818181818,0.8087834919919636,0.43006993006993,0.8867730313947504,0.6825915875169606
"frozenset({'math_cat_χαμηλό', 'gender_male', 'reading_cat_χαμηλό'})","frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced'})",0.123,0.156,0.064,0.5203252032520326,3.3354179695643116,1.0,0.044812000000000005,1.7595254237288136,0.7983893956670468,0.29767441860465116,0.4316649327630718,0.4652908067542214
"frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced'})","frozenset({'math_cat_χαμηλό', 'gender_male', 'reading_cat_χαμηλό'})",0.156,0.123,0.064,0.41025641025641024,3.335417969564311,1.0,0.044812000000000005,1.487086956521739,0.8296060426540285,0.29767441860465116,0.32754436745314736,0.4652908067542214
"frozenset({'test preparation course_completed', 'writing_cat_υψηλό', 'reading_cat_υψηλό'})",frozenset({'math_cat_υψηλό'}),0.104,0.176,0.061,0.5865384615384616,3.3326048951048954,1.0,0.042696,1.9929302325581397,0.7811768149882903,0.27853881278538817,0.49822628827483195,0.4665646853146853
frozenset({'math_cat_υψηλό'}),"frozenset({'test preparation course_completed', 'writing_cat_υψηλό', 'reading_cat_υψηλό'})",0.176,0.104,0.061,0.3465909090909091,3.3326048951048954,1.0,0.042696,1.3712695652173914,0.849434983288238,0.27853881278538817,0.27074878246753253,0.4665646853146853
frozenset({'writing_cat_χαμηλό'}),"frozenset({'math_cat_χαμηλό', 'gender_male', 'lunch_standard'})",0.301,0.062,0.062,0.20598006644518274,3.322259136212625,1.0,0.043338,1.1813305439330544,1.0,0.20598006644518274,0.15349687254283875,0.6029900332225914
frozenset({'writing_cat_χαμηλό'}),"frozenset({'math_cat_χαμηλό', 'gender_male', 'reading_cat_χαμηλό', 'lunch_standard'})",0.301,0.059,0.059,0.19601328903654486,3.322259136212625,1.0,0.041241,1.170417355371901,1.0,0.19601328903654486,0.14560392033639197,0.5980066445182725
"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced', 'gender_male', 'reading_cat_χαμηλό'})",frozenset({'writing_cat_χαμηλό'}),0.064,0.301,0.064,1.0,3.3222591362126246,1.0,0.044736,inf,0.7467948717948718,0.21262458471760798,1.0,0.606312292358804
"frozenset({'math_cat_χαμηλό', 'gender_male', 'test preparation course_none', 'lunch_standard'})",frozenset({'writing_cat_χαμηλό'}),0.051,0.301,0.051,1.0,3.3222591362126246,1.0,0.035649,inf,0.7365648050579557,0.16943521594684385,1.0,0.584717607973422
frozenset({'writing_cat_χαμηλό'}),"frozenset({'math_cat_χαμηλό', 'gender_male', 'reading_cat_χαμηλό'})",0.301,0.123,0.123,0.40863787375415284,3.3222591362126246,1.0,0.085977,1.4830168539325843,0.9999999999999999,0.40863787375415284,0.32569882982229514,0.7043189368770764
frozenset({'writing_cat_χαμηλό'}),"frozenset({'math_cat_χαμηλό', 'gender_male', 'test preparation course_none', 'reading_cat_χαμηλό'})",0.301,0.092,0.092,0.30564784053156147,3.3222591362126246,1.0,0.064308,1.3076937799043062,1.0,0.3056478405315614,0.2352949785589884,0.6528239202657807
frozenset({'writing_cat_χαμηλό'}),"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced', 'gender_male', 'reading_cat_χαμηλό'})",0.301,0.064,0.064,0.21262458471760798,3.3222591362126246,1.0,0.044736,1.188759493670886,0.9999999999999999,0.21262458471760798,0.1587869494846239,0.606312292358804
"frozenset({'math_cat_χαμηλό', 'gender_male', 'reading_cat_χαμηλό'})",frozenset({'writing_cat_χαμηλό'}),0.123,0.301,0.123,1.0,3.3222591362126246,1.0,0.085977,inf,0.7970353477765109,0.40863787375415284,1.0,0.7043189368770764
frozenset({'writing_cat_χαμηλό'}),"frozenset({'math_cat_χαμηλό', 'gender_male', 'test preparation course_none', 'lunch_standard'})",0.301,0.051,0.051,0.16943521594684385,3.3222591362126246,1.0,0.035649,1.142596,1.0,0.16943521594684385,0.12480001680383969,0.584717607973422
"frozenset({'math_cat_χαμηλό', 'gender_male', 'lunch_standard'})",frozenset({'writing_cat_χαμηλό'}),0.062,0.301,0.062,1.0,3.3222591362126246,1.0,0.043338,inf,0.7452025586353945,0.20598006644518274,1.0,0.6029900332225914
"frozenset({'math_cat_χαμηλό', 'gender_male', 'reading_cat_χαμηλό', 'lunch_standard'})",frozenset({'writing_cat_χαμηλό'}),0.059,0.301,0.059,1.0,3.3222591362126246,1.0,0.041241,inf,0.742826780021254,0.19601328903654486,1.0,0.5980066445182725
"frozenset({'math_cat_χαμηλό', 'gender_male', 'test preparation course_none', 'reading_cat_χαμηλό'})",frozenset({'writing_cat_χαμηλό'}),0.092,0.301,0.092,1.0,3.3222591362126246,1.0,0.064308,inf,0.7698237885462555,0.3056478405315614,1.0,0.6528239202657807
"frozenset({'test preparation course_none', 'reading_cat_χαμηλό'})","frozenset({'writing_cat_χαμηλό', 'gender_female'})",0.214,0.1,0.071,0.3317757009345794,3.317757009345794,1.0,0.04959999999999999,1.3468531468531468,0.888793319714726,0.29218106995884774,0.2575285565939771,0.5208878504672896
"frozenset({'writing_cat_χαμηλό', 'gender_female'})","frozenset({'test preparation course_none', 'reading_cat_χαμηλό'})",0.1,0.214,0.071,0.7099999999999999,3.3177570093457938,1.0,0.04959999999999999,2.7103448275862054,0.7762128325508606,0.29218106995884774,0.6310432569974553,0.5208878504672896
"frozenset({'math_cat_χαμηλό', 'gender_male', 'writing_cat_χαμηλό'})","frozenset({'reading_cat_χαμηλό', 'lunch_standard'})",0.131,0.136,0.059,0.45038167938931295,3.3116299955096538,1.0,0.041184,1.5719999999999998,0.8032611027676464,0.2836538461538461,0.3638676844783715,0.44210260440053883
"frozenset({'reading_cat_χαμηλό', 'lunch_standard'})","frozenset({'math_cat_χαμηλό', 'gender_male', 'writing_cat_χαμηλό'})",0.136,0.131,0.059,0.43382352941176466,3.3116299955096538,1.0,0.041184,1.534857142857143,0.8079096045197741,0.2836538461538461,0.34847356664184653,0.44210260440053883
"frozenset({'math_cat_χαμηλό', 'gender_male', 'reading_cat_χαμηλό'})","frozenset({'writing_cat_χαμηλό', 'lunch_standard'})",0.123,0.145,0.059,0.4796747967479675,3.308102046537707,1.0,0.04116499999999999,1.643203125,0.795566550064743,0.2822966507177033,0.3914325108163363,0.4432856742360527
"frozenset({'writing_cat_χαμηλό', 'lunch_standard'})","frozenset({'math_cat_χαμηλό', 'gender_male', 'reading_cat_χαμηλό'})",0.145,0.123,0.059,0.4068965517241379,3.3081020465377065,1.0,0.04116499999999999,1.4786627906976741,0.816037268312023,0.2822966507177033,0.3237132858884127,0.4432856742360527
"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced'})","frozenset({'gender_female', 'reading_cat_χαμηλό'})",0.182,0.098,0.059,0.3241758241758242,3.3079165732226956,1.0,0.04116399999999999,1.3346666666666667,0.8529277692594587,0.26696832579185514,0.25074925074925075,0.4631083202511773
"frozenset({'gender_female', 'reading_cat_χαμηλό'})","frozenset({'math_cat_χαμηλό', 'lunch_free/reduced'})",0.098,0.182,0.059,0.6020408163265305,3.307916573222695,1.0,0.04116399999999999,2.055487179487179,0.7734976887519259,0.26696832579185514,0.5134973304725311,0.4631083202511773
"frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced', 'math_cat_χαμηλό'})",frozenset({'reading_cat_χαμηλό'}),0.132,0.275,0.12,0.9090909090909091,3.305785123966942,1.0,0.0837,7.974999999999997,0.8035714285714286,0.41811846689895465,0.8746081504702193,0.6727272727272726
frozenset({'reading_cat_χαμηλό'}),"frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced', 'math_cat_χαμηλό'})",0.275,0.132,0.12,0.4363636363636363,3.3057851239669414,1.0,0.0837,1.5399999999999998,0.9620689655172414,0.41811846689895465,0.35064935064935054,0.6727272727272726
"frozenset({'math_cat_χαμηλό', 'gender_female', 'writing_cat_χαμηλό'})","frozenset({'test preparation course_none', 'reading_cat_χαμηλό'})",0.099,0.214,0.07,0.7070707070707071,3.3040687246294724,1.0,0.04881400000000001,2.683241379310345,0.7739654352306962,0.2880658436213992,0.627316421209551,0.5170867554045124
"frozenset({'test preparation course_none', 'reading_cat_χαμηλό'})","frozenset({'math_cat_χαμηλό', 'gender_female', 'writing_cat_χαμηλό'})",0.214,0.099,0.07,0.3271028037383178,3.3040687246294724,1.0,0.04881400000000001,1.3389861111111112,0.8872046528535079,0.2880658436213992,0.25316626386050806,0.5170867554045124
frozenset({'reading_cat_χαμηλό'}),"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced', 'test preparation course_none', 'writing_cat_χαμηλό'})",0.275,0.098,0.089,0.3236363636363636,3.302411873840445,1.0,0.062049999999999994,1.3336021505376343,0.9616427741185586,0.3133802816901408,0.2501511791977424,0.615899814471243
"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced', 'test preparation course_none', 'writing_cat_χαμηλό'})",frozenset({'reading_cat_χαμηλό'}),0.098,0.275,0.089,0.9081632653061223,3.3024118738404447,1.0,0.062049999999999994,7.894444444444435,0.7729390368469568,0.3133802816901408,0.873328641801548,0.615899814471243
"frozenset({'math_cat_χαμηλό', 'gender_male'})","frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced'})",0.134,0.156,0.069,0.5149253731343284,3.3008036739380024,1.0,0.048096,1.7399384615384617,0.8049000903705191,0.31221719457013575,0.4252670297800099,0.47861653272101035
"frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced'})","frozenset({'math_cat_χαμηλό', 'gender_male'})",0.156,0.134,0.069,0.44230769230769235,3.3008036739380024,1.0,0.048096,1.5528275862068965,0.8258808984133525,0.31221719457013575,0.35601350151003736,0.47861653272101035
"frozenset({'writing_cat_χαμηλό', 'math_cat_χαμηλό'})","frozenset({'test preparation course_none', 'reading_cat_χαμηλό'})",0.23,0.214,0.162,0.7043478260869566,3.2913449817147504,1.0,0.11278,2.6585294117647065,0.9041205707872375,0.5744680851063829,0.6238521960393849,0.7306785859406746
"frozenset({'test preparation course_none', 'reading_cat_χαμηλό'})","frozenset({'writing_cat_χαμηλό', 'math_cat_χαμηλό'})",0.214,0.23,0.162,0.7570093457943926,3.2913449817147504,1.0,0.11278,3.168846153846155,0.8857160807966576,0.5744680851063829,0.6844277218108995,0.7306785859406746
"frozenset({'math_cat_χαμηλό', 'gender_male', 'test preparation course_none'})",frozenset({'writing_cat_χαμηλό'}),0.099,0.301,0.098,0.98989898989899,3.288700963119568,1.0,0.06820100000000001,69.20100000000032,0.7723957507531316,0.3245033112582781,0.9855493417725176,0.6577401926239136
frozenset({'writing_cat_χαμηλό'}),"frozenset({'math_cat_χαμηλό', 'gender_male', 'test preparation course_none'})",0.301,0.099,0.098,0.32558139534883723,3.288700963119568,1.0,0.06820100000000001,1.3359655172413794,0.995605967708972,0.3245033112582781,0.25147768629171724,0.6577401926239136
"frozenset({'writing_cat_υψηλό', 'gender_female', 'reading_cat_υψηλό', 'lunch_standard'})",frozenset({'math_cat_υψηλό'}),0.102,0.176,0.059,0.5784313725490197,3.2865418894830665,1.0,0.041048,1.954604651162791,0.7747536899324299,0.26940639269406397,0.48838758804492677,0.4568293226381462
frozenset({'math_cat_υψηλό'}),"frozenset({'writing_cat_υψηλό', 'gender_female', 'reading_cat_υψηλό', 'lunch_standard'})",0.176,0.102,0.059,0.3352272727272727,3.286541889483066,1.0,0.041048,1.350837606837607,0.8443310844166529,0.26940639269406397,0.25971856651143954,0.4568293226381462
"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced', 'test preparation course_none', 'reading_cat_χαμηλό'})",frozenset({'writing_cat_χαμηλό'}),0.09,0.301,0.089,0.9888888888888889,3.2853451458102625,1.0,0.06190999999999999,62.91000000000023,0.7644153599209779,0.29470198675496684,0.9841042759497696,0.6422849760059063
frozenset({'writing_cat_χαμηλό'}),"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced', 'test preparation course_none', 'reading_cat_χαμηλό'})",0.301,0.09,0.089,0.2956810631229236,3.285345145810262,1.0,0.06190999999999999,1.2920283018867926,0.9951616273649353,0.29470198675496684,0.226023146288927,0.6422849760059063
"frozenset({'reading_cat_υψηλό', 'lunch_standard'})","frozenset({'math_cat_υψηλό', 'test preparation course_none'})",0.182,0.092,0.055,0.30219780219780223,3.284758719541329,1.0,0.038256,1.301228346456693,0.8503222938430761,0.2511415525114155,0.23149537686982624,0.450011944577162
"frozenset({'math_cat_υψηλό', 'test preparation course_none'})","frozenset({'reading_cat_υψηλό', 'lunch_standard'})",0.092,0.182,0.055,0.5978260869565217,3.2847587195413284,1.0,0.038256,2.033945945945946,0.7660392470965157,0.2511415525114155,0.5083448495800998,0.450011944577162
"frozenset({'test preparation course_none', 'reading_cat_χαμηλό'})","frozenset({'math_cat_χαμηλό', 'gender_male', 'writing_cat_χαμηλό'})",0.214,0.131,0.092,0.42990654205607476,3.281729328672326,1.0,0.063966,1.5243114754098361,0.884583471622967,0.36363636363636365,0.3439661013303507,0.5660983091959763
"frozenset({'math_cat_χαμηλό', 'gender_male', 'writing_cat_χαμηλό'})","frozenset({'test preparation course_none', 'reading_cat_χαμηλό'})",0.131,0.214,0.092,0.7022900763358778,3.281729328672326,1.0,0.063966,2.640153846153846,0.8000950617901637,0.36363636363636365,0.6212341938115494,0.5660983091959763
"frozenset({'test preparation course_completed', 'reading_cat_υψηλό'})",frozenset({'math_cat_υψηλό'}),0.125,0.176,0.072,0.576,3.2727272727272725,1.0,0.049999999999999996,1.9433962264150944,0.7936507936507936,0.314410480349345,0.4854368932038834,0.4925454545454545
frozenset({'math_cat_υψηλό'}),"frozenset({'test preparation course_completed', 'reading_cat_υψηλό'})",0.176,0.125,0.072,0.40909090909090906,3.2727272727272725,1.0,0.049999999999999996,1.4807692307692306,0.8427723840345199,0.314410480349345,0.3246753246753246,0.4925454545454545
"frozenset({'reading_cat_μέτριο', 'gender_female'})","frozenset({'math_cat_χαμηλό', 'writing_cat_μέτριο'})",0.261,0.109,0.093,0.3563218390804598,3.2690076979858693,1.0,0.064551,1.384232142857143,0.9392378541184687,0.335740072202166,0.27757782163912437,0.6047664241273858
"frozenset({'math_cat_χαμηλό', 'writing_cat_μέτριο'})","frozenset({'reading_cat_μέτριο', 'gender_female'})",0.109,0.261,0.093,0.8532110091743119,3.2690076979858693,1.0,0.064551,5.034437499999999,0.7790087252452843,0.335740072202166,0.8013680773671339,0.6047664241273858
"frozenset({'math_cat_υψηλό', 'lunch_standard'})","frozenset({'test preparation course_completed', 'writing_cat_υψηλό', 'reading_cat_υψηλό'})",0.156,0.104,0.053,0.33974358974358976,3.2667652859960556,1.0,0.036776,1.3570485436893205,0.8221407493516947,0.25603864734299514,0.263106684981685,0.42467948717948717
"frozenset({'test preparation course_completed', 'writing_cat_υψηλό', 'reading_cat_υψηλό'})","frozenset({'math_cat_υψηλό', 'lunch_standard'})",0.104,0.156,0.053,0.5096153846153846,3.266765285996055,1.0,0.036776,1.721098039215686,0.7744272237196765,0.25603864734299514,0.4189755741888443,0.42467948717948717
"frozenset({'lunch_free/reduced', 'gender_male', 'test preparation course_none', 'reading_cat_χαμηλό'})",frozenset({'writing_cat_χαμηλό'}),0.057,0.301,0.056,0.9824561403508771,3.2639738882088944,1.0,0.038843,39.84299999999988,0.7355514316012726,0.18543046357615894,0.9749014883417413,0.584251325989392
frozenset({'writing_cat_χαμηλό'}),"frozenset({'lunch_free/reduced', 'gender_male', 'test preparation course_none', 'reading_cat_χαμηλό'})",0.301,0.057,0.056,0.18604651162790697,3.263973888208894,1.0,0.038843,1.158542857142857,0.992310443490701,0.18543046357615894,0.13684677797232978,0.584251325989392
"frozenset({'writing_cat_χαμηλό', 'gender_female', 'test preparation course_none'})","frozenset({'math_cat_χαμηλό', 'lunch_free/reduced'})",0.086,0.182,0.051,0.5930232558139535,3.258369537439305,1.0,0.035348,2.0099428571428577,0.7583129531900288,0.23502304147465433,0.5024734178654688,0.4366215180168669
"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced'})","frozenset({'writing_cat_χαμηλό', 'gender_female', 'test preparation course_none'})",0.182,0.086,0.051,0.2802197802197802,3.258369537439305,1.0,0.035348,1.2698320610687024,0.8473081164005944,0.23502304147465433,0.21249428908072235,0.4366215180168669
"frozenset({'race/ethnicity_group C', 'gender_male', 'reading_cat_χαμηλό'})",frozenset({'writing_cat_χαμηλό'}),0.051,0.301,0.05,0.9803921568627452,3.257116800208456,1.0,0.034649000000000006,35.64900000000013,0.7302212855637514,0.16556291390728478,0.9719487222642993,0.5732525568366882
frozenset({'writing_cat_χαμηλό'}),"frozenset({'race/ethnicity_group C', 'gender_male', 'reading_cat_χαμηλό'})",0.301,0.051,0.05,0.16611295681063123,3.257116800208456,1.0,0.034649000000000006,1.138043824701195,0.9913876967095853,0.16556291390728478,0.12129921687105506,0.5732525568366882
frozenset({'reading_cat_χαμηλό'}),"frozenset({'writing_cat_χαμηλό', 'math_cat_χαμηλό'})",0.275,0.23,0.206,0.749090909090909,3.2569169960474302,1.0,0.14275,3.068840579710144,0.9558085035152327,0.688963210702341,0.6741440377804013,0.8223715415019762
"frozenset({'writing_cat_χαμηλό', 'math_cat_χαμηλό'})",frozenset({'reading_cat_χαμηλό'}),0.23,0.275,0.206,0.8956521739130434,3.2569169960474302,1.0,0.14275,6.9479166666666625,0.8999495649981087,0.688963210702341,0.8560719640179909,0.8223715415019762
frozenset({'writing_cat_χαμηλό'}),"frozenset({'math_cat_χαμηλό', 'gender_male'})",0.301,0.134,0.131,0.43521594684385384,3.247880200327267,1.0,0.090666,1.533329411764706,0.9901385840186088,0.430921052631579,0.34782441898828387,0.7064139435711806
"frozenset({'math_cat_χαμηλό', 'gender_male'})",frozenset({'writing_cat_χαμηλό'}),0.134,0.301,0.131,0.9776119402985074,3.247880200327267,1.0,0.090666,31.221999999999912,0.7991996192020873,0.430921052631579,0.9679713022868488,0.7064139435711806
frozenset({'math_cat_υψηλό'}),"frozenset({'writing_cat_υψηλό', 'test preparation course_none'})",0.176,0.091,0.052,0.29545454545454547,3.246753246753247,1.0,0.035984,1.2901935483870968,0.8398058252427184,0.24186046511627904,0.22492249224922495,0.4334415584415584
"frozenset({'math_cat_υψηλό', 'reading_cat_υψηλό', 'lunch_standard'})","frozenset({'writing_cat_υψηλό', 'gender_female'})",0.118,0.154,0.059,0.5,3.2467532467532467,1.0,0.040828,1.692,0.7845804988662132,0.2769953051643192,0.408983451536643,0.44155844155844154
"frozenset({'writing_cat_υψηλό', 'test preparation course_none'})",frozenset({'math_cat_υψηλό'}),0.091,0.176,0.052,0.5714285714285714,3.2467532467532467,1.0,0.035984,1.9226666666666667,0.7612761276127613,0.24186046511627904,0.47988904299583907,0.4334415584415584
"frozenset({'writing_cat_υψηλό', 'gender_female'})","frozenset({'math_cat_υψηλό', 'reading_cat_υψηλό', 'lunch_standard'})",0.154,0.118,0.059,0.3831168831168831,3.2467532467532467,1.0,0.040828,1.4297684210526316,0.8179669030732862,0.2769953051643192,0.3005860352799128,0.44155844155844154
frozenset({'writing_cat_χαμηλό'}),"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced', 'reading_cat_χαμηλό'})",0.301,0.123,0.12,0.39867109634551495,3.241228425573292,1.0,0.082977,1.4584364640883976,0.9892346208869812,0.39473684210526316,0.31433420335862594,0.6871404262215379
"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced', 'reading_cat_χαμηλό'})",frozenset({'writing_cat_χαμηλό'}),0.123,0.301,0.12,0.975609756097561,3.241228425573292,1.0,0.082977,28.658999999999978,0.7884549600912201,0.39473684210526316,0.9651069472068111,0.6871404262215379
"frozenset({'math_cat_χαμηλό', 'test preparation course_none'})","frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced', 'gender_female'})",0.246,0.064,0.051,0.2073170731707317,3.239329268292683,1.0,0.035255999999999996,1.1807999999999998,0.9168356997971603,0.1969111969111969,0.15311653116531165,0.5020960365853658
"frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced', 'gender_female'})","frozenset({'math_cat_χαμηλό', 'test preparation course_none'})",0.064,0.246,0.051,0.7968749999999999,3.2393292682926824,1.0,0.035255999999999996,3.711999999999998,0.738562091503268,0.1969111969111969,0.7306034482758619,0.5020960365853658
frozenset({'reading_cat_χαμηλό'}),"frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced', 'gender_female'})",0.275,0.064,0.057,0.20727272727272728,3.2386363636363638,1.0,0.039400000000000004,1.1807339449541283,0.9534180278281913,0.20212765957446807,0.15306915306915309,0.5489488636363636
"frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced', 'gender_female'})",frozenset({'reading_cat_χαμηλό'}),0.064,0.275,0.057,0.890625,3.2386363636363633,1.0,0.039400000000000004,6.628571428571428,0.7384915279652123,0.20212765957446807,0.8491379310344828,0.5489488636363636
"frozenset({'reading_cat_χαμηλό', 'lunch_standard'})","frozenset({'math_cat_χαμηλό', 'gender_male'})",0.136,0.134,0.059,0.43382352941176466,3.2374890254609303,1.0,0.04077599999999999,1.5295584415584416,0.7999058380414312,0.27962085308056867,0.34621654666485524,0.43706101843722556
"frozenset({'math_cat_χαμηλό', 'gender_male'})","frozenset({'reading_cat_χαμηλό', 'lunch_standard'})",0.134,0.136,0.059,0.4402985074626865,3.23748902546093,1.0,0.04077599999999999,1.5436799999999997,0.798058480447802,0.27962085308056867,0.3521973466003316,0.43706101843722556
"frozenset({'lunch_free/reduced', 'gender_male', 'reading_cat_χαμηλό'})",frozenset({'writing_cat_χαμηλό'}),0.078,0.301,0.076,0.9743589743589743,3.237073004514865,1.0,0.052522,27.26099999999998,0.7495433268637973,0.2508250825082508,0.9633175598840833,0.6134253343555669
frozenset({'writing_cat_χαμηλό'}),"frozenset({'lunch_free/reduced', 'gender_male', 'reading_cat_χαμηλό'})",0.301,0.078,0.076,0.25249169435215946,3.237073004514865,1.0,0.052522,1.2334311111111111,0.9886680219862961,0.2508250825082508,0.18925346459019463,0.6134253343555669
frozenset({'reading_cat_χαμηλό'}),"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced', 'gender_male'})",0.275,0.072,0.064,0.23272727272727273,3.2323232323232327,1.0,0.0442,1.209478672985782,0.9525862068965518,0.2261484098939929,0.1731974921630094,0.5608080808080809
"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced', 'gender_male'})",frozenset({'reading_cat_χαμηλό'}),0.072,0.275,0.064,0.888888888888889,3.2323232323232323,1.0,0.0442,6.525000000000003,0.744207974137931,0.2261484098939929,0.8467432950191571,0.5608080808080809
"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced', 'gender_female', 'writing_cat_χαμηλό'})",frozenset({'reading_cat_χαμηλό'}),0.063,0.275,0.056,0.888888888888889,3.2323232323232323,1.0,0.038675,6.525000000000003,0.7370597652081109,0.19858156028368792,0.8467432950191571,0.5462626262626262
frozenset({'reading_cat_χαμηλό'}),"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced', 'gender_female', 'writing_cat_χαμηλό'})",0.275,0.063,0.056,0.20363636363636362,3.232323232323232,1.0,0.038675,1.1765981735159818,0.9525862068965518,0.19858156028368792,0.15009217036965167,0.5462626262626262
"frozenset({'math_cat_υψηλό', 'lunch_standard'})","frozenset({'test preparation course_completed', 'reading_cat_υψηλό'})",0.156,0.125,0.063,0.40384615384615385,3.230769230769231,1.0,0.0435,1.467741935483871,0.8180997517490408,0.2889908256880734,0.3186813186813187,0.45392307692307693
"frozenset({'test preparation course_completed', 'reading_cat_υψηλό'})","frozenset({'math_cat_υψηλό', 'lunch_standard'})",0.125,0.156,0.063,0.504,3.230769230769231,1.0,0.0435,1.7016129032258065,0.7891156462585034,0.2889908256880734,0.41232227488151657,0.45392307692307693
"frozenset({'lunch_free/reduced', 'test preparation course_none', 'reading_cat_χαμηλό'})",frozenset({'writing_cat_χαμηλό'}),0.105,0.301,0.102,0.9714285714285714,3.22733744660655,1.0,0.07039499999999999,24.464999999999993,0.7711140322050607,0.3355263157894737,0.9591252810136929,0.6551495016611295
frozenset({'writing_cat_χαμηλό'}),"frozenset({'lunch_free/reduced', 'test preparation course_none', 'reading_cat_χαμηλό'})",0.301,0.105,0.102,0.3388704318936877,3.22733744660655,1.0,0.07039499999999999,1.3537437185929648,0.9873348481023309,0.3355263157894737,0.26130774513261196,0.6551495016611295
"frozenset({'writing_cat_υψηλό', 'gender_female'})","frozenset({'math_cat_υψηλό', 'reading_cat_υψηλό'})",0.154,0.131,0.065,0.4220779220779221,3.2219688708238325,1.0,0.044826000000000005,1.5036629213483146,0.8151663938897983,0.2954545454545454,0.3349573326558367,0.45913056409239617
"frozenset({'math_cat_υψηλό', 'reading_cat_υψηλό'})","frozenset({'writing_cat_υψηλό', 'gender_female'})",0.131,0.154,0.065,0.4961832061068702,3.2219688708238325,1.0,0.044826000000000005,1.6791818181818183,0.7935912189076747,0.2954545454545454,0.4044718748308159,0.45913056409239617
frozenset({'reading_cat_χαμηλό'}),"frozenset({'writing_cat_χαμηλό', 'test preparation course_none', 'math_cat_χαμηλό'})",0.275,0.183,0.162,0.5890909090909091,3.219076005961252,1.0,0.111675,1.9882743362831858,0.9508301404853129,0.5472972972972973,0.49705129631690226,0.7371684053651267
"frozenset({'writing_cat_χαμηλό', 'test preparation course_none', 'math_cat_χαμηλό'})",frozenset({'reading_cat_χαμηλό'}),0.183,0.275,0.162,0.8852459016393444,3.2190760059612518,1.0,0.111675,6.3178571428571475,0.843759916587334,0.5472972972972973,0.8417184850197853,0.7371684053651267
"frozenset({'math_cat_υψηλό', 'lunch_standard'})",frozenset({'reading_cat_υψηλό'}),0.156,0.235,0.118,0.7564102564102564,3.2187670485542825,1.0,0.08134,3.1405263157894736,0.8167322676520203,0.4322344322344322,0.681582034523211,0.6292689579923623
frozenset({'reading_cat_υψηλό'}),"frozenset({'math_cat_υψηλό', 'lunch_standard'})",0.235,0.156,0.118,0.502127659574468,3.218767048554282,1.0,0.08134,1.695213675213675,0.9010745541154314,0.4322344322344322,0.4101038620550569,0.6292689579923623
"frozenset({'math_cat_χαμηλό', 'parental level of education_high school', 'reading_cat_χαμηλό'})",frozenset({'writing_cat_χαμηλό'}),0.059,0.301,0.057,0.9661016949152543,3.2096401824427057,1.0,0.039241000000000005,20.620500000000057,0.7316031843689991,0.18811881188118812,0.9515045706942122,0.577735232839687
frozenset({'writing_cat_χαμηλό'}),"frozenset({'reading_cat_χαμηλό', 'race/ethnicity_group B'})",0.301,0.059,0.057,0.18936877076411962,3.2096401824427057,1.0,0.039241000000000005,1.1608237704918034,0.9848906959817283,0.18811881188118812,0.13854279571107292,0.577735232839687
frozenset({'writing_cat_χαμηλό'}),"frozenset({'math_cat_χαμηλό', 'parental level of education_high school', 'reading_cat_χαμηλό'})",0.301,0.059,0.057,0.18936877076411962,3.2096401824427057,1.0,0.039241000000000005,1.1608237704918034,0.9848906959817283,0.18811881188118812,0.13854279571107292,0.577735232839687
"frozenset({'reading_cat_χαμηλό', 'race/ethnicity_group B'})",frozenset({'writing_cat_χαμηλό'}),0.059,0.301,0.057,0.9661016949152543,3.2096401824427057,1.0,0.039241000000000005,20.620500000000057,0.7316031843689991,0.18811881188118812,0.9515045706942122,0.577735232839687
"frozenset({'math_cat_χαμηλό', 'gender_male'})","frozenset({'test preparation course_none', 'reading_cat_χαμηλό'})",0.134,0.214,0.092,0.6865671641791045,3.2082577765378715,1.0,0.06332399999999999,2.5077142857142856,0.7948087157345115,0.359375,0.6012304887774866,0.5582368531175896
"frozenset({'test preparation course_none', 'reading_cat_χαμηλό'})","frozenset({'math_cat_χαμηλό', 'gender_male'})",0.214,0.134,0.092,0.42990654205607476,3.208257776537871,1.0,0.06332399999999999,1.519049180327869,0.8757052771324261,0.359375,0.34169346657745353,0.5582368531175896
"frozenset({'parental level of education_some high school', 'reading_cat_χαμηλό'})",frozenset({'writing_cat_χαμηλό'}),0.058,0.301,0.056,0.9655172413793103,3.2076984763432237,1.0,0.038542,20.27099999999996,0.7306263269639066,0.18481848184818483,0.9506684426027329,0.5757818765036087
frozenset({'writing_cat_χαμηλό'}),"frozenset({'parental level of education_some high school', 'reading_cat_χαμηλό'})",0.301,0.058,0.056,0.18604651162790697,3.2076984763432237,1.0,0.038542,1.1573142857142857,0.9846208869814018,0.18481848184818483,0.1359304794351454,0.5757818765036087
"frozenset({'math_cat_υψηλό', 'lunch_standard'})","frozenset({'test preparation course_none', 'reading_cat_υψηλό'})",0.156,0.11,0.055,0.3525641025641026,3.2051282051282053,1.0,0.03784,1.3746534653465348,0.8151658767772513,0.2606635071090047,0.2725439354652838,0.4262820512820513
"frozenset({'test preparation course_none', 'reading_cat_υψηλό'})","frozenset({'math_cat_υψηλό', 'lunch_standard'})",0.11,0.156,0.055,0.5,3.2051282051282053,1.0,0.03784,1.688,0.7730337078651685,0.2606635071090047,0.4075829383886256,0.4262820512820513
frozenset({'writing_cat_χαμηλό'}),"frozenset({'gender_male', 'test preparation course_none', 'reading_cat_χαμηλό'})",0.301,0.135,0.13,0.43189368770764125,3.1992125015380832,1.0,0.089365,1.5226023391812866,0.9834378782876636,0.4248366013071896,0.34322969677183957,0.6974283253353021
"frozenset({'gender_male', 'test preparation course_none', 'reading_cat_χαμηλό'})",frozenset({'writing_cat_χαμηλό'}),0.135,0.301,0.13,0.9629629629629629,3.199212501538083,1.0,0.089365,18.872999999999976,0.7947087594486438,0.4248366013071896,0.9470142531658982,0.6974283253353021
"frozenset({'test preparation course_completed', 'writing_cat_χαμηλό'})",frozenset({'reading_cat_χαμηλό'}),0.058,0.275,0.051,0.8793103448275861,3.1974921630094038,1.0,0.03505,6.007142857142851,0.729569959618667,0.18085106382978722,0.8335315101070153,0.5323824451410657
frozenset({'reading_cat_χαμηλό'}),"frozenset({'test preparation course_completed', 'writing_cat_χαμηλό'})",0.275,0.058,0.051,0.18545454545454543,3.1974921630094038,1.0,0.03505,1.1564732142857141,0.9479377958079784,0.18085106382978722,0.13530206523837096,0.5323824451410657
"frozenset({'gender_female', 'reading_cat_χαμηλό'})","frozenset({'math_cat_χαμηλό', 'test preparation course_none'})",0.098,0.246,0.077,0.7857142857142857,3.1939605110336817,1.0,0.052891999999999995,3.5186666666666664,0.7615400120943357,0.28838951310861427,0.715801439939371,0.5493612078977932
"frozenset({'math_cat_χαμηλό', 'test preparation course_none'})","frozenset({'gender_female', 'reading_cat_χαμηλό'})",0.246,0.098,0.077,0.3130081300813008,3.1939605110336813,1.0,0.052891999999999995,1.3129704142011833,0.9110200144682903,0.28838951310861427,0.23836821516773923,0.5493612078977932
frozenset({'reading_cat_χαμηλό'}),"frozenset({'writing_cat_χαμηλό', 'math_cat_χαμηλό', 'lunch_standard'})",0.275,0.098,0.086,0.3127272727272727,3.191094619666048,1.0,0.05904999999999999,1.3124338624338625,0.9470729751403367,0.2996515679442508,0.2380568433783511,0.5951391465677179
"frozenset({'writing_cat_χαμηλό', 'math_cat_χαμηλό', 'lunch_standard'})",frozenset({'reading_cat_χαμηλό'}),0.098,0.275,0.086,0.8775510204081631,3.1910946196660475,1.0,0.05904999999999999,5.920833333333326,0.7612282782447274,0.2996515679442508,0.8311048557353974,0.5951391465677179
"frozenset({'math_cat_χαμηλό', 'reading_cat_χαμηλό'})","frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced', 'gender_male'})",0.218,0.092,0.064,0.29357798165137616,3.191065017949741,1.0,0.043944,1.2853506493506495,0.8780370843989769,0.2601626016260163,0.2220021824354363,0.4946150777822098
"frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced', 'gender_male'})","frozenset({'math_cat_χαμηλό', 'reading_cat_χαμηλό'})",0.092,0.218,0.064,0.6956521739130435,3.191065017949741,1.0,0.043944,2.5694285714285714,0.7561949339207047,0.2601626016260163,0.6108084065384187,0.4946150777822098
"frozenset({'math_cat_χαμηλό', 'gender_male'})","frozenset({'writing_cat_χαμηλό', 'lunch_standard'})",0.134,0.145,0.062,0.46268656716417905,3.19094184251158,1.0,0.04257,1.59125,0.7928555464501229,0.2857142857142857,0.37156323644933226,0.4451363870303654
"frozenset({'writing_cat_χαμηλό', 'lunch_standard'})","frozenset({'math_cat_χαμηλό', 'gender_male'})",0.145,0.134,0.062,0.42758620689655175,3.19094184251158,1.0,0.04257,1.5128915662650602,0.8030560271646858,0.2857142857142857,0.33901409572350083,0.4451363870303654
"frozenset({'test preparation course_none', 'reading_cat_χαμηλό'})","frozenset({'writing_cat_χαμηλό', 'lunch_standard'})",0.214,0.145,0.099,0.4626168224299066,3.1904608443441838,1.0,0.06797,1.5910434782608696,0.8734932017374765,0.38076923076923075,0.3714816636607095,0.5726877215597809
"frozenset({'writing_cat_χαμηλό', 'lunch_standard'})","frozenset({'test preparation course_none', 'reading_cat_χαμηλό'})",0.145,0.214,0.099,0.6827586206896552,3.1904608443441833,1.0,0.06797,2.477608695652174,0.8030007679130486,0.38076923076923075,0.5963850136000702,0.5726877215597809
"frozenset({'writing_cat_χαμηλό', 'test preparation course_none', 'race/ethnicity_group D'})",frozenset({'reading_cat_χαμηλό'}),0.057,0.275,0.05,0.8771929824561404,3.189792663476874,1.0,0.034325,5.903571428571431,0.7279957582184519,0.17730496453900707,0.8306110102843316,0.5295055821371611
frozenset({'reading_cat_χαμηλό'}),"frozenset({'writing_cat_χαμηλό', 'test preparation course_none', 'race/ethnicity_group D'})",0.275,0.057,0.05,0.18181818181818182,3.189792663476874,1.0,0.034325,1.1525555555555556,0.946896551724138,0.17730496453900707,0.13236286513062762,0.5295055821371611
frozenset({'writing_cat_χαμηλό'}),"frozenset({'math_cat_χαμηλό', 'test preparation course_none', 'reading_cat_χαμηλό'})",0.301,0.169,0.162,0.5382059800664453,3.184650769623936,1.0,0.11113100000000001,1.7995035971223023,0.9813931719034246,0.5259740259740261,0.44429119141569823,0.7483929308616251
"frozenset({'math_cat_χαμηλό', 'test preparation course_none', 'reading_cat_χαμηλό'})",frozenset({'writing_cat_χαμηλό'}),0.169,0.301,0.162,0.9585798816568047,3.184650769623936,1.0,0.11113100000000001,16.87585714285715,0.8255040038032417,0.5259740259740261,0.940743750581981,0.7483929308616251
frozenset({'writing_cat_χαμηλό'}),"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced', 'gender_male'})",0.301,0.072,0.069,0.22923588039867113,3.183831672203766,1.0,0.04732800000000001,1.2040000000000002,0.9812776015425764,0.22697368421052633,0.1694352159468439,0.5937846068660023
"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced', 'gender_male'})",frozenset({'writing_cat_χαμηλό'}),0.072,0.301,0.069,0.9583333333333335,3.1838316722037656,1.0,0.04732800000000001,16.77600000000006,0.7391304347826088,0.22697368421052633,0.9403910348116359,0.5937846068660023
"frozenset({'writing_cat_χαμηλό', 'race/ethnicity_group D'})",frozenset({'reading_cat_χαμηλό'}),0.064,0.275,0.056,0.875,3.1818181818181817,1.0,0.038400000000000004,5.8,0.7326007326007327,0.1978798586572438,0.8275862068965517,0.5393181818181818
frozenset({'reading_cat_χαμηλό'}),"frozenset({'writing_cat_χαμηλό', 'race/ethnicity_group D'})",0.275,0.064,0.056,0.20363636363636362,3.181818181818181,1.0,0.038400000000000004,1.1753424657534246,0.9458128078817736,0.1978798586572438,0.14918414918414916,0.5393181818181818
frozenset({'writing_cat_χαμηλό'}),"frozenset({'lunch_free/reduced', 'reading_cat_χαμηλό'})",0.301,0.139,0.133,0.44186046511627913,3.178852267023591,1.0,0.091161,1.5426250000000004,0.9805737519765078,0.43322475570032576,0.3517543148853416,0.6993474987451899
"frozenset({'lunch_free/reduced', 'reading_cat_χαμηλό'})",frozenset({'writing_cat_χαμηλό'}),0.139,0.301,0.133,0.9568345323741007,3.1788522670235904,1.0,0.091161,16.19349999999998,0.7960755547405098,0.43322475570032576,0.938246827430759,0.6993474987451899
"frozenset({'test preparation course_none', 'reading_cat_χαμηλό'})","frozenset({'writing_cat_χαμηλό', 'gender_male', 'lunch_standard'})",0.214,0.109,0.074,0.34579439252336447,3.172425619480408,1.0,0.050674,1.3619571428571426,0.8712261880200811,0.2971887550200803,0.26576250563789505,0.5123467375460858
"frozenset({'writing_cat_χαμηλό', 'gender_male', 'lunch_standard'})","frozenset({'test preparation course_none', 'reading_cat_χαμηλό'})",0.109,0.214,0.074,0.6788990825688073,3.172425619480408,1.0,0.050674,2.447828571428571,0.7685564352231019,0.2971887550200803,0.5914746597567523,0.5123467375460858
frozenset({'math_cat_υψηλό'}),frozenset({'reading_cat_υψηλό'}),0.176,0.235,0.131,0.7443181818181819,3.1673114119922636,1.0,0.08964000000000001,2.992000000000001,0.8304305936411472,0.4678571428571429,0.6657754010695188,0.6508824951644101
frozenset({'reading_cat_υψηλό'}),frozenset({'math_cat_υψηλό'}),0.235,0.176,0.131,0.5574468085106383,3.167311411992263,1.0,0.08964000000000001,1.8619230769230772,0.8944768747193534,0.4678571428571429,0.4629208841148523,0.6508824951644101
"frozenset({'math_cat_χαμηλό', 'reading_cat_χαμηλό', 'lunch_standard'})","frozenset({'writing_cat_χαμηλό', 'test preparation course_none'})",0.095,0.243,0.073,0.7684210526315789,3.162226554039419,1.0,0.049914999999999994,3.2688636363636356,0.7555437826383107,0.2754716981132076,0.6940832927761941,0.5344162876326619
"frozenset({'writing_cat_χαμηλό', 'test preparation course_none'})","frozenset({'math_cat_χαμηλό', 'reading_cat_χαμηλό', 'lunch_standard'})",0.243,0.095,0.073,0.3004115226337448,3.162226554039419,1.0,0.049914999999999994,1.2936176470588234,0.9032590796402525,0.2754716981132076,0.22697405815883404,0.5344162876326619
"frozenset({'gender_male', 'reading_cat_χαμηλό'})",frozenset({'writing_cat_χαμηλό'}),0.177,0.301,0.168,0.9491525423728815,3.153330705557746,1.0,0.11472300000000002,13.747000000000037,0.8297387606318349,0.5419354838709679,0.9272568560413184,0.7536460386283013
frozenset({'writing_cat_χαμηλό'}),"frozenset({'gender_male', 'reading_cat_χαμηλό'})",0.301,0.177,0.168,0.558139534883721,3.153330705557746,1.0,0.11472300000000002,1.8625789473684213,0.976931330472103,0.5419354838709679,0.46311000593404744,0.7536460386283013
frozenset({'writing_cat_χαμηλό'}),"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced', 'gender_female', 'reading_cat_χαμηλό'})",0.301,0.059,0.056,0.18604651162790697,3.1533307055577455,1.0,0.038241000000000004,1.1560857142857144,0.976931330472103,0.1842105263157895,0.13501223339841337,0.5675995270003942
"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced', 'gender_female', 'reading_cat_χαμηλό'})",frozenset({'writing_cat_χαμηλό'}),0.059,0.301,0.056,0.9491525423728814,3.1533307055577455,1.0,0.038241000000000004,13.747000000000009,0.7256907545164718,0.1842105263157895,0.927256856041318,0.5675995270003942
frozenset({'writing_cat_χαμηλό'}),"frozenset({'gender_male', 'test preparation course_none', 'reading_cat_χαμηλό', 'lunch_standard'})",0.301,0.078,0.074,0.24584717607973422,3.1518868728171054,1.0,0.050522,1.2225638766519826,0.9767235046205003,0.2426229508196721,0.18204682871988526,0.5972825623988415
"frozenset({'gender_male', 'test preparation course_none', 'reading_cat_χαμηλό', 'lunch_standard'})",frozenset({'writing_cat_χαμηλό'}),0.078,0.301,0.074,0.9487179487179487,3.1518868728171054,1.0,0.050522,13.63049999999999,0.7404877762795333,0.2426229508196721,0.9266351197681669,0.5972825623988415
"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced', 'writing_cat_χαμηλό'})","frozenset({'test preparation course_none', 'reading_cat_χαμηλό'})",0.132,0.214,0.089,0.6742424242424242,3.150665533843104,1.0,0.060751999999999994,2.4128372093023254,0.7864132967431263,0.3463035019455253,0.585550158069242,0.545065137354857
"frozenset({'test preparation course_none', 'reading_cat_χαμηλό'})","frozenset({'math_cat_χαμηλό', 'lunch_free/reduced', 'writing_cat_χαμηλό'})",0.214,0.132,0.089,0.4158878504672897,3.1506655338431035,1.0,0.060751999999999994,1.486016,0.8684564142150555,0.3463035019455253,0.32705973556139367,0.545065137354857
"frozenset({'math_cat_χαμηλό', 'reading_cat_χαμηλό'})","frozenset({'writing_cat_χαμηλό', 'parental level of education_high school'})",0.218,0.083,0.057,0.26146788990825687,3.1502155410633357,1.0,0.038905999999999996,1.2416521739130435,0.8728406694485573,0.2336065573770492,0.19462147209188313,0.47410743893003204
"frozenset({'writing_cat_χαμηλό', 'parental level of education_high school'})","frozenset({'math_cat_χαμηλό', 'reading_cat_χαμηλό'})",0.083,0.218,0.057,0.6867469879518072,3.1502155410633357,1.0,0.038905999999999996,2.496384615384615,0.7443417704566759,0.2336065573770492,0.5994207007056358,0.47410743893003204
frozenset({'writing_cat_χαμηλό'}),"frozenset({'parental level of education_high school', 'test preparation course_none', 'reading_cat_χαμηλό'})",0.301,0.057,0.054,0.17940199335548174,3.1474033922014337,1.0,0.036843,1.1491619433198381,0.9760769353044031,0.17763157894736842,0.1298006292210835,0.5633852072040566
"frozenset({'parental level of education_high school', 'test preparation course_none', 'reading_cat_χαμηλό'})",frozenset({'writing_cat_χαμηλό'}),0.057,0.301,0.054,0.9473684210526315,3.1474033922014337,1.0,0.036843,13.280999999999988,0.7235183221397432,0.17763157894736842,0.9247044650252237,0.5633852072040566
"frozenset({'math_cat_υψηλό', 'lunch_standard'})",frozenset({'writing_cat_υψηλό'}),0.156,0.208,0.102,0.6538461538461539,3.1434911242603554,1.0,0.069552,2.2880000000000003,0.8079174797881238,0.3893129770992366,0.5629370629370629,0.5721153846153846
frozenset({'writing_cat_υψηλό'}),"frozenset({'math_cat_υψηλό', 'lunch_standard'})",0.208,0.156,0.102,0.49038461538461536,3.143491124260355,1.0,0.069552,1.6561509433962265,0.8609625668449199,0.3893129770992366,0.3961903025884069,0.5721153846153846
"frozenset({'math_cat_χαμηλό', 'parental level of education_high school', 'writing_cat_χαμηλό'})",frozenset({'reading_cat_χαμηλό'}),0.066,0.275,0.057,0.8636363636363636,3.140495867768595,1.0,0.038849999999999996,5.316666666666667,0.7297419136706863,0.20070422535211266,0.8119122257053292,0.5354545454545454
frozenset({'reading_cat_χαμηλό'}),"frozenset({'math_cat_χαμηλό', 'parental level of education_high school', 'writing_cat_χαμηλό'})",0.275,0.066,0.057,0.20727272727272728,3.140495867768595,1.0,0.038849999999999996,1.1782110091743119,0.94010889292196,0.20070422535211266,0.15125559665174226,0.5354545454545454
frozenset({'writing_cat_χαμηλό'}),"frozenset({'parental level of education_high school', 'reading_cat_χαμηλό'})",0.301,0.073,0.069,0.22923588039867113,3.140217539707824,1.0,0.047027000000000006,1.2027025862068967,0.9750368020567683,0.22622950819672133,0.16853924530600986,0.587220679925363
"frozenset({'parental level of education_high school', 'reading_cat_χαμηλό'})",frozenset({'writing_cat_χαμηλό'}),0.073,0.301,0.069,0.9452054794520549,3.1402175397078236,1.0,0.047027000000000006,12.756750000000027,0.7352219251754921,0.22622950819672133,0.9216101279714661,0.587220679925363
"frozenset({'math_cat_χαμηλό', 'reading_cat_χαμηλό'})",frozenset({'writing_cat_χαμηλό'}),0.218,0.301,0.206,0.944954128440367,3.1393824865128472,1.0,0.140382,12.698500000000001,0.8714399225287415,0.6581469648562299,0.9212505414025277,0.8146697552500838
frozenset({'writing_cat_χαμηλό'}),"frozenset({'math_cat_χαμηλό', 'reading_cat_χαμηλό'})",0.301,0.218,0.206,0.6843853820598006,3.139382486512847,1.0,0.140382,2.4777052631578944,0.9749156214842285,0.6581469648562299,0.5964007443219956,0.8146697552500838
"frozenset({'test preparation course_none', 'reading_cat_χαμηλό', 'race/ethnicity_group D'})",frozenset({'writing_cat_χαμηλό'}),0.053,0.301,0.05,0.9433962264150945,3.1342067322760614,1.0,0.03404700000000001,12.349000000000029,0.7190496304118269,0.16447368421052633,0.9190217831403352,0.5547545916128629
frozenset({'writing_cat_χαμηλό'}),"frozenset({'test preparation course_none', 'reading_cat_χαμηλό', 'race/ethnicity_group D'})",0.301,0.053,0.05,0.16611295681063123,3.134206732276061,1.0,0.03404700000000001,1.135645418326693,0.9741630901287556,0.16447368421052633,0.11944346020130016,0.5547545916128629
frozenset({'reading_cat_χαμηλό'}),"frozenset({'math_cat_χαμηλό', 'test preparation course_none', 'writing_cat_χαμηλό', 'lunch_standard'})",0.275,0.085,0.073,0.26545454545454544,3.1229946524064167,1.0,0.04962499999999999,1.245668316831683,0.9376476145488899,0.2543554006968641,0.19721808246398406,0.562139037433155
"frozenset({'math_cat_χαμηλό', 'test preparation course_none', 'writing_cat_χαμηλό', 'lunch_standard'})",frozenset({'reading_cat_χαμηλό'}),0.085,0.275,0.073,0.8588235294117645,3.1229946524064163,1.0,0.04962499999999999,5.135416666666661,0.74294483119994,0.2543554006968641,0.8052738336713994,0.562139037433155
frozenset({'writing_cat_χαμηλό'}),"frozenset({'test preparation course_none', 'reading_cat_χαμηλό'})",0.301,0.214,0.201,0.6677740863787376,3.1204396559754093,1.0,0.136586,2.3658600000000005,0.9721492679663201,0.6401273885350319,0.5773207205836356,0.8035132114136678
"frozenset({'test preparation course_none', 'reading_cat_χαμηλό'})",frozenset({'writing_cat_χαμηλό'}),0.214,0.301,0.201,0.9392523364485982,3.1204396559754093,1.0,0.136586,11.506615384615396,0.8645449596799717,0.6401273885350319,0.9130934713141603,0.8035132114136678
"frozenset({'writing_cat_χαμηλό', 'test preparation course_none', 'lunch_standard'})","frozenset({'math_cat_χαμηλό', 'gender_male'})",0.122,0.134,0.051,0.41803278688524587,3.11964766332273,1.0,0.034651999999999995,1.4880563380281688,0.7738621644557594,0.24878048780487802,0.3279824328928936,0.39931490090530947
"frozenset({'math_cat_χαμηλό', 'gender_male'})","frozenset({'writing_cat_χαμηλό', 'test preparation course_none', 'lunch_standard'})",0.134,0.122,0.051,0.38059701492537307,3.11964766332273,1.0,0.034651999999999995,1.4174939759036143,0.7845854277045691,0.24878048780487802,0.2945296297555502,0.39931490090530947
"frozenset({'reading_cat_χαμηλό', 'race/ethnicity_group D'})","frozenset({'writing_cat_χαμηλό', 'test preparation course_none'})",0.066,0.243,0.05,0.7575757575757576,3.117595710188303,1.0,0.033962000000000006,3.1226249999999998,0.7272376873661672,0.19305019305019305,0.6797566150274208,0.48166853722409275
"frozenset({'writing_cat_χαμηλό', 'test preparation course_none'})","frozenset({'reading_cat_χαμηλό', 'race/ethnicity_group D'})",0.243,0.066,0.05,0.205761316872428,3.117595710188303,1.0,0.033962000000000006,1.1759689119170984,0.8972787318361957,0.19305019305019305,0.14963738423172163,0.48166853722409275
frozenset({'writing_cat_υψηλό'}),frozenset({'math_cat_υψηλό'}),0.208,0.176,0.114,0.5480769230769231,3.114073426573427,1.0,0.07739200000000002,1.8233191489361706,0.8571681729576468,0.4222222222222222,0.45154966392830476,0.5979020979020979
frozenset({'math_cat_υψηλό'}),frozenset({'writing_cat_υψηλό'}),0.176,0.208,0.114,0.6477272727272728,3.114073426573427,1.0,0.07739200000000002,2.2482580645161296,0.8238800885709421,0.4222222222222222,0.5552112029384758,0.5979020979020979
frozenset({'writing_cat_χαμηλό'}),"frozenset({'race/ethnicity_group C', 'test preparation course_none', 'reading_cat_χαμηλό'})",0.301,0.061,0.057,0.18936877076411962,3.1044060781003218,1.0,0.03863900000000001,1.1583565573770493,0.9697813919634567,0.18688524590163935,0.1367079560853244,0.5618975001361581
"frozenset({'lunch_free/reduced', 'gender_female', 'reading_cat_χαμηλό'})",frozenset({'writing_cat_χαμηλό'}),0.061,0.301,0.057,0.9344262295081968,3.1044060781003218,1.0,0.03863900000000001,10.659750000000008,0.721913943538292,0.18688524590163935,0.9061891695396233,0.5618975001361581
"frozenset({'race/ethnicity_group C', 'test preparation course_none', 'reading_cat_χαμηλό'})",frozenset({'writing_cat_χαμηλό'}),0.061,0.301,0.057,0.9344262295081968,3.1044060781003218,1.0,0.03863900000000001,10.659750000000008,0.721913943538292,0.18688524590163935,0.9061891695396233,0.5618975001361581
frozenset({'writing_cat_χαμηλό'}),"frozenset({'lunch_free/reduced', 'gender_female', 'reading_cat_χαμηλό'})",0.301,0.061,0.057,0.18936877076411962,3.1044060781003218,1.0,0.03863900000000001,1.1583565573770493,0.9697813919634567,0.18688524590163935,0.1367079560853244,0.5618975001361581
frozenset({'reading_cat_χαμηλό'}),"frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced'})",0.275,0.156,0.133,0.48363636363636364,3.1002331002331003,1.0,0.09010000000000001,1.634507042253521,0.9344049779621469,0.44630872483221473,0.3881947436449806,0.6681002331002331
"frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced'})",frozenset({'reading_cat_χαμηλό'}),0.156,0.275,0.133,0.8525641025641026,3.1002331002331003,1.0,0.09010000000000001,4.917391304347829,0.8026583045290954,0.44630872483221473,0.7966401414677278,0.6681002331002331
"frozenset({'writing_cat_χαμηλό', 'gender_male'})","frozenset({'math_cat_χαμηλό', 'reading_cat_χαμηλό', 'lunch_standard'})",0.201,0.095,0.059,0.29353233830845765,3.089814087457449,1.0,0.039904999999999996,1.2810211267605631,0.8465030440593114,0.24894514767932482,0.2193727495121079,0.45729248494370245
"frozenset({'math_cat_χαμηλό', 'reading_cat_χαμηλό', 'lunch_standard'})","frozenset({'writing_cat_χαμηλό', 'gender_male'})",0.095,0.201,0.059,0.6210526315789473,3.089814087457449,1.0,0.039904999999999996,2.108472222222222,0.7473546212192153,0.24894514767932482,0.5257229431526249,0.45729248494370245
"frozenset({'gender_male', 'reading_cat_χαμηλό', 'lunch_standard'})",frozenset({'writing_cat_χαμηλό'}),0.099,0.301,0.092,0.9292929292929293,3.087351924561227,1.0,0.062201,9.885857142857143,0.7503860444916276,0.29870129870129863,0.8988453924076242,0.6174703849122454
frozenset({'writing_cat_χαμηλό'}),"frozenset({'gender_male', 'reading_cat_χαμηλό', 'lunch_standard'})",0.301,0.099,0.092,0.30564784053156147,3.0873519245612266,1.0,0.062201,1.2976124401913876,0.9672358026995086,0.29870129870129863,0.2293538740638862,0.6174703849122454
frozenset({'writing_cat_χαμηλό'}),"frozenset({'race/ethnicity_group C', 'reading_cat_χαμηλό'})",0.301,0.083,0.077,0.2558139534883721,3.0820958251611095,1.0,0.052016999999999994,1.2322187500000001,0.966445571595786,0.250814332247557,0.18845578352058023,0.591762398430933
"frozenset({'race/ethnicity_group C', 'reading_cat_χαμηλό'})",frozenset({'writing_cat_χαμηλό'}),0.083,0.301,0.077,0.927710843373494,3.0820958251611095,1.0,0.052016999999999994,9.6695,0.7366907901259044,0.250814332247557,0.8965820362997052,0.591762398430933
frozenset({'writing_cat_χαμηλό'}),"frozenset({'race/ethnicity_group C', 'math_cat_χαμηλό', 'reading_cat_χαμηλό'})",0.301,0.069,0.064,0.21262458471760798,3.0815157205450427,1.0,0.043231,1.182409282700422,0.9663581902718168,0.20915032679738563,0.15426915651730178,0.570080408300833
"frozenset({'race/ethnicity_group C', 'math_cat_χαμηλό', 'reading_cat_χαμηλό'})",frozenset({'writing_cat_χαμηλό'}),0.069,0.301,0.064,0.9275362318840579,3.0815157205450427,1.0,0.043231,9.646199999999988,0.725547126745435,0.20915032679738563,0.8963322344550184,0.570080408300833
"frozenset({'writing_cat_χαμηλό', 'test preparation course_none'})","frozenset({'math_cat_χαμηλό', 'gender_male', 'reading_cat_χαμηλό'})",0.243,0.123,0.092,0.3786008230452675,3.078055471912744,1.0,0.062111,1.4113311258278147,0.8918356211590374,0.3357664233576642,0.2914490570641591,0.5632841513600321
"frozenset({'math_cat_χαμηλό', 'gender_male', 'reading_cat_χαμηλό'})","frozenset({'writing_cat_χαμηλό', 'test preparation course_none'})",0.123,0.243,0.092,0.7479674796747967,3.0780554719127435,1.0,0.062111,3.00358064516129,0.7698056615933766,0.3357664233576642,0.667064041842532,0.5632841513600321
"frozenset({'writing_cat_χαμηλό', 'test preparation course_none'})","frozenset({'gender_male', 'reading_cat_χαμηλό', 'lunch_standard'})",0.243,0.099,0.074,0.3045267489711934,3.076027767385792,1.0,0.049942999999999994,1.2955207100591715,0.8915527151986861,0.27611940298507465,0.22810959930210145,0.5260007482229704
"frozenset({'gender_male', 'reading_cat_χαμηλό', 'lunch_standard'})","frozenset({'writing_cat_χαμηλό', 'test preparation course_none'})",0.099,0.243,0.074,0.7474747474747474,3.0760277673857916,1.0,0.049942999999999994,2.9977199999999993,0.7490626031136575,0.27611940298507465,0.6664131406535633,0.5260007482229704
frozenset({'writing_cat_χαμηλό'}),"frozenset({'math_cat_χαμηλό', 'test preparation course_none', 'reading_cat_χαμηλό', 'lunch_standard'})",0.301,0.079,0.073,0.2425249169435216,3.0699356575129317,1.0,0.049221,1.2158815789473685,0.9646069727791169,0.23778501628664495,0.1775514841949203,0.583287774927457
"frozenset({'math_cat_χαμηλό', 'test preparation course_none', 'reading_cat_χαμηλό', 'lunch_standard'})",frozenset({'writing_cat_χαμηλό'}),0.079,0.301,0.073,0.9240506329113923,3.0699356575129313,1.0,0.049221,9.203499999999993,0.7320958457900139,0.23778501628664495,0.8913456837072851,0.583287774927457
frozenset({'reading_cat_χαμηλό'}),"frozenset({'writing_cat_χαμηλό', 'gender_male', 'lunch_standard'})",0.275,0.109,0.092,0.33454545454545453,3.0692243536280235,1.0,0.062025,1.338934426229508,0.9299100449775113,0.31506849315068486,0.25313743495561675,0.5892910758965805
"frozenset({'writing_cat_χαμηλό', 'gender_male', 'lunch_standard'})",frozenset({'reading_cat_χαμηλό'}),0.109,0.275,0.092,0.8440366972477064,3.069224353628023,1.0,0.062025,4.648529411764704,0.756660811008637,0.31506849315068486,0.7848782031002847,0.5892910758965805
"frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced', 'test preparation course_none'})",frozenset({'reading_cat_χαμηλό'}),0.121,0.275,0.102,0.8429752066115702,3.065364387678437,1.0,0.068725,4.617105263157893,0.7665239019384774,0.346938775510204,0.7834140780849245,0.6069421487603306
frozenset({'reading_cat_χαμηλό'}),"frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced', 'test preparation course_none'})",0.275,0.121,0.102,0.37090909090909085,3.065364387678437,1.0,0.068725,1.3972543352601154,0.929344151453685,0.346938775510204,0.2843106836280897,0.6069421487603306
"frozenset({'writing_cat_χαμηλό', 'test preparation course_none'})","frozenset({'math_cat_χαμηλό', 'reading_cat_χαμηλό'})",0.243,0.218,0.162,0.6666666666666667,3.0581039755351687,1.0,0.10902600000000001,2.3460000000000005,0.8890356671070013,0.5418060200668898,0.5737425404944587,0.7048929663608563
"frozenset({'math_cat_χαμηλό', 'reading_cat_χαμηλό'})","frozenset({'writing_cat_χαμηλό', 'test preparation course_none'})",0.218,0.243,0.162,0.7431192660550459,3.058103975535168,1.0,0.10902600000000001,2.946892857142857,0.860613810741688,0.5418060200668898,0.6606595324373129,0.7048929663608563
"frozenset({'writing_cat_χαμηλό', 'gender_male', 'test preparation course_none', 'lunch_standard'})",frozenset({'reading_cat_χαμηλό'}),0.088,0.275,0.074,0.8409090909090909,3.0578512396694215,1.0,0.0498,4.557142857142858,0.7379089615931722,0.2560553633217993,0.7805642633228841,0.5549999999999999
frozenset({'reading_cat_χαμηλό'}),"frozenset({'writing_cat_χαμηλό', 'gender_male', 'test preparation course_none', 'lunch_standard'})",0.275,0.088,0.074,0.26909090909090905,3.057851239669421,1.0,0.0498,1.2477611940298508,0.9282385834109972,0.2560553633217993,0.19856459330143536,0.5549999999999999
"frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced'})","frozenset({'test preparation course_none', 'reading_cat_χαμηλό'})",0.156,0.214,0.102,0.6538461538461539,3.055355859094177,1.0,0.068616,2.2706666666666666,0.7970448843044327,0.38059701492537307,0.5596007046388726,0.5652408339324227
"frozenset({'test preparation course_none', 'reading_cat_χαμηλό'})","frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced'})",0.214,0.156,0.102,0.4766355140186916,3.0553558590941767,1.0,0.068616,1.6126428571428568,0.8558599012123934,0.38059701492537307,0.3798998981264119,0.5652408339324227
frozenset({'reading_cat_χαμηλό'}),"frozenset({'writing_cat_χαμηλό', 'gender_female'})",0.275,0.1,0.084,0.3054545454545454,3.0545454545454542,1.0,0.0565,1.2958115183246073,0.9277504105090312,0.288659793814433,0.22828282828282823,0.5727272727272728
"frozenset({'writing_cat_χαμηλό', 'gender_female'})",frozenset({'reading_cat_χαμηλό'}),0.1,0.275,0.084,0.84,3.0545454545454542,1.0,0.0565,4.531249999999999,0.7473544973544973,0.288659793814433,0.7793103448275862,0.5727272727272728
frozenset({'reading_cat_χαμηλό'}),"frozenset({'writing_cat_χαμηλό', 'gender_female', 'math_cat_χαμηλό'})",0.275,0.099,0.083,0.3018181818181818,3.0486685032139578,1.0,0.055775000000000005,1.2904947916666667,0.9268799335272124,0.28522336769759454,0.22510342044193318,0.5701010101010101
"frozenset({'writing_cat_χαμηλό', 'gender_female', 'math_cat_χαμηλό'})",frozenset({'reading_cat_χαμηλό'}),0.099,0.275,0.083,0.8383838383838383,3.0486685032139573,1.0,0.055775000000000005,4.485937499999999,0.745824585801586,0.28522336769759454,0.7770811563915011,0.5701010101010101
"frozenset({'test preparation course_none', 'reading_cat_υψηλό'})",frozenset({'math_cat_υψηλό'}),0.11,0.176,0.059,0.5363636363636364,3.047520661157025,1.0,0.039639999999999995,1.7772549019607844,0.7549038278423156,0.2599118942731278,0.4373345101500441,0.43579545454545454
frozenset({'math_cat_υψηλό'}),"frozenset({'test preparation course_none', 'reading_cat_υψηλό'})",0.176,0.11,0.059,0.3352272727272727,3.0475206611570247,1.0,0.039639999999999995,1.3388034188034188,0.815369425703472,0.2599118942731278,0.2530643513789581,0.43579545454545454
frozenset({'reading_cat_χαμηλό'}),frozenset({'writing_cat_χαμηλό'}),0.275,0.301,0.252,0.9163636363636363,3.044397463002114,1.0,0.16922500000000001,8.357608695652166,0.9262452107279694,0.7777777777777777,0.8803485498764467,0.8767864693446088
frozenset({'writing_cat_χαμηλό'}),frozenset({'reading_cat_χαμηλό'}),0.301,0.275,0.252,0.8372093023255814,3.044397463002114,1.0,0.16922500000000001,4.45357142857143,0.960697822285805,0.7777777777777777,0.7754611066559743,0.8767864693446088
"frozenset({'parental level of education_high school', 'reading_cat_χαμηλό'})","frozenset({'writing_cat_χαμηλό', 'test preparation course_none'})",0.073,0.243,0.054,0.7397260273972603,3.0441400304414006,1.0,0.036261,2.908473684210527,0.7243797195253505,0.20610687022900762,0.6561770507229331,0.4809741248097413
"frozenset({'writing_cat_χαμηλό', 'test preparation course_none'})","frozenset({'parental level of education_high school', 'reading_cat_χαμηλό'})",0.243,0.073,0.054,0.22222222222222224,3.0441400304414006,1.0,0.036261,1.191857142857143,0.8870541611624836,0.20610687022900762,0.1609732710056335,0.4809741248097413
"frozenset({'test preparation course_none', 'reading_cat_χαμηλό'})","frozenset({'writing_cat_χαμηλό', 'parental level of education_high school'})",0.214,0.083,0.054,0.25233644859813087,3.0401981758810948,1.0,0.036238,1.2264875000000002,0.8537838092545471,0.22222222222222224,0.1846635208267512,0.45146942911834254
"frozenset({'writing_cat_χαμηλό', 'parental level of education_high school'})","frozenset({'test preparation course_none', 'reading_cat_χαμηλό'})",0.083,0.214,0.054,0.6506024096385542,3.0401981758810943,1.0,0.036238,2.2495862068965518,0.7318146936467547,0.22222222222222224,0.5554738036113921,0.45146942911834254
"frozenset({'writing_cat_χαμηλό', 'gender_male'})",frozenset({'reading_cat_χαμηλό'}),0.201,0.275,0.168,0.835820895522388,3.0393487109905015,1.0,0.112725,4.41590909090909,0.839777400321831,0.5454545454545454,0.7735460627895008,0.7233649932157394
"frozenset({'writing_cat_χαμηλό', 'parental level of education_some high school'})",frozenset({'reading_cat_χαμηλό'}),0.067,0.275,0.056,0.835820895522388,3.0393487109905015,1.0,0.037575,4.41590909090909,0.7191662838768947,0.19580419580419578,0.7735460627895008,0.5197286295793758
frozenset({'reading_cat_χαμηλό'}),"frozenset({'writing_cat_χαμηλό', 'parental level of education_some high school'})",0.275,0.067,0.056,0.20363636363636362,3.0393487109905015,1.0,0.037575,1.1715753424657536,0.9254926108374384,0.19580419580419578,0.14644840689856764,0.5197286295793758
frozenset({'reading_cat_χαμηλό'}),"frozenset({'writing_cat_χαμηλό', 'gender_male'})",0.275,0.201,0.168,0.6109090909090908,3.0393487109905015,1.0,0.112725,2.053504672897196,0.9254926108374384,0.5454545454545454,0.5130276481966094,0.7233649932157394
"frozenset({'math_cat_χαμηλό', 'gender_female', 'reading_cat_χαμηλό'})","frozenset({'writing_cat_χαμηλό', 'test preparation course_none'})",0.095,0.243,0.07,0.736842105263158,3.032272038119992,1.0,0.046915000000000005,2.8766000000000007,0.740568271507498,0.26119402985074636,0.6523673781547661,0.5124539744422786
"frozenset({'writing_cat_χαμηλό', 'test preparation course_none'})","frozenset({'math_cat_χαμηλό', 'gender_female', 'reading_cat_χαμηλό'})",0.243,0.095,0.07,0.2880658436213992,3.0322720381199915,1.0,0.046915000000000005,1.271184971098266,0.885355727495754,0.26119402985074636,0.21333242389104884,0.5124539744422786
frozenset({'reading_cat_χαμηλό'}),"frozenset({'writing_cat_χαμηλό', 'parental level of education_high school'})",0.275,0.083,0.069,0.2509090909090909,3.023001095290252,1.0,0.04617500000000001,1.2241504854368934,0.9230384807596204,0.23875432525951557,0.18310696936651133,0.5411171960569551
"frozenset({'writing_cat_χαμηλό', 'parental level of education_high school'})",frozenset({'reading_cat_χαμηλό'}),0.083,0.275,0.069,0.8313253012048193,3.0230010952902515,1.0,0.04617500000000001,4.298214285714286,0.7297741532723279,0.23875432525951557,0.76734524304113,0.5411171960569551
"frozenset({'gender_male', 'reading_cat_χαμηλό'})","frozenset({'writing_cat_χαμηλό', 'test preparation course_none'})",0.177,0.243,0.13,0.7344632768361583,3.0224826207249316,1.0,0.08698900000000001,2.850829787234044,0.8130572950743061,0.4482758620689656,0.6492249363753743,0.6347213503522355
"frozenset({'writing_cat_χαμηλό', 'test preparation course_none'})","frozenset({'gender_male', 'reading_cat_χαμηλό'})",0.243,0.177,0.13,0.5349794238683128,3.022482620724931,1.0,0.08698900000000001,1.7698141592920353,0.8839447210649326,0.4482758620689656,0.434968923290781,0.6347213503522355
"frozenset({'race/ethnicity_group C', 'writing_cat_χαμηλό', 'math_cat_χαμηλό'})",frozenset({'reading_cat_χαμηλό'}),0.077,0.275,0.064,0.8311688311688312,3.022432113341204,1.0,0.042825,4.29423076923077,0.7249627573131093,0.2222222222222222,0.7671294223018361,0.531948051948052
frozenset({'reading_cat_χαμηλό'}),"frozenset({'race/ethnicity_group C', 'writing_cat_χαμηλό', 'math_cat_χαμηλό'})",0.275,0.077,0.064,0.23272727272727273,3.022432113341204,1.0,0.042825,1.202962085308057,0.9229525862068967,0.2222222222222222,0.1687186053383236,0.531948051948052
"frozenset({'test preparation course_none', 'reading_cat_χαμηλό'})","frozenset({'writing_cat_χαμηλό', 'gender_male'})",0.214,0.201,0.13,0.6074766355140188,3.022271818477705,1.0,0.08698600000000001,2.035547619047619,0.8513016245840673,0.45614035087719296,0.5087317090288095,0.6271214023341238
"frozenset({'writing_cat_χαμηλό', 'gender_male'})","frozenset({'test preparation course_none', 'reading_cat_χαμηλό'})",0.201,0.214,0.13,0.6467661691542288,3.0222718184777047,1.0,0.08698600000000001,2.2251549295774646,0.8374506594781941,0.45614035087719296,0.5505930905270087,0.6271214023341238
"frozenset({'math_cat_χαμηλό', 'writing_cat_χαμηλό'})","frozenset({'gender_male', 'reading_cat_χαμηλό'})",0.23,0.177,0.123,0.5347826086956522,3.0213706705969052,1.0,0.08229,1.7690654205607477,0.8688628444726005,0.4330985915492957,0.4347297797031011,0.6148489314664702
"frozenset({'gender_male', 'reading_cat_χαμηλό'})","frozenset({'math_cat_χαμηλό', 'writing_cat_χαμηλό'})",0.177,0.23,0.123,0.6949152542372882,3.021370670596905,1.0,0.08229,2.523888888888889,0.8129093441602703,0.4330985915492957,0.6037860444640106,0.6148489314664702
frozenset({'writing_cat_χαμηλό'}),"frozenset({'math_cat_χαμηλό', 'gender_female', 'test preparation course_none', 'reading_cat_χαμηλό'})",0.301,0.077,0.07,0.23255813953488375,3.0202355783751136,1.0,0.046823000000000004,1.2026969696969696,0.9569384835479255,0.2272727272727273,0.16853536244299427,0.5708245243128964
"frozenset({'math_cat_χαμηλό', 'gender_female', 'test preparation course_none', 'reading_cat_χαμηλό'})",frozenset({'writing_cat_χαμηλό'}),0.077,0.301,0.07,0.9090909090909092,3.0202355783751136,1.0,0.046823000000000004,7.689000000000007,0.7247020585048753,0.2272727272727273,0.8699440759526598,0.5708245243128964
"frozenset({'lunch_free/reduced', 'reading_cat_χαμηλό'})","frozenset({'writing_cat_χαμηλό', 'test preparation course_none'})",0.139,0.243,0.102,0.7338129496402876,3.0198063771205255,1.0,0.06822299999999999,2.8438648648648637,0.7768326842932294,0.3642857142857142,0.6483658515723747,0.5767830180300204
"frozenset({'writing_cat_χαμηλό', 'test preparation course_none'})","frozenset({'lunch_free/reduced', 'reading_cat_χαμηλό'})",0.243,0.139,0.102,0.41975308641975306,3.0198063771205255,1.0,0.06822299999999999,1.483851063829787,0.8835573859662755,0.3642857142857142,0.3260779168638247,0.5767830180300204
frozenset({'writing_cat_χαμηλό'}),"frozenset({'test preparation course_none', 'reading_cat_χαμηλό', 'lunch_standard'})",0.301,0.109,0.099,0.3289036544850499,3.017464720046329,1.0,0.066191,1.3276782178217823,0.9565035187352782,0.3183279742765274,0.246805448355836,0.6185802676094974
"frozenset({'test preparation course_none', 'reading_cat_χαμηλό', 'lunch_standard'})",frozenset({'writing_cat_χαμηλό'}),0.109,0.301,0.099,0.908256880733945,3.0174647200463287,1.0,0.066191,7.6191,0.7503882823748143,0.3183279742765274,0.8687509023375463,0.6185802676094974
"frozenset({'writing_cat_χαμηλό', 'gender_male', 'test preparation course_none'})",frozenset({'reading_cat_χαμηλό'}),0.157,0.275,0.13,0.8280254777070064,3.011001737116387,1.0,0.086825,4.215740740740742,0.7922711926270645,0.4304635761589403,0.7627937623544916,0.6503763752171395
frozenset({'reading_cat_χαμηλό'}),"frozenset({'writing_cat_χαμηλό', 'gender_male', 'test preparation course_none'})",0.275,0.157,0.13,0.4727272727272727,3.011001737116387,1.0,0.086825,1.5987931034482756,0.9212201591511936,0.4304635761589403,0.374528200150976,0.6503763752171395
"frozenset({'test preparation course_completed', 'writing_cat_υψηλό'})",frozenset({'math_cat_υψηλό'}),0.117,0.176,0.062,0.5299145299145299,3.010878010878011,1.0,0.041408,1.7528727272727274,0.7563657618821467,0.2683982683982684,0.4295079246535557,0.4410936285936286
frozenset({'math_cat_υψηλό'}),"frozenset({'test preparation course_completed', 'writing_cat_υψηλό'})",0.176,0.117,0.062,0.3522727272727273,3.010878010878011,1.0,0.041408,1.3632280701754387,0.8105230191042906,0.2683982683982684,0.266447029753938,0.4410936285936286
"frozenset({'math_cat_χαμηλό', 'gender_male'})","frozenset({'writing_cat_χαμηλό', 'test preparation course_none'})",0.134,0.243,0.098,0.7313432835820896,3.009643142313126,1.0,0.065438,2.8177222222222222,0.7710562284960173,0.35125448028673834,0.6451034129221791,0.5673177323260242
"frozenset({'writing_cat_χαμηλό', 'test preparation course_none'})","frozenset({'math_cat_χαμηλό', 'gender_male'})",0.243,0.134,0.098,0.4032921810699589,3.009643142313126,1.0,0.065438,1.451296551724138,0.8820801768527754,0.35125448028673834,0.31096094811773545,0.5673177323260242
"frozenset({'writing_cat_χαμηλό', 'test preparation course_none'})",frozenset({'reading_cat_χαμηλό'}),0.243,0.275,0.201,0.8271604938271606,3.0078563411896746,1.0,0.134175,4.19464285714286,0.8818194365030855,0.6340694006309149,0.7616006811409112,0.7790347923681258
frozenset({'reading_cat_χαμηλό'}),"frozenset({'writing_cat_χαμηλό', 'test preparation course_none'})",0.275,0.243,0.201,0.7309090909090908,3.007856341189674,1.0,0.134175,2.813175675675675,0.9207411219763252,0.6340694006309149,0.644529842680437,0.7790347923681258
"frozenset({'math_cat_χαμηλό', 'reading_cat_χαμηλό', 'lunch_standard'})",frozenset({'writing_cat_χαμηλό'}),0.095,0.301,0.086,0.9052631578947368,3.007518796992481,1.0,0.057405,7.3783333333333285,0.7375690607734807,0.2774193548387096,0.8644680370454031,0.5954887218045113
frozenset({'writing_cat_χαμηλό'}),"frozenset({'math_cat_χαμηλό', 'reading_cat_χαμηλό', 'lunch_standard'})",0.301,0.095,0.086,0.2857142857142857,3.007518796992481,1.0,0.057405,1.267,0.9549356223175965,0.2774193548387096,0.21073401736385158,0.5954887218045113
frozenset({'reading_cat_χαμηλό'}),"frozenset({'writing_cat_χαμηλό', 'race/ethnicity_group B'})",0.275,0.069,0.057,0.20727272727272728,3.0039525691699605,1.0,0.038025,1.1744266055045873,0.9201451905626135,0.19860627177700346,0.14852065228005076,0.5166798418972331
"frozenset({'writing_cat_χαμηλό', 'race/ethnicity_group B'})",frozenset({'reading_cat_χαμηλό'}),0.069,0.275,0.057,0.8260869565217391,3.0039525691699605,1.0,0.038025,4.16875,0.7165470066142801,0.19860627177700346,0.760119940029985,0.5166798418972331
"frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced', 'gender_male'})",frozenset({'reading_cat_χαμηλό'}),0.092,0.275,0.076,0.8260869565217391,3.0039525691699605,1.0,0.050699999999999995,4.16875,0.7346974263853465,0.2611683848797251,0.760119940029985,0.5512252964426877
frozenset({'reading_cat_χαμηλό'}),"frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced', 'gender_male'})",0.275,0.092,0.076,0.2763636363636363,3.00395256916996,1.0,0.050699999999999995,1.2547738693467336,0.9201451905626135,0.2611683848797251,0.20304365238285937,0.5512252964426877
"frozenset({'writing_cat_χαμηλό', 'gender_female', 'test preparation course_none'})",frozenset({'reading_cat_χαμηλό'}),0.086,0.275,0.071,0.8255813953488372,3.0021141649048624,1.0,0.04734999999999999,4.156666666666667,0.7296514315653219,0.24482758620689654,0.7594226142742583,0.5418816067653277
frozenset({'reading_cat_χαμηλό'}),"frozenset({'writing_cat_χαμηλό', 'gender_female', 'test preparation course_none'})",0.275,0.086,0.071,0.2581818181818181,3.002114164904862,1.0,0.04734999999999999,1.2321078431372547,0.9198640116561437,0.24482758620689654,0.18838273324050123,0.5418816067653277
"frozenset({'race/ethnicity_group C', 'reading_cat_χαμηλό'})","frozenset({'writing_cat_χαμηλό', 'gender_male'})",0.083,0.201,0.05,0.6024096385542169,2.9970628783791886,1.0,0.033317,2.009606060606061,0.7266521264994547,0.21367521367521364,0.502390035737443,0.42558292872984477
"frozenset({'writing_cat_χαμηλό', 'gender_male'})","frozenset({'race/ethnicity_group C', 'reading_cat_χαμηλό'})",0.201,0.083,0.05,0.24875621890547264,2.997062878379188,1.0,0.033317,1.2206423841059604,0.8339674593241552,0.21367521367521364,0.18075923544762557,0.42558292872984477
"frozenset({'race/ethnicity_group C', 'writing_cat_χαμηλό'})","frozenset({'math_cat_χαμηλό', 'reading_cat_χαμηλό'})",0.098,0.218,0.064,0.6530612244897959,2.9956936903201647,1.0,0.042636,2.2539999999999996,0.7385670731707317,0.25396825396825395,0.5563442768411712,0.473319603070586
"frozenset({'math_cat_χαμηλό', 'reading_cat_χαμηλό'})","frozenset({'race/ethnicity_group C', 'writing_cat_χαμηλό'})",0.218,0.098,0.064,0.29357798165137616,2.9956936903201647,1.0,0.042636,1.276857142857143,0.8519021739130435,0.25396825396825395,0.21682703065562767,0.473319603070586
"frozenset({'writing_cat_χαμηλό', 'test preparation course_none'})","frozenset({'reading_cat_χαμηλό', 'lunch_standard'})",0.243,0.136,0.099,0.40740740740740744,2.9956427015250546,1.0,0.06595200000000001,1.458,0.8800288219046476,0.35357142857142854,0.3141289437585734,0.5676742919389979
"frozenset({'reading_cat_χαμηλό', 'lunch_standard'})","frozenset({'writing_cat_χαμηλό', 'test preparation course_none'})",0.136,0.243,0.099,0.7279411764705882,2.9956427015250546,1.0,0.06595200000000001,2.782486486486486,0.7710437710437712,0.35357142857142854,0.6406092159452949,0.5676742919389979
"frozenset({'math_cat_χαμηλό', 'gender_female', 'test preparation course_none', 'writing_cat_χαμηλό'})",frozenset({'reading_cat_χαμηλό'}),0.085,0.275,0.07,0.823529411764706,2.9946524064171123,1.0,0.046625,4.108333333333335,0.7279469164715066,0.24137931034482757,0.7565922920892496,0.5390374331550802
frozenset({'reading_cat_χαμηλό'}),"frozenset({'math_cat_χαμηλό', 'gender_female', 'test preparation course_none', 'writing_cat_χαμηλό'})",0.275,0.085,0.07,0.2545454545454546,2.9946524064171123,1.0,0.046625,1.227439024390244,0.91871921182266,0.24137931034482757,0.18529557873820168,0.5390374331550802
"frozenset({'writing_cat_χαμηλό', 'parental level of education_some college'})",frozenset({'reading_cat_χαμηλό'}),0.062,0.275,0.051,0.8225806451612903,2.9912023460410553,1.0,0.033949999999999994,4.086363636363635,0.7096868598185543,0.17832167832167828,0.7552836484983314,0.5040175953079178
frozenset({'reading_cat_χαμηλό'}),"frozenset({'writing_cat_χαμηλό', 'parental level of education_some college'})",0.275,0.062,0.051,0.18545454545454543,2.9912023460410553,1.0,0.033949999999999994,1.1515624999999998,0.9181879648411089,0.17832167832167828,0.1316146540027137,0.5040175953079178
frozenset({'writing_cat_χαμηλό'}),"frozenset({'gender_female', 'test preparation course_none', 'reading_cat_χαμηλό'})",0.301,0.079,0.071,0.23588039867109634,2.9858278312797006,1.0,0.047221,1.205308695652174,0.9514799814624514,0.2297734627831715,0.17033702352996344,0.5673072879431431
"frozenset({'gender_female', 'test preparation course_none', 'reading_cat_χαμηλό'})",frozenset({'writing_cat_χαμηλό'}),0.079,0.301,0.071,0.8987341772151898,2.9858278312797,1.0,0.047221,6.902624999999994,0.7221330152467466,0.2297734627831715,0.8551275782763802,0.5673072879431431
frozenset({'reading_cat_χαμηλό'}),"frozenset({'writing_cat_χαμηλό', 'lunch_standard'})",0.275,0.145,0.119,0.4327272727272727,2.9843260188087775,1.0,0.079125,1.507211538461538,0.9171254708780064,0.39534883720930225,0.3365231259968102,0.6267084639498433
"frozenset({'writing_cat_χαμηλό', 'lunch_standard'})",frozenset({'reading_cat_χαμηλό'}),0.145,0.275,0.119,0.8206896551724138,2.984326018808777,1.0,0.079125,4.04326923076923,0.7776794928497716,0.39534883720930225,0.7526753864447087,0.6267084639498433
"frozenset({'gender_female', 'reading_cat_χαμηλό'})","frozenset({'writing_cat_χαμηλό', 'test preparation course_none'})",0.098,0.243,0.071,0.7244897959183673,2.981439489375997,1.0,0.04718599999999999,2.7476296296296288,0.7367977264919895,0.26296296296296295,0.6360499285579488,0.5083354329386075
"frozenset({'writing_cat_χαμηλό', 'test preparation course_none'})","frozenset({'gender_female', 'reading_cat_χαμηλό'})",0.243,0.098,0.071,0.29218106995884774,2.981439489375997,1.0,0.04718599999999999,1.2743372093023255,0.8779280704039294,0.26296296296296295,0.21527834806967597,0.5083354329386075
"frozenset({'writing_cat_χαμηλό', 'lunch_standard'})","frozenset({'math_cat_χαμηλό', 'test preparation course_none', 'reading_cat_χαμηλό'})",0.145,0.169,0.073,0.503448275862069,2.978983880840645,1.0,0.048494999999999996,1.6735416666666667,0.7769766882960827,0.3029045643153527,0.4024648325656666,0.46770046929198117
"frozenset({'math_cat_χαμηλό', 'test preparation course_none', 'reading_cat_χαμηλό'})","frozenset({'writing_cat_χαμηλό', 'lunch_standard'})",0.169,0.145,0.073,0.4319526627218934,2.9789838808406444,1.0,0.048494999999999996,1.50515625,0.7994164482468721,0.3029045643153527,0.33561714938233156,0.46770046929198117
"frozenset({'writing_cat_χαμηλό', 'test preparation course_none'})","frozenset({'math_cat_χαμηλό', 'lunch_free/reduced', 'reading_cat_χαμηλό'})",0.243,0.123,0.089,0.3662551440329218,2.9776840978286327,1.0,0.059111,1.3838376623376623,0.8773692725572558,0.32129963898916963,0.2773718860124536,0.5449161899026398
"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced', 'reading_cat_χαμηλό'})","frozenset({'writing_cat_χαμηλό', 'test preparation course_none'})",0.123,0.243,0.089,0.7235772357723577,2.9776840978286323,1.0,0.059111,2.738558823529411,0.7573187449553508,0.32129963898916963,0.6348444329885834,0.5449161899026398
"frozenset({'gender_male', 'test preparation course_none', 'reading_cat_χαμηλό'})","frozenset({'math_cat_χαμηλό', 'writing_cat_χαμηλό'})",0.135,0.23,0.092,0.6814814814814815,2.962962962962963,1.0,0.06094999999999999,2.417441860465116,0.7658959537572253,0.33699633699633696,0.5863395863395864,0.5407407407407407
"frozenset({'math_cat_χαμηλό', 'writing_cat_χαμηλό'})","frozenset({'gender_male', 'test preparation course_none', 'reading_cat_χαμηλό'})",0.23,0.135,0.092,0.39999999999999997,2.9629629629629624,1.0,0.06094999999999999,1.4416666666666664,0.8603896103896103,0.33699633699633696,0.30635838150289013,0.5407407407407407
"frozenset({'lunch_free/reduced', 'gender_male', 'reading_cat_χαμηλό'})","frozenset({'writing_cat_χαμηλό', 'test preparation course_none'})",0.078,0.243,0.056,0.717948717948718,2.954521473039992,1.0,0.037046,2.683909090909091,0.717500774713356,0.2113207547169811,0.6274091386376723,0.4742006964229186
"frozenset({'writing_cat_χαμηλό', 'test preparation course_none'})","frozenset({'lunch_free/reduced', 'gender_male', 'reading_cat_χαμηλό'})",0.243,0.078,0.056,0.23045267489711935,2.954521473039992,1.0,0.037046,1.1981069518716578,0.8738913002453293,0.2113207547169811,0.1653499727734483,0.4742006964229186
"frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced', 'gender_male', 'test preparation course_none'})",frozenset({'reading_cat_χαμηλό'}),0.069,0.275,0.056,0.8115942028985507,2.9512516469038204,1.0,0.037025,3.848076923076922,0.7101618843025932,0.19444444444444442,0.7401299350324837,0.5076152832674572
frozenset({'reading_cat_χαμηλό'}),"frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced', 'gender_male', 'test preparation course_none'})",0.275,0.069,0.056,0.20363636363636362,2.9512516469038204,1.0,0.037025,1.1690639269406393,0.9119458128078819,0.19444444444444442,0.14461478371252803,0.5076152832674572
frozenset({'reading_cat_χαμηλό'}),"frozenset({'writing_cat_χαμηλό', 'test preparation course_none', 'lunch_standard'})",0.275,0.122,0.099,0.36,2.9508196721311477,1.0,0.06545000000000001,1.371875,0.9118773946360154,0.3322147651006711,0.27107061503416857,0.5857377049180328
"frozenset({'writing_cat_χαμηλό', 'test preparation course_none', 'lunch_standard'})",frozenset({'reading_cat_χαμηλό'}),0.122,0.275,0.099,0.8114754098360656,2.9508196721311473,1.0,0.06545000000000001,3.845652173913044,0.7529739306504684,0.3322147651006711,0.7399660825325043,0.5857377049180328
"frozenset({'math_cat_υψηλό', 'gender_male'})","frozenset({'reading_cat_υψηλό', 'lunch_standard'})",0.108,0.182,0.058,0.5370370370370371,2.950752950752951,1.0,0.038344,1.7668800000000002,0.741147363537962,0.25000000000000006,0.4340306076247397,0.42785917785917793
"frozenset({'reading_cat_υψηλό', 'lunch_standard'})","frozenset({'math_cat_υψηλό', 'gender_male'})",0.182,0.108,0.058,0.3186813186813187,2.950752950752951,1.0,0.038344,1.309225806451613,0.8081949245426187,0.25000000000000006,0.23618981914946047,0.42785917785917793
"frozenset({'math_cat_χαμηλό', 'reading_cat_μέτριο'})","frozenset({'writing_cat_μέτριο', 'gender_female'})",0.12,0.264,0.093,0.775,2.9356060606060606,1.0,0.06132,3.2711111111111113,0.749266862170088,0.31958762886597936,0.6942934782608696,0.5636363636363636
"frozenset({'writing_cat_μέτριο', 'gender_female'})","frozenset({'math_cat_χαμηλό', 'reading_cat_μέτριο'})",0.264,0.12,0.093,0.35227272727272724,2.9356060606060606,1.0,0.06132,1.3585964912280701,0.8958625525946705,0.31958762886597936,0.2639462809917355,0.5636363636363636
"frozenset({'reading_cat_χαμηλό', 'lunch_standard'})","frozenset({'math_cat_χαμηλό', 'test preparation course_none', 'writing_cat_χαμηλό'})",0.136,0.183,0.073,0.5367647058823529,2.933140469302475,1.0,0.048111999999999995,1.7636825396825395,0.762810755961441,0.2967479674796748,0.4330045359637123,0.46783590485374477
"frozenset({'math_cat_χαμηλό', 'test preparation course_none', 'writing_cat_χαμηλό'})","frozenset({'reading_cat_χαμηλό', 'lunch_standard'})",0.183,0.136,0.073,0.3989071038251366,2.933140469302475,1.0,0.048111999999999995,1.437381818181818,0.8066933820693818,0.2967479674796748,0.30429062942724144,0.46783590485374477
frozenset({'reading_cat_χαμηλό'}),"frozenset({'writing_cat_χαμηλό', 'parental level of education_high school', 'test preparation course_none'})",0.275,0.067,0.054,0.19636363636363635,2.930800542740841,1.0,0.035574999999999996,1.160972850678733,0.9086845466155811,0.18749999999999997,0.13865341518074634,0.5011668928086839
"frozenset({'writing_cat_χαμηλό', 'parental level of education_high school', 'test preparation course_none'})",frozenset({'reading_cat_χαμηλό'}),0.067,0.275,0.054,0.8059701492537313,2.930800542740841,1.0,0.035574999999999996,3.736538461538461,0.7061053550871341,0.18749999999999997,0.7323726196603191,0.5011668928086839
frozenset({'writing_cat_χαμηλό'}),"frozenset({'parental level of education_some college', 'reading_cat_χαμηλό'})",0.301,0.058,0.051,0.16943521594684385,2.9212968266697215,1.0,0.033541999999999995,1.1341679999999998,0.9408959578108782,0.16558441558441558,0.11829640758688308,0.5243727803872149
"frozenset({'parental level of education_some college', 'reading_cat_χαμηλό'})",frozenset({'writing_cat_χαμηλό'}),0.058,0.301,0.051,0.8793103448275861,2.9212968266697215,1.0,0.033541999999999995,5.79171428571428,0.6981807585029766,0.16558441558441558,0.8273395491095652,0.5243727803872149
frozenset({'math_cat_χαμηλό'}),"frozenset({'writing_cat_χαμηλό', 'gender_female'})",0.339,0.1,0.099,0.2920353982300885,2.920353982300885,1.0,0.06509999999999999,1.27125,0.9948196029890429,0.29117647058823526,0.21337266470009833,0.6410176991150442
"frozenset({'writing_cat_χαμηλό', 'gender_female'})",frozenset({'math_cat_χαμηλό'}),0.1,0.339,0.099,0.99,2.9203539823008846,1.0,0.06509999999999999,66.09999999999994,0.7306397306397304,0.29117647058823526,0.9848714069591528,0.6410176991150442
"frozenset({'gender_female', 'reading_cat_μέτριο', 'test preparation course_none'})","frozenset({'math_cat_χαμηλό', 'writing_cat_μέτριο'})",0.173,0.109,0.055,0.3179190751445087,2.9166887627936577,1.0,0.036143,1.3062966101694915,0.7946136088820491,0.24229074889867844,0.23447707648093008,0.4112531155539057
"frozenset({'math_cat_χαμηλό', 'writing_cat_μέτριο'})","frozenset({'gender_female', 'reading_cat_μέτριο', 'test preparation course_none'})",0.109,0.173,0.055,0.5045871559633027,2.9166887627936577,1.0,0.036143,1.6693148148148147,0.7375369860218345,0.24229074889867844,0.40095182099552934,0.4112531155539057
"frozenset({'writing_cat_χαμηλό', 'gender_female', 'test preparation course_none'})",frozenset({'math_cat_χαμηλό'}),0.086,0.339,0.085,0.988372093023256,2.915551896823764,1.0,0.05584600000000001,56.846000000000984,0.718831252413438,0.25,0.9824086127432012,0.6195547780750499
frozenset({'math_cat_χαμηλό'}),"frozenset({'writing_cat_χαμηλό', 'gender_female', 'test preparation course_none'})",0.339,0.086,0.085,0.25073746312684364,2.9155518968237635,1.0,0.05584600000000001,1.2198661417322836,0.9939663611284151,0.25,0.18023792464643723,0.6195547780750499
"frozenset({'writing_cat_χαμηλό', 'gender_female', 'reading_cat_χαμηλό'})",frozenset({'math_cat_χαμηλό'}),0.084,0.339,0.083,0.9880952380952381,2.9147352156201713,1.0,0.054524,55.5240000000002,0.717156836954806,0.24411764705882352,0.9819897701894676,0.6164664981036663
frozenset({'math_cat_χαμηλό'}),"frozenset({'writing_cat_χαμηλό', 'gender_female', 'reading_cat_χαμηλό'})",0.339,0.084,0.083,0.24483775811209438,2.914735215620171,1.0,0.054524,1.212984375,0.993820972239943,0.24411764705882352,0.17558707217477554,0.6164664981036663
"frozenset({'test preparation course_none', 'reading_cat_χαμηλό', 'lunch_standard'})","frozenset({'math_cat_χαμηλό', 'writing_cat_χαμηλό'})",0.109,0.23,0.073,0.6697247706422018,2.9118468288791384,1.0,0.04792999999999999,2.3313888888888887,0.7368971295911934,0.27443609022556387,0.5710711307041582,0.4935580374950139
"frozenset({'math_cat_χαμηλό', 'writing_cat_χαμηλό'})","frozenset({'test preparation course_none', 'reading_cat_χαμηλό', 'lunch_standard'})",0.23,0.109,0.073,0.31739130434782603,2.911846828879138,1.0,0.04792999999999999,1.3052866242038215,0.8526952499555239,0.27443609022556387,0.2338847411311179,0.4935580374950139
"frozenset({'writing_cat_χαμηλό', 'gender_female', 'test preparation course_none', 'reading_cat_χαμηλό'})",frozenset({'math_cat_χαμηλό'}),0.071,0.339,0.07,0.9859154929577466,2.9083052889609045,1.0,0.04593100000000001,46.93100000000044,0.7063047824081193,0.2058823529411765,0.97869212247768,0.5962025842369854
frozenset({'math_cat_χαμηλό'}),"frozenset({'writing_cat_χαμηλό', 'gender_female', 'test preparation course_none', 'reading_cat_χαμηλό'})",0.339,0.071,0.07,0.20648967551622419,2.9083052889609045,1.0,0.04593100000000001,1.1707472118959108,0.9926734385130754,0.2058823529411765,0.1458446453350099,0.5962025842369854
"frozenset({'reading_cat_χαμηλό', 'lunch_standard'})",frozenset({'writing_cat_χαμηλό'}),0.136,0.301,0.119,0.8749999999999999,2.9069767441860463,1.0,0.078064,5.591999999999995,0.7592592592592593,0.3742138364779874,0.8211731044349068,0.6351744186046511
frozenset({'writing_cat_χαμηλό'}),"frozenset({'reading_cat_χαμηλό', 'lunch_standard'})",0.301,0.136,0.119,0.3953488372093023,2.9069767441860463,1.0,0.078064,1.428923076923077,0.9384835479256078,0.3742138364779874,0.30017226528854435,0.6351744186046511
"frozenset({'math_cat_υψηλό', 'lunch_standard'})","frozenset({'test preparation course_completed', 'writing_cat_υψηλό'})",0.156,0.117,0.053,0.33974358974358976,2.903791365329827,1.0,0.034748,1.337359223300971,0.7768040776178129,0.24090909090909088,0.2522577460289805,0.39636752136752135
"frozenset({'test preparation course_completed', 'writing_cat_υψηλό'})","frozenset({'math_cat_υψηλό', 'lunch_standard'})",0.117,0.156,0.053,0.45299145299145294,2.9037913653298264,1.0,0.034748,1.5429374999999999,0.7424944977456783,0.24090909090909088,0.35188560780977834,0.39636752136752135
"frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced', 'gender_female'})",frozenset({'math_cat_χαμηλό'}),0.064,0.339,0.063,0.984375,2.9037610619469025,1.0,0.041303999999999993,42.304,0.7004477004477003,0.1852941176470588,0.9763615733736762,0.5851078539823009
frozenset({'math_cat_χαμηλό'}),"frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced', 'gender_female'})",0.339,0.064,0.063,0.18584070796460175,2.9037610619469025,1.0,0.041303999999999993,1.1496521739130434,0.9918593761256392,0.1852941176470588,0.13017169654337796,0.5851078539823009
frozenset({'writing_cat_χαμηλό'}),"frozenset({'math_cat_χαμηλό', 'gender_female', 'reading_cat_χαμηλό'})",0.301,0.095,0.083,0.27574750830564787,2.902605350585767,1.0,0.05440500000000001,1.2495642201834862,0.9377423858524226,0.26517571884984026,0.19972100365264958,0.5747158594159818
"frozenset({'math_cat_χαμηλό', 'gender_female', 'reading_cat_χαμηλό'})",frozenset({'writing_cat_χαμηλό'}),0.095,0.301,0.083,0.8736842105263158,2.902605350585767,1.0,0.05440500000000001,5.533750000000003,0.7242894228849099,0.26517571884984026,0.8192907160605377,0.5747158594159818
frozenset({'math_cat_χαμηλό'}),"frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced', 'gender_female', 'reading_cat_χαμηλό'})",0.339,0.057,0.056,0.16519174041297935,2.8981007089996376,1.0,0.036677,1.1296007067137808,0.9908417981413443,0.16470588235294117,0.11473143203921458,0.5738239403819283
"frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced', 'gender_female', 'reading_cat_χαμηλό'})",frozenset({'math_cat_χαμηλό'}),0.057,0.339,0.056,0.9824561403508771,2.8981007089996376,1.0,0.036677,37.676999999999886,0.694534918951674,0.16470588235294117,0.973458608700268,0.5738239403819283
frozenset({'math_cat_χαμηλό'}),"frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced', 'gender_female', 'test preparation course_none'})",0.339,0.052,0.051,0.15044247787610618,2.8931245745405034,1.0,0.033372,1.115875,0.9899439352140251,0.14999999999999997,0.10384227624061834,0.5656058543226684
"frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced', 'gender_female', 'test preparation course_none'})",frozenset({'math_cat_χαμηλό'}),0.052,0.339,0.051,0.9807692307692307,2.8931245745405034,1.0,0.033372,34.37199999999993,0.6902457185405808,0.14999999999999997,0.9709065518445243,0.5656058543226684
"frozenset({'test preparation course_completed', 'gender_female'})","frozenset({'writing_cat_υψηλό', 'math_cat_μέτριο'})",0.184,0.094,0.05,0.27173913043478265,2.8908418131359856,1.0,0.032704000000000004,1.2440597014925374,0.8015686274509805,0.21929824561403508,0.19618005566753052,0.401827012025902
"frozenset({'writing_cat_υψηλό', 'math_cat_μέτριο'})","frozenset({'test preparation course_completed', 'gender_female'})",0.094,0.184,0.05,0.5319148936170213,2.890841813135985,1.0,0.032704000000000004,1.7432727272727273,0.7219426048565121,0.21929824561403508,0.42636629119732994,0.401827012025902
"frozenset({'writing_cat_υψηλό', 'gender_female', 'lunch_standard'})",frozenset({'math_cat_υψηλό'}),0.116,0.176,0.059,0.5086206896551724,2.8898902821316614,1.0,0.03858399999999999,1.6769122807017545,0.7397806580259221,0.2532188841201717,0.40366588550384996,0.42192398119122254
frozenset({'math_cat_υψηλό'}),"frozenset({'writing_cat_υψηλό', 'gender_female', 'lunch_standard'})",0.176,0.116,0.059,0.3352272727272727,2.889890282131661,1.0,0.03858399999999999,1.3297777777777777,0.7936481816685863,0.2532188841201717,0.2479946524064171,0.42192398119122254
frozenset({'reading_cat_χαμηλό'}),"frozenset({'race/ethnicity_group C', 'writing_cat_χαμηλό', 'gender_male'})",0.275,0.063,0.05,0.18181818181818182,2.886002886002886,1.0,0.032675,1.1452222222222224,0.9013793103448278,0.1736111111111111,0.12680702435238186,0.4877344877344878
"frozenset({'race/ethnicity_group C', 'writing_cat_χαμηλό', 'gender_male'})",frozenset({'reading_cat_χαμηλό'}),0.063,0.275,0.05,0.7936507936507937,2.886002886002886,1.0,0.032675,3.5134615384615393,0.6974386339381003,0.1736111111111111,0.7153804050355775,0.4877344877344878
"frozenset({'race/ethnicity_group C', 'writing_cat_χαμηλό'})","frozenset({'gender_male', 'reading_cat_χαμηλό'})",0.098,0.177,0.05,0.5102040816326531,2.882508935777701,1.0,0.032654,1.6802916666666665,0.7240354767184036,0.2222222222222222,0.40486522677114595,0.39634497866943386
"frozenset({'gender_male', 'reading_cat_χαμηλό'})","frozenset({'race/ethnicity_group C', 'writing_cat_χαμηλό'})",0.177,0.098,0.05,0.2824858757062147,2.8825089357777007,1.0,0.032654,1.2571181102362206,0.793535844471446,0.2222222222222222,0.20452979568316484,0.39634497866943386
frozenset({'math_cat_χαμηλό'}),"frozenset({'gender_female', 'test preparation course_none', 'reading_cat_χαμηλό'})",0.339,0.079,0.077,0.2271386430678466,2.875172697061349,1.0,0.050219,1.191675572519084,0.9866789791146825,0.22580645161290322,0.16084543221264558,0.600911093685822
"frozenset({'gender_female', 'test preparation course_none', 'reading_cat_χαμηλό'})",frozenset({'math_cat_χαμηλό'}),0.079,0.339,0.077,0.9746835443037974,2.875172697061349,1.0,0.050219,26.109499999999976,0.70813768207905,0.22580645161290322,0.9616997644535512,0.600911093685822
frozenset({'math_cat_χαμηλό'}),"frozenset({'gender_female', 'reading_cat_χαμηλό'})",0.339,0.098,0.095,0.28023598820059,2.85955090000602,1.0,0.061778,1.2531885245901642,0.9838044430289035,0.27777777777777773,0.20203546363701771,0.6248118716513154
"frozenset({'gender_female', 'reading_cat_χαμηλό'})",frozenset({'math_cat_χαμηλό'}),0.098,0.339,0.095,0.9693877551020408,2.85955090000602,1.0,0.061778,21.592666666666645,0.7209476018205158,0.27777777777777773,0.9536879804872023,0.6248118716513154
"frozenset({'race/ethnicity_group C', 'writing_cat_χαμηλό'})",frozenset({'reading_cat_χαμηλό'}),0.098,0.275,0.077,0.7857142857142857,2.8571428571428568,1.0,0.05005,3.383333333333333,0.720620842572062,0.26013513513513514,0.7044334975369457,0.5328571428571428
frozenset({'reading_cat_χαμηλό'}),"frozenset({'race/ethnicity_group C', 'writing_cat_χαμηλό'})",0.275,0.098,0.077,0.27999999999999997,2.8571428571428568,1.0,0.05005,1.2527777777777778,0.8965517241379309,0.26013513513513514,0.20177383592017734,0.5328571428571428
"frozenset({'lunch_free/reduced', 'gender_female', 'reading_cat_χαμηλό'})",frozenset({'math_cat_χαμηλό'}),0.061,0.339,0.059,0.9672131147540983,2.853136031723004,1.0,0.038320999999999994,20.160499999999978,0.6917023158426743,0.17302052785923752,0.9503980556037797,0.5706272063446007
frozenset({'math_cat_χαμηλό'}),"frozenset({'lunch_free/reduced', 'gender_female', 'reading_cat_χαμηλό'})",0.339,0.061,0.059,0.17404129793510323,2.853136031723004,1.0,0.038320999999999994,1.1368607142857143,0.9826149388445856,0.17302052785923752,0.12038476883397574,0.5706272063446007
"frozenset({'gender_female', 'reading_cat_χαμηλό'})",frozenset({'writing_cat_χαμηλό'}),0.098,0.301,0.084,0.8571428571428572,2.84765068818225,1.0,0.05450200000000001,4.8930000000000025,0.7193274205469329,0.26666666666666666,0.7956264050684653,0.5681063122923589
frozenset({'writing_cat_χαμηλό'}),"frozenset({'gender_female', 'reading_cat_χαμηλό'})",0.301,0.098,0.084,0.2790697674418605,2.84765068818225,1.0,0.05450200000000001,1.2511612903225808,0.9282308059132093,0.26666666666666666,0.20074253596658592,0.5681063122923589
"frozenset({'test preparation course_none', 'reading_cat_χαμηλό'})","frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced', 'gender_male'})",0.214,0.092,0.056,0.26168224299065423,2.8443722064201546,1.0,0.036312,1.2298227848101266,0.8249727371864776,0.224,0.18687471695005972,0.43518894758228366
"frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced', 'gender_male'})","frozenset({'test preparation course_none', 'reading_cat_χαμηλό'})",0.092,0.214,0.056,0.6086956521739131,2.8443722064201546,1.0,0.036312,2.008666666666667,0.7141283826305852,0.224,0.5021573182874213,0.43518894758228366
frozenset({'math_cat_υψηλό'}),"frozenset({'gender_female', 'reading_cat_υψηλό', 'lunch_standard'})",0.176,0.12,0.06,0.34090909090909094,2.8409090909090913,1.0,0.03888,1.3351724137931036,0.7864077669902911,0.2542372881355932,0.2510330578512397,0.42045454545454547
"frozenset({'gender_female', 'reading_cat_υψηλό', 'lunch_standard'})",frozenset({'math_cat_υψηλό'}),0.12,0.176,0.06,0.5,2.8409090909090913,1.0,0.03888,1.6480000000000001,0.7363636363636363,0.2542372881355932,0.3932038834951456,0.42045454545454547
"frozenset({'math_cat_χαμηλό', 'test preparation course_none', 'writing_cat_χαμηλό'})","frozenset({'gender_male', 'reading_cat_χαμηλό'})",0.183,0.177,0.092,0.5027322404371585,2.8402951437127597,1.0,0.059609,1.655043956043956,0.7930525251450163,0.34328358208955223,0.39578644038536875,0.5112531258682967
"frozenset({'gender_male', 'reading_cat_χαμηλό'})","frozenset({'math_cat_χαμηλό', 'test preparation course_none', 'writing_cat_χαμηλό'})",0.177,0.183,0.092,0.519774011299435,2.8402951437127597,1.0,0.059609,1.7012823529411762,0.7872708542448097,0.34328358208955223,0.41220809216577114,0.5112531258682967
"frozenset({'writing_cat_χαμηλό', 'test preparation course_none'})","frozenset({'race/ethnicity_group C', 'reading_cat_χαμηλό'})",0.243,0.083,0.057,0.23456790123456792,2.826119291982746,1.0,0.036831,1.1980161290322582,0.8535771396787876,0.21189591078066913,0.1652866970933129,0.4606574445931876
"frozenset({'race/ethnicity_group C', 'reading_cat_χαμηλό'})","frozenset({'writing_cat_χαμηλό', 'test preparation course_none'})",0.083,0.243,0.057,0.6867469879518072,2.826119291982746,1.0,0.036831,2.416576923076923,0.7046432876083338,0.21189591078066913,0.5861915296589263,0.4606574445931876
"frozenset({'reading_cat_χαμηλό', 'race/ethnicity_group D'})",frozenset({'writing_cat_χαμηλό'}),0.066,0.301,0.056,0.8484848484848485,2.8188865398167726,1.0,0.036134,4.613400000000001,0.690845824411135,0.18006430868167203,0.783240126587766,0.5172656800563777
frozenset({'writing_cat_χαμηλό'}),"frozenset({'reading_cat_χαμηλό', 'race/ethnicity_group D'})",0.301,0.066,0.056,0.18604651162790697,2.818886539816772,1.0,0.036134,1.1474857142857142,0.9231044349070099,0.18006430868167203,0.1285294557043972,0.5172656800563777
"frozenset({'writing_cat_χαμηλό', 'gender_male'})","frozenset({'math_cat_χαμηλό', 'reading_cat_χαμηλό'})",0.201,0.218,0.123,0.6119402985074627,2.807065589483774,1.0,0.079182,2.0151538461538463,0.8057022497634239,0.41554054054054046,0.503759972515937,0.5880802409968506
"frozenset({'math_cat_χαμηλό', 'reading_cat_χαμηλό'})","frozenset({'writing_cat_χαμηλό', 'gender_male'})",0.218,0.201,0.123,0.5642201834862385,2.8070655894837735,1.0,0.079182,1.8334947368421048,0.8232175160626286,0.41554054054054046,0.4545934711967941,0.5880802409968506
"frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό'})","frozenset({'math_cat_χαμηλό', 'lunch_free/reduced', 'test preparation course_none'})",0.252,0.126,0.089,0.35317460317460314,2.8029730410682787,1.0,0.05724799999999999,1.3512147239263803,0.859941116385267,0.30795847750865046,0.259925175256983,0.5297619047619047
"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced', 'test preparation course_none'})","frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό'})",0.126,0.252,0.089,0.7063492063492063,2.8029730410682787,1.0,0.05724799999999999,2.547243243243243,0.7359679119635923,0.30795847750865046,0.6074187250657838,0.5297619047619047
"frozenset({'math_cat_χαμηλό', 'parental level of education_high school', 'test preparation course_none'})",frozenset({'writing_cat_χαμηλό'}),0.062,0.301,0.052,0.8387096774193548,2.7864108884363947,1.0,0.033338,4.333799999999998,0.6834918812530754,0.16720257234726688,0.7692556186256864,0.5057335762512056
frozenset({'writing_cat_χαμηλό'}),"frozenset({'math_cat_χαμηλό', 'parental level of education_high school', 'test preparation course_none'})",0.301,0.062,0.052,0.17275747508305647,2.7864108884363947,1.0,0.033338,1.1338875502008032,0.9171893914383185,0.16720257234726688,0.11807833164504955,0.5057335762512056
"frozenset({'writing_cat_υψηλό', 'gender_female', 'reading_cat_υψηλό'})","frozenset({'math_cat_υψηλό', 'lunch_standard'})",0.136,0.156,0.059,0.43382352941176466,2.780920060331825,1.0,0.037784,1.4907012987012986,0.7412115505335845,0.25321888412017163,0.3291747978812378,0.40601432880844646
"frozenset({'math_cat_υψηλό', 'lunch_standard'})","frozenset({'writing_cat_υψηλό', 'gender_female', 'reading_cat_υψηλό'})",0.156,0.136,0.059,0.3782051282051282,2.780920060331825,1.0,0.037784,1.3895257731958763,0.7587758052855652,0.25321888412017163,0.28033000949667614,0.40601432880844646
frozenset({'writing_cat_χαμηλό'}),"frozenset({'test preparation course_completed', 'reading_cat_χαμηλό'})",0.301,0.061,0.051,0.16943521594684385,2.7776264909318664,1.0,0.032639,1.1305560000000001,0.9155656540155405,0.1639871382636656,0.11547946320217663,0.5027503948586678
"frozenset({'test preparation course_completed', 'reading_cat_χαμηλό'})",frozenset({'writing_cat_χαμηλό'}),0.061,0.301,0.051,0.8360655737704917,2.7776264909318664,1.0,0.032639,4.263899999999999,0.6815552632128464,0.1639871382636656,0.7654729238490582,0.5027503948586678
frozenset({'reading_cat_υψηλό'}),"frozenset({'math_cat_υψηλό', 'test preparation course_none', 'lunch_standard'})",0.235,0.085,0.055,0.23404255319148937,2.7534418022528158,1.0,0.035025,1.1945833333333333,0.8324420677361853,0.20754716981132074,0.16288803627485174,0.4405506883604505
"frozenset({'math_cat_υψηλό', 'test preparation course_none', 'lunch_standard'})",frozenset({'reading_cat_υψηλό'}),0.085,0.235,0.055,0.6470588235294117,2.7534418022528158,1.0,0.035025,2.1674999999999995,0.6959761549925484,0.20754716981132074,0.5386389850057669,0.4405506883604505
"frozenset({'gender_male', 'reading_cat_χαμηλό'})","frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced'})",0.177,0.156,0.076,0.4293785310734463,2.7524264812400405,1.0,0.048388,1.479089108910891,0.7736138645520242,0.2957198443579767,0.3239082121723298,0.45827900912646674
"frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced'})","frozenset({'gender_male', 'reading_cat_χαμηλό'})",0.156,0.177,0.076,0.48717948717948717,2.7524264812400405,1.0,0.048388,1.60485,0.7543651783487155,0.2957198443579767,0.37688880580739637,0.45827900912646674
"frozenset({'reading_cat_χαμηλό', 'lunch_standard'})","frozenset({'writing_cat_χαμηλό', 'math_cat_χαμηλό'})",0.136,0.23,0.086,0.6323529411764705,2.749360613810741,1.0,0.05471999999999999,2.0943999999999994,0.7364341085271318,0.3071428571428571,0.5225362872421694,0.5031329923273656
"frozenset({'writing_cat_χαμηλό', 'math_cat_χαμηλό'})","frozenset({'reading_cat_χαμηλό', 'lunch_standard'})",0.23,0.136,0.086,0.3739130434782608,2.749360613810741,1.0,0.05471999999999999,1.38,0.8263364542434308,0.3071428571428571,0.27536231884057966,0.5031329923273656
"frozenset({'math_cat_χαμηλό', 'reading_cat_χαμηλό'})","frozenset({'writing_cat_χαμηλό', 'test preparation course_none', 'lunch_standard'})",0.218,0.122,0.073,0.3348623853211009,2.7447736501729585,1.0,0.046404,1.3200275862068966,0.8128788144203483,0.27340823970037453,0.24244007439760923,0.4666115205294029
"frozenset({'writing_cat_χαμηλό', 'test preparation course_none', 'lunch_standard'})","frozenset({'math_cat_χαμηλό', 'reading_cat_χαμηλό'})",0.122,0.218,0.073,0.5983606557377049,2.7447736501729585,1.0,0.046404,1.9470204081632654,0.7239991262832715,0.27340823970037453,0.48639470043184774,0.4666115205294029
"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced', 'writing_cat_χαμηλό'})","frozenset({'gender_male', 'reading_cat_χαμηλό'})",0.132,0.177,0.064,0.48484848484848486,2.7392569765451125,1.0,0.040636000000000005,1.5975882352941175,0.7314948156682028,0.2612244897959184,0.37405648219743,0.42321520287621983
"frozenset({'gender_male', 'reading_cat_χαμηλό'})","frozenset({'math_cat_χαμηλό', 'lunch_free/reduced', 'writing_cat_χαμηλό'})",0.177,0.132,0.064,0.36158192090395486,2.7392569765451125,1.0,0.040636000000000005,1.3596106194690265,0.7714914945321995,0.2612244897959184,0.2644953005805932,0.42321520287621983
"frozenset({'math_cat_υψηλό', 'test preparation course_none'})",frozenset({'reading_cat_υψηλό'}),0.092,0.235,0.059,0.6413043478260869,2.72895467160037,1.0,0.03738,2.1327272727272724,0.6977525573060553,0.2201492537313433,0.5311167945439045,0.4461840888066605
frozenset({'reading_cat_υψηλό'}),"frozenset({'math_cat_υψηλό', 'test preparation course_none'})",0.235,0.092,0.059,0.251063829787234,2.72895467160037,1.0,0.03738,1.2123863636363637,0.8281821203057493,0.2201492537313433,0.17518042928109473,0.4461840888066605
"frozenset({'writing_cat_χαμηλό', 'lunch_standard'})","frozenset({'math_cat_χαμηλό', 'reading_cat_χαμηλό'})",0.145,0.218,0.086,0.593103448275862,2.720658019614046,1.0,0.054389999999999994,1.9218644067796609,0.7396980824153406,0.31046931407942235,0.47967192874151154,0.49379943055994935
"frozenset({'math_cat_χαμηλό', 'reading_cat_χαμηλό'})","frozenset({'writing_cat_χαμηλό', 'lunch_standard'})",0.218,0.145,0.086,0.39449541284403666,2.720658019614046,1.0,0.054389999999999994,1.4120454545454544,0.8087491821804556,0.31046931407942235,0.29180750040238207,0.49379943055994935
"frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced', 'reading_cat_χαμηλό'})","frozenset({'math_cat_χαμηλό', 'test preparation course_none'})",0.133,0.246,0.089,0.669172932330827,2.7202151720765326,1.0,0.05628199999999999,2.279136363636363,0.7293910293793657,0.3068965517241379,0.5612373107835902,0.5154807751085029
"frozenset({'math_cat_χαμηλό', 'test preparation course_none'})","frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced', 'reading_cat_χαμηλό'})",0.246,0.133,0.089,0.36178861788617883,2.7202151720765326,1.0,0.05628199999999999,1.358484076433121,0.8387029475754775,0.3068965517241379,0.26388537241773796,0.5154807751085029
"frozenset({'writing_cat_χαμηλό', 'gender_male'})","frozenset({'lunch_free/reduced', 'reading_cat_χαμηλό'})",0.201,0.139,0.076,0.37810945273631835,2.7202118901893404,1.0,0.04806099999999999,1.384488,0.7914663065674198,0.28787878787878785,0.27771132721988195,0.4624360213321879
"frozenset({'lunch_free/reduced', 'reading_cat_χαμηλό'})","frozenset({'writing_cat_χαμηλό', 'gender_male'})",0.139,0.201,0.076,0.5467625899280575,2.7202118901893404,1.0,0.04806099999999999,1.7628730158730155,0.7344733785683721,0.28787878787878785,0.4327441676195964,0.4624360213321879
"frozenset({'test preparation course_none', 'reading_cat_χαμηλό'})","frozenset({'race/ethnicity_group C', 'writing_cat_χαμηλό'})",0.214,0.098,0.057,0.2663551401869159,2.71790959374404,1.0,0.036028000000000004,1.2294777070063696,0.8041605285478327,0.2235294117647059,0.1866464968812815,0.42399389662407017
"frozenset({'race/ethnicity_group C', 'writing_cat_χαμηλό'})","frozenset({'test preparation course_none', 'reading_cat_χαμηλό'})",0.098,0.214,0.057,0.5816326530612245,2.71790959374404,1.0,0.036028000000000004,1.878731707317073,0.700742988291127,0.2235294117647059,0.4677260217063925,0.42399389662407017
"frozenset({'math_cat_υψηλό', 'test preparation course_none'})",frozenset({'writing_cat_υψηλό'}),0.092,0.208,0.052,0.5652173913043478,2.717391304347826,1.0,0.032864,1.8215999999999999,0.6960352422907489,0.2096774193548387,0.4510320597277119,0.4076086956521739
frozenset({'writing_cat_υψηλό'}),"frozenset({'math_cat_υψηλό', 'test preparation course_none'})",0.208,0.092,0.052,0.25,2.717391304347826,1.0,0.032864,1.2106666666666668,0.7979797979797979,0.2096774193548387,0.17400881057268722,0.4076086956521739
frozenset({'math_cat_υψηλό'}),"frozenset({'writing_cat_υψηλό', 'gender_female', 'reading_cat_υψηλό'})",0.176,0.136,0.065,0.3693181818181819,2.7155748663101607,1.0,0.041064,1.369945945945946,0.7666915608663182,0.2631578947368421,0.270044191919192,0.42362967914438504
"frozenset({'writing_cat_υψηλό', 'gender_female', 'reading_cat_υψηλό'})",frozenset({'math_cat_υψηλό'}),0.136,0.176,0.065,0.4779411764705882,2.71557486631016,1.0,0.041064,1.5783661971830987,0.7311965811965813,0.2631578947368421,0.3664334665905197,0.42362967914438504
"frozenset({'math_cat_χαμηλό', 'test preparation course_none', 'reading_cat_χαμηλό'})","frozenset({'writing_cat_χαμηλό', 'gender_male'})",0.169,0.201,0.092,0.5443786982248521,2.7083517324619506,1.0,0.05803099999999999,1.7536493506493505,0.7590514309632187,0.3309352517985611,0.42976057349793756,0.5010450705054609
"frozenset({'writing_cat_χαμηλό', 'gender_male'})","frozenset({'math_cat_χαμηλό', 'test preparation course_none', 'reading_cat_χαμηλό'})",0.201,0.169,0.092,0.4577114427860696,2.70835173246195,1.0,0.05803099999999999,1.5323944954128437,0.7894514882733852,0.3309352517985611,0.3474265256150056,0.5010450705054609
"frozenset({'math_cat_χαμηλό', 'reading_cat_μέτριο', 'test preparation course_none'})","frozenset({'writing_cat_μέτριο', 'gender_female'})",0.077,0.264,0.055,0.7142857142857143,2.7056277056277054,1.0,0.034671999999999994,2.576,0.6829902491874321,0.1923076923076923,0.6118012422360248,0.46130952380952384
"frozenset({'writing_cat_μέτριο', 'gender_female'})","frozenset({'math_cat_χαμηλό', 'reading_cat_μέτριο', 'test preparation course_none'})",0.264,0.077,0.055,0.20833333333333331,2.7056277056277054,1.0,0.034671999999999994,1.1658947368421053,0.8565217391304346,0.1923076923076923,0.1422896352473817,0.46130952380952384
"frozenset({'writing_cat_χαμηλό', 'math_cat_μέτριο'})","frozenset({'gender_male', 'test preparation course_none'})",0.071,0.308,0.059,0.8309859154929577,2.6980062191329797,1.0,0.037132,4.094333333333333,0.6774552553319589,0.18437499999999998,0.7557599934869331,0.5112721785256996
"frozenset({'gender_male', 'test preparation course_none'})","frozenset({'writing_cat_χαμηλό', 'math_cat_μέτριο'})",0.308,0.071,0.059,0.19155844155844154,2.6980062191329797,1.0,0.037132,1.149124497991968,0.9094738904673264,0.18437499999999998,0.12977227293696614,0.5112721785256996
frozenset({'reading_cat_χαμηλό'}),"frozenset({'race/ethnicity_group C', 'writing_cat_χαμηλό', 'test preparation course_none'})",0.275,0.077,0.057,0.20727272727272728,2.69185360094451,1.0,0.035824999999999996,1.1643348623853211,0.866908650937689,0.19322033898305083,0.1411405495912538,0.4737662337662338
"frozenset({'race/ethnicity_group C', 'writing_cat_χαμηλό', 'test preparation course_none'})",frozenset({'reading_cat_χαμηλό'}),0.077,0.275,0.057,0.7402597402597403,2.6918536009445098,1.0,0.035824999999999996,2.7912500000000002,0.6809412480279788,0.19322033898305083,0.6417375727720556,0.4737662337662338
"frozenset({'writing_cat_χαμηλό', 'gender_male', 'test preparation course_none'})","frozenset({'math_cat_χαμηλό', 'reading_cat_χαμηλό'})",0.157,0.218,0.092,0.5859872611464968,2.6880149593876,1.0,0.057774,1.8888307692307693,0.744932693795451,0.3250883392226148,0.4705719452001238,0.504002804885175
"frozenset({'math_cat_χαμηλό', 'reading_cat_χαμηλό'})","frozenset({'writing_cat_χαμηλό', 'gender_male', 'test preparation course_none'})",0.218,0.157,0.092,0.4220183486238532,2.6880149593876,1.0,0.057774,1.4585238095238093,0.8030412543089068,0.3250883392226148,0.31437526527147475,0.504002804885175
frozenset({'math_cat_χαμηλό'}),"frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced', 'reading_cat_χαμηλό'})",0.339,0.133,0.12,0.3539823008849557,2.661521059285381,1.0,0.074913,1.3420684931506848,0.9444402420574886,0.3409090909090909,0.25488154658011036,0.62811896999135
"frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced', 'reading_cat_χαμηλό'})",frozenset({'math_cat_χαμηλό'}),0.133,0.339,0.12,0.9022556390977443,2.661521059285381,1.0,0.074913,6.762538461538458,0.720040369088812,0.3409090909090909,0.8521265341872077,0.62811896999135
"frozenset({'math_cat_χαμηλό', 'parental level of education_high school'})","frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό'})",0.085,0.252,0.057,0.6705882352941176,2.661064425770308,1.0,0.03558,2.2707142857142855,0.6821972965199884,0.20357142857142857,0.5596099402327775,0.4483893557422969
"frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό'})","frozenset({'math_cat_χαμηλό', 'parental level of education_high school'})",0.252,0.085,0.057,0.2261904761904762,2.661064425770308,1.0,0.03558,1.1824615384615385,0.8345060512243175,0.20357142857142857,0.15430653135571168,0.4483893557422969
"frozenset({'gender_male', 'test preparation course_none', 'reading_cat_χαμηλό'})","frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced'})",0.135,0.156,0.056,0.4148148148148148,2.6590693257359925,1.0,0.03494,1.442278481012658,0.7213047068538397,0.2382978723404255,0.30665262418816924,0.3868945868945869
"frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced'})","frozenset({'gender_male', 'test preparation course_none', 'reading_cat_χαμηλό'})",0.156,0.135,0.056,0.358974358974359,2.659069325735992,1.0,0.03494,1.3494000000000002,0.7392518618821936,0.2382978723404255,0.25892989476804507,0.3868945868945869
"frozenset({'writing_cat_χαμηλό', 'gender_male'})","frozenset({'lunch_free/reduced', 'test preparation course_none', 'reading_cat_χαμηλό'})",0.201,0.105,0.056,0.27860696517412936,2.653399668325042,1.0,0.034895,1.2406551724137933,0.779881101376721,0.224,0.19397426276439036,0.4059701492537313
"frozenset({'lunch_free/reduced', 'test preparation course_none', 'reading_cat_χαμηλό'})","frozenset({'writing_cat_χαμηλό', 'gender_male'})",0.105,0.201,0.056,0.5333333333333333,2.6533996683250414,1.0,0.034895,1.712142857142857,0.6962290502793296,0.224,0.4159365874009178,0.4059701492537313
"frozenset({'lunch_free/reduced', 'reading_cat_χαμηλό'})","frozenset({'math_cat_χαμηλό', 'test preparation course_none'})",0.139,0.246,0.09,0.6474820143884892,2.6320407088962976,1.0,0.055805999999999994,2.1388979591836734,0.7201703445605885,0.30508474576271183,0.532469515104097,0.5066678364625372
"frozenset({'math_cat_χαμηλό', 'test preparation course_none'})","frozenset({'lunch_free/reduced', 'reading_cat_χαμηλό'})",0.246,0.139,0.09,0.36585365853658536,2.632040708896297,1.0,0.055805999999999994,1.3577307692307692,0.8223695844385498,0.30508474576271183,0.2634769553270445,0.5066678364625372
"frozenset({'math_cat_υψηλό', 'gender_male', 'lunch_standard'})",frozenset({'reading_cat_υψηλό'}),0.094,0.235,0.058,0.6170212765957447,2.625622453598914,1.0,0.035910000000000004,1.9975,0.6833751998173099,0.21402214022140226,0.4993742177722153,0.4319148936170213
frozenset({'reading_cat_υψηλό'}),"frozenset({'math_cat_υψηλό', 'gender_male', 'lunch_standard'})",0.235,0.094,0.058,0.2468085106382979,2.625622453598914,1.0,0.035910000000000004,1.2028813559322034,0.8093306288032456,0.21402214022140226,0.1686628152740595,0.4319148936170213
"frozenset({'math_cat_χαμηλό', 'reading_cat_μέτριο'})","frozenset({'writing_cat_μέτριο', 'gender_female', 'test preparation course_none'})",0.12,0.175,0.055,0.45833333333333337,2.6190476190476195,1.0,0.034,1.523076923076923,0.7024793388429753,0.22916666666666669,0.34343434343434354,0.3863095238095239
"frozenset({'writing_cat_μέτριο', 'gender_female', 'test preparation course_none'})","frozenset({'math_cat_χαμηλό', 'reading_cat_μέτριο'})",0.175,0.12,0.055,0.31428571428571433,2.6190476190476195,1.0,0.034,1.2833333333333334,0.7493112947658402,0.22916666666666669,0.22077922077922085,0.3863095238095239
"frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό'})","frozenset({'math_cat_χαμηλό', 'lunch_free/reduced'})",0.252,0.182,0.12,0.47619047619047616,2.6164311878597593,1.0,0.074136,1.5616363636363637,0.8259358288770053,0.38216560509554137,0.3596460589125625,0.5677655677655677
"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced'})","frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό'})",0.182,0.252,0.12,0.6593406593406593,2.6164311878597593,1.0,0.074136,2.1957419354838708,0.7552567237163814,0.38216560509554137,0.5445730739848387,0.5677655677655677
"frozenset({'gender_male', 'reading_cat_χαμηλό'})","frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced', 'test preparation course_none'})",0.177,0.121,0.056,0.3163841807909605,2.6147452957930617,1.0,0.034583,1.2858099173553719,0.7503688595729909,0.23140495867768596,0.22228006915922693,0.38959704907316617
"frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced', 'test preparation course_none'})","frozenset({'gender_male', 'reading_cat_χαμηλό'})",0.121,0.177,0.056,0.4628099173553719,2.6147452957930617,1.0,0.034583,1.5320461538461538,0.7025637900211279,0.23140495867768596,0.3472781498850206,0.38959704907316617
"frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό'})","frozenset({'math_cat_χαμηλό', 'test preparation course_none'})",0.252,0.246,0.162,0.6428571428571429,2.613240418118467,1.0,0.10000800000000001,2.1112,0.8253119429590019,0.4821428571428572,0.526335733232285,0.6506968641114983
"frozenset({'math_cat_χαμηλό', 'test preparation course_none'})","frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό'})",0.246,0.252,0.162,0.6585365853658537,2.613240418118467,1.0,0.10000800000000001,2.190571428571429,0.8187444739168878,0.4821428571428572,0.5434981087778793,0.6506968641114983
"frozenset({'lunch_free/reduced', 'reading_cat_χαμηλό'})",frozenset({'math_cat_χαμηλό'}),0.139,0.339,0.123,0.8848920863309352,2.6103011396192777,1.0,0.07587899999999999,5.742437499999998,0.716495283419733,0.3464788732394366,0.8258579218319744,0.6238619723690074
frozenset({'math_cat_χαμηλό'}),"frozenset({'lunch_free/reduced', 'reading_cat_χαμηλό'})",0.339,0.139,0.123,0.3628318584070796,2.6103011396192777,1.0,0.07587899999999999,1.3512916666666663,0.9332865945906054,0.3464788732394366,0.25996731522308897,0.6238619723690074
"frozenset({'gender_male', 'lunch_standard'})","frozenset({'math_cat_υψηλό', 'writing_cat_μέτριο'})",0.316,0.062,0.051,0.16139240506329114,2.6031033074724377,1.0,0.03140799999999999,1.118520754716981,0.9003554638229561,0.15596330275229356,0.10596205230628054,0.4919865251122907
"frozenset({'math_cat_υψηλό', 'writing_cat_μέτριο'})","frozenset({'gender_male', 'lunch_standard'})",0.062,0.316,0.051,0.8225806451612903,2.6031033074724377,1.0,0.03140799999999999,3.8552727272727254,0.6565491868389146,0.15596330275229356,0.7406149783059799,0.4919865251122907
"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced', 'test preparation course_none'})",frozenset({'reading_cat_χαμηλό'}),0.126,0.275,0.09,0.7142857142857143,2.5974025974025974,1.0,0.055349999999999996,2.5375,0.7036613272311213,0.2893890675241157,0.6059113300492611,0.5207792207792208
frozenset({'reading_cat_χαμηλό'}),"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced', 'test preparation course_none'})",0.275,0.126,0.09,0.3272727272727272,2.597402597402597,1.0,0.055349999999999996,1.299189189189189,0.8482758620689654,0.2893890675241157,0.23028916163927599,0.5207792207792208
"frozenset({'gender_male', 'reading_cat_χαμηλό', 'lunch_standard'})","frozenset({'math_cat_χαμηλό', 'writing_cat_χαμηλό'})",0.099,0.23,0.059,0.5959595959595959,2.5911286780851994,1.0,0.03623,1.9057499999999998,0.6815402848059595,0.2185185185185185,0.47527220254492974,0.42624066754501533
"frozenset({'math_cat_χαμηλό', 'writing_cat_χαμηλό'})","frozenset({'gender_male', 'reading_cat_χαμηλό', 'lunch_standard'})",0.23,0.099,0.059,0.25652173913043474,2.5911286780851994,1.0,0.03623,1.2118713450292398,0.797490644948272,0.2185185185185185,0.17482989914587652,0.42624066754501533
"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced', 'reading_cat_χαμηλό'})","frozenset({'writing_cat_χαμηλό', 'gender_male'})",0.123,0.201,0.064,0.5203252032520326,2.588682603243943,1.0,0.039277,1.6657118644067797,0.6997755131128848,0.24615384615384614,0.3996560741577379,0.41936658172551877
"frozenset({'writing_cat_χαμηλό', 'gender_male'})","frozenset({'math_cat_χαμηλό', 'lunch_free/reduced', 'reading_cat_χαμηλό'})",0.201,0.123,0.064,0.31840796019900497,2.5886826032439427,1.0,0.039277,1.2866934306569342,0.7680890175219025,0.24615384615384614,0.2228140937274857,0.41936658172551877
frozenset({'writing_cat_χαμηλό'}),"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced', 'test preparation course_none'})",0.301,0.126,0.098,0.32558139534883723,2.5839793281653747,1.0,0.060074,1.2959310344827586,0.876967095851216,0.29787234042553196,0.2283539992549625,0.5516795865633075
"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced', 'test preparation course_none'})",frozenset({'writing_cat_χαμηλό'}),0.126,0.301,0.098,0.7777777777777778,2.5839793281653747,1.0,0.060074,3.1455000000000006,0.7013729977116705,0.29787234042553196,0.6820855189953902,0.5516795865633075
"frozenset({'math_cat_χαμηλό', 'parental level of education_high school'})",frozenset({'writing_cat_χαμηλό'}),0.085,0.301,0.066,0.7764705882352941,2.5796365057650967,1.0,0.040415000000000006,3.1271052631578953,0.6692333167743004,0.20625000000000002,0.6802154338130102,0.4978698456126637
frozenset({'writing_cat_χαμηλό'}),"frozenset({'math_cat_χαμηλό', 'parental level of education_high school'})",0.301,0.085,0.066,0.21926910299003324,2.5796365057650967,1.0,0.040415000000000006,1.1719787234042554,0.8760350283955434,0.20625000000000002,0.14674218906014563,0.4978698456126637
frozenset({'math_cat_χαμηλό'}),"frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced', 'test preparation course_none', 'reading_cat_χαμηλό'})",0.339,0.102,0.089,0.26253687315634217,2.5738909132974723,1.0,0.054422,1.217688,0.9250879668190858,0.2528409090909091,0.17877157367075966,0.5675429463820927
"frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced', 'test preparation course_none', 'reading_cat_χαμηλό'})",frozenset({'math_cat_χαμηλό'}),0.102,0.339,0.089,0.8725490196078431,2.5738909132974723,1.0,0.054422,5.186307692307693,0.6809389154425566,0.2528409090909091,0.8071845984990063,0.5675429463820927
"frozenset({'lunch_free/reduced', 'reading_cat_χαμηλό'})","frozenset({'writing_cat_χαμηλό', 'gender_male', 'test preparation course_none'})",0.139,0.157,0.056,0.4028776978417266,2.5660999862530356,1.0,0.034177,1.4117710843373492,0.7088310934129749,0.23333333333333328,0.29166986695341235,0.37978279796544934
"frozenset({'writing_cat_χαμηλό', 'gender_male', 'test preparation course_none'})","frozenset({'lunch_free/reduced', 'reading_cat_χαμηλό'})",0.157,0.139,0.056,0.356687898089172,2.5660999862530356,1.0,0.034177,1.3383861386138614,0.7239662769022199,0.23333333333333328,0.25283147280972357,0.37978279796544934
"frozenset({'math_cat_υψηλό', 'gender_male'})",frozenset({'reading_cat_υψηλό'}),0.108,0.235,0.065,0.6018518518518519,2.5610717100078806,1.0,0.03962,1.9213953488372093,0.6833390824422214,0.23381294964028781,0.4795449043815057,0.4392237982663515
frozenset({'reading_cat_υψηλό'}),"frozenset({'math_cat_υψηλό', 'gender_male'})",0.235,0.108,0.065,0.2765957446808511,2.56107171000788,1.0,0.03962,1.233058823529412,0.7967823026646556,0.23381294964028781,0.18900868237763574,0.4392237982663515
"frozenset({'math_cat_χαμηλό', 'test preparation course_none'})","frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced'})",0.246,0.156,0.098,0.3983739837398374,2.5536793829476756,1.0,0.059624,1.402864864864865,0.8069073783359498,0.3223684210526315,0.287172966516395,0.5132895559724828
"frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced'})","frozenset({'math_cat_χαμηλό', 'test preparation course_none'})",0.156,0.246,0.098,0.6282051282051282,2.5536793829476756,1.0,0.059624,2.028,0.7208627526840121,0.3223684210526315,0.5069033530571992,0.5132895559724828
"frozenset({'math_cat_χαμηλό', 'test preparation course_none'})","frozenset({'writing_cat_χαμηλό', 'parental level of education_high school'})",0.246,0.083,0.052,0.2113821138211382,2.5467724556763636,1.0,0.031582,1.16279381443299,0.8054988777800449,0.18772563176895304,0.14000230514846038,0.4189440689587618
"frozenset({'writing_cat_χαμηλό', 'parental level of education_high school'})","frozenset({'math_cat_χαμηλό', 'test preparation course_none'})",0.083,0.246,0.052,0.6265060240963854,2.5467724556763636,1.0,0.031582,2.0187741935483867,0.6623185974331013,0.18772563176895304,0.5046498993320762,0.4189440689587618
frozenset({'writing_cat_υψηλό'}),"frozenset({'test preparation course_completed', 'math_cat_μέτριο', 'gender_female'})",0.208,0.095,0.05,0.24038461538461542,2.530364372469636,1.0,0.030240000000000003,1.1913924050632911,0.7636363636363637,0.1976284584980237,0.16064598385040377,0.3833502024291498
"frozenset({'test preparation course_completed', 'math_cat_μέτριο', 'gender_female'})",frozenset({'writing_cat_υψηλό'}),0.095,0.208,0.05,0.5263157894736842,2.5303643724696356,1.0,0.030240000000000003,1.672,0.6682872928176795,0.1976284584980237,0.4019138755980861,0.3833502024291498
frozenset({'writing_cat_χαμηλό'}),"frozenset({'math_cat_χαμηλό', 'parental level of education_some high school'})",0.301,0.067,0.051,0.16943521594684385,2.528883820102147,1.0,0.030832999999999996,1.123332,0.8649050464248645,0.1608832807570978,0.10979122823884656,0.465314622898795
"frozenset({'math_cat_χαμηλό', 'parental level of education_some high school'})",frozenset({'writing_cat_χαμηλό'}),0.067,0.301,0.051,0.7611940298507461,2.5288838201021466,1.0,0.030832999999999996,2.9270624999999986,0.64798352352731,0.1608832807570978,0.6583605577263892,0.465314622898795
"frozenset({'lunch_free/reduced', 'test preparation course_none', 'reading_cat_χαμηλό'})",frozenset({'math_cat_χαμηλό'}),0.105,0.339,0.09,0.8571428571428571,2.528445006321112,1.0,0.054404999999999995,4.626999999999999,0.6754189944134078,0.2542372881355932,0.7838772422736112,0.561314791403287
frozenset({'math_cat_χαμηλό'}),"frozenset({'lunch_free/reduced', 'test preparation course_none', 'reading_cat_χαμηλό'})",0.339,0.105,0.09,0.2654867256637168,2.528445006321112,1.0,0.054404999999999995,1.2184939759036144,0.9145234493192133,0.2542372881355932,0.17931477727789585,0.561314791403287
"frozenset({'math_cat_χαμηλό', 'parental level of education_high school'})",frozenset({'reading_cat_χαμηλό'}),0.085,0.275,0.059,0.6941176470588234,2.524064171122994,1.0,0.03562499999999999,2.3701923076923066,0.6599055293136981,0.1960132890365448,0.5780933062880322,0.45433155080213894
frozenset({'reading_cat_χαμηλό'}),"frozenset({'math_cat_χαμηλό', 'parental level of education_high school'})",0.275,0.085,0.059,0.2145454545454545,2.524064171122994,1.0,0.03562499999999999,1.1649305555555556,0.8328462887200466,0.1960132890365448,0.14157973174366614,0.45433155080213894
"frozenset({'writing_cat_χαμηλό', 'test preparation course_none'})","frozenset({'math_cat_χαμηλό', 'parental level of education_high school'})",0.243,0.085,0.052,0.2139917695473251,2.5175502299685304,1.0,0.031345,1.1641099476439791,0.7962859465501474,0.18840579710144925,0.14097461152713125,0.412878237714839
"frozenset({'math_cat_χαμηλό', 'parental level of education_high school'})","frozenset({'writing_cat_χαμηλό', 'test preparation course_none'})",0.085,0.243,0.052,0.6117647058823529,2.5175502299685304,1.0,0.031345,1.9498484848484845,0.6587852038671711,0.18840579710144925,0.4871396378895018,0.412878237714839
frozenset({'reading_cat_χαμηλό'}),"frozenset({'math_cat_χαμηλό', 'test preparation course_none'})",0.275,0.246,0.169,0.6145454545454545,2.498152254249815,1.0,0.10135000000000001,1.956132075471698,0.827178126912875,0.4801136363636364,0.4887870749939715,0.6507686622320769
"frozenset({'math_cat_χαμηλό', 'test preparation course_none'})",frozenset({'reading_cat_χαμηλό'}),0.246,0.275,0.169,0.6869918699186992,2.498152254249815,1.0,0.10135000000000001,2.316233766233766,0.7953635835700721,0.4801136363636364,0.568264648163723,0.6507686622320769
"frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced'})",frozenset({'math_cat_χαμηλό'}),0.156,0.339,0.132,0.8461538461538461,2.496029044701611,1.0,0.079116,4.2965,0.7101464885825075,0.36363636363636365,0.7672524147561968,0.6177671885636488
frozenset({'math_cat_χαμηλό'}),"frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced'})",0.339,0.156,0.132,0.3893805309734513,2.496029044701611,1.0,0.079116,1.3822028985507244,0.9067528538027781,0.36363636363636365,0.2765172167931888,0.6177671885636488
"frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό', 'lunch_standard'})","frozenset({'math_cat_χαμηλό', 'test preparation course_none'})",0.119,0.246,0.073,0.6134453781512605,2.493680398988864,1.0,0.043726,1.9505652173913046,0.6798936451417288,0.25,0.4873280877337673,0.4550966728154677
"frozenset({'math_cat_χαμηλό', 'test preparation course_none'})","frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό', 'lunch_standard'})",0.246,0.119,0.073,0.2967479674796748,2.493680398988864,1.0,0.043726,1.2527514450867052,0.794411540278333,0.25,0.2017570572981553,0.4550966728154677
"frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced', 'gender_male', 'reading_cat_χαμηλό'})",frozenset({'math_cat_χαμηλό'}),0.076,0.339,0.064,0.8421052631578948,2.4840863219996896,1.0,0.038236,4.1863333333333355,0.6465773809523809,0.18233618233618232,0.7611274783024126,0.5154479118149355
frozenset({'math_cat_χαμηλό'}),"frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced', 'gender_male', 'reading_cat_χαμηλό'})",0.339,0.076,0.064,0.1887905604719764,2.4840863219996896,1.0,0.038236,1.13904,0.903838880484115,0.18233618233618232,0.1220677061385026,0.5154479118149355
"frozenset({'math_cat_χαμηλό', 'reading_cat_χαμηλό'})","frozenset({'writing_cat_χαμηλό', 'gender_male', 'lunch_standard'})",0.218,0.109,0.059,0.2706422018348624,2.482955980136352,1.0,0.035238,1.221622641509434,0.7637522215960814,0.22014925373134325,0.18141661260927316,0.40596330275229353
"frozenset({'writing_cat_χαμηλό', 'gender_male', 'lunch_standard'})","frozenset({'math_cat_χαμηλό', 'reading_cat_χαμηλό'})",0.109,0.218,0.059,0.5412844036697247,2.482955980136352,1.0,0.035238,1.70476,0.6703190093020601,0.22014925373134325,0.41340716581806236,0.40596330275229353
frozenset({'writing_cat_χαμηλό'}),"frozenset({'math_cat_χαμηλό', 'test preparation course_none'})",0.301,0.246,0.183,0.6079734219269103,2.4714366744996354,1.0,0.108954,1.9233389830508474,0.8517554351649897,0.5027472527472528,0.48007085136195,0.6759379304756503
"frozenset({'math_cat_χαμηλό', 'test preparation course_none'})",frozenset({'writing_cat_χαμηλό'}),0.246,0.301,0.183,0.7439024390243902,2.4714366744996354,1.0,0.108954,2.7294285714285715,0.7896247336609123,0.5027472527472528,0.6336229456715168,0.6759379304756503
frozenset({'reading_cat_χαμηλό'}),"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced'})",0.275,0.182,0.123,0.4472727272727272,2.4575424575424574,1.0,0.07294999999999999,1.4799342105263158,0.818054387440426,0.3682634730538922,0.3242942876194709,0.5615484515484516
"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced'})",frozenset({'reading_cat_χαμηλό'}),0.182,0.275,0.123,0.6758241758241759,2.4575424575424574,1.0,0.07294999999999999,2.236440677966102,0.7250482040272723,0.3682634730538922,0.5528609321712771,0.5615484515484516
"frozenset({'math_cat_υψηλό', 'lunch_standard'})","frozenset({'writing_cat_υψηλό', 'gender_female'})",0.156,0.154,0.059,0.3782051282051282,2.455877455877456,1.0,0.03497599999999999,1.3605773195876287,0.702385733793879,0.23505976095617528,0.26501788203915866,0.38066100566100564
"frozenset({'writing_cat_υψηλό', 'gender_female'})","frozenset({'math_cat_υψηλό', 'lunch_standard'})",0.154,0.156,0.059,0.3831168831168831,2.4558774558774554,1.0,0.03497599999999999,1.3681684210526315,0.700725247425572,0.23505976095617528,0.2690958330768757,0.38066100566100564
frozenset({'math_cat_χαμηλό'}),"frozenset({'race/ethnicity_group C', 'reading_cat_χαμηλό'})",0.339,0.083,0.069,0.20353982300884957,2.452287024203007,1.0,0.040863000000000003,1.1513444444444445,0.895941590475564,0.1954674220963173,0.13145018866832012,0.5174325621068344
"frozenset({'race/ethnicity_group C', 'reading_cat_χαμηλό'})",frozenset({'math_cat_χαμηλό'}),0.083,0.339,0.069,0.8313253012048193,2.4522870242030064,1.0,0.040863000000000003,3.9187857142857148,0.645820492153051,0.1954674220963173,0.7448189125640231,0.5174325621068344
frozenset({'math_cat_χαμηλό'}),"frozenset({'race/ethnicity_group C', 'writing_cat_χαμηλό', 'reading_cat_χαμηλό'})",0.339,0.077,0.064,0.1887905604719764,2.451825460675018,1.0,0.037897,1.1378072727272728,0.8958254538577912,0.1818181818181818,0.12111653355577075,0.5099796958204038
"frozenset({'race/ethnicity_group C', 'writing_cat_χαμηλό', 'reading_cat_χαμηλό'})",frozenset({'math_cat_χαμηλό'}),0.077,0.339,0.064,0.8311688311688312,2.451825460675018,1.0,0.037897,3.9151538461538475,0.6415391386782231,0.1818181818181818,0.7445821954142681,0.5099796958204038
frozenset({'reading_cat_χαμηλό'}),"frozenset({'math_cat_χαμηλό', 'race/ethnicity_group D'})",0.275,0.076,0.051,0.18545454545454543,2.4401913875598082,1.0,0.030099999999999995,1.134375,0.8140635564570656,0.16999999999999996,0.11845730027548206,0.4282535885167464
"frozenset({'math_cat_χαμηλό', 'race/ethnicity_group D'})",frozenset({'reading_cat_χαμηλό'}),0.076,0.275,0.051,0.6710526315789473,2.4401913875598082,1.0,0.030099999999999995,2.2039999999999997,0.6387403446226975,0.16999999999999996,0.5462794918330308,0.4282535885167464
"frozenset({'writing_cat_χαμηλό', 'parental level of education_high school', 'reading_cat_χαμηλό'})",frozenset({'math_cat_χαμηλό'}),0.069,0.339,0.057,0.8260869565217391,2.43683468000513,1.0,0.033609,3.8007500000000003,0.6333314489230595,0.16239316239316237,0.7368940340722225,0.4971142747210465
frozenset({'math_cat_χαμηλό'}),"frozenset({'writing_cat_χαμηλό', 'parental level of education_high school', 'reading_cat_χαμηλό'})",0.339,0.069,0.057,0.16814159292035397,2.4368346800051297,1.0,0.033609,1.11918085106383,0.8920296201926905,0.16239316239316237,0.106489358668479,0.4971142747210465
"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced'})","frozenset({'writing_cat_χαμηλό', 'test preparation course_none', 'reading_cat_χαμηλό'})",0.182,0.201,0.089,0.489010989010989,2.432890492591985,1.0,0.05241799999999999,1.5636344086021505,0.7200076920963708,0.3027210884353741,0.3604643166595607,0.4658985293313651
"frozenset({'writing_cat_χαμηλό', 'test preparation course_none', 'reading_cat_χαμηλό'})","frozenset({'math_cat_χαμηλό', 'lunch_free/reduced'})",0.201,0.182,0.089,0.4427860696517412,2.432890492591985,1.0,0.05241799999999999,1.4680178571428568,0.7371292767644949,0.3027210884353741,0.3188093761023731,0.4658985293313651
"frozenset({'lunch_free/reduced', 'gender_male', 'reading_cat_χαμηλό'})",frozenset({'math_cat_χαμηλό'}),0.078,0.339,0.064,0.8205128205128205,2.4203918009227743,1.0,0.037558,3.6827142857142854,0.636489967462039,0.18130311614730876,0.7284611505488963,0.5046516904923984
frozenset({'math_cat_χαμηλό'}),"frozenset({'lunch_free/reduced', 'gender_male', 'reading_cat_χαμηλό'})",0.339,0.078,0.064,0.1887905604719764,2.4203918009227743,1.0,0.037558,1.1365745454545455,0.8878120272314675,0.18130311614730876,0.12016329769194836,0.5046516904923984
"frozenset({'math_cat_υψηλό', 'lunch_standard'})","frozenset({'gender_female', 'reading_cat_υψηλό'})",0.156,0.159,0.06,0.3846153846153846,2.418964683115626,1.0,0.035196,1.366625,0.6950236966824644,0.23529411764705882,0.26827037409677124,0.3809869375907111
"frozenset({'gender_female', 'reading_cat_υψηλό'})","frozenset({'math_cat_υψηλό', 'lunch_standard'})",0.159,0.156,0.06,0.3773584905660377,2.418964683115626,1.0,0.035196,1.3555151515151513,0.6975029726516052,0.23529411764705882,0.2622730930877224,0.3809869375907111
"frozenset({'math_cat_χαμηλό', 'test preparation course_none', 'lunch_standard'})","frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό'})",0.12,0.252,0.073,0.6083333333333333,2.4140211640211637,1.0,0.04275999999999999,1.909787234042553,0.6656288916562889,0.24414715719063546,0.47638146167557927,0.44900793650793647
"frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό'})","frozenset({'math_cat_χαμηλό', 'test preparation course_none', 'lunch_standard'})",0.252,0.12,0.073,0.28968253968253965,2.4140211640211637,1.0,0.04275999999999999,1.2388826815642457,0.783092813713281,0.24414715719063546,0.19282106782106778,0.44900793650793647
frozenset({'math_cat_χαμηλό'}),"frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό'})",0.339,0.252,0.206,0.607669616519174,2.411387367139579,1.0,0.12057199999999998,1.9065563909774432,0.8854780194762274,0.535064935064935,0.475494139731516,0.7125649669897457
"frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό'})",frozenset({'math_cat_χαμηλό'}),0.252,0.339,0.206,0.8174603174603174,2.411387367139579,1.0,0.12057199999999998,3.6211304347826085,0.7824879289756501,0.535064935064935,0.72384314290517,0.7125649669897457
frozenset({'writing_cat_χαμηλό'}),"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced'})",0.301,0.182,0.132,0.4385382059800665,2.4095505823080576,1.0,0.07721800000000001,1.4569112426035504,0.836888195257294,0.3760683760683761,0.31361638872868763,0.5819064656273959
"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced'})",frozenset({'writing_cat_χαμηλό'}),0.182,0.301,0.132,0.7252747252747254,2.4095505823080576,1.0,0.07721800000000001,2.544360000000001,0.7151404015707193,0.3760683760683761,0.6069738559008946,0.5819064656273959
"frozenset({'race/ethnicity_group C', 'math_cat_χαμηλό', 'test preparation course_none'})",frozenset({'writing_cat_χαμηλό'}),0.08,0.301,0.058,0.725,2.4086378737541527,1.0,0.033920000000000006,2.541818181818182,0.6356821589205398,0.17956656346749225,0.6065808297567954,0.4588455149501661
frozenset({'writing_cat_χαμηλό'}),"frozenset({'race/ethnicity_group C', 'math_cat_χαμηλό', 'test preparation course_none'})",0.301,0.08,0.058,0.19269102990033224,2.4086378737541527,1.0,0.033920000000000006,1.1395884773662552,0.8366632134576488,0.17956656346749225,0.12249024989166547,0.4588455149501661
"frozenset({'reading_cat_μέτριο', 'race/ethnicity_group D'})","frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'gender_male'})",0.136,0.165,0.054,0.39705882352941174,2.406417112299465,1.0,0.03156,1.3848780487804875,0.676440329218107,0.21862348178137647,0.27791475871785837,0.36216577540106953
"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'gender_male'})","frozenset({'reading_cat_μέτριο', 'race/ethnicity_group D'})",0.165,0.136,0.054,0.32727272727272727,2.406417112299465,1.0,0.03156,1.2843243243243243,0.6999334664005322,0.21862348178137647,0.22138047138047137,0.36216577540106953
"frozenset({'race/ethnicity_group C', 'writing_cat_χαμηλό'})","frozenset({'math_cat_χαμηλό', 'test preparation course_none'})",0.098,0.246,0.058,0.5918367346938775,2.4058403849344616,1.0,0.033892000000000005,1.8473,0.6478324030889212,0.2027972027972028,0.4586694094083257,0.4138045462087274
"frozenset({'math_cat_χαμηλό', 'test preparation course_none'})","frozenset({'race/ethnicity_group C', 'writing_cat_χαμηλό'})",0.246,0.098,0.058,0.23577235772357724,2.4058403849344616,1.0,0.033892000000000005,1.180276595744681,0.7749931400347573,0.2027972027972028,0.15274097308600582,0.4138045462087274
frozenset({'writing_cat_υψηλό'}),"frozenset({'test preparation course_completed', 'gender_female', 'lunch_standard'})",0.208,0.114,0.057,0.27403846153846156,2.403846153846154,1.0,0.033288,1.220450331125828,0.7373737373737372,0.21509433962264152,0.18063031776350064,0.3870192307692308
"frozenset({'test preparation course_completed', 'gender_female', 'lunch_standard'})",frozenset({'writing_cat_υψηλό'}),0.114,0.208,0.057,0.5,2.4038461538461537,1.0,0.033288,1.584,0.6591422121896161,0.21509433962264152,0.36868686868686873,0.3870192307692308
"frozenset({'writing_cat_υψηλό', 'gender_female'})",frozenset({'math_cat_υψηλό'}),0.154,0.176,0.065,0.4220779220779221,2.398170011806376,1.0,0.037896,1.425797752808989,0.6891434806328424,0.24528301886792458,0.2986382549489346,0.39569805194805197
frozenset({'math_cat_υψηλό'}),"frozenset({'writing_cat_υψηλό', 'gender_female'})",0.176,0.154,0.065,0.3693181818181819,2.398170011806376,1.0,0.037896,1.3414054054054054,0.7075429424943988,0.24528301886792458,0.2545132172791748,0.39569805194805197
frozenset({'reading_cat_χαμηλό'}),"frozenset({'math_cat_χαμηλό', 'test preparation course_none', 'lunch_standard'})",0.275,0.12,0.079,0.28727272727272724,2.3939393939393936,1.0,0.046,1.2346938775510203,0.8031427324312527,0.25,0.19008264462809912,0.4728030303030303
"frozenset({'math_cat_χαμηλό', 'test preparation course_none', 'lunch_standard'})",frozenset({'reading_cat_χαμηλό'}),0.12,0.275,0.079,0.6583333333333333,2.3939393939393936,1.0,0.046,2.1219512195121952,0.6616800920598389,0.25,0.5287356321839081,0.4728030303030303
"frozenset({'reading_cat_μέτριο', 'gender_male'})","frozenset({'test preparation course_completed', 'writing_cat_μέτριο', 'math_cat_μέτριο'})",0.229,0.115,0.063,0.27510917030567683,2.3922536548319724,1.0,0.036665,1.2208734939759036,0.7548432256603463,0.22419928825622773,0.18091431672957836,0.41146762863109926
"frozenset({'test preparation course_completed', 'writing_cat_μέτριο', 'math_cat_μέτριο'})","frozenset({'reading_cat_μέτριο', 'gender_male'})",0.115,0.229,0.063,0.5478260869565217,2.3922536548319724,1.0,0.036665,1.7050961538461538,0.6576091830329119,0.22419928825622773,0.4135228105791462,0.41146762863109926
"frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced', 'test preparation course_none'})",frozenset({'math_cat_χαμηλό'}),0.121,0.339,0.098,0.8099173553719009,2.389136741510032,1.0,0.056981000000000004,3.477434782608697,0.6614775603073993,0.2707182320441989,0.7124317025293507,0.5495014505473074
frozenset({'math_cat_χαμηλό'}),"frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced', 'test preparation course_none'})",0.339,0.121,0.098,0.28908554572271383,2.3891367415100317,1.0,0.056981000000000004,1.2364356846473028,0.8796350612862391,0.2707182320441989,0.1912236015047939,0.5495014505473074
frozenset({'math_cat_χαμηλό'}),"frozenset({'parental level of education_high school', 'reading_cat_χαμηλό'})",0.339,0.073,0.059,0.17404129793510323,2.384127368974017,1.0,0.03425299999999999,1.122332142857143,0.8783045719121001,0.16713881019830026,0.10899816389978774,0.4911302380086475
"frozenset({'parental level of education_high school', 'reading_cat_χαμηλό'})",frozenset({'math_cat_χαμηλό'}),0.073,0.339,0.059,0.8082191780821918,2.384127368974017,1.0,0.03425299999999999,3.4466428571428573,0.6262775857970854,0.16713881019830026,0.709862599216629,0.4911302380086475
"frozenset({'math_cat_χαμηλό', 'test preparation course_none'})","frozenset({'writing_cat_χαμηλό', 'lunch_standard'})",0.246,0.145,0.085,0.3455284552845529,2.3829548640313996,1.0,0.049330000000000006,1.3063975155279504,0.7696988609767514,0.2777777777777778,0.234536205011173,0.46586767591813855
"frozenset({'writing_cat_χαμηλό', 'lunch_standard'})","frozenset({'math_cat_χαμηλό', 'test preparation course_none'})",0.145,0.246,0.085,0.5862068965517242,2.382954864031399,1.0,0.049330000000000006,1.822166666666667,0.6787753697970417,0.2777777777777778,0.4512027805725785,0.46586767591813855
"frozenset({'writing_cat_μέτριο', 'gender_male'})","frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'race/ethnicity_group D'})",0.227,0.1,0.054,0.23788546255506607,2.3788546255506606,1.0,0.031299999999999994,1.1809248554913294,0.7498442815389774,0.1978021978021978,0.15320606950562896,0.388942731277533
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'race/ethnicity_group D'})","frozenset({'writing_cat_μέτριο', 'gender_male'})",0.1,0.227,0.054,0.5399999999999999,2.37885462555066,1.0,0.031299999999999994,1.6804347826086954,0.6440329218106995,0.1978021978021978,0.40491591203104776,0.388942731277533
"frozenset({'writing_cat_χαμηλό', 'test preparation course_none', 'reading_cat_χαμηλό'})",frozenset({'math_cat_χαμηλό'}),0.201,0.339,0.162,0.8059701492537313,2.377493065645225,1.0,0.093861,3.4066923076923077,0.7251425392852177,0.4285714285714286,0.7064601350283378,0.6419231277242108
frozenset({'math_cat_χαμηλό'}),"frozenset({'writing_cat_χαμηλό', 'test preparation course_none', 'reading_cat_χαμηλό'})",0.339,0.201,0.162,0.47787610619469023,2.3774930656452247,1.0,0.093861,1.53028813559322,0.8765338712388636,0.4285714285714286,0.34652829310975,0.6419231277242108
"frozenset({'reading_cat_μέτριο', 'race/ethnicity_group D', 'gender_male'})","frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})",0.071,0.32,0.054,0.7605633802816902,2.376760563380282,1.0,0.03128,2.8400000000000007,0.6235298807957581,0.1602373887240356,0.6478873239436622,0.4646566901408451
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})","frozenset({'reading_cat_μέτριο', 'race/ethnicity_group D', 'gender_male'})",0.32,0.071,0.054,0.16874999999999998,2.3767605633802815,1.0,0.03128,1.117593984962406,0.851851851851852,0.1602373887240356,0.10522066738428416,0.4646566901408451
"frozenset({'test preparation course_completed', 'math_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'writing_cat_μέτριο', 'gender_male'})",0.117,0.227,0.063,0.5384615384615384,2.3720772619451034,1.0,0.036441,1.6748333333333332,0.6550719948228443,0.22419928825622773,0.4029256642451985,0.4079972890545578
"frozenset({'writing_cat_μέτριο', 'gender_male'})","frozenset({'test preparation course_completed', 'math_cat_μέτριο', 'reading_cat_μέτριο'})",0.227,0.117,0.063,0.2775330396475771,2.372077261945103,1.0,0.036441,1.222201219512195,0.7482905193125116,0.22419928825622773,0.18180412191118583,0.4079972890545578
"frozenset({'math_cat_χαμηλό', 'test preparation course_none'})","frozenset({'reading_cat_χαμηλό', 'lunch_standard'})",0.246,0.136,0.079,0.32113821138211385,2.3613103778096605,1.0,0.045544,1.2727185628742514,0.7645972534667428,0.2607260726072607,0.21428033724781695,0.4510102821616451
"frozenset({'reading_cat_χαμηλό', 'lunch_standard'})","frozenset({'math_cat_χαμηλό', 'test preparation course_none'})",0.136,0.246,0.079,0.5808823529411764,2.36131037780966,1.0,0.045544,1.799017543859649,0.6672526957337085,0.2607260726072607,0.44414105164612255,0.4510102821616451
"frozenset({'gender_female', 'reading_cat_υψηλό'})",frozenset({'math_cat_υψηλό'}),0.159,0.176,0.066,0.41509433962264153,2.358490566037736,1.0,0.03801600000000001,1.4087741935483873,0.6848989298454222,0.24535315985130116,0.2901630335226232,0.3950471698113208
frozenset({'math_cat_υψηλό'}),"frozenset({'gender_female', 'reading_cat_υψηλό'})",0.176,0.159,0.066,0.37500000000000006,2.358490566037736,1.0,0.03801600000000001,1.3456,0.6990291262135923,0.24535315985130116,0.25683709869203336,0.3950471698113208
"frozenset({'reading_cat_μέτριο', 'gender_male'})","frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'race/ethnicity_group D'})",0.229,0.1,0.054,0.23580786026200873,2.358078602620087,1.0,0.031099999999999996,1.1777142857142857,0.7469856367392035,0.19636363636363635,0.15089762251334302,0.3879039301310043
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'race/ethnicity_group D'})","frozenset({'reading_cat_μέτριο', 'gender_male'})",0.1,0.229,0.054,0.5399999999999999,2.358078602620087,1.0,0.031099999999999996,1.676086956521739,0.639917695473251,0.19636363636363635,0.4033722438391698,0.3879039301310043
"frozenset({'math_cat_χαμηλό', 'test preparation course_none', 'lunch_standard'})",frozenset({'writing_cat_χαμηλό'}),0.12,0.301,0.085,0.7083333333333334,2.353266888150609,1.0,0.04888000000000001,2.3965714285714292,0.6534759358288771,0.2529761904761905,0.5827372436814497,0.49536267995570327
frozenset({'writing_cat_χαμηλό'}),"frozenset({'math_cat_χαμηλό', 'test preparation course_none', 'lunch_standard'})",0.301,0.12,0.085,0.2823920265780731,2.353266888150609,1.0,0.04888000000000001,1.2262962962962962,0.8226878734326348,0.2529761904761905,0.18453639383871945,0.49536267995570327
"frozenset({'test preparation course_none', 'reading_cat_χαμηλό'})","frozenset({'math_cat_χαμηλό', 'lunch_standard'})",0.214,0.157,0.079,0.3691588785046729,2.351330436335496,1.0,0.045402,1.336311111111111,0.7311817566914677,0.2705479452054795,0.25167126750257757,0.43617179594023453
"frozenset({'math_cat_χαμηλό', 'lunch_standard'})","frozenset({'test preparation course_none', 'reading_cat_χαμηλό'})",0.157,0.214,0.079,0.5031847133757962,2.351330436335496,1.0,0.045402,1.5820769230769232,0.681742420829767,0.2705479452054795,0.3679194826664074,0.43617179594023453
frozenset({'math_cat_χαμηλό'}),"frozenset({'writing_cat_χαμηλό', 'race/ethnicity_group D'})",0.339,0.064,0.051,0.15044247787610618,2.350663716814159,1.0,0.029303999999999993,1.10175,0.8692711577823261,0.14488636363636362,0.0923530746539596,0.47365873893805305
"frozenset({'writing_cat_χαμηλό', 'race/ethnicity_group D'})",frozenset({'math_cat_χαμηλό'}),0.064,0.339,0.051,0.7968749999999999,2.3506637168141586,1.0,0.029303999999999993,3.2541538461538444,0.6138763197586726,0.14488636363636362,0.692700453857791,0.47365873893805305
frozenset({'math_cat_χαμηλό'}),"frozenset({'writing_cat_χαμηλό', 'parental level of education_high school'})",0.339,0.083,0.066,0.19469026548672566,2.3456658492376583,1.0,0.037863,1.1386923076923077,0.8678998762206023,0.18539325842696627,0.12179963520907923,0.49493549418914595
"frozenset({'writing_cat_χαμηλό', 'parental level of education_high school'})",frozenset({'math_cat_χαμηλό'}),0.083,0.339,0.066,0.7951807228915663,2.3456658492376583,1.0,0.037863,3.2272352941176474,0.6256072172102706,0.18539325842696627,0.6901372509705994,0.49493549418914595
"frozenset({'test preparation course_completed', 'reading_cat_μέτριο', 'gender_male'})","frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})",0.084,0.32,0.063,0.75,2.34375,1.0,0.03612,2.7199999999999998,0.6259097525473071,0.1847507331378299,0.6323529411764707,0.4734375
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})","frozenset({'test preparation course_completed', 'reading_cat_μέτριο', 'gender_male'})",0.32,0.084,0.063,0.196875,2.34375,1.0,0.03612,1.140544747081712,0.8431372549019609,0.1847507331378299,0.12322598253275108,0.4734375
frozenset({'math_cat_χαμηλό'}),frozenset({'reading_cat_χαμηλό'}),0.339,0.275,0.218,0.6430678466076696,2.3384285331187984,1.0,0.12477499999999998,2.0311983471074377,0.865903759941151,0.5505050505050503,0.5076797884243718,0.717897559667471
frozenset({'reading_cat_χαμηλό'}),frozenset({'math_cat_χαμηλό'}),0.275,0.339,0.218,0.7927272727272726,2.338428533118798,1.0,0.12477499999999998,3.189035087719297,0.7894653590635874,0.5505050505050503,0.6864255260624396,0.717897559667471
frozenset({'math_cat_χαμηλό'}),"frozenset({'test preparation course_none', 'reading_cat_χαμηλό'})",0.339,0.214,0.169,0.49852507374631266,2.329556419375293,1.0,0.09645400000000001,1.5673764705882354,0.8634398302732994,0.4401041666666667,0.3619911879724079,0.6441223499572685
"frozenset({'test preparation course_none', 'reading_cat_χαμηλό'})",frozenset({'math_cat_χαμηλό'}),0.214,0.339,0.169,0.7897196261682243,2.329556419375293,1.0,0.09645400000000001,3.143422222222223,0.7261243356369604,0.4401041666666667,0.681875379982185,0.6441223499572685
"frozenset({'lunch_free/reduced', 'test preparation course_none'})","frozenset({'writing_cat_χαμηλό', 'gender_female'})",0.224,0.1,0.052,0.23214285714285712,2.321428571428571,1.0,0.029599999999999994,1.172093023255814,0.733544805709754,0.19117647058823528,0.1468253968253968,0.3760714285714285
"frozenset({'writing_cat_χαμηλό', 'gender_female'})","frozenset({'lunch_free/reduced', 'test preparation course_none'})",0.1,0.224,0.052,0.5199999999999999,2.321428571428571,1.0,0.029599999999999994,1.6166666666666665,0.6324786324786323,0.19117647058823528,0.3814432989690721,0.3760714285714285
frozenset({'math_cat_χαμηλό'}),"frozenset({'race/ethnicity_group C', 'writing_cat_χαμηλό'})",0.339,0.098,0.077,0.2271386430678466,2.317741255794353,1.0,0.043778,1.167091603053435,0.8601292807041672,0.21388888888888885,0.1431692273479452,0.5064264643910661
"frozenset({'race/ethnicity_group C', 'writing_cat_χαμηλό'})",frozenset({'math_cat_χαμηλό'}),0.098,0.339,0.077,0.7857142857142857,2.317741255794353,1.0,0.043778,3.0846666666666667,0.6303164684539407,0.21388888888888885,0.675815863410417,0.5064264643910661
"frozenset({'math_cat_χαμηλό', 'lunch_standard'})","frozenset({'writing_cat_χαμηλό', 'test preparation course_none', 'reading_cat_χαμηλό'})",0.157,0.201,0.073,0.464968152866242,2.313274392369363,1.0,0.041442999999999994,1.493369047619047,0.6734428573749979,0.256140350877193,0.33037315752971463,0.414076116234116
"frozenset({'writing_cat_χαμηλό', 'test preparation course_none', 'reading_cat_χαμηλό'})","frozenset({'math_cat_χαμηλό', 'lunch_standard'})",0.201,0.157,0.073,0.36318407960199,2.313274392369363,1.0,0.041442999999999994,1.3237734374999999,0.710528571673496,0.256140350877193,0.24458372432027284,0.414076116234116
"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced'})","frozenset({'test preparation course_none', 'reading_cat_χαμηλό'})",0.182,0.214,0.09,0.4945054945054945,2.3107733388107223,1.0,0.051052,1.554913043478261,0.6934528660690029,0.2941176470588235,0.3568772194726394,0.45753312108452293
"frozenset({'test preparation course_none', 'reading_cat_χαμηλό'})","frozenset({'math_cat_χαμηλό', 'lunch_free/reduced'})",0.214,0.182,0.09,0.4205607476635514,2.310773338810722,1.0,0.051052,1.4117096774193547,0.7216850438224485,0.2941176470588235,0.29163905582341243,0.45753312108452293
"frozenset({'lunch_free/reduced', 'test preparation course_none'})","frozenset({'math_cat_χαμηλό', 'gender_female', 'writing_cat_χαμηλό'})",0.224,0.099,0.051,0.2276785714285714,2.2997835497835495,1.0,0.028823999999999995,1.1666127167630058,0.7283201940570041,0.18749999999999997,0.1428175043602346,0.37141504329004327
"frozenset({'math_cat_χαμηλό', 'gender_female', 'writing_cat_χαμηλό'})","frozenset({'lunch_free/reduced', 'test preparation course_none'})",0.099,0.224,0.051,0.5151515151515151,2.2997835497835495,1.0,0.028823999999999995,1.6005,0.6272768818959326,0.18749999999999997,0.37519525148391125,0.37141504329004327
"frozenset({'writing_cat_χαμηλό', 'parental level of education_high school', 'test preparation course_none'})",frozenset({'math_cat_χαμηλό'}),0.067,0.339,0.052,0.7761194029850745,2.289437766917624,1.0,0.029286999999999994,2.9524666666666657,0.6036565256822489,0.14689265536723162,0.6613001558019281,0.46475586668427765
frozenset({'math_cat_χαμηλό'}),"frozenset({'writing_cat_χαμηλό', 'parental level of education_high school', 'test preparation course_none'})",0.339,0.067,0.052,0.1533923303834808,2.2894377669176236,1.0,0.029286999999999994,1.1020452961672473,0.8520598161294075,0.14689265536723162,0.0925962812255957,0.46475586668427765
"frozenset({'test preparation course_completed', 'reading_cat_μέτριο'})","frozenset({'lunch_free/reduced', 'writing_cat_μέτριο'})",0.172,0.153,0.06,0.3488372093023256,2.279981760145919,1.0,0.033684,1.3007499999999999,0.6780193236714975,0.22641509433962267,0.231212761868153,0.3704970360237118
"frozenset({'lunch_free/reduced', 'writing_cat_μέτριο'})","frozenset({'test preparation course_completed', 'reading_cat_μέτριο'})",0.153,0.172,0.06,0.39215686274509803,2.279981760145919,1.0,0.033684,1.3621935483870968,0.6628099173553719,0.22641509433962267,0.26588993085156765,0.3704970360237118
frozenset({'math_cat_χαμηλό'}),"frozenset({'reading_cat_χαμηλό', 'race/ethnicity_group D'})",0.339,0.066,0.051,0.15044247787610618,2.279431482971306,1.0,0.028625999999999995,1.0993958333333333,0.8491590282103764,0.14406779661016947,0.09040950522067043,0.4615848753016894
"frozenset({'reading_cat_χαμηλό', 'race/ethnicity_group D'})",frozenset({'math_cat_χαμηλό'}),0.066,0.339,0.051,0.7727272727272726,2.2794314829713054,1.0,0.028625999999999995,2.9083999999999985,0.6009572994079858,0.14406779661016947,0.6561683399807452,0.4615848753016894
frozenset({'writing_cat_χαμηλό'}),frozenset({'math_cat_χαμηλό'}),0.301,0.339,0.23,0.7641196013289037,2.25404012191417,1.0,0.127961,2.802267605633803,0.795925856814082,0.5609756097560975,0.6431461442192188,0.7212928390125344
frozenset({'math_cat_χαμηλό'}),frozenset({'writing_cat_χαμηλό'}),0.339,0.301,0.23,0.6784660766961652,2.25404012191417,1.0,0.127961,2.1739541284403674,0.841682562652108,0.5609756097560975,0.5400086934136841,0.7212928390125344
"frozenset({'lunch_free/reduced', 'gender_female'})","frozenset({'math_cat_χαμηλό', 'reading_cat_μέτριο'})",0.189,0.12,0.051,0.2698412698412698,2.248677248677249,1.0,0.028319999999999998,1.2052173913043478,0.6847029810691231,0.19767441860465115,0.17027417027417027,0.3474206349206349
"frozenset({'math_cat_χαμηλό', 'reading_cat_μέτριο'})","frozenset({'lunch_free/reduced', 'gender_female'})",0.12,0.189,0.051,0.425,2.2486772486772484,1.0,0.028319999999999998,1.4104347826086956,0.6310160427807486,0.19767441860465115,0.2909987669543773,0.3474206349206349
frozenset({'math_cat_χαμηλό'}),"frozenset({'writing_cat_χαμηλό', 'parental level of education_some high school'})",0.339,0.067,0.051,0.15044247787610618,2.2454101175538232,1.0,0.028286999999999993,1.09821875,0.8391029634244014,0.1436619718309859,0.08943459579432601,0.45581825386342617
"frozenset({'writing_cat_χαμηλό', 'parental level of education_some high school'})",frozenset({'math_cat_χαμηλό'}),0.067,0.339,0.051,0.7611940298507461,2.2454101175538232,1.0,0.028286999999999993,2.7679374999999986,0.5944770191034612,0.1436619718309859,0.6387201661887233,0.45581825386342617
"frozenset({'writing_cat_χαμηλό', 'gender_male', 'reading_cat_χαμηλό'})","frozenset({'math_cat_χαμηλό', 'lunch_standard'})",0.168,0.157,0.059,0.35119047619047616,2.2368820139520773,1.0,0.032624,1.2993027522935778,0.6646023468057367,0.22180451127819548,0.2303564367621307,0.3634933272672126
"frozenset({'math_cat_χαμηλό', 'lunch_standard'})","frozenset({'writing_cat_χαμηλό', 'gender_male', 'reading_cat_χαμηλό'})",0.157,0.168,0.059,0.375796178343949,2.2368820139520773,1.0,0.032624,1.3328979591836732,0.6559301928142027,0.22180451127819548,0.24975502204801564,0.3634933272672126
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'gender_female'})","frozenset({'writing_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",0.15,0.221,0.074,0.49333333333333335,2.2322775263951735,1.0,0.04085,1.5375000000000003,0.6494435612082671,0.24915824915824916,0.3495934959349593,0.41408748114630467
"frozenset({'writing_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'gender_female'})",0.221,0.15,0.074,0.334841628959276,2.2322775263951735,1.0,0.04085,1.2778911564625848,0.7086354647330257,0.24915824915824916,0.21746073995208942,0.41408748114630467
"frozenset({'math_cat_χαμηλό', 'race/ethnicity_group D'})",frozenset({'writing_cat_χαμηλό'}),0.076,0.301,0.051,0.6710526315789473,2.2294107361426825,1.0,0.028124,2.12496,0.5968084203378321,0.15644171779141103,0.52940290640765,0.4202439237628956
frozenset({'writing_cat_χαμηλό'}),"frozenset({'math_cat_χαμηλό', 'race/ethnicity_group D'})",0.301,0.076,0.051,0.16943521594684385,2.2294107361426825,1.0,0.028124,1.1124960000000002,0.788914135038851,0.15644171779141103,0.10112036357883533,0.4202439237628956
"frozenset({'writing_cat_χαμηλό', 'test preparation course_none'})","frozenset({'math_cat_χαμηλό', 'lunch_standard'})",0.243,0.157,0.085,0.3497942386831276,2.2279887814211947,1.0,0.04684900000000001,1.2965126582278481,0.7280907607428706,0.2698412698412699,0.22870016451142064,0.4455977562842389
"frozenset({'math_cat_χαμηλό', 'lunch_standard'})","frozenset({'writing_cat_χαμηλό', 'test preparation course_none'})",0.157,0.243,0.085,0.5414012738853503,2.2279887814211947,1.0,0.04684900000000001,1.6506805555555555,0.6538134114855908,0.2698412698412699,0.3941892653703439,0.4455977562842389
"frozenset({'math_cat_χαμηλό', 'test preparation course_none'})","frozenset({'writing_cat_χαμηλό', 'gender_male', 'reading_cat_χαμηλό'})",0.246,0.168,0.092,0.37398373983739835,2.2260936895083234,1.0,0.050671999999999995,1.329038961038961,0.7304809133894591,0.28571428571428564,0.2475766103814884,0.46080139372822293
"frozenset({'writing_cat_χαμηλό', 'gender_male', 'reading_cat_χαμηλό'})","frozenset({'math_cat_χαμηλό', 'test preparation course_none'})",0.168,0.246,0.092,0.5476190476190476,2.2260936895083234,1.0,0.050671999999999995,1.6667368421052629,0.6619983277591972,0.28571428571428564,0.40002526209422756,0.46080139372822293
"frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced'})","frozenset({'math_cat_χαμηλό', 'gender_female', 'test preparation course_none'})",0.156,0.147,0.051,0.3269230769230769,2.2239665096807957,1.0,0.028068,1.2673142857142856,0.6520769445218846,0.20238095238095236,0.2109297502029038,0.3369309262166405
"frozenset({'math_cat_χαμηλό', 'gender_female', 'test preparation course_none'})","frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced'})",0.147,0.156,0.051,0.3469387755102041,2.2239665096807952,1.0,0.028068,1.2923749999999998,0.6451968829735881,0.20238095238095236,0.22623077667085792,0.3369309262166405
frozenset({'math_cat_χαμηλό'}),"frozenset({'race/ethnicity_group C', 'writing_cat_χαμηλό', 'test preparation course_none'})",0.339,0.077,0.058,0.1710914454277286,2.2219668237367354,1.0,0.031897,1.1135124555160143,0.8319943659032814,0.16201117318435754,0.10194089428789666,0.46216909933724093
"frozenset({'race/ethnicity_group C', 'writing_cat_χαμηλό', 'test preparation course_none'})",frozenset({'math_cat_χαμηλό'}),0.077,0.339,0.058,0.7532467532467533,2.2219668237367354,1.0,0.031897,2.678789473684211,0.5958269510964994,0.16201117318435754,0.6266970548362378,0.46216909933724093
frozenset({'math_cat_χαμηλό'}),"frozenset({'writing_cat_χαμηλό', 'test preparation course_none'})",0.339,0.243,0.183,0.5398230088495575,2.221493863578426,1.0,0.10062299999999999,1.6450192307692306,0.831849408496813,0.4586466165413533,0.3921043710033784,0.646454714301322
"frozenset({'writing_cat_χαμηλό', 'test preparation course_none'})",frozenset({'math_cat_χαμηλό'}),0.243,0.339,0.183,0.7530864197530864,2.221493863578426,1.0,0.10062299999999999,2.6770500000000004,0.726357277432488,0.4586466165413533,0.6264544928185876,0.646454714301322
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'gender_male'})","frozenset({'writing_cat_μέτριο', 'race/ethnicity_group D'})",0.175,0.139,0.054,0.3085714285714286,2.219938335046249,1.0,0.029675,1.2452479338842977,0.6661054994388328,0.20769230769230768,0.19694707151153149,0.348530318602261
"frozenset({'writing_cat_μέτριο', 'race/ethnicity_group D'})","frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'gender_male'})",0.139,0.175,0.054,0.3884892086330935,2.2199383350462485,1.0,0.029675,1.3491176470588235,0.638254398417,0.20769230769230768,0.25877479834314365,0.348530318602261
"frozenset({'test preparation course_completed', 'reading_cat_μέτριο'})","frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'gender_male'})",0.172,0.165,0.063,0.3662790697674419,2.2198731501057085,1.0,0.03462,1.3176146788990826,0.6636760984587071,0.2299270072992701,0.24105277816460108,0.37404862579281184
"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'gender_male'})","frozenset({'test preparation course_completed', 'reading_cat_μέτριο'})",0.165,0.172,0.063,0.3818181818181818,2.219873150105708,1.0,0.03462,1.3394117647058825,0.6581123467351012,0.2299270072992701,0.25340360122968814,0.37404862579281184
"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced'})","frozenset({'writing_cat_χαμηλό', 'test preparation course_none'})",0.182,0.243,0.098,0.5384615384615385,2.215891104779994,1.0,0.05377400000000001,1.640166666666667,0.6707998602864128,0.29969418960244654,0.3903058632252821,0.4708768597657487
"frozenset({'writing_cat_χαμηλό', 'test preparation course_none'})","frozenset({'math_cat_χαμηλό', 'lunch_free/reduced'})",0.243,0.182,0.098,0.4032921810699589,2.215891104779994,1.0,0.05377400000000001,1.3708551724137934,0.7248537459898095,0.29969418960244654,0.27052833871633114,0.4708768597657487
"frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced', 'gender_male'})",frozenset({'math_cat_χαμηλό'}),0.092,0.339,0.069,0.7500000000000001,2.212389380530974,1.0,0.037812000000000005,2.6440000000000015,0.6035242290748898,0.19060773480662982,0.62178517397882,0.4767699115044248
frozenset({'math_cat_χαμηλό'}),"frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced', 'gender_male'})",0.339,0.092,0.069,0.20353982300884957,2.212389380530974,1.0,0.037812000000000005,1.1400444444444444,0.8290468986384266,0.19060773480662982,0.12284121476745546,0.4767699115044248
frozenset({'reading_cat_χαμηλό'}),"frozenset({'math_cat_χαμηλό', 'lunch_standard'})",0.275,0.157,0.095,0.3454545454545454,2.200347423277359,1.0,0.051824999999999996,1.2879166666666666,0.7524500907441016,0.28189910979228483,0.22355224846328045,0.4752750434279096
"frozenset({'math_cat_χαμηλό', 'lunch_standard'})",frozenset({'reading_cat_χαμηλό'}),0.157,0.275,0.095,0.6050955414012739,2.200347423277359,1.0,0.051824999999999996,1.8358870967741934,0.6471249297621277,0.28189910979228483,0.4553041950362398,0.4752750434279096
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'writing_cat_μέτριο', 'gender_male', 'race/ethnicity_group D'})",0.325,0.076,0.054,0.16615384615384615,2.1862348178137654,1.0,0.0293,1.1081180811808118,0.803840877914952,0.15561959654178673,0.09756909756909757,0.4383400809716599
"frozenset({'writing_cat_μέτριο', 'gender_male', 'race/ethnicity_group D'})","frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο'})",0.076,0.325,0.054,0.7105263157894737,2.186234817813765,1.0,0.0293,2.331818181818182,0.5872214205547538,0.15561959654178673,0.5711500974658869,0.4383400809716599
frozenset({'writing_cat_χαμηλό'}),"frozenset({'lunch_free/reduced', 'gender_male', 'test preparation course_none'})",0.301,0.105,0.069,0.22923588039867113,2.1831988609397253,1.0,0.037395000000000005,1.1611853448275864,0.7753312185109161,0.2047477744807122,0.13881103955158786,0.4431893687707642
"frozenset({'lunch_free/reduced', 'gender_male', 'test preparation course_none'})",frozenset({'writing_cat_χαμηλό'}),0.105,0.301,0.069,0.6571428571428573,2.1831988609397253,1.0,0.037395000000000005,2.0387500000000007,0.6055380131163469,0.2047477744807122,0.5095033721643165,0.4431893687707642
"frozenset({'writing_cat_χαμηλό', 'race/ethnicity_group B'})",frozenset({'math_cat_χαμηλό'}),0.069,0.339,0.051,0.7391304347826085,2.180325766320379,1.0,0.027608999999999995,2.533833333333332,0.5814746951412142,0.14285714285714282,0.6053410511083336,0.44478645632935737
frozenset({'math_cat_χαμηλό'}),"frozenset({'writing_cat_χαμηλό', 'race/ethnicity_group B'})",0.339,0.069,0.051,0.15044247787610618,2.180325766320379,1.0,0.027608999999999995,1.0958645833333334,0.8189908338524516,0.14285714285714282,0.08747849395929772,0.44478645632935737
frozenset({'math_cat_χαμηλό'}),"frozenset({'writing_cat_χαμηλό', 'test preparation course_none', 'reading_cat_χαμηλό', 'lunch_standard'})",0.339,0.099,0.073,0.21533923303834807,2.175143768064122,1.0,0.039438999999999995,1.1482669172932332,0.817337782106812,0.19999999999999996,0.12912234521459273,0.4763564852060427
"frozenset({'writing_cat_χαμηλό', 'test preparation course_none', 'reading_cat_χαμηλό', 'lunch_standard'})",frozenset({'math_cat_χαμηλό'}),0.099,0.339,0.073,0.7373737373737373,2.175143768064122,1.0,0.039438999999999995,2.5168846153846154,0.5996229455855746,0.19999999999999996,0.6026834150888613,0.4763564852060427
"frozenset({'math_cat_χαμηλό', 'lunch_standard'})","frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό'})",0.157,0.252,0.086,0.5477707006369427,2.1736932564958042,1.0,0.04643599999999999,1.6540281690140846,0.6405142210819609,0.26625386996904016,0.39541537518307845,0.44452027095339197
"frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό'})","frozenset({'math_cat_χαμηλό', 'lunch_standard'})",0.252,0.157,0.086,0.3412698412698412,2.173693256495804,1.0,0.04643599999999999,1.2797349397590358,0.7218629523691082,0.26625386996904016,0.21858818655971676,0.44452027095339197
"frozenset({'math_cat_χαμηλό', 'race/ethnicity_group B'})",frozenset({'writing_cat_χαμηλό'}),0.078,0.301,0.051,0.6538461538461539,2.17224635829287,1.0,0.027521999999999998,2.0193333333333334,0.5853004976394027,0.15548780487804875,0.5047870584351271,0.41164068489649885
"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced', 'gender_female', 'test preparation course_none'})",frozenset({'writing_cat_χαμηλό'}),0.078,0.301,0.051,0.6538461538461539,2.17224635829287,1.0,0.027521999999999998,2.0193333333333334,0.5853004976394027,0.15548780487804875,0.5047870584351271,0.41164068489649885
frozenset({'writing_cat_χαμηλό'}),"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced', 'gender_female', 'test preparation course_none'})",0.301,0.078,0.051,0.16943521594684385,2.17224635829287,1.0,0.027521999999999998,1.110088,0.772027265841959,0.15548780487804875,0.09917051621132739,0.41164068489649885
frozenset({'writing_cat_χαμηλό'}),"frozenset({'math_cat_χαμηλό', 'race/ethnicity_group B'})",0.301,0.078,0.051,0.16943521594684385,2.17224635829287,1.0,0.027521999999999998,1.110088,0.772027265841959,0.15548780487804875,0.09917051621132739,0.41164068489649885
frozenset({'writing_cat_χαμηλό'}),"frozenset({'race/ethnicity_group C', 'math_cat_χαμηλό'})",0.301,0.118,0.077,0.2558139534883721,2.16791486007095,1.0,0.041482000000000005,1.1851875,0.770711405904539,0.22514619883040937,0.15625164794600013,0.45417816318486404
"frozenset({'race/ethnicity_group C', 'math_cat_χαμηλό'})",frozenset({'writing_cat_χαμηλό'}),0.118,0.301,0.077,0.652542372881356,2.16791486007095,1.0,0.041482000000000005,2.011756097560976,0.6108018965161823,0.22514619883040937,0.5029218496156738,0.45417816318486404
"frozenset({'writing_cat_μέτριο', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none', 'gender_female'})",0.338,0.101,0.074,0.21893491124260353,2.167672388540629,1.0,0.039861999999999995,1.1509924242424243,0.8137094798726219,0.20273972602739723,0.13118455088165018,0.47580408928466805
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none', 'gender_female'})","frozenset({'writing_cat_μέτριο', 'lunch_standard'})",0.101,0.338,0.074,0.7326732673267325,2.1676723885406286,1.0,0.039861999999999995,2.476370370370369,0.5991942999729428,0.20273972602739723,0.5961831832730099,0.47580408928466805
"frozenset({'writing_cat_χαμηλό', 'gender_male', 'reading_cat_χαμηλό'})",frozenset({'math_cat_χαμηλό'}),0.168,0.339,0.123,0.7321428571428571,2.1597134428992835,1.0,0.066048,2.467733333333333,0.6454033771106942,0.3203125,0.5947698292630212,0.5474873577749684
frozenset({'math_cat_χαμηλό'}),"frozenset({'writing_cat_χαμηλό', 'gender_male', 'reading_cat_χαμηλό'})",0.339,0.168,0.123,0.3628318584070796,2.159713442899283,1.0,0.066048,1.3057777777777775,0.812368547286078,0.3203125,0.23417290673927837,0.5474873577749684
"frozenset({'race/ethnicity_group C', 'math_cat_χαμηλό'})","frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό'})",0.118,0.252,0.064,0.5423728813559322,2.1522733387140165,1.0,0.034264,1.6345185185185185,0.6070011337868482,0.20915032679738563,0.38819903924589866,0.3981705676620931
"frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό'})","frozenset({'race/ethnicity_group C', 'math_cat_χαμηλό'})",0.252,0.118,0.064,0.25396825396825395,2.1522733387140165,1.0,0.034264,1.1822553191489362,0.7157419786096257,0.20915032679738563,0.1541590181045963,0.3981705676620931
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'gender_female', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'test preparation course_none'})",0.112,0.308,0.074,0.6607142857142857,2.1451762523191094,1.0,0.039504,2.0395789473684207,0.6011687363038715,0.2138728323699422,0.5097027250206441,0.450487012987013
"frozenset({'writing_cat_μέτριο', 'test preparation course_none'})","frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'gender_female', 'lunch_standard'})",0.308,0.112,0.074,0.24025974025974026,2.1451762523191094,1.0,0.039504,1.168820512820513,0.7714419621934073,0.2138728323699422,0.14443664443664442,0.450487012987013
"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'gender_female'})","frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",0.155,0.223,0.074,0.4774193548387097,2.1408939678865906,1.0,0.039435,1.4868518518518519,0.6306572845034384,0.24342105263157895,0.3274380371154565,0.4046289599305656
"frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'gender_female'})",0.223,0.155,0.074,0.3318385650224215,2.14089396788659,1.0,0.039435,1.2646644295302012,0.6858499426066993,0.24342105263157895,0.20927640831055797,0.4046289599305656
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})","frozenset({'reading_cat_μέτριο', 'race/ethnicity_group D', 'lunch_standard'})",0.32,0.092,0.063,0.196875,2.139945652173913,1.0,0.03356,1.1305836575875488,0.7833800186741364,0.1805157593123209,0.11550110132158589,0.4408288043478261
"frozenset({'reading_cat_μέτριο', 'race/ethnicity_group D', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})",0.092,0.32,0.063,0.6847826086956522,2.139945652173913,1.0,0.03356,2.157241379310345,0.5866722606810711,0.1805157593123209,0.5364450127877239,0.4408288043478261
frozenset({'math_cat_χαμηλό'}),"frozenset({'test preparation course_none', 'reading_cat_χαμηλό', 'lunch_standard'})",0.339,0.109,0.079,0.23303834808259585,2.137966496170604,1.0,0.042048999999999996,1.161726923076923,0.8052433022463087,0.2140921409214092,0.13921251187721195,0.47890449514221534
"frozenset({'test preparation course_none', 'reading_cat_χαμηλό', 'lunch_standard'})",frozenset({'math_cat_χαμηλό'}),0.109,0.339,0.079,0.7247706422018348,2.137966496170604,1.0,0.042048999999999996,2.4016333333333333,0.5973802724857576,0.2140921409214092,0.5836167052977833,0.47890449514221534
"frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό', 'lunch_standard'})",frozenset({'math_cat_χαμηλό'}),0.119,0.339,0.086,0.7226890756302521,2.1318261818001534,1.0,0.04565899999999999,2.3836060606060605,0.6026317873452471,0.23118279569892472,0.5804675879428927,0.48818819563223514
frozenset({'math_cat_χαμηλό'}),"frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό', 'lunch_standard'})",0.339,0.119,0.086,0.25368731563421826,2.1318261818001534,1.0,0.04565899999999999,1.1804703557312253,0.8032051507581887,0.23118279569892472,0.1528800404474668,0.48818819563223514
frozenset({'reading_cat_χαμηλό'}),"frozenset({'race/ethnicity_group C', 'math_cat_χαμηλό'})",0.275,0.118,0.069,0.2509090909090909,2.1263482280431436,1.0,0.036550000000000006,1.1774271844660196,0.7306346826586708,0.21296296296296297,0.15069057926200785,0.4178274268104777
"frozenset({'race/ethnicity_group C', 'math_cat_χαμηλό'})",frozenset({'reading_cat_χαμηλό'}),0.118,0.275,0.069,0.5847457627118645,2.1263482280431436,1.0,0.036550000000000006,1.7459183673469392,0.6005783956094516,0.21296296296296297,0.4272355347749855,0.4178274268104777
"frozenset({'math_cat_χαμηλό', 'lunch_standard'})","frozenset({'gender_male', 'reading_cat_χαμηλό'})",0.157,0.177,0.059,0.375796178343949,2.1231422505307855,1.0,0.031211,1.3184795918367345,0.627520759193357,0.21454545454545457,0.2415506419732066,0.35456475583864117
"frozenset({'gender_male', 'reading_cat_χαμηλό'})","frozenset({'math_cat_χαμηλό', 'lunch_standard'})",0.177,0.157,0.059,0.3333333333333333,2.1231422505307855,1.0,0.031211,1.2644999999999997,0.6427703523693804,0.21454545454545457,0.20917358639778566,0.35456475583864117
"frozenset({'math_cat_υψηλό', 'writing_cat_υψηλό', 'reading_cat_υψηλό'})","frozenset({'test preparation course_completed', 'lunch_standard'})",0.11,0.227,0.053,0.4818181818181818,2.122547056467761,1.0,0.028029999999999996,1.4917543859649123,0.5942336230655076,0.18661971830985913,0.3296483594025638,0.35764917901481774
"frozenset({'test preparation course_completed', 'lunch_standard'})","frozenset({'math_cat_υψηλό', 'writing_cat_υψηλό', 'reading_cat_υψηλό'})",0.227,0.11,0.053,0.23348017621145373,2.122547056467761,1.0,0.028029999999999996,1.1610919540229885,0.684175840269472,0.18661971830985913,0.13874177102410531,0.35764917901481774
"frozenset({'math_cat_υψηλό', 'reading_cat_υψηλό'})","frozenset({'test preparation course_completed', 'lunch_standard'})",0.131,0.227,0.063,0.4809160305343511,2.1185728217372297,1.0,0.033263,1.4891617647058824,0.6075766708678102,0.21355932203389832,0.32848128141571936,0.3792245350909641
"frozenset({'test preparation course_completed', 'lunch_standard'})","frozenset({'math_cat_υψηλό', 'reading_cat_υψηλό'})",0.227,0.131,0.063,0.2775330396475771,2.1185728217372297,1.0,0.033263,1.2028231707317072,0.6830325058009405,0.21355932203389832,0.16862260028489884,0.3792245350909641
frozenset({'writing_cat_υψηλό'}),"frozenset({'test preparation course_completed', 'gender_female'})",0.208,0.184,0.081,0.3894230769230769,2.116429765886288,1.0,0.042728,1.3364409448818897,0.666043147524629,0.2604501607717042,0.2517439668174962,0.4148202341137124
"frozenset({'test preparation course_completed', 'gender_female'})",frozenset({'writing_cat_υψηλό'}),0.184,0.208,0.081,0.44021739130434784,2.116429765886288,1.0,0.042728,1.414834951456311,0.6464536431856693,0.2604501607717042,0.2932037768994291,0.4148202341137124
"frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό'})","frozenset({'lunch_free/reduced', 'gender_male', 'test preparation course_none'})",0.252,0.105,0.056,0.22222222222222224,2.1164021164021167,1.0,0.02954,1.1507142857142858,0.7052139037433155,0.18604651162790697,0.1309745499689634,0.37777777777777777
"frozenset({'lunch_free/reduced', 'gender_male', 'test preparation course_none'})","frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό'})",0.105,0.252,0.056,0.5333333333333333,2.1164021164021163,1.0,0.02954,1.6028571428571428,0.5893854748603351,0.18604651162790697,0.3761140819964349,0.37777777777777777
"frozenset({'writing_cat_χαμηλό', 'gender_male'})","frozenset({'math_cat_χαμηλό', 'test preparation course_none', 'lunch_standard'})",0.201,0.12,0.051,0.25373134328358204,2.1144278606965172,1.0,0.026879999999999998,1.1792,0.6596480895236694,0.18888888888888886,0.15196743554952505,0.339365671641791
"frozenset({'math_cat_χαμηλό', 'test preparation course_none', 'lunch_standard'})","frozenset({'writing_cat_χαμηλό', 'gender_male'})",0.12,0.201,0.051,0.425,2.1144278606965172,1.0,0.026879999999999998,1.3895652173913042,0.5989304812834224,0.18888888888888886,0.28035043804755944,0.339365671641791
"frozenset({'math_cat_χαμηλό', 'test preparation course_none'})","frozenset({'gender_male', 'reading_cat_χαμηλό'})",0.246,0.177,0.092,0.37398373983739835,2.112902484957053,1.0,0.048458,1.3146623376623376,0.6985641794487372,0.27794561933534745,0.23934840806488258,0.44687887556841666
"frozenset({'gender_male', 'reading_cat_χαμηλό'})","frozenset({'math_cat_χαμηλό', 'test preparation course_none'})",0.177,0.246,0.092,0.519774011299435,2.112902484957053,1.0,0.048458,1.5700941176470589,0.6399968302604471,0.27794561933534745,0.3630955057021685,0.44687887556841666
frozenset({'math_cat_υψηλό'}),frozenset({'race/ethnicity_group E'}),0.176,0.14,0.052,0.29545454545454547,2.1103896103896105,1.0,0.027359999999999995,1.2206451612903224,0.6385362210604928,0.19696969696969696,0.18076109936575052,0.3334415584415584
frozenset({'race/ethnicity_group E'}),frozenset({'math_cat_υψηλό'}),0.14,0.176,0.052,0.3714285714285714,2.1103896103896105,1.0,0.027359999999999995,1.310909090909091,0.6118067978533094,0.19696969696969696,0.23717059639389731,0.3334415584415584
"frozenset({'writing_cat_μέτριο', 'gender_male'})","frozenset({'reading_cat_μέτριο', 'race/ethnicity_group D'})",0.227,0.136,0.065,0.28634361233480177,2.105467737755895,1.0,0.034128,1.2106666666666666,0.679231764354662,0.21812080536912754,0.17400881057268722,0.382142394402695
"frozenset({'reading_cat_μέτριο', 'race/ethnicity_group D'})","frozenset({'writing_cat_μέτριο', 'gender_male'})",0.136,0.227,0.065,0.4779411764705882,2.105467737755895,1.0,0.034128,1.4806760563380281,0.6076923076923076,0.21812080536912754,0.32463282855186054,0.382142394402695
"frozenset({'reading_cat_μέτριο', 'gender_female', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'test preparation course_none'})",0.172,0.205,0.074,0.43023255813953487,2.098695405558707,1.0,0.038740000000000004,1.3953061224489796,0.6322626974800889,0.2442244224422442,0.2833113938862074,0.3956040839478162
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'test preparation course_none'})","frozenset({'reading_cat_μέτριο', 'gender_female', 'lunch_standard'})",0.205,0.172,0.074,0.36097560975609755,2.098695405558707,1.0,0.038740000000000004,1.2957251908396947,0.6585075641679415,0.2442244224422442,0.22823141274891007,0.3956040839478162
"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced'})","frozenset({'writing_cat_χαμηλό', 'gender_male', 'reading_cat_χαμηλό'})",0.182,0.168,0.064,0.3516483516483517,2.0931449502878077,1.0,0.033423999999999995,1.2832542372881357,0.6384474327628361,0.2237762237762238,0.22073119188503806,0.36630036630036633
"frozenset({'writing_cat_χαμηλό', 'gender_male', 'reading_cat_χαμηλό'})","frozenset({'math_cat_χαμηλό', 'lunch_free/reduced'})",0.168,0.182,0.064,0.38095238095238093,2.0931449502878072,1.0,0.033423999999999995,1.3213846153846154,0.6277043269230769,0.2237762237762238,0.24321806962393755,0.36630036630036633
"frozenset({'test preparation course_completed', 'gender_female', 'lunch_standard'})",frozenset({'reading_cat_υψηλό'}),0.114,0.235,0.056,0.49122807017543857,2.0903322135125046,1.0,0.029210000000000003,1.5036206896551723,0.5887213801999356,0.19112627986348124,0.3349386538241027,0.36476297125793206
frozenset({'reading_cat_υψηλό'}),"frozenset({'test preparation course_completed', 'gender_female', 'lunch_standard'})",0.235,0.114,0.056,0.23829787234042554,2.0903322135125046,1.0,0.029210000000000003,1.1631843575418994,0.6818394024276377,0.19112627986348124,0.14029105230296335,0.36476297125793206
"frozenset({'reading_cat_μέτριο', 'gender_female'})","frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'test preparation course_none'})",0.261,0.099,0.054,0.20689655172413793,2.089864158829676,1.0,0.028161,1.1360434782608695,0.7056833558863329,0.17647058823529413,0.11975199969382677,0.37617554858934166
"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'test preparation course_none'})","frozenset({'reading_cat_μέτριο', 'gender_female'})",0.099,0.261,0.054,0.5454545454545454,2.089864158829676,1.0,0.028161,1.6258,0.578801331853496,0.17647058823529413,0.38491819411981787,0.37617554858934166
"frozenset({'writing_cat_χαμηλό', 'gender_male', 'test preparation course_none', 'reading_cat_χαμηλό'})",frozenset({'math_cat_χαμηλό'}),0.13,0.339,0.092,0.7076923076923076,2.0875879282958927,1.0,0.04792999999999999,2.261315789473684,0.5988255872063967,0.2440318302387268,0.557779588036774,0.4895393691853869
frozenset({'math_cat_χαμηλό'}),"frozenset({'writing_cat_χαμηλό', 'gender_male', 'test preparation course_none', 'reading_cat_χαμηλό'})",0.339,0.13,0.092,0.2713864306784661,2.0875879282958927,1.0,0.04792999999999999,1.1940485829959515,0.788166809182398,0.2440318302387268,0.16251313871088055,0.4895393691853869
"frozenset({'writing_cat_μέτριο', 'test preparation course_none'})","frozenset({'reading_cat_μέτριο', 'race/ethnicity_group D', 'lunch_standard'})",0.308,0.092,0.059,0.19155844155844154,2.0821569734613212,1.0,0.030663999999999997,1.12314859437751,0.7510531987851474,0.17302052785923752,0.10964586074718231,0.4164313946922642
"frozenset({'reading_cat_μέτριο', 'race/ethnicity_group D', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'test preparation course_none'})",0.092,0.308,0.059,0.6413043478260869,2.0821569734613212,1.0,0.030663999999999997,1.929212121212121,0.5723885611886805,0.17302052785923752,0.48165368182960544,0.4164313946922642
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'test preparation course_none'})","frozenset({'reading_cat_μέτριο', 'race/ethnicity_group D'})",0.205,0.136,0.058,0.2829268292682927,2.0803443328550935,1.0,0.03012,1.2048979591836735,0.6532205595315549,0.20494699646643114,0.17005420054200546,0.35469870875179343
"frozenset({'reading_cat_μέτριο', 'race/ethnicity_group D'})","frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'test preparation course_none'})",0.136,0.205,0.058,0.4264705882352941,2.0803443328550935,1.0,0.03012,1.3861538461538463,0.6010536398467433,0.20494699646643114,0.2785793562708102,0.35469870875179343
frozenset({'writing_cat_χαμηλό'}),"frozenset({'math_cat_χαμηλό', 'lunch_standard'})",0.301,0.157,0.098,0.32558139534883723,2.073766849355651,1.0,0.050743,1.2499655172413793,0.740752094829348,0.27222222222222225,0.19997793042566694,0.4748926085024441
"frozenset({'math_cat_χαμηλό', 'lunch_standard'})",frozenset({'writing_cat_χαμηλό'}),0.157,0.301,0.098,0.6242038216560509,2.073766849355651,1.0,0.050743,1.8600508474576272,0.6142179291645484,0.27222222222222225,0.46238028849220447,0.4748926085024441
"frozenset({'lunch_free/reduced', 'reading_cat_χαμηλό'})","frozenset({'math_cat_χαμηλό', 'gender_female'})",0.139,0.205,0.059,0.4244604316546762,2.0705386909984207,1.0,0.030504999999999997,1.3813125,0.6005039469280892,0.20701754385964913,0.27605085742726565,0.3561326548517283
"frozenset({'math_cat_χαμηλό', 'gender_female'})","frozenset({'lunch_free/reduced', 'reading_cat_χαμηλό'})",0.205,0.139,0.059,0.28780487804878047,2.0705386909984203,1.0,0.030504999999999997,1.2089383561643836,0.6503571047862701,0.20701754385964913,0.17282796521345,0.3561326548517283
"frozenset({'writing_cat_χαμηλό', 'gender_male', 'test preparation course_none'})","frozenset({'math_cat_χαμηλό', 'lunch_standard'})",0.157,0.157,0.051,0.3248407643312102,2.069049454338918,1.0,0.026350999999999996,1.2485943396226413,0.6129137301421161,0.19391634980988592,0.19909936456845811,0.3248407643312102
"frozenset({'math_cat_χαμηλό', 'lunch_standard'})","frozenset({'writing_cat_χαμηλό', 'gender_male', 'test preparation course_none'})",0.157,0.157,0.051,0.3248407643312102,2.069049454338918,1.0,0.026350999999999996,1.2485943396226413,0.6129137301421161,0.19391634980988592,0.19909936456845811,0.3248407643312102
"frozenset({'reading_cat_μέτριο', 'gender_female'})","frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο'})",0.261,0.154,0.083,0.31800766283524906,2.0649848236055135,1.0,0.042806000000000004,1.2404831460674157,0.6978821918254887,0.25,0.19386248562086178,0.428484350898144
"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο'})","frozenset({'reading_cat_μέτριο', 'gender_female'})",0.154,0.261,0.083,0.538961038961039,2.064984823605513,1.0,0.042806000000000004,1.6029014084507043,0.6096157680366858,0.25,0.3761313111786725,0.428484350898144
"frozenset({'writing_cat_μέτριο', 'race/ethnicity_group D', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο'})",0.094,0.325,0.063,0.6702127659574468,2.062193126022913,1.0,0.03245,2.046774193548387,0.5685202705070255,0.17696629213483145,0.5114263199369582,0.4320294599018003
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'writing_cat_μέτριο', 'race/ethnicity_group D', 'lunch_standard'})",0.325,0.094,0.063,0.19384615384615383,2.062193126022913,1.0,0.03245,1.123854961832061,0.7630805408583187,0.17696629213483145,0.11020546782136184,0.4320294599018003
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'test preparation course_completed', 'writing_cat_μέτριο', 'gender_male'})",0.325,0.094,0.063,0.19384615384615383,2.062193126022913,1.0,0.03245,1.123854961832061,0.7630805408583187,0.17696629213483145,0.11020546782136184,0.4320294599018003
"frozenset({'test preparation course_completed', 'writing_cat_μέτριο', 'gender_male'})","frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο'})",0.094,0.325,0.063,0.6702127659574468,2.062193126022913,1.0,0.03245,2.046774193548387,0.5685202705070255,0.17696629213483145,0.5114263199369582,0.4320294599018003
frozenset({'math_cat_χαμηλό'}),"frozenset({'reading_cat_χαμηλό', 'lunch_standard'})",0.339,0.136,0.095,0.28023598820059,2.060558736769044,1.0,0.048895999999999995,1.200393442622951,0.7786607213950154,0.25,0.16693980115809023,0.48938269998264794
"frozenset({'reading_cat_χαμηλό', 'lunch_standard'})",frozenset({'math_cat_χαμηλό'}),0.136,0.339,0.095,0.6985294117647058,2.0605587367690434,1.0,0.048895999999999995,2.1925853658536583,0.5957115009746589,0.25,0.5439174156803416,0.48938269998264794
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'lunch_standard'})","frozenset({'reading_cat_μέτριο', 'race/ethnicity_group D'})",0.225,0.136,0.063,0.27999999999999997,2.0588235294117645,1.0,0.0324,1.2,0.663594470046083,0.21140939597315436,0.16666666666666663,0.3716176470588235
"frozenset({'reading_cat_μέτριο', 'race/ethnicity_group D'})","frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'lunch_standard'})",0.136,0.225,0.063,0.463235294117647,2.0588235294117645,1.0,0.0324,1.443835616438356,0.5952380952380952,0.21140939597315436,0.3074003795066413,0.3716176470588235
"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'gender_female', 'test preparation course_none'})","frozenset({'reading_cat_μέτριο', 'lunch_standard'})",0.11,0.327,0.074,0.6727272727272727,2.057269947178204,1.0,0.038029999999999994,2.056388888888889,0.577436987549347,0.20385674931129477,0.5137106578414156,0.44951348345843756
"frozenset({'reading_cat_μέτριο', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'gender_female', 'test preparation course_none'})",0.327,0.11,0.074,0.2262996941896024,2.0572699471782037,1.0,0.038029999999999994,1.1503162055335967,0.7636239508453475,0.20385674931129477,0.13067381369618247,0.44951348345843756
"frozenset({'test preparation course_completed', 'gender_female'})","frozenset({'writing_cat_υψηλό', 'reading_cat_υψηλό'})",0.184,0.185,0.07,0.3804347826086957,2.056404230317274,1.0,0.035960000000000006,1.315438596491228,0.6295518207282913,0.23411371237458198,0.23979727927447325,0.37940658049353704
"frozenset({'writing_cat_υψηλό', 'reading_cat_υψηλό'})","frozenset({'test preparation course_completed', 'gender_female'})",0.185,0.184,0.07,0.3783783783783784,2.056404230317274,1.0,0.035960000000000006,1.3126956521739133,0.6303242769500439,0.23411371237458198,0.23820879703232645,0.37940658049353704
"frozenset({'math_cat_χαμηλό', 'gender_female'})","frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced', 'test preparation course_none'})",0.205,0.121,0.051,0.24878048780487805,2.0560370892965127,1.0,0.026195,1.1700974025974027,0.6460722653841411,0.18545454545454546,0.1453702932933766,0.33513404555533155
"frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced', 'test preparation course_none'})","frozenset({'math_cat_χαμηλό', 'gender_female'})",0.121,0.205,0.051,0.4214876033057851,2.0560370892965127,1.0,0.026195,1.3742142857142856,0.5843315710812198,0.18545454545454546,0.2723114506991008,0.33513404555533155
"frozenset({'writing_cat_χαμηλό', 'test preparation course_none', 'lunch_standard'})",frozenset({'math_cat_χαμηλό'}),0.122,0.339,0.085,0.6967213114754099,2.0552251075970793,1.0,0.04364200000000001,2.1795135135135144,0.5847782393139489,0.22606382978723405,0.541182014334962,0.4737293873011268
frozenset({'math_cat_χαμηλό'}),"frozenset({'writing_cat_χαμηλό', 'test preparation course_none', 'lunch_standard'})",0.339,0.122,0.085,0.25073746312684364,2.055225107597079,1.0,0.04364200000000001,1.1718188976377952,0.7767553617513572,0.22606382978723405,0.1466258122173618,0.4737293873011268
"frozenset({'math_cat_χαμηλό', 'gender_female'})","frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced', 'reading_cat_χαμηλό'})",0.205,0.133,0.056,0.2731707317073171,2.053915275994865,1.0,0.028735,1.1928523489932885,0.645440251572327,0.19858156028368798,0.1616732776324303,0.3471116816431322
"frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced', 'reading_cat_χαμηλό'})","frozenset({'math_cat_χαμηλό', 'gender_female'})",0.133,0.205,0.056,0.42105263157894735,2.053915275994865,1.0,0.028735,1.3731818181818183,0.5918396770472896,0.19858156028368798,0.2717643164515061,0.3471116816431322
"frozenset({'gender_male', 'reading_cat_χαμηλό'})",frozenset({'math_cat_χαμηλό'}),0.177,0.339,0.123,0.6949152542372882,2.049897505124744,1.0,0.062997,2.1666111111111115,0.6223216667160596,0.3129770992366412,0.53844970383856,0.5288735563221839
frozenset({'math_cat_χαμηλό'}),"frozenset({'gender_male', 'reading_cat_χαμηλό'})",0.339,0.177,0.123,0.3628318584070796,2.0498975051247434,1.0,0.062997,1.2916527777777775,0.7748422567432935,0.3129770992366412,0.22579812686157913,0.5288735563221839
"frozenset({'math_cat_υψηλό', 'writing_cat_υψηλό'})","frozenset({'test preparation course_completed', 'lunch_standard'})",0.114,0.227,0.053,0.46491228070175433,2.0480717211531028,1.0,0.027121999999999997,1.444622950819672,0.5775799650751735,0.18402777777777776,0.3077778534304713,0.34919622845660403
"frozenset({'test preparation course_completed', 'lunch_standard'})","frozenset({'math_cat_υψηλό', 'writing_cat_υψηλό'})",0.227,0.114,0.053,0.23348017621145373,2.0480717211531028,1.0,0.027121999999999997,1.155873563218391,0.6620127413410138,0.18402777777777776,0.1348534720219568,0.34919622845660403
"frozenset({'writing_cat_χαμηλό', 'math_cat_μέτριο'})",frozenset({'gender_male'}),0.071,0.482,0.07,0.9859154929577466,2.0454678277131673,1.0,0.03577800000000001,36.77800000000034,0.5501768414577889,0.1449275362318841,0.9728098319647618,0.5655718543626908
frozenset({'gender_male'}),"frozenset({'writing_cat_χαμηλό', 'math_cat_μέτριο'})",0.482,0.071,0.07,0.14522821576763487,2.0454678277131673,1.0,0.03577800000000001,1.0868398058252429,0.9867071152785439,0.1449275362318841,0.07990120104158759,0.5655718543626908
"frozenset({'reading_cat_μέτριο', 'race/ethnicity_group D'})","frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})",0.136,0.32,0.089,0.6544117647058822,2.045036764705882,1.0,0.04547999999999999,1.9676595744680843,0.5914481897627965,0.24250681198910082,0.4917820069204151,0.4662683823529411
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})","frozenset({'reading_cat_μέτριο', 'race/ethnicity_group D'})",0.32,0.136,0.089,0.27812499999999996,2.045036764705882,1.0,0.04547999999999999,1.1968831168831169,0.751487111698612,0.24250681198910082,0.1644965277777777,0.4662683823529411
"frozenset({'writing_cat_μέτριο', 'race/ethnicity_group D'})","frozenset({'reading_cat_μέτριο', 'gender_male'})",0.139,0.229,0.065,0.4676258992805755,2.042034494675002,1.0,0.033169,1.4482297297297297,0.5926739926739926,0.21452145214521454,0.3095018148905,0.3757343470202004
"frozenset({'reading_cat_μέτριο', 'gender_male'})","frozenset({'writing_cat_μέτριο', 'race/ethnicity_group D'})",0.229,0.139,0.065,0.2838427947598253,2.042034494675002,1.0,0.033169,1.20225,0.661857727227377,0.21452145214521454,0.1682262424620503,0.3757343470202004
frozenset({'gender_male'}),"frozenset({'writing_cat_χαμηλό', 'math_cat_μέτριο', 'test preparation course_none'})",0.482,0.06,0.059,0.12240663900414937,2.040110650069156,1.0,0.03008,1.0711111111111111,0.9842287808389504,0.12215320910973083,0.06639004149377593,0.5528699861687413
"frozenset({'writing_cat_χαμηλό', 'math_cat_μέτριο', 'test preparation course_none'})",frozenset({'gender_male'}),0.06,0.482,0.059,0.9833333333333333,2.040110650069156,1.0,0.03008,31.079999999999906,0.5423728813559322,0.12215320910973083,0.9678249678249677,0.5528699861687413
"frozenset({'reading_cat_μέτριο', 'lunch_standard', 'gender_male'})","frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})",0.155,0.32,0.101,0.6516129032258065,2.036290322580645,1.0,0.05140000000000001,1.9518518518518517,0.60226140957291,0.2700534759358289,0.4876660341555978,0.48361895161290325
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})","frozenset({'reading_cat_μέτριο', 'lunch_standard', 'gender_male'})",0.32,0.155,0.101,0.315625,2.036290322580645,1.0,0.05140000000000001,1.234703196347032,0.7483983692486896,0.2700534759358289,0.1900887573964497,0.48361895161290325
"frozenset({'writing_cat_μέτριο', 'test preparation course_none'})","frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'gender_female'})",0.308,0.15,0.094,0.3051948051948052,2.034632034632035,1.0,0.0478,1.2233644859813084,0.7348419628581971,0.25824175824175827,0.18258212375859434,0.465930735930736
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'gender_female'})","frozenset({'writing_cat_μέτριο', 'test preparation course_none'})",0.15,0.308,0.094,0.6266666666666667,2.034632034632035,1.0,0.0478,1.8535714285714286,0.5982478097622028,0.25824175824175827,0.4605009633911369,0.465930735930736
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})","frozenset({'reading_cat_μέτριο', 'gender_male'})",0.32,0.229,0.149,0.46562499999999996,2.033296943231441,1.0,0.07572,1.4428070175438596,0.7473351756810108,0.37249999999999994,0.3069066147859921,0.5581400109170305
"frozenset({'reading_cat_μέτριο', 'gender_male'})","frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})",0.229,0.32,0.149,0.6506550218340611,2.033296943231441,1.0,0.07572,1.9464999999999997,0.6591283002115269,0.37249999999999994,0.48625738505008986,0.5581400109170305
"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο'})","frozenset({'gender_female', 'reading_cat_μέτριο', 'test preparation course_none'})",0.154,0.173,0.054,0.35064935064935066,2.026874859244802,1.0,0.027358,1.27358,0.5988529901059453,0.19780219780219782,0.21481179038615558,0.331394039486525
"frozenset({'gender_female', 'reading_cat_μέτριο', 'test preparation course_none'})","frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο'})",0.173,0.154,0.054,0.31213872832369943,2.0268748592448014,1.0,0.027358,1.2298991596638655,0.6126114022123696,0.19780219780219782,0.1869252107845147,0.331394039486525
"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'gender_female', 'lunch_standard'})","frozenset({'reading_cat_μέτριο', 'test preparation course_none'})",0.115,0.318,0.074,0.6434782608695652,2.023516543614985,1.0,0.03742999999999999,1.9129268292682922,0.5715376393342494,0.20612813370473537,0.477240851714905,0.4380913316926442
"frozenset({'reading_cat_μέτριο', 'test preparation course_none'})","frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'gender_female', 'lunch_standard'})",0.318,0.115,0.074,0.23270440251572325,2.0235165436149845,1.0,0.03742999999999999,1.1534016393442623,0.7416580803677577,0.20612813370473537,0.13299932487652344,0.4380913316926442
"frozenset({'writing_cat_χαμηλό', 'test preparation course_none'})","frozenset({'race/ethnicity_group C', 'math_cat_χαμηλό'})",0.243,0.118,0.058,0.23868312757201648,2.022738369254377,1.0,0.029326000000000005,1.158518918918919,0.6679269348152873,0.19141914191419143,0.13682894282541552,0.36510427565041503
"frozenset({'race/ethnicity_group C', 'math_cat_χαμηλό'})","frozenset({'writing_cat_χαμηλό', 'test preparation course_none'})",0.118,0.243,0.058,0.49152542372881364,2.022738369254377,1.0,0.029326000000000005,1.4887666666666668,0.5732660880444133,0.19141914191419143,0.3283030696549718,0.36510427565041503
"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced', 'gender_female'})","frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό'})",0.11,0.252,0.056,0.5090909090909091,2.0202020202020203,1.0,0.02828,1.5237037037037038,0.5674157303370786,0.18300653594771243,0.34370442391832773,0.3656565656565657
"frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό'})","frozenset({'math_cat_χαμηλό', 'lunch_free/reduced', 'gender_female'})",0.252,0.11,0.056,0.22222222222222224,2.0202020202020203,1.0,0.02828,1.1442857142857144,0.6751336898395721,0.18300653594771243,0.12609238451935081,0.3656565656565657
"frozenset({'writing_cat_μέτριο', 'gender_male'})","frozenset({'reading_cat_μέτριο', 'math_cat_μέτριο'})",0.227,0.325,0.149,0.6563876651982379,2.0196543544561165,1.0,0.07522499999999999,1.964423076923077,0.6531251899250717,0.3697270471464019,0.4909446891825746,0.5574246018298882
"frozenset({'reading_cat_μέτριο', 'math_cat_μέτριο'})","frozenset({'writing_cat_μέτριο', 'gender_male'})",0.325,0.227,0.149,0.4584615384615384,2.0196543544561165,1.0,0.07522499999999999,1.4274147727272728,0.7479492915734526,0.3697270471464019,0.29943277938103285,0.5574246018298882
"frozenset({'gender_male', 'test preparation course_none'})","frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό', 'lunch_standard'})",0.308,0.119,0.074,0.24025974025974026,2.018989413947397,1.0,0.037348,1.1596068376068376,0.7293391657553507,0.2096317280453258,0.13763875171366657,0.4310542398777693
"frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό', 'lunch_standard'})","frozenset({'gender_male', 'test preparation course_none'})",0.119,0.308,0.074,0.6218487394957983,2.018989413947397,1.0,0.037348,1.8299555555555556,0.5728748044298555,0.2096317280453258,0.453538640889882,0.4310542398777693
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'test preparation course_none'})","frozenset({'reading_cat_μέτριο', 'lunch_standard'})",0.205,0.327,0.135,0.6585365853658537,2.013873349742672,1.0,0.06796500000000001,1.9709285714285718,0.6332634521313767,0.34005037783375314,0.4926249411082521,0.5356903110315507
"frozenset({'reading_cat_μέτριο', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'test preparation course_none'})",0.327,0.205,0.135,0.41284403669724773,2.013873349742672,1.0,0.06796500000000001,1.353984375,0.7480600957569755,0.34005037783375314,0.26143903987075184,0.5356903110315507
"frozenset({'writing_cat_μέτριο', 'gender_male', 'test preparation course_none'})","frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})",0.133,0.228,0.061,0.45864661654135336,2.0116079672866376,1.0,0.030675999999999995,1.4260555555555556,0.5800291186870119,0.20333333333333334,0.29876504733335923,0.3630952380952381
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'gender_male', 'test preparation course_none'})",0.228,0.133,0.061,0.2675438596491228,2.0116079672866376,1.0,0.030675999999999995,1.1836886227544912,0.6514057589399472,0.20333333333333334,0.15518322912240232,0.3630952380952381
"frozenset({'test preparation course_completed', 'writing_cat_μέτριο'})","frozenset({'lunch_free/reduced', 'reading_cat_μέτριο'})",0.183,0.163,0.06,0.32786885245901637,2.011465352509303,1.0,0.030170999999999996,1.2452926829268292,0.6154834761321909,0.2097902097902098,0.19697592886381884,0.34798350598410943
"frozenset({'lunch_free/reduced', 'reading_cat_μέτριο'})","frozenset({'test preparation course_completed', 'writing_cat_μέτριο'})",0.163,0.183,0.06,0.36809815950920244,2.011465352509303,1.0,0.030170999999999996,1.2929223300970873,0.6007765830346475,0.2097902097902098,0.22655833477258563,0.34798350598410943
frozenset({'math_cat_χαμηλό'}),"frozenset({'gender_male', 'test preparation course_none', 'reading_cat_χαμηλό'})",0.339,0.135,0.092,0.2713864306784661,2.0102698568775264,1.0,0.04623499999999999,1.187186234817814,0.7602940209169241,0.24083769633507854,0.15767217419475846,0.47643395607997374
"frozenset({'gender_male', 'test preparation course_none', 'reading_cat_χαμηλό'})",frozenset({'math_cat_χαμηλό'}),0.135,0.339,0.092,0.6814814814814815,2.0102698568775264,1.0,0.04623499999999999,2.0752325581395348,0.5809876853480773,0.24083769633507854,0.5181262957359779,0.47643395607997374
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})","frozenset({'writing_cat_μέτριο', 'gender_female', 'lunch_standard'})",0.208,0.177,0.074,0.3557692307692308,2.009995654063451,1.0,0.037184,1.277492537313433,0.6344526344526344,0.23794212218649516,0.21721656229554168,0.38692416340721425
"frozenset({'writing_cat_μέτριο', 'gender_female', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})",0.177,0.208,0.074,0.4180790960451977,2.0099956540634505,1.0,0.037184,1.3610097087378639,0.6105546615874685,0.23794212218649516,0.26525138389545166,0.38692416340721425
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})","frozenset({'writing_cat_μέτριο', 'race/ethnicity_group D'})",0.208,0.139,0.058,0.27884615384615385,2.006087437742114,1.0,0.029088000000000003,1.19392,0.6332288401253918,0.20069204152249137,0.1624229429107478,0.34805617044825676
"frozenset({'writing_cat_μέτριο', 'race/ethnicity_group D'})","frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})",0.139,0.208,0.058,0.41726618705035967,2.006087437742114,1.0,0.029088000000000003,1.3591111111111112,0.5824822780247507,0.20069204152249137,0.264224983649444,0.34805617044825676
"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο'})","frozenset({'reading_cat_μέτριο', 'gender_female', 'lunch_standard'})",0.154,0.172,0.053,0.34415584415584416,2.000906070673513,1.0,0.026512,1.2624950495049505,0.5912841785985102,0.19413919413919417,0.20791768617855574,0.32614768951978257
"frozenset({'reading_cat_μέτριο', 'gender_female', 'lunch_standard'})","frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο'})",0.172,0.154,0.053,0.3081395348837209,2.0009060706735124,1.0,0.026512,1.2227899159663864,0.6041381824810865,0.19413919413919417,0.1821980317774479,0.32614768951978257
"frozenset({'writing_cat_χαμηλό', 'lunch_standard'})",frozenset({'math_cat_χαμηλό'}),0.145,0.339,0.098,0.6758620689655173,1.993693418777337,1.0,0.048845000000000006,2.0392553191489364,0.5829454588853086,0.2538860103626943,0.5096249152277114,0.48247380734411555
frozenset({'math_cat_χαμηλό'}),"frozenset({'writing_cat_χαμηλό', 'lunch_standard'})",0.339,0.145,0.098,0.28908554572271383,1.993693418777337,1.0,0.048845000000000006,1.2026763485477177,0.7540368643675323,0.2538860103626943,0.16852110610843724,0.48247380734411555
"frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'race/ethnicity_group D'})","frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})",0.091,0.32,0.058,0.6373626373626374,1.991758241758242,1.0,0.028880000000000003,1.8751515151515152,0.5477789158226167,0.16430594900849857,0.466709760827408,0.4093063186813187
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})","frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'race/ethnicity_group D'})",0.32,0.091,0.058,0.18125,1.9917582417582418,1.0,0.028880000000000003,1.110229007633588,0.7322515212981746,0.16430594900849857,0.09928492849284928,0.4093063186813187
"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'lunch_standard'})","frozenset({'reading_cat_μέτριο', 'gender_female'})",0.102,0.261,0.053,0.5196078431372549,1.9908346480354595,1.0,0.026378,1.5383265306122451,0.554229524730008,0.17096774193548386,0.34994295417761156,0.3613364886184359
"frozenset({'reading_cat_μέτριο', 'gender_female'})","frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'lunch_standard'})",0.261,0.102,0.053,0.20306513409961685,1.9908346480354595,1.0,0.026378,1.1268173076923076,0.6734751193606863,0.17096774193548386,0.11254469276126598,0.3613364886184359
"frozenset({'writing_cat_μέτριο', 'gender_male', 'test preparation course_none'})","frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο'})",0.133,0.325,0.086,0.6466165413533834,1.9895893580104105,1.0,0.04277499999999999,1.9101063829787233,0.5736836458249509,0.23118279569892472,0.4764689501531606,0.455615962984384
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'writing_cat_μέτριο', 'gender_male', 'test preparation course_none'})",0.325,0.133,0.086,0.2646153846153846,1.9895893580104103,1.0,0.04277499999999999,1.1789748953974895,0.7368647717484925,0.23118279569892472,0.15180551858752547,0.455615962984384
"frozenset({'gender_female', 'lunch_standard'})","frozenset({'writing_cat_υψηλό', 'test preparation course_none', 'reading_cat_υψηλό'})",0.329,0.081,0.053,0.16109422492401215,1.988817591654471,1.0,0.026350999999999996,1.0954746376811595,0.7409667350898403,0.14845938375350137,0.0871536723873908,0.4077076062891665
"frozenset({'writing_cat_υψηλό', 'test preparation course_none', 'reading_cat_υψηλό'})","frozenset({'gender_female', 'lunch_standard'})",0.081,0.329,0.053,0.6543209876543209,1.9888175916544708,1.0,0.026350999999999996,1.9411071428571425,0.5410105323670108,0.14845938375350137,0.48483008592298193,0.4077076062891665
"frozenset({'writing_cat_μέτριο', 'race/ethnicity_group D'})","frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})",0.139,0.228,0.063,0.4532374100719424,1.9878833775085192,1.0,0.031307999999999996,1.4119473684210526,0.5771804656822077,0.20723684210526316,0.2917583032019979,0.3647765997728133
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'race/ethnicity_group D'})",0.228,0.139,0.063,0.2763157894736842,1.9878833775085192,1.0,0.031307999999999996,1.1897454545454544,0.6437207007155193,0.20723684210526316,0.1594840760437679,0.3647765997728133
"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced'})","frozenset({'gender_male', 'reading_cat_χαμηλό'})",0.182,0.177,0.064,0.3516483516483517,1.986713851120631,1.0,0.031786,1.2693728813559324,0.6071592298288508,0.2169491525423729,0.2122094187707797,0.35661513627615327
"frozenset({'gender_male', 'reading_cat_χαμηλό'})","frozenset({'math_cat_χαμηλό', 'lunch_free/reduced'})",0.177,0.182,0.064,0.36158192090395486,1.986713851120631,1.0,0.031786,1.2812920353982302,0.6034705346294047,0.2169491525423729,0.21953780061608172,0.35661513627615327
"frozenset({'reading_cat_μέτριο', 'gender_female'})","frozenset({'writing_cat_μέτριο', 'parental level of education_some college'})",0.261,0.114,0.059,0.2260536398467433,1.9829266653223094,1.0,0.029245999999999994,1.1447821782178218,0.6707644320084402,0.18670886075949367,0.1264713768021933,0.37179874974793303
"frozenset({'writing_cat_μέτριο', 'parental level of education_some college'})","frozenset({'reading_cat_μέτριο', 'gender_female'})",0.114,0.261,0.059,0.5175438596491228,1.9829266653223092,1.0,0.029245999999999994,1.5317454545454543,0.5594750736503805,0.18670886075949367,0.34715001305699966,0.37179874974793303
"frozenset({'reading_cat_μέτριο', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'test preparation course_none', 'race/ethnicity_group D'})",0.327,0.091,0.059,0.18042813455657492,1.9827267533689552,1.0,0.029242999999999998,1.1091156716417911,0.7364696401138338,0.1643454038997214,0.09838078609084148,0.41438989145411165
"frozenset({'writing_cat_μέτριο', 'test preparation course_none', 'race/ethnicity_group D'})","frozenset({'reading_cat_μέτριο', 'lunch_standard'})",0.091,0.327,0.059,0.6483516483516484,1.9827267533689552,1.0,0.029242999999999998,1.9138437500000003,0.5452630008763588,0.1643454038997214,0.4774913051287494,0.41438989145411165
"frozenset({'writing_cat_μέτριο', 'test preparation course_none'})","frozenset({'reading_cat_μέτριο', 'gender_female', 'lunch_standard'})",0.308,0.172,0.105,0.3409090909090909,1.982029598308668,1.0,0.052024,1.2562758620689656,0.7159922928709057,0.27999999999999997,0.20399648660518221,0.4756871035940803
"frozenset({'reading_cat_μέτριο', 'gender_female', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'test preparation course_none'})",0.172,0.308,0.105,0.6104651162790697,1.982029598308668,1.0,0.052024,1.7764776119402983,0.5983896940418679,0.27999999999999997,0.4370883183223552,0.4756871035940803
"frozenset({'math_cat_χαμηλό', 'test preparation course_none'})","frozenset({'writing_cat_χαμηλό', 'gender_male'})",0.246,0.201,0.098,0.3983739837398374,1.9819601181086437,1.0,0.048554,1.3280675675675675,0.6570941373897039,0.28080229226361036,0.24702626250292542,0.4429680863972819
"frozenset({'writing_cat_χαμηλό', 'gender_male'})","frozenset({'math_cat_χαμηλό', 'test preparation course_none'})",0.201,0.246,0.098,0.48756218905472637,1.9819601181086437,1.0,0.048554,1.471398058252427,0.6200863324053025,0.28080229226361036,0.32037425604075115,0.4429680863972819
"frozenset({'reading_cat_μέτριο', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'gender_male', 'test preparation course_none'})",0.327,0.133,0.086,0.26299694189602446,1.9774206157595824,1.0,0.04250899999999999,1.1763858921161827,0.7344586889664465,0.2299465240641711,0.14993880264824044,0.4548067416247039
"frozenset({'writing_cat_μέτριο', 'gender_male', 'test preparation course_none'})","frozenset({'reading_cat_μέτριο', 'lunch_standard'})",0.133,0.327,0.086,0.6466165413533834,1.9774206157595822,1.0,0.04250899999999999,1.9044468085106383,0.5701161449531933,0.2299465240641711,0.47491313722642403,0.4548067416247039
"frozenset({'math_cat_υψηλό', 'writing_cat_μέτριο'})",frozenset({'gender_male'}),0.062,0.482,0.059,0.9516129032258064,1.9743006290991834,1.0,0.029116,10.705333333333318,0.5261103682555744,0.12164948453608246,0.9065886162660355,0.5370097711149778
frozenset({'gender_male'}),"frozenset({'math_cat_υψηλό', 'writing_cat_μέτριο'})",0.482,0.062,0.059,0.12240663900414937,1.9743006290991834,1.0,0.029116,1.0688321513002363,0.952686342516851,0.12164948453608246,0.06439940192340018,0.5370097711149778
"frozenset({'lunch_free/reduced', 'gender_male', 'test preparation course_none'})",frozenset({'reading_cat_χαμηλό'}),0.105,0.275,0.057,0.5428571428571429,1.974025974025974,1.0,0.028125,1.5859375000000002,0.5513084386945015,0.17647058823529413,0.36945812807881784,0.3750649350649351
frozenset({'reading_cat_χαμηλό'}),"frozenset({'lunch_free/reduced', 'gender_male', 'test preparation course_none'})",0.275,0.105,0.057,0.20727272727272728,1.974025974025974,1.0,0.028125,1.12901376146789,0.6805807622504537,0.17647058823529413,0.11427120365667852,0.3750649350649351
"frozenset({'writing_cat_μέτριο', 'race/ethnicity_group D', 'lunch_standard'})","frozenset({'reading_cat_μέτριο', 'test preparation course_none'})",0.094,0.318,0.059,0.6276595744680851,1.9737722467549845,1.0,0.029107999999999995,1.8316571428571424,0.5445429715269202,0.16713881019830026,0.45404629687402503,0.4065970828315268
"frozenset({'reading_cat_μέτριο', 'test preparation course_none'})","frozenset({'writing_cat_μέτριο', 'race/ethnicity_group D', 'lunch_standard'})",0.318,0.094,0.059,0.18553459119496854,1.9737722467549845,1.0,0.029107999999999995,1.1123861003861004,0.7233957950196331,0.16713881019830026,0.10103155761034055,0.4065970828315268
"frozenset({'writing_cat_υψηλό', 'test preparation course_none'})","frozenset({'gender_female', 'lunch_standard'})",0.091,0.329,0.059,0.6483516483516484,1.9706737031964996,1.0,0.029060999999999997,1.9081562500000002,0.5418694411814062,0.16343490304709138,0.47593390216341036,0.4138414776712649
"frozenset({'gender_female', 'lunch_standard'})","frozenset({'writing_cat_υψηλό', 'test preparation course_none'})",0.329,0.091,0.059,0.17933130699088143,1.9706737031964994,1.0,0.029060999999999997,1.1076333333333332,0.7340675440147515,0.16343490304709138,0.09717415510548012,0.4138414776712649
"frozenset({'writing_cat_χαμηλό', 'lunch_standard'})","frozenset({'gender_male', 'test preparation course_none'})",0.145,0.308,0.088,0.6068965517241379,1.9704433497536946,1.0,0.04334,1.7603508771929823,0.5760233918128655,0.24109589041095889,0.431931433127367,0.4463054187192118
"frozenset({'gender_male', 'test preparation course_none'})","frozenset({'writing_cat_χαμηλό', 'lunch_standard'})",0.308,0.145,0.088,0.2857142857142857,1.9704433497536946,1.0,0.04334,1.1969999999999998,0.7117052023121387,0.24109589041095889,0.164578111946533,0.4463054187192118
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'writing_cat_μέτριο', 'race/ethnicity_group D'})",0.325,0.139,0.089,0.27384615384615385,1.9701162147205311,1.0,0.04382499999999999,1.1856991525423728,0.7295047856845608,0.2373333333333333,0.15661574198159564,0.4570669618151632
"frozenset({'writing_cat_μέτριο', 'race/ethnicity_group D'})","frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο'})",0.139,0.325,0.089,0.6402877697841726,1.970116214720531,1.0,0.04382499999999999,1.8764999999999996,0.5719114173485234,0.2373333333333333,0.4670929922728482,0.4570669618151632
"frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced'})","frozenset({'math_cat_χαμηλό', 'gender_female'})",0.156,0.205,0.063,0.40384615384615385,1.9699812382739215,1.0,0.03102,1.3335483870967744,0.5833897540058678,0.21140939597315436,0.2501209482341558,0.35558161350844275
"frozenset({'math_cat_χαμηλό', 'gender_female'})","frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced'})",0.205,0.156,0.063,0.3073170731707317,1.9699812382739212,1.0,0.03102,1.218450704225352,0.6193471099131476,0.21140939597315436,0.17928563171887643,0.35558161350844275
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'gender_male'})","frozenset({'test preparation course_completed', 'writing_cat_μέτριο'})",0.175,0.183,0.063,0.36000000000000004,1.9672131147540985,1.0,0.030975000000000003,1.2765625,0.595959595959596,0.21355932203389832,0.21664626682986543,0.3521311475409836
"frozenset({'test preparation course_completed', 'writing_cat_μέτριο'})","frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'gender_male'})",0.183,0.175,0.063,0.3442622950819672,1.9672131147540985,1.0,0.030975000000000003,1.2581249999999997,0.601795185638515,0.21355932203389832,0.2051664182811724,0.3521311475409836
frozenset({'gender_male'}),"frozenset({'math_cat_μέτριο', 'reading_cat_χαμηλό'})",0.482,0.057,0.054,0.11203319502074689,1.9654946494867875,1.0,0.026526,1.0619766355140188,0.9483054483054484,0.111340206185567,0.05835969779506563,0.5297008080366892
"frozenset({'math_cat_μέτριο', 'reading_cat_χαμηλό'})",frozenset({'gender_male'}),0.057,0.482,0.054,0.9473684210526315,1.9654946494867875,1.0,0.026526,9.84199999999999,0.5209143395781785,0.111340206185567,0.8983946352367403,0.5297008080366892
"frozenset({'writing_cat_χαμηλό', 'gender_male'})","frozenset({'math_cat_χαμηλό', 'lunch_standard'})",0.201,0.157,0.062,0.30845771144278605,1.9646987989986373,1.0,0.030442999999999998,1.2190143884892084,0.6145383342080827,0.20945945945945946,0.17966513812904633,0.35168108502075607
"frozenset({'math_cat_χαμηλό', 'lunch_standard'})","frozenset({'writing_cat_χαμηλό', 'gender_male'})",0.157,0.201,0.062,0.3949044585987261,1.964698798998637,1.0,0.030442999999999998,1.320452631578947,0.5824627865151341,0.20945945945945946,0.24268392815860587,0.35168108502075607
"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'gender_male', 'test preparation course_none'})","frozenset({'reading_cat_μέτριο', 'lunch_standard'})",0.095,0.327,0.061,0.6421052631578947,1.9636246579752132,1.0,0.029934999999999996,1.8804411764705884,0.5422516076442351,0.16897506925207753,0.46820990068037843,0.41432480283277
"frozenset({'reading_cat_μέτριο', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'gender_male', 'test preparation course_none'})",0.327,0.095,0.061,0.18654434250764526,1.9636246579752132,1.0,0.029934999999999996,1.1125375939849624,0.7291793535186222,0.16897506925207753,0.10115396962170746,0.41432480283277
"frozenset({'writing_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})","frozenset({'reading_cat_μέτριο', 'race/ethnicity_group D'})",0.221,0.136,0.059,0.2669683257918552,1.9630023955283469,1.0,0.028943999999999994,1.1786666666666665,0.6297513108940187,0.19798657718120805,0.1515837104072398,0.35039592760180993
"frozenset({'reading_cat_μέτριο', 'race/ethnicity_group D'})","frozenset({'writing_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",0.136,0.221,0.059,0.43382352941176466,1.9630023955283469,1.0,0.028943999999999994,1.375896103896104,0.5677966101694915,0.19798657718120805,0.2732009363437287,0.35039592760180993
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'writing_cat_μέτριο', 'test preparation course_none', 'race/ethnicity_group D'})",0.325,0.091,0.058,0.17846153846153848,1.9611158072696537,1.0,0.028425000000000002,1.1064606741573033,0.7260536398467433,0.16201117318435754,0.09621731403909624,0.40791208791208794
"frozenset({'writing_cat_μέτριο', 'test preparation course_none', 'race/ethnicity_group D'})","frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο'})",0.091,0.325,0.058,0.6373626373626374,1.9611158072696535,1.0,0.028425000000000002,1.8613636363636368,0.539148742460453,0.16201117318435754,0.46275946275946284,0.40791208791208794
"frozenset({'reading_cat_μέτριο', 'parental level of education_some college'})","frozenset({'writing_cat_μέτριο', 'gender_female'})",0.114,0.264,0.059,0.5175438596491228,1.9603934077618286,1.0,0.028903999999999996,1.5255272727272726,0.5529326242491486,0.18495297805642633,0.3444889397406559,0.37051435406698563
"frozenset({'writing_cat_μέτριο', 'gender_female'})","frozenset({'reading_cat_μέτριο', 'parental level of education_some college'})",0.264,0.114,0.059,0.22348484848484845,1.9603934077618284,1.0,0.028903999999999996,1.1409951219512193,0.6656226971260132,0.18495297805642633,0.12357206375265062,0.37051435406698563
"frozenset({'reading_cat_μέτριο', 'gender_male'})","frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'lunch_standard'})",0.229,0.225,0.101,0.44104803493449785,1.9602134885977682,1.0,0.049475000000000005,1.3865234375000002,0.6353456357308882,0.2861189801699717,0.2787716579799972,0.44496846191169337
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'lunch_standard'})","frozenset({'reading_cat_μέτριο', 'gender_male'})",0.225,0.229,0.101,0.4488888888888889,1.960213488597768,1.0,0.049475000000000005,1.398991935483871,0.6320664324496965,0.2861189801699717,0.2851995964836432,0.44496846191169337
frozenset({'gender_male'}),"frozenset({'math_cat_υψηλό', 'writing_cat_μέτριο', 'lunch_standard'})",0.482,0.054,0.051,0.10580912863070539,1.9594283079760257,1.0,0.024971999999999998,1.0579396751740138,0.9452645923234158,0.10515463917525772,0.0547665207512742,0.5251267865375749
"frozenset({'math_cat_υψηλό', 'writing_cat_μέτριο', 'lunch_standard'})",frozenset({'gender_male'}),0.054,0.482,0.051,0.9444444444444444,1.9594283079760257,1.0,0.024971999999999998,9.323999999999996,0.5175973137669444,0.10515463917525772,0.8927498927498927,0.5251267865375749
"frozenset({'reading_cat_μέτριο', 'gender_female'})","frozenset({'lunch_free/reduced', 'writing_cat_μέτριο'})",0.261,0.153,0.078,0.29885057471264365,1.9532717301479978,1.0,0.038067,1.208016393442623,0.6604038721765378,0.23214285714285712,0.17219666435967373,0.40432724814063553
"frozenset({'lunch_free/reduced', 'writing_cat_μέτριο'})","frozenset({'reading_cat_μέτριο', 'gender_female'})",0.153,0.261,0.078,0.5098039215686274,1.9532717301479976,1.0,0.038067,1.5075599999999998,0.5761965307419853,0.23214285714285712,0.33667648385470555,0.40432724814063553
"frozenset({'writing_cat_μέτριο', 'gender_male'})","frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})",0.227,0.228,0.101,0.44493392070484583,1.9514645644949378,1.0,0.049244,1.390825396825397,0.6307430225558132,0.28531073446327687,0.28100248795964483,0.4439581884225984
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'gender_male'})",0.228,0.227,0.101,0.44298245614035087,1.9514645644949378,1.0,0.049244,1.3877480314960629,0.6315600471964294,0.28531073446327687,0.2794080933251628,0.4439581884225984
"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced', 'gender_female'})",frozenset({'reading_cat_χαμηλό'}),0.11,0.275,0.059,0.5363636363636364,1.9504132231404958,1.0,0.028749999999999994,1.5637254901960784,0.5475147590935059,0.18098159509202452,0.3605015673981191,0.37545454545454543
frozenset({'reading_cat_χαμηλό'}),"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced', 'gender_female'})",0.275,0.11,0.059,0.2145454545454545,1.9504132231404956,1.0,0.028749999999999994,1.1331018518518519,0.6721215663354763,0.18098159509202452,0.11746680286006124,0.37545454545454543
"frozenset({'math_cat_μέτριο', 'race/ethnicity_group D'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'gender_male'})",0.145,0.191,0.054,0.3724137931034483,1.949810435096588,1.0,0.026305000000000002,1.289065934065934,0.56974225687676,0.1914893617021277,0.22424449085716724,0.3275681530962268
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'gender_male'})","frozenset({'math_cat_μέτριο', 'race/ethnicity_group D'})",0.191,0.145,0.054,0.2827225130890052,1.9498104350965877,1.0,0.026305000000000002,1.192007299270073,0.6021379847090602,0.1914893617021277,0.16107896267719907,0.3275681530962268
"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'math_cat_μέτριο'})","frozenset({'reading_cat_μέτριο', 'test preparation course_none'})",0.1,0.318,0.062,0.62,1.949685534591195,1.0,0.030199999999999998,1.794736842105263,0.5412186379928314,0.1741573033707865,0.4428152492668622,0.4074842767295597
"frozenset({'reading_cat_μέτριο', 'test preparation course_none'})","frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'math_cat_μέτριο'})",0.318,0.1,0.062,0.19496855345911948,1.9496855345911948,1.0,0.030199999999999998,1.11796875,0.71421814397881,0.1741573033707865,0.1055206149545772,0.4074842767295597
"frozenset({'writing_cat_μέτριο', 'gender_male'})","frozenset({'test preparation course_completed', 'reading_cat_μέτριο'})",0.227,0.172,0.076,0.33480176211453744,1.9465218727589388,1.0,0.036956,1.2447417218543047,0.6290597126710696,0.23529411764705882,0.1966204856455766,0.3883311136154083
"frozenset({'test preparation course_completed', 'reading_cat_μέτριο'})","frozenset({'writing_cat_μέτριο', 'gender_male'})",0.172,0.227,0.076,0.4418604651162791,1.9465218727589386,1.0,0.036956,1.3849583333333333,0.5872743452834986,0.23529411764705882,0.2779566172267517,0.3883311136154083
"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο'})","frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})",0.154,0.208,0.062,0.4025974025974026,1.9355644355644357,1.0,0.029968,1.3257391304347828,0.5713414169145123,0.20666666666666667,0.24570379115833665,0.3503371628371629
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})","frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο'})",0.208,0.154,0.062,0.2980769230769231,1.9355644355644355,1.0,0.029968,1.2052602739726028,0.61029651352232,0.20666666666666667,0.17030369158028733,0.3503371628371629
"frozenset({'reading_cat_μέτριο', 'race/ethnicity_group D'})","frozenset({'writing_cat_μέτριο', 'test preparation course_none'})",0.136,0.308,0.081,0.5955882352941176,1.9337280366692131,1.0,0.039112,1.7111272727272726,0.5588705989940558,0.2231404958677686,0.4155899353961238,0.4292876241405653
"frozenset({'writing_cat_μέτριο', 'test preparation course_none'})","frozenset({'reading_cat_μέτριο', 'race/ethnicity_group D'})",0.308,0.136,0.081,0.262987012987013,1.9337280366692131,1.0,0.039112,1.1722995594713657,0.6977806322700351,0.2231404958677686,0.14697570947570948,0.4292876241405653
frozenset({'math_cat_χαμηλό'}),"frozenset({'lunch_free/reduced', 'gender_female', 'test preparation course_none'})",0.339,0.119,0.078,0.23008849557522124,1.9335167695396744,1.0,0.037659,1.1442873563218392,0.730420109391365,0.20526315789473684,0.1260936385643828,0.4427753402245854
"frozenset({'lunch_free/reduced', 'gender_female', 'test preparation course_none'})",frozenset({'math_cat_χαμηλό'}),0.119,0.339,0.078,0.6554621848739496,1.9335167695396742,1.0,0.037659,1.9185121951219513,0.5480223522221251,0.20526315789473684,0.4787627607775333,0.4427753402245854
"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'gender_female'})","frozenset({'reading_cat_μέτριο', 'lunch_standard'})",0.155,0.327,0.098,0.6322580645161291,1.9335109006609452,1.0,0.047315,1.830087719298246,0.5713681922473132,0.2552083333333333,0.453578104778795,0.4659761270592878
"frozenset({'reading_cat_μέτριο', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'gender_female'})",0.327,0.155,0.098,0.2996941896024465,1.933510900660945,1.0,0.047315,1.2066157205240176,0.7173939412317675,0.2552083333333333,0.17123572734017337,0.4659761270592878
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'gender_female'})","frozenset({'writing_cat_μέτριο', 'lunch_standard'})",0.15,0.338,0.098,0.6533333333333334,1.9329388560157792,1.0,0.0473,1.909615384615385,0.5678271308523409,0.2512820512820513,0.4763343403826789,0.47163708086785017
"frozenset({'writing_cat_μέτριο', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'gender_female'})",0.338,0.15,0.098,0.28994082840236685,1.932938856015779,1.0,0.0473,1.1970833333333333,0.7290831740551207,0.2512820512820513,0.16463626870866688,0.47163708086785017
"frozenset({'math_cat_χαμηλό', 'writing_cat_μέτριο', 'lunch_standard'})",frozenset({'gender_female'}),0.059,0.518,0.059,1.0,1.9305019305019304,1.0,0.028437999999999998,inf,0.5122210414452709,0.1138996138996139,1.0,0.5569498069498069
frozenset({'gender_female'}),"frozenset({'math_cat_χαμηλό', 'writing_cat_μέτριο', 'lunch_standard'})",0.518,0.059,0.059,0.1138996138996139,1.9305019305019304,1.0,0.028437999999999998,1.0619564270152506,1.0,0.1138996138996139,0.05834177885187449,0.5569498069498069
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'writing_cat_μέτριο', 'gender_male', 'lunch_standard'})",0.325,0.161,0.101,0.3107692307692308,1.9302436693741043,1.0,0.048675,1.2172991071428572,0.7139713971397139,0.26233766233766237,0.1785092142660677,0.46904921165790736
"frozenset({'writing_cat_μέτριο', 'gender_male', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο'})",0.161,0.325,0.101,0.6273291925465839,1.9302436693741043,1.0,0.048675,1.8112500000000005,0.5744108379848711,0.26233766233766237,0.4478951000690132,0.46904921165790736
"frozenset({'math_cat_χαμηλό', 'reading_cat_χαμηλό', 'writing_cat_χαμηλό'})","frozenset({'lunch_free/reduced', 'test preparation course_none'})",0.206,0.224,0.089,0.43203883495145634,1.92874479889043,1.0,0.042856,1.3662905982905984,0.6064585514957688,0.26099706744868034,0.26809128215393857,0.41468013176144247
"frozenset({'lunch_free/reduced', 'test preparation course_none'})","frozenset({'math_cat_χαμηλό', 'reading_cat_χαμηλό', 'writing_cat_χαμηλό'})",0.224,0.206,0.089,0.39732142857142855,1.92874479889043,1.0,0.042856,1.317451851851852,0.6205258890304645,0.26099706744868034,0.24095897804965813,0.41468013176144247
"frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'gender_female', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})",0.12,0.32,0.074,0.6166666666666667,1.9270833333333335,1.0,0.0356,1.7739130434782608,0.5466830466830467,0.20218579234972678,0.43627450980392163,0.4239583333333333
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})","frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'gender_female', 'lunch_standard'})",0.32,0.12,0.074,0.23124999999999998,1.9270833333333333,1.0,0.0356,1.1447154471544714,0.7074721780604135,0.20218579234972678,0.12642045454545453,0.4239583333333333
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'test preparation course_none'})",0.325,0.099,0.062,0.19076923076923077,1.926961926961927,1.0,0.029824999999999997,1.113403041825095,0.712664277180406,0.17127071823204418,0.1018526423631862,0.4085159285159285
"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'test preparation course_none'})","frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο'})",0.099,0.325,0.062,0.6262626262626262,1.9269619269619267,1.0,0.029824999999999997,1.8060810810810808,0.5339049801296051,0.17127071823204418,0.4463150018705573,0.4085159285159285
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'race/ethnicity_group D'})","frozenset({'reading_cat_μέτριο', 'lunch_standard'})",0.1,0.327,0.063,0.63,1.926605504587156,1.0,0.0303,1.818918918918919,0.5343915343915344,0.17307692307692304,0.450222882615156,0.4113302752293578
"frozenset({'reading_cat_μέτριο', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'race/ethnicity_group D'})",0.327,0.1,0.063,0.19266055045871558,1.9266055045871557,1.0,0.0303,1.1147727272727272,0.7146394962145333,0.17307692307692304,0.10295616717635063,0.4113302752293578
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})","frozenset({'test preparation course_completed', 'reading_cat_μέτριο', 'lunch_standard'})",0.32,0.104,0.064,0.2,1.9230769230769234,1.0,0.030720000000000004,1.1199999999999999,0.7058823529411766,0.17777777777777778,0.10714285714285716,0.4076923076923077
"frozenset({'test preparation course_completed', 'reading_cat_μέτριο', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})",0.104,0.32,0.064,0.6153846153846154,1.9230769230769231,1.0,0.030720000000000004,1.768,0.5357142857142858,0.17777777777777778,0.4343891402714933,0.4076923076923077
"frozenset({'writing_cat_χαμηλό', 'gender_male'})",frozenset({'math_cat_χαμηλό'}),0.201,0.339,0.131,0.6517412935323383,1.9225406888859538,1.0,0.062861,1.8980142857142859,0.6005694140576484,0.3202933985330073,0.4731335756918885,0.5190859859992075
frozenset({'math_cat_χαμηλό'}),"frozenset({'writing_cat_χαμηλό', 'gender_male'})",0.339,0.201,0.131,0.3864306784660767,1.9225406888859535,1.0,0.062861,1.302216346153846,0.7259530436188518,0.3202933985330073,0.23207844614027123,0.5190859859992075
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'test preparation course_none'})",0.228,0.308,0.135,0.5921052631578948,1.9224196855775806,1.0,0.06477600000000001,1.6965161290322581,0.6215313759355211,0.33665835411471323,0.41055673866747805,0.5152084757347916
"frozenset({'writing_cat_μέτριο', 'test preparation course_none'})","frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})",0.308,0.228,0.135,0.4383116883116883,1.9224196855775804,1.0,0.06477600000000001,1.37442774566474,0.6933847141939629,0.33665835411471323,0.2724244667249848,0.5152084757347916
"frozenset({'math_cat_χαμηλό', 'gender_female', 'test preparation course_none'})",frozenset({'writing_cat_χαμηλό'}),0.147,0.301,0.085,0.5782312925170069,1.9210341944086609,1.0,0.04075300000000001,1.6573064516129037,0.5620715812702574,0.23415977961432513,0.39661129115451627,0.43031165954754
frozenset({'writing_cat_χαμηλό'}),"frozenset({'math_cat_χαμηλό', 'gender_female', 'test preparation course_none'})",0.301,0.147,0.085,0.2823920265780731,1.9210341944086606,1.0,0.04075300000000001,1.1886712962962962,0.6859042329378104,0.23415977961432513,0.15872453291685007,0.43031165954754
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})","frozenset({'writing_cat_μέτριο', 'lunch_standard'})",0.208,0.338,0.135,0.6490384615384616,1.9202321347291762,1.0,0.064696,1.8862465753424658,0.6050879161990272,0.3284671532846715,0.46984661863815946,0.5242233727810651
"frozenset({'writing_cat_μέτριο', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})",0.338,0.208,0.135,0.3994082840236686,1.9202321347291762,1.0,0.064696,1.3186995073891625,0.7239118272350902,0.3284671532846715,0.2416771262925109,0.5242233727810651
"frozenset({'reading_cat_μέτριο', 'lunch_standard', 'gender_male'})","frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'test preparation course_none'})",0.155,0.205,0.061,0.3935483870967742,1.919748229740362,1.0,0.029225,1.310904255319149,0.5669803084683287,0.2040133779264214,0.23716778251166568,0.34555468135326517
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'test preparation course_none'})","frozenset({'reading_cat_μέτριο', 'lunch_standard', 'gender_male'})",0.205,0.155,0.061,0.2975609756097561,1.9197482297403619,1.0,0.029225,1.202951388888889,0.6026394473657077,0.2040133779264214,0.16871121373935632,0.34555468135326517
"frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'race/ethnicity_group D'})","frozenset({'writing_cat_μέτριο', 'lunch_standard'})",0.091,0.338,0.059,0.6483516483516484,1.918200143052214,1.0,0.028241999999999996,1.8825625,0.5265984225541198,0.15945945945945944,0.46880913648285255,0.41145393068469993
"frozenset({'writing_cat_μέτριο', 'lunch_standard'})","frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'race/ethnicity_group D'})",0.338,0.091,0.059,0.17455621301775145,1.9182001430522138,1.0,0.028241999999999996,1.101225806451613,0.7230784986430436,0.15945945945945944,0.09192102642216882,0.41145393068469993
"frozenset({'reading_cat_μέτριο', 'lunch_standard'})","frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'test preparation course_none'})",0.327,0.099,0.062,0.18960244648318042,1.9151762271028325,1.0,0.029626999999999994,1.1118000000000001,0.7100369074437999,0.1703296703296703,0.10055765425436228,0.4079325363729033
"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'test preparation course_none'})","frozenset({'reading_cat_μέτριο', 'lunch_standard'})",0.099,0.327,0.062,0.6262626262626262,1.9151762271028323,1.0,0.029626999999999994,1.8007297297297296,0.5303605313092978,0.1703296703296703,0.44466957839914734,0.4079325363729033
frozenset({'writing_cat_μέτριο'}),"frozenset({'race/ethnicity_group E', 'reading_cat_μέτριο'})",0.491,0.066,0.062,0.12627291242362526,1.9132259458125038,1.0,0.029593999999999995,1.068983682983683,0.9377653843716329,0.12525252525252525,0.06453202614949172,0.5328334259087824
"frozenset({'race/ethnicity_group E', 'reading_cat_μέτριο'})",frozenset({'writing_cat_μέτριο'}),0.066,0.491,0.062,0.9393939393939393,1.9132259458125038,1.0,0.029593999999999995,8.398499999999991,0.5110520135387165,0.12525252525252525,0.8809311186521401,0.5328334259087824
"frozenset({'test preparation course_completed', 'gender_female'})","frozenset({'writing_cat_υψηλό', 'lunch_standard'})",0.184,0.162,0.057,0.3097826086956522,1.9122383252818038,1.0,0.027192,1.2141102362204725,0.5846233230134159,0.19723183391003463,0.17635156168932245,0.33081723027375204
"frozenset({'writing_cat_υψηλό', 'lunch_standard'})","frozenset({'test preparation course_completed', 'gender_female'})",0.162,0.184,0.057,0.35185185185185186,1.9122383252818036,1.0,0.027192,1.2589714285714286,0.5692752166813214,0.19723183391003463,0.20570079883805373,0.33081723027375204
frozenset({'lunch_free/reduced'}),"frozenset({'writing_cat_χαμηλό', 'gender_female', 'reading_cat_χαμηλό'})",0.355,0.084,0.057,0.16056338028169015,1.9114688128772637,1.0,0.027180000000000003,1.0912080536912752,0.7392900856793146,0.14921465968586387,0.08358447629005474,0.4195674044265594
"frozenset({'writing_cat_χαμηλό', 'gender_female', 'reading_cat_χαμηλό'})",frozenset({'lunch_free/reduced'}),0.084,0.355,0.057,0.6785714285714286,1.9114688128772637,1.0,0.027180000000000003,2.006666666666667,0.5205699839117445,0.14921465968586387,0.5016611295681064,0.4195674044265594
"frozenset({'reading_cat_μέτριο', 'test preparation course_none'})","frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'lunch_standard'})",0.318,0.102,0.062,0.19496855345911948,1.911456406461956,1.0,0.029564,1.1154843749999999,0.6991769936619053,0.17318435754189945,0.10352845596783906,0.40140584535701074
"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'lunch_standard'})","frozenset({'reading_cat_μέτριο', 'test preparation course_none'})",0.102,0.318,0.062,0.607843137254902,1.911456406461956,1.0,0.029564,1.7391,0.5310007902866585,0.17318435754189945,0.4249899373239033,0.40140584535701074
"frozenset({'writing_cat_χαμηλό', 'test preparation course_none'})","frozenset({'math_cat_χαμηλό', 'lunch_free/reduced', 'gender_female'})",0.243,0.11,0.051,0.20987654320987653,1.9079685746352413,1.0,0.024269999999999996,1.12640625,0.6286424741627166,0.16887417218543047,0.112220835067277,0.3367564534231201
"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced', 'gender_female'})","frozenset({'writing_cat_χαμηλό', 'test preparation course_none'})",0.11,0.243,0.051,0.4636363636363636,1.9079685746352413,1.0,0.024269999999999996,1.4113559322033897,0.5346992729676139,0.16887417218543047,0.2914615107481686,0.3367564534231201
"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'gender_female'})","frozenset({'reading_cat_μέτριο', 'test preparation course_none'})",0.155,0.318,0.094,0.6064516129032258,1.9070805437208358,1.0,0.04471,1.732950819672131,0.5628855596122372,0.24802110817941952,0.422949579036988,0.45102454858997765
"frozenset({'reading_cat_μέτριο', 'test preparation course_none'})","frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'gender_female'})",0.318,0.155,0.094,0.29559748427672955,1.9070805437208358,1.0,0.04471,1.1995982142857142,0.6974168590503526,0.24802110817941952,0.1663875553570764,0.45102454858997765
frozenset({'reading_cat_χαμηλό'}),"frozenset({'math_cat_χαμηλό', 'gender_female', 'test preparation course_none'})",0.275,0.147,0.077,0.27999999999999997,1.9047619047619047,1.0,0.036574999999999996,1.1847222222222222,0.6551724137931034,0.22318840579710142,0.15592028135990618,0.4019047619047619
"frozenset({'math_cat_χαμηλό', 'gender_female', 'test preparation course_none'})",frozenset({'reading_cat_χαμηλό'}),0.147,0.275,0.077,0.5238095238095238,1.9047619047619047,1.0,0.036574999999999996,1.5225,0.5568581477139507,0.22318840579710142,0.34318555008210183,0.4019047619047619
"frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'race/ethnicity_group D'})",0.223,0.139,0.059,0.2645739910313901,1.9034100074200726,1.0,0.028002999999999993,1.17075,0.6108457125406276,0.19471947194719472,0.14584667947896646,0.34451721134303315
"frozenset({'writing_cat_μέτριο', 'race/ethnicity_group D'})","frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",0.139,0.223,0.059,0.4244604316546762,1.9034100074200726,1.0,0.028002999999999993,1.3500374999999998,0.5512510088781274,0.19471947194719472,0.25927983481940303,0.34451721134303315
"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced', 'gender_female'})",frozenset({'writing_cat_χαμηλό'}),0.11,0.301,0.063,0.5727272727272728,1.9027484143763216,1.0,0.02989,1.635957446808511,0.533083645443196,0.1810344827586207,0.38873715697750033,0.39101479915433407
frozenset({'writing_cat_χαμηλό'}),"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced', 'gender_female'})",0.301,0.11,0.063,0.20930232558139536,1.9027484143763214,1.0,0.02989,1.1255882352941178,0.6787474169448418,0.1810344827586207,0.11157564672066894,0.39101479915433407
"frozenset({'lunch_free/reduced', 'test preparation course_none'})","frozenset({'writing_cat_χαμηλό', 'math_cat_χαμηλό'})",0.224,0.23,0.098,0.4375,1.9021739130434783,1.0,0.04648,1.3688888888888888,0.6111929307805596,0.2752808988764045,0.2694805194805195,0.43179347826086956
"frozenset({'writing_cat_χαμηλό', 'math_cat_χαμηλό'})","frozenset({'lunch_free/reduced', 'test preparation course_none'})",0.23,0.224,0.098,0.4260869565217391,1.902173913043478,1.0,0.04648,1.352121212121212,0.6159554730983302,0.2752808988764045,0.2604213357238906,0.43179347826086956
"frozenset({'math_cat_χαμηλό', 'test preparation course_none'})","frozenset({'writing_cat_χαμηλό', 'gender_male', 'lunch_standard'})",0.246,0.109,0.051,0.2073170731707317,1.9019914969791898,1.0,0.024185999999999996,1.1240307692307692,0.6289592760180995,0.16776315789473684,0.11034463879992334,0.3376034907138062
"frozenset({'writing_cat_χαμηλό', 'gender_male', 'lunch_standard'})","frozenset({'math_cat_χαμηλό', 'test preparation course_none'})",0.109,0.246,0.051,0.4678899082568807,1.9019914969791898,1.0,0.024185999999999996,1.417,0.5322506106819832,0.16776315789473684,0.2942836979534227,0.3376034907138062
"frozenset({'reading_cat_μέτριο', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})",0.327,0.32,0.199,0.6085626911314985,1.9017584097859328,1.0,0.09436,1.7371875,0.70456293353842,0.4441964285714286,0.424356898722792,0.6152188455657492
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})","frozenset({'reading_cat_μέτριο', 'lunch_standard'})",0.32,0.327,0.199,0.6218750000000001,1.9017584097859328,1.0,0.09436,1.7798347107438022,0.6973100798108188,0.4441964285714286,0.4381500742942051,0.6152188455657492
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'lunch_standard'})","frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'gender_female'})",0.225,0.173,0.074,0.32888888888888884,1.901091843288375,1.0,0.035074999999999995,1.2322847682119202,0.6115954664341761,0.22839506172839505,0.1884992610506516,0.37831727681438665
"frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'gender_female'})","frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'lunch_standard'})",0.173,0.225,0.074,0.4277456647398844,1.901091843288375,1.0,0.035074999999999995,1.3542929292929293,0.5731396450864407,0.22839506172839505,0.26160730934178633,0.37831727681438665
frozenset({'lunch_free/reduced'}),"frozenset({'math_cat_χαμηλό', 'gender_female', 'reading_cat_χαμηλό', 'writing_cat_χαμηλό'})",0.355,0.083,0.056,0.15774647887323945,1.9005599864245715,1.0,0.026535,1.0887458193979933,0.7346345514950167,0.14659685863874344,0.0815119725989525,0.41622263702698115
"frozenset({'math_cat_χαμηλό', 'gender_female', 'reading_cat_χαμηλό', 'writing_cat_χαμηλό'})",frozenset({'lunch_free/reduced'}),0.083,0.355,0.056,0.6746987951807228,1.9005599864245715,1.0,0.026535,1.9827777777777775,0.5167276834397881,0.14659685863874344,0.4956570467918184,0.41622263702698115
frozenset({'gender_female'}),"frozenset({'math_cat_χαμηλό', 'writing_cat_μέτριο', 'test preparation course_none'})",0.518,0.063,0.062,0.11969111969111969,1.8998590427161854,1.0,0.029365999999999996,1.0643991228070175,0.9826663097309597,0.11946050096339116,0.06050279582830276,0.5519090519090519
"frozenset({'math_cat_χαμηλό', 'writing_cat_μέτριο', 'test preparation course_none'})",frozenset({'gender_female'}),0.063,0.518,0.062,0.9841269841269841,1.8998590427161854,1.0,0.029365999999999996,30.365999999999893,0.5054911006300133,0.11946050096339116,0.9670684317987222,0.5519090519090519
"frozenset({'writing_cat_μέτριο', 'lunch_standard'})","frozenset({'reading_cat_μέτριο', 'parental level of education_some high school'})",0.338,0.081,0.052,0.15384615384615383,1.8993352326685657,1.0,0.024621999999999995,1.0860909090909092,0.7152567975830815,0.1416893732970027,0.07926676153009121,0.39791073124406456
"frozenset({'reading_cat_μέτριο', 'parental level of education_some high school'})","frozenset({'writing_cat_μέτριο', 'lunch_standard'})",0.081,0.338,0.052,0.6419753086419753,1.8993352326685657,1.0,0.024621999999999995,1.8490344827586203,0.5152339499455929,0.1416893732970027,0.4591772033866696,0.39791073124406456
"frozenset({'reading_cat_μέτριο', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'test preparation course_none'})",0.327,0.308,0.191,0.5840978593272171,1.8964216211922635,1.0,0.090284,1.6638529411764704,0.7023641894152151,0.43018018018018017,0.3989853458485797,0.6021138647285437
"frozenset({'writing_cat_μέτριο', 'test preparation course_none'})","frozenset({'reading_cat_μέτριο', 'lunch_standard'})",0.308,0.327,0.191,0.6201298701298702,1.8964216211922635,1.0,0.090284,1.77165811965812,0.6830796235208668,0.43018018018018017,0.4355570135659289,0.6021138647285437
frozenset({'writing_cat_μέτριο'}),"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'race/ethnicity_group D', 'gender_male'})",0.491,0.058,0.054,0.109979633401222,1.8962005758831377,1.0,0.025522,1.0584027459954233,0.9285454413155788,0.10909090909090907,0.055180077920617826,0.5205070580799214
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'race/ethnicity_group D', 'gender_male'})",frozenset({'writing_cat_μέτριο'}),0.058,0.491,0.054,0.9310344827586207,1.8962005758831377,1.0,0.025522,7.380499999999997,0.5017299677596918,0.10909090909090907,0.864507824673125,0.5205070580799214
frozenset({'gender_female'}),"frozenset({'math_cat_χαμηλό', 'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})",0.518,0.056,0.055,0.10617760617760617,1.8960286817429672,1.0,0.025991999999999998,1.0561382289416845,0.9804602036967182,0.10597302504816955,0.05315424383220993,0.5441602316602316
"frozenset({'math_cat_χαμηλό', 'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})",frozenset({'gender_female'}),0.056,0.518,0.055,0.9821428571428571,1.8960286817429672,1.0,0.025991999999999998,26.991999999999926,0.5006163328197226,0.10597302504816955,0.9629519857735624,0.5441602316602316
"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'math_cat_μέτριο'})","frozenset({'reading_cat_μέτριο', 'lunch_standard'})",0.1,0.327,0.062,0.62,1.896024464831804,1.0,0.0293,1.7710526315789474,0.525089605734767,0.1698630136986301,0.4353640416047548,0.4048012232415902
"frozenset({'reading_cat_μέτριο', 'lunch_standard'})","frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'math_cat_μέτριο'})",0.327,0.1,0.062,0.18960244648318042,1.896024464831804,1.0,0.0293,1.110566037735849,0.7022000671044433,0.1698630136986301,0.09955827387020046,0.4048012232415902
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'writing_cat_μέτριο', 'gender_male', 'test preparation course_none', 'lunch_standard'})",0.325,0.099,0.061,0.18769230769230769,1.8958818958818957,1.0,0.028824999999999996,1.109185606060606,0.7000607164541589,0.1680440771349862,0.09843763339878765,0.4019269619269619
"frozenset({'writing_cat_μέτριο', 'gender_male', 'test preparation course_none', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο'})",0.099,0.325,0.061,0.6161616161616161,1.8958818958818957,1.0,0.028824999999999996,1.7585526315789473,0.5244628008951802,0.1680440771349862,0.4313505424616535,0.4019269619269619
frozenset({'writing_cat_μέτριο'}),"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none', 'gender_female'})",0.491,0.101,0.094,0.1914460285132383,1.8955052328043396,1.0,0.044409,1.1118614609571789,0.9281653638757681,0.18875502008032127,0.10060737320716162,0.5610695489100844
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none', 'gender_female'})",frozenset({'writing_cat_μέτριο'}),0.101,0.491,0.094,0.9306930693069306,1.8955052328043394,1.0,0.044409,7.344142857142851,0.5255129813267697,0.18875502008032127,0.8638370713299227,0.5610695489100844
"frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})",0.223,0.32,0.135,0.6053811659192826,1.891816143497758,1.0,0.06364,1.7231818181818184,0.6067019400352733,0.33088235294117646,0.4196781851754156,0.5136280829596414
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})","frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",0.32,0.223,0.135,0.421875,1.8918161434977578,1.0,0.06364,1.344,0.6932461873638345,0.33088235294117646,0.25595238095238093,0.5136280829596414
frozenset({'math_cat_χαμηλό'}),"frozenset({'writing_cat_χαμηλό', 'gender_male', 'reading_cat_χαμηλό', 'lunch_standard'})",0.339,0.092,0.059,0.17404129793510323,1.891753238425035,1.0,0.027811999999999996,1.0993285714285714,0.713146490935665,0.15860215053763438,0.09035385235143527,0.4076728228805951
"frozenset({'writing_cat_χαμηλό', 'gender_male', 'reading_cat_χαμηλό', 'lunch_standard'})",frozenset({'math_cat_χαμηλό'}),0.092,0.339,0.059,0.6413043478260869,1.891753238425035,1.0,0.027811999999999996,1.8427878787878786,0.519151795714179,0.15860215053763438,0.457343945273959,0.4076728228805951
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})","frozenset({'reading_cat_μέτριο', 'gender_female'})",0.15,0.261,0.074,0.49333333333333335,1.8901660280970625,1.0,0.03485,1.4585526315789477,0.5540540540540541,0.21958456973293766,0.3143888137122237,0.38842911877394637
"frozenset({'reading_cat_μέτριο', 'gender_female'})","frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",0.261,0.15,0.074,0.28352490421455934,1.8901660280970622,1.0,0.03485,1.1863636363636363,0.6372746223896427,0.21958456973293766,0.15708812260536395,0.38842911877394637
"frozenset({'math_cat_χαμηλό', 'gender_female', 'test preparation course_none'})","frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό'})",0.147,0.252,0.07,0.4761904761904763,1.8896447467876043,1.0,0.032956000000000006,1.4280000000000002,0.5519343493552169,0.2127659574468085,0.2997198879551822,0.37698412698412703
"frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό'})","frozenset({'math_cat_χαμηλό', 'gender_female', 'test preparation course_none'})",0.252,0.147,0.07,0.2777777777777778,1.889644746787604,1.0,0.032956000000000006,1.1810769230769231,0.6294117647058824,0.2127659574468085,0.15331509704311583,0.37698412698412703
"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",frozenset({'reading_cat_μέτριο'}),0.067,0.49,0.062,0.9253731343283581,1.8885166006701186,1.0,0.02917,6.833999999999991,0.5042699581647824,0.12525252525252525,0.8536728124085453,0.5259518732866281
frozenset({'reading_cat_μέτριο'}),"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'math_cat_μέτριο', 'test preparation course_none'})",0.49,0.067,0.062,0.12653061224489795,1.8885166006701186,1.0,0.02917,1.0681542056074766,0.9225173940543959,0.12525252525252525,0.06380558654329897,0.5259518732866281
frozenset({'reading_cat_μέτριο'}),"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",0.49,0.067,0.062,0.12653061224489795,1.8885166006701186,1.0,0.02917,1.0681542056074766,0.9225173940543959,0.12525252525252525,0.06380558654329897,0.5259518732866281
"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'math_cat_μέτριο', 'test preparation course_none'})",frozenset({'reading_cat_μέτριο'}),0.067,0.49,0.062,0.9253731343283581,1.8885166006701186,1.0,0.02917,6.833999999999991,0.5042699581647824,0.12525252525252525,0.8536728124085453,0.5259518732866281
frozenset({'writing_cat_μέτριο'}),"frozenset({'test preparation course_completed', 'math_cat_μέτριο', 'reading_cat_μέτριο', 'gender_male'})",0.491,0.068,0.063,0.12830957230142567,1.8869054750209657,1.0,0.029612,1.0691869158878504,0.9234415442666917,0.1270161290322581,0.06470984152513484,0.5273900802683599
"frozenset({'test preparation course_completed', 'math_cat_μέτριο', 'reading_cat_μέτριο', 'gender_male'})",frozenset({'writing_cat_μέτριο'}),0.068,0.491,0.063,0.926470588235294,1.8869054750209655,1.0,0.029612,6.9223999999999934,0.5043259077593841,0.1270161290322581,0.8555414307176701,0.5273900802683599
"frozenset({'reading_cat_μέτριο', 'test preparation course_none'})","frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'lunch_standard'})",0.318,0.225,0.135,0.42452830188679247,1.8867924528301887,1.0,0.06345,1.34672131147541,0.6891495601173021,0.33088235294117646,0.25745587340231285,0.5122641509433963
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'lunch_standard'})","frozenset({'reading_cat_μέτριο', 'test preparation course_none'})",0.225,0.318,0.135,0.6,1.8867924528301885,1.0,0.06345,1.7049999999999998,0.6064516129032258,0.33088235294117646,0.41348973607038125,0.5122641509433963
"frozenset({'gender_male', 'writing_cat_μέτριο', 'lunch_standard', 'test preparation course_none', 'math_cat_μέτριο'})",frozenset({'reading_cat_μέτριο'}),0.066,0.49,0.061,0.9242424242424242,1.886209029066172,1.0,0.028659999999999998,6.731999999999997,0.5030364727770562,0.12323232323232322,0.8514557338086749,0.5243661100803958
frozenset({'reading_cat_μέτριο'}),"frozenset({'gender_male', 'writing_cat_μέτριο', 'lunch_standard', 'test preparation course_none', 'math_cat_μέτριο'})",0.49,0.066,0.061,0.12448979591836734,1.8862090290661717,1.0,0.028659999999999998,1.0668065268065268,0.9212471873995499,0.12323232323232322,0.06262290783551107,0.5243661100803958
"frozenset({'writing_cat_χαμηλό', 'gender_male'})","frozenset({'math_cat_χαμηλό', 'lunch_free/reduced'})",0.201,0.182,0.069,0.34328358208955223,1.8861735279645728,1.0,0.032418,1.2455909090909092,0.5880176307340698,0.2197452229299363,0.1971681932635113,0.3612022306052157
"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced'})","frozenset({'writing_cat_χαμηλό', 'gender_male'})",0.182,0.201,0.069,0.37912087912087916,1.8861735279645728,1.0,0.032418,1.2868849557522122,0.5743595195067502,0.2197452229299363,0.22292976110247706,0.3612022306052157
frozenset({'writing_cat_μέτριο'}),"frozenset({'reading_cat_μέτριο', 'lunch_standard', 'test preparation course_none', 'gender_female', 'math_cat_μέτριο'})",0.491,0.08,0.074,0.15071283095723015,1.8839103869653768,1.0,0.034719999999999994,1.0832613908872901,0.9217862263048903,0.1488933601609658,0.07686177277959798,0.5378564154786151
"frozenset({'reading_cat_μέτριο', 'lunch_standard', 'test preparation course_none', 'gender_female', 'math_cat_μέτριο'})",frozenset({'writing_cat_μέτριο'}),0.08,0.491,0.074,0.9249999999999999,1.8839103869653766,1.0,0.034719999999999994,6.786666666666661,0.5099882491186838,0.1488933601609658,0.8526522593320235,0.5378564154786151
"frozenset({'writing_cat_μέτριο', 'test preparation course_none'})","frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'race/ethnicity_group D'})",0.308,0.1,0.058,0.18831168831168832,1.8831168831168832,1.0,0.027200000000000002,1.1088,0.677695834163843,0.1657142857142857,0.09812409812409813,0.38415584415584414
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'race/ethnicity_group D'})","frozenset({'writing_cat_μέτριο', 'test preparation course_none'})",0.1,0.308,0.058,0.58,1.883116883116883,1.0,0.027200000000000002,1.6476190476190473,0.5210727969348659,0.1657142857142857,0.39306358381502887,0.38415584415584414
"frozenset({'writing_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο'})",0.221,0.325,0.135,0.6108597285067874,1.8795683954054996,1.0,0.06317500000000001,1.7345930232558142,0.6007226738934056,0.3284671532846715,0.4234958940841294,0.5131221719457014
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'writing_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",0.325,0.221,0.135,0.4153846153846154,1.8795683954054996,1.0,0.06317500000000001,1.3325000000000002,0.693278463648834,0.3284671532846715,0.24953095684803003,0.5131221719457014
"frozenset({'reading_cat_μέτριο', 'lunch_standard'})","frozenset({'parental level of education_high school', 'writing_cat_μέτριο'})",0.327,0.096,0.059,0.18042813455657492,1.879459734964322,1.0,0.027607999999999994,1.1030149253731343,0.6952930213816202,0.16208791208791207,0.09339395415550322,0.3975057339449541
"frozenset({'parental level of education_high school', 'writing_cat_μέτριο'})","frozenset({'reading_cat_μέτριο', 'lunch_standard'})",0.096,0.327,0.059,0.6145833333333333,1.8794597349643218,1.0,0.027607999999999994,1.746162162162162,0.5176241187940601,0.16208791208791207,0.4273155027241207,0.3975057339449541
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'test preparation course_none', 'race/ethnicity_group D'})",frozenset({'reading_cat_μέτριο'}),0.063,0.49,0.058,0.9206349206349207,1.8788467768059607,1.0,0.027130000000000005,6.426000000000005,0.49920877341478676,0.11717171717171719,0.8443821973233739,0.5195011337868481
frozenset({'reading_cat_μέτριο'}),"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'test preparation course_none', 'race/ethnicity_group D'})",0.49,0.063,0.058,0.11836734693877551,1.8788467768059605,1.0,0.027130000000000005,1.062800925925926,0.9171737660581475,0.11717171717171719,0.059090018077668637,0.5195011337868481
"frozenset({'reading_cat_μέτριο', 'gender_female'})","frozenset({'writing_cat_μέτριο', 'test preparation course_none'})",0.261,0.308,0.151,0.578544061302682,1.8783898094242921,1.0,0.070612,1.6419272727272727,0.6327863857548683,0.36124401913875603,0.3909596261599451,0.5344019007812111
"frozenset({'writing_cat_μέτριο', 'test preparation course_none'})","frozenset({'reading_cat_μέτριο', 'gender_female'})",0.308,0.261,0.151,0.49025974025974023,1.878389809424292,1.0,0.070612,1.4497579617834393,0.6757646518393753,0.36124401913875603,0.3102296891200815,0.5344019007812111
frozenset({'reading_cat_μέτριο'}),"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'math_cat_μέτριο'})",0.49,0.1,0.092,0.18775510204081633,1.8775510204081631,1.0,0.043,1.1080402010050252,0.9164535379369138,0.18473895582329317,0.09750566893424036,0.5538775510204081
"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'math_cat_μέτριο'})",frozenset({'reading_cat_μέτριο'}),0.1,0.49,0.092,0.9199999999999999,1.8775510204081631,1.0,0.043,6.374999999999995,0.5193236714975845,0.18473895582329317,0.8431372549019607,0.5538775510204081
frozenset({'writing_cat_μέτριο'}),"frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'race/ethnicity_group D', 'lunch_standard'})",0.491,0.064,0.059,0.12016293279022403,1.8775458248472503,1.0,0.027575999999999996,1.0638333333333332,0.9182511404881621,0.11895161290322581,0.06000313332288892,0.5210189663951119
"frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'race/ethnicity_group D', 'lunch_standard'})",frozenset({'writing_cat_μέτριο'}),0.064,0.491,0.059,0.9218749999999999,1.8775458248472503,1.0,0.027575999999999996,6.515199999999991,0.499348109517601,0.11895161290322581,0.8465127701375244,0.5210189663951119
"frozenset({'math_cat_χαμηλό', 'writing_cat_μέτριο'})",frozenset({'gender_female'}),0.109,0.518,0.106,0.9724770642201834,1.8773688498459138,1.0,0.049538,17.51266666666663,0.5245113609893484,0.20345489443378117,0.9428984734858578,0.588555134426694
frozenset({'gender_female'}),"frozenset({'math_cat_χαμηλό', 'writing_cat_μέτριο'})",0.518,0.109,0.106,0.2046332046332046,1.8773688498459138,1.0,0.049538,1.12023786407767,0.9695842793392312,0.20345489443378117,0.10733244066577398,0.588555134426694
frozenset({'reading_cat_μέτριο'}),"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'test preparation course_none'})",0.49,0.099,0.091,0.18571428571428572,1.875901875901876,1.0,0.04248999999999999,1.1064912280701755,0.9155354449472095,0.1827309236947791,0.0962422704931029,0.5524531024531024
"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'test preparation course_none'})",frozenset({'reading_cat_μέτριο'}),0.099,0.49,0.091,0.9191919191919191,1.8759018759018757,1.0,0.04248999999999999,6.311249999999994,0.5182276103474771,0.1827309236947791,0.8415527827292532,0.5524531024531024
"frozenset({'math_cat_μέτριο', 'gender_male'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'race/ethnicity_group D'})",0.24,0.12,0.054,0.225,1.8750000000000002,1.0,0.0252,1.135483870967742,0.6140350877192983,0.17647058823529413,0.11931818181818182,0.3375
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'race/ethnicity_group D'})","frozenset({'math_cat_μέτριο', 'gender_male'})",0.12,0.24,0.054,0.45,1.8750000000000002,1.0,0.0252,1.3818181818181816,0.5303030303030303,0.17647058823529413,0.27631578947368424,0.3375
frozenset({'reading_cat_μέτριο'}),"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'gender_male', 'lunch_standard'})",0.49,0.11,0.101,0.20612244897959187,1.873840445269017,1.0,0.04710000000000001,1.1210796915167096,0.9143855562026791,0.20240480961923848,0.10800275166246277,0.562152133580705
"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'gender_male', 'lunch_standard'})",frozenset({'reading_cat_μέτριο'}),0.11,0.49,0.101,0.9181818181818182,1.8738404452690167,1.0,0.04710000000000001,6.233333333333335,0.5239737456891757,0.20240480961923848,0.839572192513369,0.562152133580705
frozenset({'reading_cat_μέτριο'}),"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'gender_male'})",0.49,0.061,0.056,0.1142857142857143,1.8735362997658083,1.0,0.02611,1.0601612903225808,0.9142156862745098,0.11313131313131315,0.056747299558801166,0.5161592505854801
"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'gender_male'})",frozenset({'reading_cat_μέτριο'}),0.061,0.49,0.056,0.9180327868852459,1.873536299765808,1.0,0.02611,6.222000000000001,0.49653887113951006,0.11313131313131315,0.839279974284796,0.5161592505854801
"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'gender_male'})","frozenset({'reading_cat_μέτριο', 'lunch_standard'})",0.165,0.327,0.101,0.6121212121212122,1.8719303122972848,1.0,0.047045000000000003,1.7350781250000005,0.5578348253987075,0.25831202046035806,0.42365707596019636,0.4604948568251321
"frozenset({'reading_cat_μέτριο', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'gender_male'})",0.327,0.165,0.101,0.308868501529052,1.8719303122972846,1.0,0.047045000000000003,1.2081637168141592,0.6921130448854692,0.25831202046035806,0.17229760662161914,0.4604948568251321
"frozenset({'math_cat_χαμηλό', 'reading_cat_χαμηλό', 'writing_cat_χαμηλό'})","frozenset({'lunch_free/reduced', 'gender_male'})",0.206,0.166,0.064,0.31067961165048547,1.871563925605334,1.0,0.029804000000000004,1.209887323943662,0.5865081863979849,0.2077922077922078,0.17347675257851974,0.3481108901625921
"frozenset({'lunch_free/reduced', 'gender_male'})","frozenset({'math_cat_χαμηλό', 'reading_cat_χαμηλό', 'writing_cat_χαμηλό'})",0.166,0.206,0.064,0.38554216867469876,1.8715639256053338,1.0,0.029804000000000004,1.2921960784313724,0.5583782973621104,0.2077922077922078,0.22612363812934352,0.3481108901625921
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'lunch_standard'})",0.325,0.102,0.062,0.19076923076923077,1.8702865761689293,1.0,0.02885,1.1096958174904943,0.6893667861409796,0.16986301369863013,0.09885215007709441,0.3993061840120664
"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο'})",0.102,0.325,0.062,0.607843137254902,1.8702865761689291,1.0,0.02885,1.7212500000000004,0.5181765931460593,0.16986301369863013,0.4190268700072622,0.3993061840120664
frozenset({'gender_female'}),"frozenset({'math_cat_χαμηλό', 'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.518,0.096,0.093,0.17953667953667954,1.8701737451737452,1.0,0.043272,1.1018164705882354,0.9653326194619194,0.1785028790786948,0.09240783134588444,0.5741433397683398
"frozenset({'math_cat_χαμηλό', 'writing_cat_μέτριο', 'reading_cat_μέτριο'})",frozenset({'gender_female'}),0.096,0.518,0.093,0.96875,1.870173745173745,1.0,0.043272,15.424,0.5147016842706251,0.1785028790786948,0.9351659751037344,0.5741433397683398
"frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'gender_male'})","frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'lunch_standard'})",0.145,0.225,0.061,0.4206896551724138,1.8697318007662835,1.0,0.028374999999999997,1.337797619047619,0.5440513852938357,0.19741100323624594,0.25250278086763067,0.3459003831417624
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'lunch_standard'})","frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'gender_male'})",0.225,0.145,0.061,0.2711111111111111,1.8697318007662835,1.0,0.028374999999999997,1.1730182926829267,0.600211528291909,0.19741100323624594,0.147498375568551,0.3459003831417624
frozenset({'reading_cat_μέτριο'}),"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'gender_female', 'test preparation course_none'})",0.49,0.059,0.054,0.11020408163265306,1.8678657903839504,1.0,0.02509,1.057545871559633,0.9110384894698621,0.1090909090909091,0.054414539460842785,0.5127291594603943
"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'gender_female', 'test preparation course_none'})",frozenset({'reading_cat_μέτριο'}),0.059,0.49,0.054,0.9152542372881356,1.8678657903839502,1.0,0.02509,6.018000000000001,0.49376156177431413,0.1090909090909091,0.8338318378198737,0.5127291594603943
"frozenset({'writing_cat_μέτριο', 'gender_female', 'test preparation course_none', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο'})",0.122,0.325,0.074,0.6065573770491803,1.8663303909205549,1.0,0.03435,1.7156250000000002,0.528689281536662,0.19839142091152814,0.4171220400728597,0.417124842370744
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'writing_cat_μέτριο', 'gender_female', 'test preparation course_none', 'lunch_standard'})",0.325,0.122,0.074,0.22769230769230767,1.8663303909205546,1.0,0.03435,1.136852589641434,0.6876876876876876,0.19839142091152814,0.12037848256527069,0.417124842370744
"frozenset({'reading_cat_μέτριο', 'test preparation course_none'})","frozenset({'writing_cat_μέτριο', 'gender_female', 'lunch_standard'})",0.318,0.177,0.105,0.330188679245283,1.865472764097644,1.0,0.048714,1.2287042253521128,0.6802681189777965,0.2692307692307692,0.18613448267956625,0.46170450911416694
"frozenset({'writing_cat_μέτριο', 'gender_female', 'lunch_standard'})","frozenset({'reading_cat_μέτριο', 'test preparation course_none'})",0.177,0.318,0.105,0.5932203389830508,1.865472764097644,1.0,0.048714,1.676583333333333,0.5637215761152579,0.2692307692307692,0.40354888413937073,0.46170450911416694
frozenset({'writing_cat_μέτριο'}),"frozenset({'reading_cat_μέτριο', 'race/ethnicity_group D', 'gender_male'})",0.491,0.071,0.065,0.1323828920570265,1.8645477754510775,1.0,0.030139000000000006,1.0707488262910798,0.910956626870183,0.130784708249497,0.06607415721961948,0.5239379249017527
"frozenset({'reading_cat_μέτριο', 'race/ethnicity_group D', 'gender_male'})",frozenset({'writing_cat_μέτριο'}),0.071,0.491,0.065,0.915492957746479,1.8645477754510773,1.0,0.030139000000000006,6.0231666666666746,0.49911401838204855,0.130784708249497,0.8339743767121395,0.5239379249017527
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'race/ethnicity_group D'})","frozenset({'writing_cat_μέτριο', 'lunch_standard'})",0.1,0.338,0.063,0.63,1.863905325443787,1.0,0.029199999999999997,1.789189189189189,0.5149911816578483,0.16799999999999998,0.4410876132930514,0.4081952662721893
"frozenset({'writing_cat_μέτριο', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'race/ethnicity_group D'})",0.338,0.1,0.063,0.18639053254437868,1.8639053254437867,1.0,0.029199999999999997,1.1061818181818182,0.7001390687191292,0.16799999999999998,0.09598948060486519,0.4081952662721893
"frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'test preparation course_none'})",0.108,0.308,0.062,0.5740740740740741,1.863876863876864,1.0,0.028735999999999998,1.6246956521739129,0.5196007522059887,0.1751412429378531,0.38450010704345966,0.3876863876863877
"frozenset({'writing_cat_μέτριο', 'test preparation course_none'})","frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο', 'lunch_standard'})",0.308,0.108,0.062,0.2012987012987013,1.863876863876864,1.0,0.028735999999999998,1.1168130081300813,0.6697743800111877,0.1751412429378531,0.10459495661289385,0.3876863876863877
"frozenset({'writing_cat_μέτριο', 'test preparation course_none'})","frozenset({'race/ethnicity_group C', 'math_cat_μέτριο', 'reading_cat_μέτριο'})",0.308,0.108,0.062,0.2012987012987013,1.863876863876864,1.0,0.028735999999999998,1.1168130081300813,0.6697743800111877,0.1751412429378531,0.10459495661289385,0.3876863876863877
"frozenset({'race/ethnicity_group C', 'math_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'writing_cat_μέτριο', 'test preparation course_none'})",0.108,0.308,0.062,0.5740740740740741,1.863876863876864,1.0,0.028735999999999998,1.6246956521739129,0.5196007522059887,0.1751412429378531,0.38450010704345966,0.3876863876863877
"frozenset({'lunch_free/reduced', 'gender_female', 'test preparation course_none'})","frozenset({'math_cat_χαμηλό', 'writing_cat_χαμηλό'})",0.119,0.23,0.051,0.42857142857142855,1.8633540372670805,1.0,0.023629999999999998,1.3475000000000001,0.5259175179720015,0.17114093959731544,0.2578849721706864,0.3251552795031056
"frozenset({'math_cat_χαμηλό', 'writing_cat_χαμηλό'})","frozenset({'lunch_free/reduced', 'gender_female', 'test preparation course_none'})",0.23,0.119,0.051,0.22173913043478258,1.8633540372670805,1.0,0.023629999999999998,1.1320111731843574,0.6017316017316017,0.17114093959731544,0.11661649311553074,0.3251552795031056
"frozenset({'gender_male', 'test preparation course_none'})","frozenset({'reading_cat_χαμηλό', 'lunch_standard'})",0.308,0.136,0.078,0.2532467532467533,1.8621084797555387,1.0,0.036112,1.157008695652174,0.6690380910034089,0.21311475409836067,0.1357022607022607,0.41338808250572956
"frozenset({'reading_cat_χαμηλό', 'lunch_standard'})","frozenset({'gender_male', 'test preparation course_none'})",0.136,0.308,0.078,0.5735294117647058,1.8621084797555385,1.0,0.036112,1.6226206896551723,0.5358499525166192,0.21311475409836067,0.3837130227813669,0.41338808250572956
"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'math_cat_μέτριο', 'lunch_standard'})",frozenset({'reading_cat_μέτριο'}),0.068,0.49,0.062,0.9117647058823529,1.8607442977190876,1.0,0.028679999999999997,5.7799999999999985,0.49633116433614843,0.12499999999999999,0.8269896193771626,0.5191476590636255
frozenset({'reading_cat_μέτριο'}),"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'math_cat_μέτριο', 'lunch_standard'})",0.49,0.068,0.062,0.12653061224489795,1.8607442977190873,1.0,0.028679999999999997,1.0670093457943923,0.907020872865275,0.12499999999999999,0.06280108609967591,0.5191476590636255
"frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο'})","frozenset({'writing_cat_μέτριο', 'gender_female'})",0.169,0.264,0.083,0.4911242603550296,1.8603191680114757,1.0,0.038384,1.446325581395349,0.5565076189233468,0.23714285714285713,0.30859274504759454,0.4027590998744845
"frozenset({'writing_cat_μέτριο', 'gender_female'})","frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο'})",0.264,0.169,0.083,0.3143939393939394,1.8603191680114757,1.0,0.038384,1.2120662983425414,0.6283394447354635,0.23714285714285713,0.17496262261605222,0.4027590998744845
"frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'gender_female'})",0.108,0.264,0.053,0.49074074074074076,1.8588664421997756,1.0,0.024487999999999996,1.4452363636363634,0.517979524494458,0.16614420062695923,0.30807165861513686,0.34574915824915825
"frozenset({'writing_cat_μέτριο', 'gender_female'})","frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο', 'lunch_standard'})",0.264,0.108,0.053,0.20075757575757575,1.8588664421997754,1.0,0.024487999999999996,1.1160568720379147,0.627768662838392,0.16614420062695923,0.10398831362956922,0.34574915824915825
"frozenset({'reading_cat_μέτριο', 'test preparation course_none'})","frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο'})",0.318,0.154,0.091,0.2861635220125786,1.8582046883933676,1.0,0.042027999999999996,1.1851453744493392,0.6771937739679675,0.23884514435695536,0.15622165722527023,0.43853630646083475
"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο'})","frozenset({'reading_cat_μέτριο', 'test preparation course_none'})",0.154,0.318,0.091,0.5909090909090909,1.8582046883933676,1.0,0.042027999999999996,1.667111111111111,0.5459174395344608,0.23884514435695536,0.40015995734470816,0.43853630646083475
"frozenset({'test preparation course_completed', 'lunch_standard'})","frozenset({'writing_cat_υψηλό', 'reading_cat_υψηλό'})",0.227,0.185,0.078,0.3436123348017621,1.857363971901417,1.0,0.036005,1.2416442953020133,0.5971572627458785,0.2335329341317365,0.19461636171995353,0.3826169782116919
"frozenset({'writing_cat_υψηλό', 'reading_cat_υψηλό'})","frozenset({'test preparation course_completed', 'lunch_standard'})",0.185,0.227,0.078,0.42162162162162165,1.8573639719014168,1.0,0.036005,1.3364953271028037,0.566383514236275,0.2335329341317365,0.2517744134820461,0.3826169782116919
frozenset({'reading_cat_μέτριο'}),"frozenset({'test preparation course_completed', 'lunch_free/reduced', 'writing_cat_μέτριο'})",0.49,0.066,0.06,0.12244897959183673,1.8552875695732838,1.0,0.027659999999999997,1.0643255813953487,0.9039215686274509,0.12096774193548386,0.06043787964864746,0.5157699443413729
"frozenset({'test preparation course_completed', 'lunch_free/reduced', 'writing_cat_μέτριο'})",frozenset({'reading_cat_μέτριο'}),0.066,0.49,0.06,0.9090909090909091,1.8552875695732838,1.0,0.027659999999999997,5.6099999999999985,0.493576017130621,0.12096774193548386,0.8217468805704099,0.5157699443413729
"frozenset({'writing_cat_μέτριο', 'gender_female', 'test preparation course_none'})","frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})",0.175,0.228,0.074,0.4228571428571429,1.8546365914786969,1.0,0.0341,1.3376237623762375,0.5585585585585586,0.2249240121580547,0.2524056254626203,0.3737092731829574
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'gender_female', 'test preparation course_none'})",0.228,0.175,0.074,0.3245614035087719,1.8546365914786966,1.0,0.0341,1.221428571428571,0.5969051953507912,0.2249240121580547,0.1812865497076023,0.3737092731829574
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})","frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'gender_male'})",0.32,0.145,0.086,0.26875,1.853448275862069,1.0,0.039599999999999996,1.1692307692307693,0.677154582763338,0.22691292875989444,0.14473684210526316,0.430926724137931
"frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'gender_male'})","frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})",0.145,0.32,0.086,0.593103448275862,1.8534482758620687,1.0,0.039599999999999996,1.6711864406779657,0.5385556915544675,0.22691292875989444,0.4016227180527383,0.430926724137931
"frozenset({'writing_cat_μέτριο', 'gender_male'})","frozenset({'math_cat_μέτριο', 'race/ethnicity_group D'})",0.227,0.145,0.061,0.2687224669603524,1.8532583928300166,1.0,0.028085,1.1691867469879518,0.5956142769282972,0.19614147909967844,0.14470463971971043,0.34470606106638313
"frozenset({'math_cat_μέτριο', 'race/ethnicity_group D'})","frozenset({'writing_cat_μέτριο', 'gender_male'})",0.145,0.227,0.061,0.4206896551724138,1.8532583928300166,1.0,0.028085,1.334345238095238,0.5384910363340044,0.19614147909967844,0.25056876477673196,0.34470606106638313
"frozenset({'writing_cat_μέτριο', 'test preparation course_none'})","frozenset({'reading_cat_μέτριο', 'parental level of education_some college'})",0.308,0.114,0.065,0.21103896103896105,1.8512189564821144,1.0,0.029888000000000005,1.1229958847736625,0.6644730991551802,0.18207282913165268,0.10952478672568967,0.3906071998177261
"frozenset({'reading_cat_μέτριο', 'parental level of education_some college'})","frozenset({'writing_cat_μέτριο', 'test preparation course_none'})",0.114,0.308,0.065,0.5701754385964912,1.8512189564821144,1.0,0.029888000000000005,1.6099591836734692,0.5189789894078833,0.18207282913165268,0.37886624074637465,0.3906071998177261
"frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'lunch_standard', 'gender_male'})","frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})",0.103,0.32,0.061,0.5922330097087379,1.850728155339806,1.0,0.028040000000000002,1.6676190476190478,0.5124549957051739,0.1685082872928177,0.40034266133637936,0.39142900485436893
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})","frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'lunch_standard', 'gender_male'})",0.32,0.103,0.061,0.190625,1.8507281553398058,1.0,0.028040000000000002,1.1082625482625483,0.6759884281581486,0.1685082872928177,0.09768673355629877,0.39142900485436893
"frozenset({'reading_cat_μέτριο', 'parental level of education_some college', 'gender_female'})",frozenset({'writing_cat_μέτριο'}),0.065,0.491,0.059,0.9076923076923076,1.8486605044649849,1.0,0.027084999999999998,5.51416666666666,0.49098160065258767,0.1187122736418511,0.8186489345624903,0.5139276202412658
frozenset({'writing_cat_μέτριο'}),"frozenset({'reading_cat_μέτριο', 'parental level of education_some college', 'gender_female'})",0.491,0.065,0.059,0.12016293279022403,1.8486605044649849,1.0,0.027084999999999998,1.0626967592592593,0.9019013685857946,0.1187122736418511,0.0589977890804535,0.5139276202412658
frozenset({'reading_cat_μέτριο'}),"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'gender_male', 'test preparation course_none'})",0.49,0.095,0.086,0.17551020408163265,1.8474758324382383,1.0,0.03944999999999999,1.0976485148514852,0.8994528043775649,0.1723446893787575,0.08896155147141728,0.5403866809881848
"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'gender_male', 'test preparation course_none'})",frozenset({'reading_cat_μέτριο'}),0.095,0.49,0.086,0.9052631578947368,1.8474758324382383,1.0,0.03944999999999999,5.383333333333329,0.5068739560580752,0.1723446893787575,0.8142414860681113,0.5403866809881848
frozenset({'reading_cat_μέτριο'}),"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', ""parental level of education_associate's degree""})",0.49,0.063,0.057,0.1163265306122449,1.8464528668610303,1.0,0.026130000000000004,1.0603464203233257,0.8988648090815274,0.1149193548387097,0.056911985712107684,0.5105442176870748
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', ""parental level of education_associate's degree""})",frozenset({'reading_cat_μέτριο'}),0.063,0.49,0.057,0.9047619047619048,1.8464528668610303,1.0,0.026130000000000004,5.355,0.4892433859461889,0.1149193548387097,0.8132586367880486,0.5105442176870748
"frozenset({'writing_cat_μέτριο', ""parental level of education_associate's degree""})","frozenset({'reading_cat_μέτριο', 'gender_female'})",0.11,0.261,0.053,0.4818181818181818,1.8460466736328804,1.0,0.024289999999999996,1.426140350877193,0.5149459402162391,0.16666666666666666,0.2988067412965924,0.34244165795889936
"frozenset({'reading_cat_μέτριο', 'gender_female'})","frozenset({'writing_cat_μέτριο', ""parental level of education_associate's degree""})",0.261,0.11,0.053,0.20306513409961685,1.8460466736328804,1.0,0.024289999999999996,1.1167788461538461,0.6201649347665125,0.16666666666666666,0.10456756640406388,0.34244165795889936
frozenset({'writing_cat_μέτριο'}),"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none', 'race/ethnicity_group D'})",0.491,0.064,0.058,0.11812627291242363,1.8457230142566192,1.0,0.026576000000000002,1.0613764434180137,0.9002100128717566,0.11670020120724348,0.057827214650025244,0.5121881364562119
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none', 'race/ethnicity_group D'})",frozenset({'writing_cat_μέτριο'}),0.064,0.491,0.058,0.90625,1.8457230142566192,1.0,0.026576000000000002,5.429333333333333,0.48953728264073093,0.11670020120724348,0.8158153241650294,0.5121881364562119
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'gender_female'})",0.152,0.264,0.074,0.4868421052631579,1.8440988835725678,1.0,0.03387199999999999,1.4342564102564104,0.539775624681285,0.2163742690058479,0.3027745995423341,0.3835725677830941
"frozenset({'writing_cat_μέτριο', 'gender_female'})","frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",0.264,0.152,0.074,0.2803030303030303,1.8440988835725676,1.0,0.03387199999999999,1.1782736842105261,0.621915393654524,0.2163742690058479,0.15130074328187534,0.3835725677830941
"frozenset({'writing_cat_υψηλό', 'math_cat_μέτριο'})","frozenset({'gender_female', 'lunch_standard'})",0.094,0.329,0.057,0.6063829787234043,1.8431093578218976,1.0,0.026074,1.704702702702703,0.5048991131249758,0.15573770491803277,0.41338744966230145,0.3898176291793313
"frozenset({'gender_female', 'lunch_standard'})","frozenset({'writing_cat_υψηλό', 'math_cat_μέτριο'})",0.329,0.094,0.057,0.17325227963525835,1.8431093578218973,1.0,0.026074,1.0958602941176472,0.6817266713729181,0.15573770491803277,0.08747492233472225,0.3898176291793313
"frozenset({'lunch_free/reduced', 'test preparation course_none'})","frozenset({'math_cat_χαμηλό', 'reading_cat_χαμηλό'})",0.224,0.218,0.09,0.40178571428571425,1.84305373525557,1.0,0.041167999999999996,1.3072238805970147,0.5894616265750285,0.2556818181818182,0.23502009499451948,0.40731487549148093
"frozenset({'math_cat_χαμηλό', 'reading_cat_χαμηλό'})","frozenset({'lunch_free/reduced', 'test preparation course_none'})",0.218,0.224,0.09,0.4128440366972477,1.84305373525557,1.0,0.041167999999999996,1.321625,0.5849389030974709,0.2556818181818182,0.2433557173933604,0.40731487549148093
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'gender_male'})",frozenset({'reading_cat_μέτριο'}),0.165,0.49,0.149,0.903030303030303,1.8429189857761286,1.0,0.06814999999999999,5.259374999999996,0.5477635333360125,0.2944664031620553,0.8098633392751039,0.6035559678416821
frozenset({'reading_cat_μέτριο'}),"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'gender_male'})",0.49,0.165,0.149,0.3040816326530612,1.8429189857761283,1.0,0.06814999999999999,1.1998533724340175,0.8968285300697458,0.2944664031620553,0.1665648295246242,0.6035559678416821
"frozenset({'test preparation course_completed', 'reading_cat_μέτριο', 'gender_male'})",frozenset({'writing_cat_μέτριο'}),0.084,0.491,0.076,0.9047619047619047,1.8426922703908446,1.0,0.034755999999999995,5.344499999999994,0.4992530452769478,0.1523046092184369,0.8128917578819345,0.5297740277373678
frozenset({'writing_cat_μέτριο'}),"frozenset({'test preparation course_completed', 'reading_cat_μέτριο', 'gender_male'})",0.491,0.084,0.076,0.15478615071283094,1.8426922703908444,1.0,0.034755999999999995,1.0837493975903616,0.8984593113431908,0.1523046092184369,0.07727745711007744,0.5297740277373678
frozenset({'writing_cat_υψηλό'}),"frozenset({'test preparation course_completed', 'lunch_standard'})",0.208,0.227,0.087,0.4182692307692308,1.8425957302609286,1.0,0.03978399999999999,1.3287933884297523,0.5773830256588877,0.25,0.24743755597571898,0.4007645713317519
"frozenset({'test preparation course_completed', 'lunch_standard'})",frozenset({'writing_cat_υψηλό'}),0.227,0.208,0.087,0.3832599118942731,1.8425957302609286,1.0,0.03978399999999999,1.2841714285714285,0.5915748464706844,0.25,0.22128776754327412,0.4007645713317519
"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο'})",frozenset({'reading_cat_μέτριο'}),0.154,0.49,0.139,0.9025974025974027,1.8420355155049035,1.0,0.06354000000000001,5.236000000000007,0.540333690494413,0.27524752475247527,0.8090145148968682,0.5931354359925789
frozenset({'reading_cat_μέτριο'}),"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο'})",0.49,0.154,0.139,0.2836734693877551,1.8420355155049033,1.0,0.06354000000000001,1.181025641025641,0.8963182395260263,0.27524752475247527,0.15327833260963963,0.5931354359925789
frozenset({'math_cat_χαμηλό'}),"frozenset({'writing_cat_χαμηλό', 'gender_male', 'test preparation course_none'})",0.339,0.157,0.098,0.28908554572271383,1.841309208424929,1.0,0.044777,1.1857966804979252,0.6912377659081786,0.24623115577889446,0.15668510761887763,0.4566446836893824
"frozenset({'writing_cat_χαμηλό', 'gender_male', 'test preparation course_none'})",frozenset({'math_cat_χαμηλό'}),0.157,0.339,0.098,0.6242038216560509,1.841309208424929,1.0,0.044777,1.7589322033898305,0.5420025661510155,0.24623115577889446,0.43147325515287577,0.4566446836893824
"frozenset({'lunch_free/reduced', 'gender_male'})",frozenset({'writing_cat_χαμηλό'}),0.166,0.301,0.092,0.5542168674698795,1.8412520513949486,1.0,0.042033999999999995,1.5680270270270271,0.5478313001772495,0.24533333333333332,0.3622558905148491,0.4299323540007205
frozenset({'writing_cat_χαμηλό'}),"frozenset({'lunch_free/reduced', 'gender_male'})",0.301,0.166,0.092,0.30564784053156147,1.8412520513949484,1.0,0.042033999999999995,1.2011196172248804,0.6536356285376623,0.24533333333333332,0.16744345387477394,0.4299323540007205
"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο'})","frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο'})",0.154,0.325,0.092,0.5974025974025974,1.838161838161838,1.0,0.041949999999999994,1.6766129032258064,0.5389813958269092,0.23772609819121446,0.4035594035594035,0.4402397602397602
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο'})",0.325,0.154,0.092,0.28307692307692306,1.838161838161838,1.0,0.041949999999999994,1.1800429184549355,0.6755233494363928,0.23772609819121446,0.1525731951263866,0.4402397602397602
frozenset({'reading_cat_μέτριο'}),"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",0.49,0.15,0.135,0.2755102040816327,1.8367346938775513,1.0,0.06150000000000001,1.1732394366197183,0.8932461873638345,0.26732673267326734,0.14765906362545023,0.5877551020408164
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",frozenset({'reading_cat_μέτριο'}),0.15,0.49,0.135,0.9000000000000001,1.8367346938775513,1.0,0.06150000000000001,5.100000000000007,0.5359477124183007,0.26732673267326734,0.8039215686274512,0.5877551020408164
frozenset({'reading_cat_μέτριο'}),"frozenset({'test preparation course_completed', 'math_cat_μέτριο', 'writing_cat_μέτριο', 'gender_male'})",0.49,0.07,0.063,0.1285714285714286,1.836734693877551,1.0,0.028699999999999996,1.0672131147540982,0.8932461873638343,0.1267605633802817,0.06298003072196622,0.5142857142857142
frozenset({'reading_cat_μέτριο'}),"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'race/ethnicity_group D', 'lunch_standard'})",0.49,0.07,0.063,0.1285714285714286,1.836734693877551,1.0,0.028699999999999996,1.0672131147540982,0.8932461873638343,0.1267605633802817,0.06298003072196622,0.5142857142857142
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'race/ethnicity_group D', 'lunch_standard'})",frozenset({'reading_cat_μέτριο'}),0.07,0.49,0.063,0.8999999999999999,1.8367346938775508,1.0,0.028699999999999996,5.099999999999995,0.48984468339307047,0.1267605633802817,0.8039215686274508,0.5142857142857142
"frozenset({'test preparation course_completed', 'math_cat_μέτριο', 'writing_cat_μέτριο', 'gender_male'})",frozenset({'reading_cat_μέτριο'}),0.07,0.49,0.063,0.8999999999999999,1.8367346938775508,1.0,0.028699999999999996,5.099999999999995,0.48984468339307047,0.1267605633802817,0.8039215686274508,0.5142857142857142
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})","frozenset({'reading_cat_μέτριο', 'parental level of education_some college'})",0.32,0.114,0.067,0.209375,1.8366228070175439,1.0,0.03052,1.1206324110671937,0.6698858647936787,0.18256130790190736,0.10764672686230248,0.39854714912280703
"frozenset({'reading_cat_μέτριο', 'parental level of education_some college'})","frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})",0.114,0.32,0.067,0.5877192982456141,1.8366228070175439,1.0,0.03052,1.6493617021276596,0.5141336208348775,0.18256130790190736,0.39370485036119723,0.39854714912280703
frozenset({'gender_female'}),"frozenset({'math_cat_χαμηλό', 'reading_cat_μέτριο', 'lunch_standard'})",0.518,0.061,0.058,0.11196911196911197,1.835559212608393,1.0,0.026402000000000002,1.0573956521739132,0.9444126484475605,0.11132437619961615,0.05428020443994885,0.5313943920501298
"frozenset({'math_cat_χαμηλό', 'reading_cat_μέτριο', 'lunch_standard'})",frozenset({'gender_female'}),0.061,0.518,0.058,0.9508196721311476,1.835559212608393,1.0,0.026402000000000002,9.800666666666677,0.48477837758437076,0.11132437619961615,0.8979661247534182,0.5313943920501298
"frozenset({'test preparation course_completed', 'reading_cat_μέτριο'})","frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})",0.172,0.32,0.101,0.5872093023255814,1.835029069767442,1.0,0.04596000000000001,1.6473239436619718,0.5495766968001148,0.25831202046035806,0.3929548563611492,0.4514171511627907
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})","frozenset({'test preparation course_completed', 'reading_cat_μέτριο'})",0.32,0.172,0.101,0.315625,1.8350290697674418,1.0,0.04596000000000001,1.2098630136986304,0.6691904484566105,0.25831202046035806,0.17346014492753623,0.4514171511627907
"frozenset({'reading_cat_μέτριο', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'gender_female', 'test preparation course_none'})",0.327,0.175,0.105,0.3211009174311926,1.8348623853211008,1.0,0.047775,1.2152027027027026,0.676077265973254,0.2644836272040302,0.17709202112871836,0.4605504587155963
"frozenset({'writing_cat_μέτριο', 'gender_female', 'test preparation course_none'})","frozenset({'reading_cat_μέτριο', 'lunch_standard'})",0.175,0.327,0.105,0.6,1.8348623853211008,1.0,0.047775,1.6825,0.5515151515151515,0.2644836272040302,0.4056463595839524,0.4605504587155963
"frozenset({'writing_cat_υψηλό', 'math_cat_μέτριο', 'lunch_standard'})",frozenset({'gender_female'}),0.06,0.518,0.057,0.9500000000000001,1.833976833976834,1.0,0.025920000000000002,9.640000000000013,0.4837625979843226,0.10940499040307101,0.8962655601659753,0.530019305019305
frozenset({'gender_female'}),"frozenset({'writing_cat_υψηλό', 'math_cat_μέτριο', 'lunch_standard'})",0.518,0.06,0.057,0.11003861003861004,1.833976833976834,1.0,0.025920000000000002,1.0562255965292842,0.9434374317536581,0.10940499040307101,0.05323256387086175,0.530019305019305
"frozenset({'writing_cat_μέτριο', 'gender_female', ""parental level of education_associate's degree""})",frozenset({'reading_cat_μέτριο'}),0.059,0.49,0.053,0.8983050847457628,1.833275683154618,1.0,0.02409,5.015000000000002,0.48302688829627255,0.10685483870967744,0.8005982053838485,0.5032341750259426
frozenset({'reading_cat_μέτριο'}),"frozenset({'writing_cat_μέτριο', 'gender_female', ""parental level of education_associate's degree""})",0.49,0.059,0.053,0.10816326530612244,1.8332756831546178,1.0,0.02409,1.0551258581235698,0.8912319644839068,0.10685483870967744,0.05224576546878049,0.5032341750259426
frozenset({'writing_cat_μέτριο'}),"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'race/ethnicity_group D', 'lunch_standard'})",0.491,0.07,0.063,0.12830957230142567,1.8329938900203666,1.0,0.028629999999999996,1.066892523364486,0.8928181619733682,0.12650602409638556,0.06269846484024266,0.5141547861507128
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'race/ethnicity_group D', 'lunch_standard'})",frozenset({'writing_cat_μέτριο'}),0.07,0.491,0.063,0.8999999999999999,1.8329938900203664,1.0,0.028629999999999996,5.089999999999995,0.4886499402628435,0.12650602409638556,0.8035363457760313,0.5141547861507128
"frozenset({'writing_cat_μέτριο', 'race/ethnicity_group D'})","frozenset({'reading_cat_μέτριο', 'test preparation course_none'})",0.139,0.318,0.081,0.5827338129496402,1.8324962671372333,1.0,0.036798,1.6344482758620686,0.5276379747924463,0.21542553191489364,0.38817274626046955,0.41872539704085787
"frozenset({'reading_cat_μέτριο', 'test preparation course_none'})","frozenset({'writing_cat_μέτριο', 'race/ethnicity_group D'})",0.318,0.139,0.081,0.25471698113207547,1.8324962671372333,1.0,0.036798,1.1552658227848103,0.6661236016074725,0.21542553191489364,0.13439835206977405,0.41872539704085787
"frozenset({'reading_cat_μέτριο', 'gender_male'})","frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'test preparation course_none'})",0.229,0.205,0.086,0.3755458515283842,1.831930983065289,1.0,0.03905499999999999,1.2731118881118881,0.5890115524990197,0.2471264367816092,0.21452308368350215,0.3975290233251677
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'test preparation course_none'})","frozenset({'reading_cat_μέτριο', 'gender_male'})",0.205,0.229,0.086,0.4195121951219512,1.831930983065289,1.0,0.03905499999999999,1.3281932773109242,0.5712300716688605,0.2471264367816092,0.2470975293410521,0.3975290233251677
frozenset({'reading_cat_μέτριο'}),"frozenset({'lunch_free/reduced', 'gender_female', 'writing_cat_μέτριο'})",0.49,0.087,0.078,0.15918367346938775,1.8296973961998593,1.0,0.035370000000000006,1.0858495145631069,0.8891402714932127,0.15631262525050102,0.07906207389856271,0.5278676988036594
"frozenset({'lunch_free/reduced', 'gender_female', 'writing_cat_μέτριο'})",frozenset({'reading_cat_μέτριο'}),0.087,0.49,0.078,0.896551724137931,1.8296973961998593,1.0,0.035370000000000006,4.930000000000001,0.4966720026960992,0.15631262525050102,0.797160243407708,0.5278676988036594
"frozenset({""parental level of education_associate's degree"", 'reading_cat_μέτριο', 'gender_female'})",frozenset({'writing_cat_μέτριο'}),0.059,0.491,0.053,0.8983050847457628,1.8295419241257898,1.0,0.024031,5.005166666666669,0.48184388346399853,0.10663983903420522,0.8002064533315575,0.5031240291345922
frozenset({'writing_cat_μέτριο'}),"frozenset({""parental level of education_associate's degree"", 'reading_cat_μέτριο', 'gender_female'})",0.491,0.059,0.053,0.10794297352342158,1.8295419241257895,1.0,0.024031,1.0548652968036532,0.8907958631426771,0.10663983903420522,0.05201166155517702,0.5031240291345922
"frozenset({'writing_cat_μέτριο', 'race/ethnicity_group D'})","frozenset({'math_cat_μέτριο', 'gender_male'})",0.139,0.24,0.061,0.4388489208633093,1.8285371702637887,1.0,0.027639999999999998,1.3543589743589741,0.5262656842025094,0.19182389937106917,0.261643316925407,0.34650779376498797
"frozenset({'math_cat_μέτριο', 'gender_male'})","frozenset({'writing_cat_μέτριο', 'race/ethnicity_group D'})",0.24,0.139,0.061,0.25416666666666665,1.8285371702637887,1.0,0.027639999999999998,1.154413407821229,0.5962036238136325,0.19182389937106917,0.13375919473480447,0.34650779376498797
"frozenset({'writing_cat_μέτριο', 'parental level of education_some high school'})","frozenset({'reading_cat_μέτριο', 'lunch_standard'})",0.087,0.327,0.052,0.5977011494252874,1.827832261239411,1.0,0.023551,1.6728857142857145,0.49606116774791476,0.14364640883977897,0.40223053406432,0.37836127807655806
"frozenset({'reading_cat_μέτριο', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'parental level of education_some high school'})",0.327,0.087,0.052,0.15902140672782875,1.827832261239411,1.0,0.023551,1.08564,0.6729626242999199,0.14364640883977897,0.07888434471832284,0.37836127807655806
"frozenset({'writing_cat_υψηλό', 'math_cat_μέτριο'})",frozenset({'gender_female'}),0.094,0.518,0.089,0.9468085106382979,1.8278156576028914,1.0,0.040308,9.061599999999997,0.499888384552422,0.1701720841300191,0.8896442129425267,0.5593115912264848
frozenset({'gender_female'}),"frozenset({'writing_cat_υψηλό', 'math_cat_μέτριο'})",0.518,0.094,0.089,0.1718146718146718,1.8278156576028914,1.0,0.040308,1.093958041958042,0.9396242249055899,0.1701720841300191,0.08588815873584084,0.5593115912264848
"frozenset({'writing_cat_υψηλό', 'math_cat_μέτριο', 'reading_cat_υψηλό'})",frozenset({'gender_female'}),0.075,0.518,0.071,0.9466666666666667,1.8275418275418274,1.0,0.03214999999999999,9.037499999999998,0.4895317853064331,0.13601532567049807,0.8893499308437067,0.5418661518661518
frozenset({'gender_female'}),"frozenset({'writing_cat_υψηλό', 'math_cat_μέτριο', 'reading_cat_υψηλό'})",0.518,0.075,0.071,0.13706563706563704,1.8275418275418274,1.0,0.03214999999999999,1.071923937360179,0.9394541522996901,0.13601532567049807,0.0670979860169049,0.5418661518661518
"frozenset({'parental level of education_high school', 'writing_cat_μέτριο'})","frozenset({'reading_cat_μέτριο', 'math_cat_μέτριο'})",0.096,0.325,0.057,0.59375,1.8269230769230769,1.0,0.0258,1.6615384615384616,0.5006986492780624,0.1565934065934066,0.3981481481481481,0.3845673076923077
"frozenset({'reading_cat_μέτριο', 'math_cat_μέτριο'})","frozenset({'parental level of education_high school', 'writing_cat_μέτριο'})",0.325,0.096,0.057,0.1753846153846154,1.8269230769230769,1.0,0.0258,1.096268656716418,0.6705653021442495,0.1565934065934066,0.0878148400272294,0.3845673076923077
"frozenset({'writing_cat_μέτριο', 'test preparation course_none'})","frozenset({""parental level of education_associate's degree"", 'reading_cat_μέτριο'})",0.308,0.112,0.063,0.20454545454545456,1.8262987012987013,1.0,0.028504,1.1163428571428573,0.6538214515093129,0.17647058823529413,0.10421785421785423,0.3835227272727273
"frozenset({""parental level of education_associate's degree"", 'reading_cat_μέτριο'})","frozenset({'writing_cat_μέτριο', 'test preparation course_none'})",0.112,0.308,0.063,0.5625,1.8262987012987013,1.0,0.028504,1.5817142857142856,0.5095095095095096,0.17647058823529413,0.3677745664739885,0.3835227272727273
"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'gender_female'})","frozenset({'reading_cat_μέτριο', 'test preparation course_none'})",0.093,0.318,0.054,0.5806451612903226,1.8259281801582472,1.0,0.024426,1.6263076923076922,0.49871370819551636,0.15126050420168066,0.38511020717056105,0.3752282410225198
"frozenset({'reading_cat_μέτριο', 'test preparation course_none'})","frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'gender_female'})",0.318,0.093,0.054,0.16981132075471697,1.825928180158247,1.0,0.024426,1.0925227272727274,0.6632453567937439,0.15126050420168066,0.08468723346716314,0.3752282410225198
"frozenset({'writing_cat_μέτριο', 'gender_female', 'test preparation course_none'})","frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο'})",0.175,0.169,0.054,0.3085714285714286,1.8258664412510568,1.0,0.024425,1.2018595041322313,0.5482603815937149,0.18620689655172415,0.16795599106068423,0.31404902789518174
"frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο'})","frozenset({'writing_cat_μέτριο', 'gender_female', 'test preparation course_none'})",0.169,0.175,0.054,0.3195266272189349,1.8258664412510566,1.0,0.024425,1.212391304347826,0.5443018228818469,0.18620689655172415,0.17518379056840594,0.31404902789518174
frozenset({'reading_cat_μέτριο'}),"frozenset({'writing_cat_μέτριο', 'test preparation course_none', 'race/ethnicity_group D', 'lunch_standard'})",0.49,0.066,0.059,0.12040816326530612,1.8243661100803956,1.0,0.026659999999999996,1.0618561484918794,0.8860086407444333,0.1187122736418511,0.058252851461783856,0.50717377860235
"frozenset({'writing_cat_μέτριο', 'test preparation course_none', 'race/ethnicity_group D', 'lunch_standard'})",frozenset({'reading_cat_μέτριο'}),0.066,0.49,0.059,0.8939393939393938,1.8243661100803956,1.0,0.026659999999999996,4.8085714285714225,0.48379486807244215,0.1187122736418511,0.7920380273321447,0.50717377860235
"frozenset({'parental level of education_high school', 'gender_male'})",frozenset({'writing_cat_χαμηλό'}),0.102,0.301,0.056,0.5490196078431373,1.8239854081167353,1.0,0.025298000000000005,1.5499565217391307,0.5030623608017818,0.1613832853025937,0.3548206120788802,0.36753305973552214
frozenset({'writing_cat_χαμηλό'}),"frozenset({'parental level of education_high school', 'gender_male'})",0.301,0.102,0.056,0.18604651162790697,1.8239854081167353,1.0,0.025298000000000005,1.103257142857143,0.6462804005722461,0.1613832853025937,0.09359299735847103,0.36753305973552214
"frozenset({'reading_cat_μέτριο', 'test preparation course_none'})","frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'race/ethnicity_group D'})",0.318,0.1,0.058,0.18238993710691825,1.8238993710691824,1.0,0.0262,1.1007692307692307,0.6623521084032764,0.1611111111111111,0.09154437456324249,0.38119496855345913
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'race/ethnicity_group D'})","frozenset({'reading_cat_μέτριο', 'test preparation course_none'})",0.1,0.318,0.058,0.58,1.8238993710691822,1.0,0.0262,1.6238095238095236,0.5019157088122606,0.1611111111111111,0.3841642228739003,0.38119496855345913
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'gender_male'})","frozenset({'test preparation course_completed', 'math_cat_μέτριο'})",0.191,0.181,0.063,0.3298429319371728,1.8223366405368664,1.0,0.028429000000000003,1.2221015624999998,0.557792296976475,0.20388349514563106,0.18173740163268962,0.33895461513985714
"frozenset({'test preparation course_completed', 'math_cat_μέτριο'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'gender_male'})",0.181,0.191,0.063,0.34806629834254144,1.8223366405368662,1.0,0.028429000000000003,1.2409237288135593,0.5509816462197415,0.20388349514563106,0.19414870005258522,0.33895461513985714
"frozenset({""parental level of education_bachelor's degree"", 'writing_cat_μέτριο'})",frozenset({'reading_cat_μέτριο'}),0.056,0.49,0.05,0.8928571428571429,1.8221574344023326,1.0,0.022560000000000004,4.7600000000000025,0.4779661016949153,0.10080645161290322,0.7899159663865547,0.49744897959183676
frozenset({'reading_cat_μέτριο'}),"frozenset({""parental level of education_bachelor's degree"", 'writing_cat_μέτριο'})",0.49,0.056,0.05,0.10204081632653061,1.8221574344023324,1.0,0.022560000000000004,1.0512727272727271,0.8847058823529412,0.10080645161290322,0.0487720511933587,0.49744897959183676
"frozenset({'writing_cat_μέτριο', 'gender_male', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})",0.161,0.208,0.061,0.37888198757763975,1.8215480172001912,1.0,0.027512,1.27512,0.5375642353308975,0.19805194805194806,0.21576008532530272,0.33607560917343526
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})","frozenset({'writing_cat_μέτριο', 'gender_male', 'lunch_standard'})",0.208,0.161,0.061,0.2932692307692308,1.8215480172001912,1.0,0.027512,1.1871564625850342,0.569465143235635,0.19805194805194806,0.15765104978454206,0.33607560917343526
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})","frozenset({'writing_cat_μέτριο', 'gender_male'})",0.208,0.227,0.086,0.41346153846153844,1.8214164689935612,1.0,0.03878399999999999,1.3179016393442622,0.569415081042988,0.24641833810888253,0.24121803164493974,0.3961580820060996
"frozenset({'writing_cat_μέτριο', 'gender_male'})","frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})",0.227,0.208,0.086,0.37885462555066074,1.8214164689935612,1.0,0.03878399999999999,1.275063829787234,0.5834110532807846,0.24641833810888253,0.21572553731144034,0.3961580820060996
"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'gender_female'})",frozenset({'reading_cat_μέτριο'}),0.093,0.49,0.083,0.89247311827957,1.8213737107746326,1.0,0.037430000000000005,4.743000000000004,0.4972038097262258,0.16600000000000004,0.7891629770187647,0.5309304366908054
frozenset({'reading_cat_μέτριο'}),"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'gender_female'})",0.49,0.093,0.083,0.16938775510204082,1.8213737107746324,1.0,0.037430000000000005,1.091965601965602,0.8842428537680134,0.16600000000000004,0.08422023715770763,0.5309304366908054
"frozenset({'writing_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})","frozenset({'reading_cat_μέτριο', 'gender_female'})",0.221,0.261,0.105,0.4751131221719457,1.8203567899308264,1.0,0.04731899999999999,1.4079224137931035,0.5785072437190536,0.27851458885941643,0.2897335888659617,0.43870598637332914
"frozenset({'reading_cat_μέτριο', 'gender_female'})","frozenset({'writing_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",0.261,0.221,0.105,0.4022988505747126,1.8203567899308262,1.0,0.04731899999999999,1.303326923076923,0.6098202203750241,0.27851458885941643,0.2327327991973204,0.43870598637332914
"frozenset({'parental level of education_high school', 'reading_cat_μέτριο'})","frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})",0.098,0.32,0.057,0.5816326530612245,1.8176020408163265,1.0,0.025640000000000003,1.6253658536585363,0.49869685299723815,0.15789473684210525,0.3847539015606242,0.3798788265306122
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})","frozenset({'parental level of education_high school', 'reading_cat_μέτριο'})",0.32,0.098,0.057,0.178125,1.8176020408163265,1.0,0.025640000000000003,1.097490494296578,0.6615067079463366,0.15789473684210525,0.08883037694013304,0.3798788265306122
"frozenset({'writing_cat_μέτριο', 'parental level of education_some college', 'test preparation course_none'})",frozenset({'reading_cat_μέτριο'}),0.073,0.49,0.065,0.8904109589041097,1.8171652222532853,1.0,0.029230000000000006,4.653750000000005,0.4851049705418638,0.13052208835341367,0.7851195272629602,0.5115320100642997
frozenset({'reading_cat_μέτριο'}),"frozenset({'writing_cat_μέτριο', 'parental level of education_some college', 'test preparation course_none'})",0.49,0.073,0.065,0.1326530612244898,1.817165222253285,1.0,0.029230000000000006,1.0687764705882354,0.8817496229260937,0.13052208835341367,0.06435065935759418,0.5115320100642997
"frozenset({'lunch_free/reduced', 'gender_male'})","frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό'})",0.166,0.252,0.076,0.4578313253012048,1.8167909734174794,1.0,0.034168,1.3796444444444442,0.5390634860532626,0.2222222222222222,0.2751755685844984,0.3797093134442532
"frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό'})","frozenset({'lunch_free/reduced', 'gender_male'})",0.252,0.166,0.076,0.30158730158730157,1.8167909734174792,1.0,0.034168,1.1941363636363636,0.6010413734871939,0.2222222222222222,0.16257470214304745,0.3797093134442532
frozenset({'reading_cat_μέτριο'}),"frozenset({'writing_cat_μέτριο', 'test preparation course_none', 'race/ethnicity_group D'})",0.49,0.091,0.081,0.1653061224489796,1.816550796142633,1.0,0.036410000000000005,1.0890220048899755,0.8813846526264828,0.16200000000000003,0.08174490918479604,0.5277080062794349
"frozenset({'writing_cat_μέτριο', 'test preparation course_none', 'race/ethnicity_group D'})",frozenset({'reading_cat_μέτριο'}),0.091,0.49,0.081,0.8901098901098902,1.816550796142633,1.0,0.036410000000000005,4.641000000000003,0.49450624074753163,0.16200000000000003,0.7845291962939023,0.5277080062794349
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'race/ethnicity_group D'})",frozenset({'reading_cat_μέτριο'}),0.1,0.49,0.089,0.8899999999999999,1.8163265306122447,1.0,0.039999999999999994,4.636363636363632,0.49937578027465657,0.17764471057884232,0.7843137254901958,0.5358163265306122
frozenset({'reading_cat_μέτριο'}),"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'race/ethnicity_group D'})",0.49,0.1,0.089,0.18163265306122447,1.8163265306122447,1.0,0.039999999999999994,1.0997506234413965,0.8812513769552763,0.17764471057884232,0.09070294784580496,0.5358163265306122
frozenset({'writing_cat_μέτριο'}),"frozenset({'reading_cat_μέτριο', 'race/ethnicity_group D', 'lunch_standard'})",0.491,0.092,0.082,0.16700610997963342,1.8152838041264503,1.0,0.03682800000000001,1.090044009779951,0.8823613972878432,0.16367265469061876,0.08260584799519098,0.5291552289028603
"frozenset({'reading_cat_μέτριο', 'race/ethnicity_group D', 'lunch_standard'})",frozenset({'writing_cat_μέτριο'}),0.092,0.491,0.082,0.891304347826087,1.8152838041264503,1.0,0.03682800000000001,4.682800000000003,0.4946276995809606,0.16367265469061876,0.7864525497565561,0.5291552289028603
frozenset({'reading_cat_μέτριο'}),"frozenset({'math_cat_χαμηλό', 'writing_cat_μέτριο', 'test preparation course_none'})",0.49,0.063,0.056,0.1142857142857143,1.8140589569160999,1.0,0.025130000000000003,1.0579032258064518,0.8799019607843138,0.11267605633802819,0.05473395334654674,0.5015873015873016
"frozenset({'math_cat_χαμηλό', 'writing_cat_μέτριο', 'test preparation course_none'})",frozenset({'reading_cat_μέτριο'}),0.063,0.49,0.056,0.888888888888889,1.8140589569160999,1.0,0.025130000000000003,4.5900000000000025,0.4789220917822839,0.11267605633802819,0.7821350762527234,0.5015873015873016
frozenset({'writing_cat_μέτριο'}),"frozenset({'math_cat_μέτριο', ""parental level of education_associate's degree"", 'reading_cat_μέτριο'})",0.491,0.064,0.057,0.11608961303462323,1.813900203665988,1.0,0.025576,1.0589308755760367,0.8815358632337228,0.11445783132530123,0.05565129597716157,0.5033573065173116
"frozenset({'math_cat_μέτριο', ""parental level of education_associate's degree"", 'reading_cat_μέτριο'})",frozenset({'writing_cat_μέτριο'}),0.064,0.491,0.057,0.890625,1.813900203665988,1.0,0.025576,4.653714285714286,0.47938221622432153,0.11445783132530123,0.7851178781925344,0.5033573065173116
"frozenset({'reading_cat_μέτριο', 'gender_male'})","frozenset({'test preparation course_completed', 'writing_cat_μέτριο'})",0.229,0.183,0.076,0.3318777292576419,1.8135395041401199,1.0,0.034093,1.222830065359477,0.581831524336132,0.22619047619047616,0.18222488281229124,0.3735891378528647
"frozenset({'test preparation course_completed', 'writing_cat_μέτριο'})","frozenset({'reading_cat_μέτριο', 'gender_male'})",0.183,0.229,0.076,0.41530054644808745,1.8135395041401199,1.0,0.034093,1.3186261682242992,0.5490723442633512,0.22619047619047616,0.24163494999751936,0.3735891378528647
"frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'race/ethnicity_group D'})",frozenset({'writing_cat_μέτριο'}),0.091,0.491,0.081,0.8901098901098902,1.812851100020143,1.0,0.036319000000000004,4.631900000000003,0.49327031468578963,0.16167664670658682,0.7841058744791556,0.5275396701058616
frozenset({'writing_cat_μέτριο'}),"frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'race/ethnicity_group D'})",0.491,0.091,0.081,0.164969450101833,1.812851100020143,1.0,0.036319000000000004,1.0885829268292684,0.8809090688592981,0.16167664670658682,0.08137453256527283,0.5275396701058616
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'race/ethnicity_group D'})",frozenset({'writing_cat_μέτριο'}),0.1,0.491,0.089,0.8899999999999999,1.8126272912423624,1.0,0.03989999999999999,4.627272727272723,0.4981273408239699,0.17729083665338644,0.7838899803536344,0.535631364562118
frozenset({'writing_cat_μέτριο'}),"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'race/ethnicity_group D'})",0.491,0.1,0.089,0.18126272912423624,1.8126272912423624,1.0,0.03989999999999999,1.0992537313432835,0.8807752588243083,0.17729083665338644,0.09029192124915138,0.535631364562118
"frozenset({'lunch_free/reduced', 'reading_cat_μέτριο'})","frozenset({'writing_cat_μέτριο', 'gender_female'})",0.163,0.264,0.078,0.47852760736196315,1.8126045733407694,1.0,0.034968,1.4113882352941174,0.5356125356125356,0.22349570200573063,0.29147772739397165,0.38699107640825425
"frozenset({'writing_cat_μέτριο', 'gender_female'})","frozenset({'lunch_free/reduced', 'reading_cat_μέτριο'})",0.264,0.163,0.078,0.2954545454545454,1.8126045733407694,1.0,0.034968,1.188,0.6091137123745819,0.22349570200573063,0.1582491582491582,0.38699107640825425
"frozenset({'writing_cat_μέτριο', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο'})",0.338,0.325,0.199,0.5887573964497042,1.8115612198452435,1.0,0.08915,1.6413669064748202,0.6767219784724225,0.42887931034482757,0.3907516984440061,0.6005325443786982
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'writing_cat_μέτριο', 'lunch_standard'})",0.325,0.338,0.199,0.6123076923076923,1.8115612198452435,1.0,0.08915,1.7075396825396825,0.6636888144425831,0.42887931034482757,0.41436207297234495,0.6005325443786982
frozenset({'reading_cat_μέτριο'}),"frozenset({'writing_cat_μέτριο', 'test preparation course_none', ""parental level of education_associate's degree""})",0.49,0.071,0.063,0.1285714285714286,1.8108651911468816,1.0,0.028210000000000006,1.0660655737704918,0.8779956427015253,0.12650602409638556,0.06197139781639245,0.5079476861167003
"frozenset({'writing_cat_μέτριο', 'test preparation course_none', ""parental level of education_associate's degree""})",frozenset({'reading_cat_μέτριο'}),0.071,0.49,0.063,0.8873239436619719,1.8108651911468814,1.0,0.028210000000000006,4.526250000000002,0.48199976079416346,0.12650602409638556,0.7790665561999448,0.5079476861167003
"frozenset({'math_cat_χαμηλό', 'writing_cat_μέτριο', 'gender_female', 'test preparation course_none'})",frozenset({'reading_cat_μέτριο'}),0.062,0.49,0.055,0.8870967741935484,1.8104015799868334,1.0,0.02462,4.517142857142857,0.4772242682690444,0.11066398390342051,0.7786211258697027,0.49967083607636603
frozenset({'reading_cat_μέτριο'}),"frozenset({'math_cat_χαμηλό', 'writing_cat_μέτριο', 'gender_female', 'test preparation course_none'})",0.49,0.062,0.055,0.11224489795918367,1.8104015799868334,1.0,0.02462,1.0565977011494252,0.8777183600713012,0.11066398390342051,0.05356598929550498,0.49967083607636603
frozenset({'writing_cat_μέτριο'}),"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",0.491,0.152,0.135,0.274949083503055,1.8088755493622042,1.0,0.06036800000000002,1.1695730337078651,0.8785272502364841,0.265748031496063,0.14498712677247055,0.5815534891199485
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",frozenset({'writing_cat_μέτριο'}),0.152,0.491,0.135,0.8881578947368421,1.808875549362204,1.0,0.06036800000000002,4.551058823529414,0.5273235499650596,0.265748031496063,0.7802709130389827,0.5815534891199485
"frozenset({'writing_cat_μέτριο', 'parental level of education_some college'})","frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο'})",0.114,0.325,0.067,0.5877192982456141,1.8083670715249665,1.0,0.029950000000000004,1.6372340425531917,0.5045315184798356,0.18010752688172044,0.3892137751786875,0.3969365721997301
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'writing_cat_μέτριο', 'parental level of education_some college'})",0.325,0.114,0.067,0.20615384615384616,1.8083670715249662,1.0,0.029950000000000004,1.1160852713178295,0.6622443338861249,0.18010752688172044,0.10401111304045842,0.3969365721997301
frozenset({'gender_female'}),"frozenset({'math_cat_μέτριο', 'reading_cat_υψηλό', 'lunch_standard'})",0.518,0.063,0.059,0.1138996138996139,1.8079303793589507,1.0,0.026365999999999994,1.057442265795207,0.9271397425979322,0.11302681992337164,0.054321893169278435,0.5252037752037751
"frozenset({'math_cat_μέτριο', 'reading_cat_υψηλό', 'lunch_standard'})",frozenset({'gender_female'}),0.063,0.518,0.059,0.9365079365079364,1.8079303793589505,1.0,0.026365999999999994,7.591499999999987,0.4769278078251903,0.11302681992337164,0.8682737271948888,0.5252037752037751
"frozenset({'lunch_free/reduced', 'gender_male'})","frozenset({'math_cat_χαμηλό', 'writing_cat_χαμηλό'})",0.166,0.23,0.069,0.41566265060240964,1.8072289156626506,1.0,0.03082,1.3177319587628868,0.5355715427657873,0.21100917431192662,0.24112032545767484,0.3578313253012048
"frozenset({'math_cat_χαμηλό', 'writing_cat_χαμηλό'})","frozenset({'lunch_free/reduced', 'gender_male'})",0.23,0.166,0.069,0.3,1.8072289156626504,1.0,0.03082,1.1914285714285715,0.5800865800865801,0.21100917431192662,0.16067146282973618,0.3578313253012048
"frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό'})","frozenset({'lunch_free/reduced', 'test preparation course_none'})",0.252,0.224,0.102,0.4047619047619047,1.806972789115646,1.0,0.045551999999999995,1.3036799999999997,0.5970430953129915,0.2727272727272727,0.23294059891998029,0.4300595238095237
"frozenset({'lunch_free/reduced', 'test preparation course_none'})","frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό'})",0.224,0.252,0.102,0.4553571428571428,1.806972789115646,1.0,0.045551999999999995,1.3733770491803277,0.5755003032140691,0.2727272727272727,0.27186783804430853,0.4300595238095237
"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'gender_male', 'race/ethnicity_group D'})",frozenset({'reading_cat_μέτριο'}),0.061,0.49,0.054,0.8852459016393442,1.8066242890598863,1.0,0.02411,4.444285714285714,0.47548613576302606,0.1086519114688129,0.7749919639987142,0.49772499163599865
frozenset({'reading_cat_μέτριο'}),"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'gender_male', 'race/ethnicity_group D'})",0.49,0.061,0.054,0.11020408163265306,1.8066242890598863,1.0,0.02411,1.0552981651376148,0.8754538852578069,0.1086519114688129,0.05240051292082328,0.49772499163599865
"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο'})","frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",0.154,0.223,0.062,0.4025974025974026,1.8053695183740028,1.0,0.027658000000000002,1.3006304347826088,0.527301151529017,0.19682539682539682,0.23114208828494545,0.3403121542134995
"frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})","frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο'})",0.223,0.154,0.062,0.2780269058295964,1.8053695183740026,1.0,0.027658000000000002,1.1717888198757762,0.5741271225142193,0.19682539682539682,0.14660390759999573,0.3403121542134995
"frozenset({'writing_cat_μέτριο', 'gender_male'})","frozenset({'reading_cat_μέτριο', 'lunch_standard'})",0.227,0.327,0.134,0.5903083700440529,1.8052243732234032,1.0,0.059771000000000005,1.642698924731183,0.5770404124268695,0.319047619047619,0.39124572071924646,0.5000471513828827
"frozenset({'reading_cat_μέτριο', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'gender_male'})",0.327,0.227,0.134,0.40978593272171254,1.8052243732234032,1.0,0.059771000000000005,1.309694300518135,0.6627819298751414,0.319047619047619,0.2364630436244664,0.5000471513828827
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'lunch_standard'})",frozenset({'reading_cat_μέτριο'}),0.225,0.49,0.199,0.8844444444444445,1.8049886621315194,1.0,0.08875000000000001,4.41346153846154,0.5754579348354677,0.3856589147286822,0.7734204793028323,0.6452834467120182
frozenset({'reading_cat_μέτριο'}),"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'lunch_standard'})",0.49,0.225,0.199,0.4061224489795919,1.8049886621315194,1.0,0.08875000000000001,1.3049828178694158,0.874470391171544,0.3856589147286822,0.23370638578011854,0.6452834467120182
"frozenset({'writing_cat_μέτριο', 'race/ethnicity_group D'})","frozenset({'reading_cat_μέτριο', 'lunch_standard'})",0.139,0.327,0.082,0.5899280575539568,1.8040613380854948,1.0,0.036546999999999996,1.6411754385964914,0.5176482252627403,0.21354166666666666,0.39068062043678575,0.4203462917739203
"frozenset({'reading_cat_μέτριο', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'race/ethnicity_group D'})",0.327,0.139,0.082,0.25076452599388377,1.8040613380854946,1.0,0.036546999999999996,1.1491714285714285,0.6622512956184539,0.21354166666666666,0.12980781183958626,0.4203462917739203
frozenset({'lunch_free/reduced'}),"frozenset({'writing_cat_χαμηλό', 'gender_female'})",0.355,0.1,0.064,0.18028169014084508,1.8028169014084507,1.0,0.028500000000000004,1.097938144329897,0.690406976744186,0.1636828644501279,0.0892018779342723,0.41014084507042253
"frozenset({'writing_cat_χαμηλό', 'gender_female'})",frozenset({'lunch_free/reduced'}),0.1,0.355,0.064,0.64,1.8028169014084507,1.0,0.028500000000000004,1.7916666666666667,0.4947916666666667,0.1636828644501279,0.44186046511627913,0.41014084507042253
frozenset({'writing_cat_μέτριο'}),"frozenset({'test preparation course_completed', 'reading_cat_μέτριο', 'lunch_standard'})",0.491,0.104,0.092,0.18737270875763748,1.8016606611311297,1.0,0.040936,1.1025964912280701,0.8741778423165627,0.18290258449304175,0.09304989816700612,0.5359940466865111
"frozenset({'test preparation course_completed', 'reading_cat_μέτριο', 'lunch_standard'})",frozenset({'writing_cat_μέτριο'}),0.104,0.491,0.092,0.8846153846153847,1.8016606611311297,1.0,0.040936,4.411333333333336,0.4966032608695652,0.18290258449304175,0.7733111682031133,0.5359940466865111
"frozenset({'gender_male', 'reading_cat_μέτριο', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'test preparation course_none'})",0.155,0.308,0.086,0.5548387096774193,1.8014243820695433,1.0,0.038259999999999995,1.5544927536231883,0.5264896105683226,0.22811671087533156,0.356703337684132,0.41702974444909924
"frozenset({'writing_cat_μέτριο', 'test preparation course_none'})","frozenset({'gender_male', 'reading_cat_μέτριο', 'lunch_standard'})",0.308,0.155,0.086,0.2792207792207792,1.8014243820695433,1.0,0.038259999999999995,1.1723423423423425,0.6428955504772147,0.22811671087533156,0.1470068393145316,0.41702974444909924
"frozenset({'writing_cat_μέτριο', ""parental level of education_associate's degree""})","frozenset({'reading_cat_μέτριο', 'test preparation course_none'})",0.11,0.318,0.063,0.5727272727272728,1.8010291595197256,1.0,0.028019999999999996,1.5961702127659574,0.4997324772605671,0.1726027397260274,0.3735003998933618,0.3854202401372213
"frozenset({'reading_cat_μέτριο', 'test preparation course_none'})","frozenset({'writing_cat_μέτριο', ""parental level of education_associate's degree""})",0.318,0.11,0.063,0.19811320754716982,1.8010291595197256,1.0,0.028019999999999996,1.1098823529411765,0.6521435553693617,0.1726027397260274,0.09900360398558407,0.3854202401372213
frozenset({'reading_cat_μέτριο'}),"frozenset({'lunch_free/reduced', 'writing_cat_μέτριο'})",0.49,0.153,0.135,0.2755102040816327,1.8007202881152464,1.0,0.060030000000000014,1.1690985915492957,0.8718954248366014,0.265748031496063,0.1446401464954341,0.5789315726290517
frozenset({'reading_cat_μέτριο'}),"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'lunch_standard'})",0.49,0.102,0.09,0.1836734693877551,1.8007202881152462,1.0,0.04002,1.10005,0.8718954248366014,0.1792828685258964,0.09095041134493888,0.5330132052821128
"frozenset({'lunch_free/reduced', 'writing_cat_μέτριο'})",frozenset({'reading_cat_μέτριο'}),0.153,0.49,0.135,0.8823529411764707,1.8007202881152462,1.0,0.060030000000000014,4.3350000000000035,0.5249901613537978,0.265748031496063,0.7693194925028837,0.5789315726290517
"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'lunch_standard'})",frozenset({'reading_cat_μέτριο'}),0.102,0.49,0.09,0.8823529411764706,1.800720288115246,1.0,0.04002,4.334999999999999,0.49517446176688934,0.1792828685258964,0.7693194925028835,0.5330132052821128
frozenset({'writing_cat_μέτριο'}),"frozenset({'test preparation course_completed', 'reading_cat_μέτριο'})",0.491,0.172,0.152,0.3095723014256619,1.7998389617771042,1.0,0.06754800000000001,1.1992566371681417,0.8730741391789888,0.2974559686888454,0.16615012249476074,0.59664661582911
"frozenset({'test preparation course_completed', 'reading_cat_μέτριο'})",frozenset({'writing_cat_μέτριο'}),0.172,0.491,0.152,0.8837209302325582,1.7998389617771042,1.0,0.06754800000000001,4.377400000000001,0.5367086193745234,0.2974559686888454,0.7715538904372459,0.59664661582911
frozenset({'math_cat_υψηλό'}),"frozenset({'writing_cat_μέτριο', 'gender_male', 'lunch_standard'})",0.176,0.161,0.051,0.2897727272727273,1.7998306041784304,1.0,0.022663999999999997,1.181312,0.5393108699790595,0.17832167832167833,0.15348358435366782,0.30327145680406553
"frozenset({'writing_cat_μέτριο', 'gender_male', 'lunch_standard'})",frozenset({'math_cat_υψηλό'}),0.161,0.176,0.051,0.3167701863354037,1.7998306041784302,1.0,0.022663999999999997,1.2060363636363638,0.5296688401224614,0.17832167832167833,0.17083760477597537,0.30327145680406553
frozenset({'reading_cat_μέτριο'}),"frozenset({'writing_cat_μέτριο', ""parental level of education_associate's degree""})",0.49,0.11,0.097,0.1979591836734694,1.7996289424860854,1.0,0.043100000000000006,1.1096692111959288,0.8712350919749344,0.19284294234592445,0.0988305434533364,0.5398886827458256
"frozenset({'writing_cat_μέτριο', ""parental level of education_associate's degree""})",frozenset({'reading_cat_μέτριο'}),0.11,0.49,0.097,0.8818181818181818,1.7996289424860854,1.0,0.043100000000000006,4.315384615384616,0.4992470751766478,0.19284294234592445,0.768270944741533,0.5398886827458256
"frozenset({'test preparation course_completed', 'lunch_standard'})",frozenset({'reading_cat_υψηλό'}),0.227,0.235,0.096,0.42290748898678415,1.7996063361139751,1.0,0.042655000000000005,1.3256106870229007,0.5748032557136696,0.26229508196721313,0.2456307045578878,0.4157090636423283
frozenset({'reading_cat_υψηλό'}),"frozenset({'test preparation course_completed', 'lunch_standard'})",0.235,0.227,0.096,0.4085106382978724,1.7996063361139751,1.0,0.042655000000000005,1.3068705035971224,0.5808142701525055,0.26229508196721313,0.23481324488728636,0.4157090636423283
"frozenset({'reading_cat_μέτριο', 'test preparation course_none'})","frozenset({'writing_cat_μέτριο', 'gender_female'})",0.318,0.264,0.151,0.4748427672955975,1.7986468458166571,1.0,0.067048,1.4014850299401198,0.6510652347012099,0.35034802784222735,0.2864711512168444,0.5234062321326471
"frozenset({'writing_cat_μέτριο', 'gender_female'})","frozenset({'reading_cat_μέτριο', 'test preparation course_none'})",0.264,0.318,0.151,0.5719696969696969,1.798646845816657,1.0,0.067048,1.5933451327433625,0.6032968615030233,0.35034802784222735,0.3723895849995556,0.5234062321326471
"frozenset({'writing_cat_μέτριο', 'test preparation course_none'})","frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο'})",0.308,0.325,0.18,0.5844155844155844,1.7982017982017982,1.0,0.07989999999999998,1.62421875,0.6414579319203596,0.3973509933774834,0.3843193843193842,0.5691308691308691
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'writing_cat_μέτριο', 'test preparation course_none'})",0.325,0.308,0.18,0.5538461538461538,1.798201798201798,1.0,0.07989999999999998,1.5510344827586202,0.6576131687242798,0.3973509933774834,0.35526900844819914,0.5691308691308691
frozenset({'reading_cat_μέτριο'}),"frozenset({'writing_cat_μέτριο', 'lunch_standard', 'gender_female', 'test preparation course_none', 'math_cat_μέτριο'})",0.49,0.084,0.074,0.1510204081632653,1.7978620019436347,1.0,0.032839999999999994,1.0789423076923077,0.8701642819289878,0.14800000000000002,0.07316638445771322,0.515986394557823
"frozenset({'writing_cat_μέτριο', 'lunch_standard', 'gender_female', 'test preparation course_none', 'math_cat_μέτριο'})",frozenset({'reading_cat_μέτριο'}),0.084,0.49,0.074,0.8809523809523808,1.7978620019436344,1.0,0.032839999999999994,4.283999999999995,0.48448011330107393,0.14800000000000002,0.7665732959850604,0.515986394557823
"frozenset({'math_cat_χαμηλό', 'writing_cat_μέτριο'})",frozenset({'reading_cat_μέτριο'}),0.109,0.49,0.096,0.8807339449541285,1.797416214192099,1.0,0.04259,4.276153846153848,0.4979190048634493,0.1908548707753479,0.7661449901061343,0.5383261561505337
frozenset({'reading_cat_μέτριο'}),"frozenset({'math_cat_χαμηλό', 'writing_cat_μέτριο'})",0.49,0.109,0.096,0.1959183673469388,1.797416214192099,1.0,0.04259,1.1080964467005077,0.8698937908496732,0.1908548707753479,0.09755147850385948,0.5383261561505337
"frozenset({'writing_cat_μέτριο', 'gender_female', 'parental level of education_some college'})",frozenset({'reading_cat_μέτριο'}),0.067,0.49,0.059,0.880597014925373,1.7971367651538226,1.0,0.02617,4.271249999999996,0.4754119207222918,0.11847389558232932,0.7658764998536726,0.5005025890953396
frozenset({'reading_cat_μέτριο'}),"frozenset({'writing_cat_μέτριο', 'gender_female', 'parental level of education_some college'})",0.49,0.067,0.059,0.12040816326530612,1.7971367651538226,1.0,0.02617,1.0607192575406033,0.869724160850781,0.11847389558232932,0.05724347616860248,0.5005025890953396
frozenset({'writing_cat_μέτριο'}),"frozenset({'reading_cat_μέτριο', 'race/ethnicity_group D'})",0.491,0.136,0.12,0.24439918533604887,1.7970528333533005,1.0,0.053223999999999994,1.1434609164420484,0.8713817943680419,0.23668639053254437,0.12546202006487137,0.5633760632562597
frozenset({'writing_cat_μέτριο'}),"frozenset({'test preparation course_completed', 'lunch_free/reduced', 'reading_cat_μέτριο'})",0.491,0.068,0.06,0.12219959266802444,1.7970528333533005,1.0,0.026611999999999997,1.0617447795823665,0.8713817943680419,0.12024048096192386,0.05815406938629231,0.5022762669222475
"frozenset({'test preparation course_completed', 'lunch_free/reduced', 'reading_cat_μέτριο'})",frozenset({'writing_cat_μέτριο'}),0.068,0.491,0.06,0.8823529411764705,1.7970528333533002,1.0,0.026611999999999997,4.326499999999995,0.4758941344778254,0.12024048096192386,0.768866289148272,0.5022762669222475
"frozenset({'reading_cat_μέτριο', 'race/ethnicity_group D'})",frozenset({'writing_cat_μέτριο'}),0.136,0.491,0.12,0.8823529411764705,1.7970528333533002,1.0,0.053223999999999994,4.326499999999995,0.5133487654320987,0.23668639053254437,0.768866289148272,0.5633760632562597
"frozenset({'writing_cat_μέτριο', 'lunch_standard'})","frozenset({'gender_female', 'reading_cat_μέτριο', 'test preparation course_none'})",0.338,0.173,0.105,0.3106508875739645,1.795669870369737,1.0,0.046526,1.199682403433476,0.6693425406416343,0.2586206896551724,0.16644605510757496,0.45879365187946775
"frozenset({'gender_female', 'reading_cat_μέτριο', 'test preparation course_none'})","frozenset({'writing_cat_μέτριο', 'lunch_standard'})",0.173,0.338,0.105,0.6069364161849711,1.7956698703697367,1.0,0.046526,1.6842058823529409,0.535797777393908,0.2586206896551724,0.40624836281717686,0.45879365187946775
frozenset({'writing_cat_χαμηλό'}),"frozenset({'lunch_free/reduced', 'test preparation course_none'})",0.301,0.224,0.121,0.4019933554817276,1.7946131941148553,1.0,0.053576,1.2976444444444446,0.6334432897054825,0.2995049504950495,0.22937288077542214,0.4710859634551495
"frozenset({'lunch_free/reduced', 'test preparation course_none'})",frozenset({'writing_cat_χαμηλό'}),0.224,0.301,0.121,0.5401785714285714,1.794613194114855,1.0,0.053576,1.5201553398058252,0.5705887364743972,0.2995049504950495,0.34217249131412214,0.4710859634551495
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})","frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο', 'lunch_standard'})",0.32,0.108,0.062,0.19375,1.7939814814814816,1.0,0.02744,1.1063565891472869,0.650853889943074,0.16939890710382513,0.0961322869955157,0.383912037037037
"frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})",0.108,0.32,0.062,0.5740740740740741,1.7939814814814814,1.0,0.02744,1.5965217391304347,0.49616664255750037,0.16939890710382513,0.37363834422657954,0.383912037037037
"frozenset({'test preparation course_completed', 'math_cat_μέτριο'})","frozenset({'writing_cat_υψηλό', 'gender_female'})",0.181,0.154,0.05,0.27624309392265195,1.7937863241730647,1.0,0.022126000000000003,1.1689007633587785,0.5403174603174604,0.1754385964912281,0.14449538288729546,0.3004592092989883
"frozenset({'writing_cat_υψηλό', 'gender_female'})","frozenset({'test preparation course_completed', 'math_cat_μέτριο'})",0.154,0.181,0.05,0.3246753246753247,1.7937863241730645,1.0,0.022126000000000003,1.21275,0.5230732860520095,0.1754385964912281,0.1754277468563183,0.3004592092989883
"frozenset({'writing_cat_μέτριο', 'parental level of education_some college'})","frozenset({'reading_cat_μέτριο', 'test preparation course_none'})",0.114,0.318,0.065,0.5701754385964912,1.7930045238883372,1.0,0.028748000000000003,1.5866938775510202,0.49918388609133535,0.1771117166212534,0.36975870761948865,0.38728897715988087
"frozenset({'reading_cat_μέτριο', 'test preparation course_none'})","frozenset({'writing_cat_μέτριο', 'parental level of education_some college'})",0.318,0.114,0.065,0.20440251572327045,1.7930045238883372,1.0,0.028748000000000003,1.1136284584980238,0.6484998872095648,0.1771117166212534,0.1020344421255874,0.38728897715988087
"frozenset({'writing_cat_χαμηλό', 'gender_female', 'math_cat_χαμηλό'})",frozenset({'lunch_free/reduced'}),0.099,0.355,0.063,0.6363636363636364,1.792573623559539,1.0,0.027854999999999998,1.77375,0.4907245917234818,0.16112531969309465,0.43622269203664554,0.40691421254801535
frozenset({'lunch_free/reduced'}),"frozenset({'writing_cat_χαμηλό', 'gender_female', 'math_cat_χαμηλό'})",0.355,0.099,0.063,0.17746478873239438,1.792573623559539,1.0,0.027854999999999998,1.0953938356164383,0.6854928017718714,0.16112531969309465,0.08708633599599819,0.40691421254801535
"frozenset({'writing_cat_μέτριο', 'gender_female'})","frozenset({""parental level of education_associate's degree"", 'reading_cat_μέτριο'})",0.264,0.112,0.053,0.20075757575757575,1.7924783549783547,1.0,0.023431999999999998,1.1110521327014218,0.6006972928630024,0.16408668730650153,0.09995222495222493,0.33698593073593075
"frozenset({""parental level of education_associate's degree"", 'reading_cat_μέτριο'})","frozenset({'writing_cat_μέτριο', 'gender_female'})",0.112,0.264,0.053,0.4732142857142857,1.7924783549783547,1.0,0.023431999999999998,1.3971525423728812,0.49787523372429027,0.16408668730650153,0.2842585403726708,0.33698593073593075
frozenset({'reading_cat_μέτριο'}),"frozenset({'test preparation course_completed', 'writing_cat_μέτριο', 'math_cat_μέτριο'})",0.49,0.115,0.101,0.20612244897959187,1.7923691215616684,1.0,0.04465,1.1147814910025708,0.8668219763152785,0.2003968253968254,0.1029632191859795,0.5421916592724046
"frozenset({'test preparation course_completed', 'writing_cat_μέτριο', 'math_cat_μέτριο'})",frozenset({'reading_cat_μέτριο'}),0.115,0.49,0.101,0.8782608695652174,1.7923691215616682,1.0,0.04465,4.189285714285716,0.49952452872405884,0.2003968253968254,0.7612958226768969,0.5421916592724046
"frozenset({'reading_cat_μέτριο', 'math_cat_μέτριο', 'gender_female'})",frozenset({'writing_cat_μέτριο'}),0.15,0.491,0.132,0.8800000000000001,1.7922606924643587,1.0,0.05835000000000001,4.241666666666671,0.520053475935829,0.2593320235756385,0.7642436149312379,0.574419551934827
frozenset({'writing_cat_μέτριο'}),"frozenset({'reading_cat_μέτριο', 'math_cat_μέτριο', 'gender_female'})",0.491,0.15,0.132,0.26883910386965376,1.7922606924643585,1.0,0.05835000000000001,1.1625348189415041,0.8684586533309522,0.2593320235756385,0.1398107104348868,0.574419551934827
frozenset({'reading_cat_μέτριο'}),"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})",0.49,0.32,0.281,0.5734693877551021,1.792091836734694,1.0,0.12420000000000003,1.5942583732057418,0.8666527109064267,0.5311909262759925,0.37274909963985614,0.7257971938775512
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})",frozenset({'reading_cat_μέτριο'}),0.32,0.49,0.281,0.878125,1.792091836734694,1.0,0.12420000000000003,4.184615384615387,0.6499895331798201,0.5311909262759925,0.761029411764706,0.7257971938775512
frozenset({'reading_cat_μέτριο'}),"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'test preparation course_none'})",0.49,0.205,0.18,0.3673469387755102,1.791936286709806,1.0,0.07955,1.2566129032258064,0.8665577342047931,0.3495145631067962,0.20420998588114495,0.6226978596316576
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'test preparation course_none'})",frozenset({'reading_cat_μέτριο'}),0.205,0.49,0.18,0.8780487804878049,1.7919362867098059,1.0,0.07955,4.182,0.5559049615653389,0.3495145631067962,0.7608799617407939,0.6226978596316576
frozenset({'reading_cat_μέτριο'}),"frozenset({'math_cat_χαμηλό', 'writing_cat_μέτριο', 'gender_female'})",0.49,0.106,0.093,0.18979591836734694,1.790527531767424,1.0,0.04106,1.1034256926952142,0.865696816360953,0.1848906560636183,0.09373145231246861,0.5335772044666923
"frozenset({'math_cat_χαμηλό', 'writing_cat_μέτριο', 'gender_female'})",frozenset({'reading_cat_μέτριο'}),0.106,0.49,0.093,0.8773584905660378,1.790527531767424,1.0,0.04106,4.158461538461539,0.4938538885280604,0.1848906560636183,0.7595264520902701,0.5335772044666923
"frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο'})","frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'test preparation course_none'})",0.169,0.205,0.062,0.3668639053254438,1.7895800259777745,1.0,0.027354999999999997,1.2556542056074766,0.5309382399751562,0.1987179487179487,0.20360239663577834,0.3346514648578438
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'test preparation course_none'})","frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο'})",0.205,0.169,0.062,0.3024390243902439,1.7895800259777743,1.0,0.027354999999999997,1.1912937062937063,0.5549807263136538,0.1987179487179487,0.16057644330955942,0.3346514648578438
frozenset({'reading_cat_μέτριο'}),"frozenset({'writing_cat_μέτριο', ""parental level of education_associate's degree"", 'lunch_standard'})",0.49,0.073,0.064,0.1306122448979592,1.7892088342186192,1.0,0.028230000000000005,1.066267605633803,0.864889705882353,0.12825651302605212,0.06214913149725911,0.5036622868325413
"frozenset({'writing_cat_μέτριο', ""parental level of education_associate's degree"", 'lunch_standard'})",frozenset({'reading_cat_μέτριο'}),0.073,0.49,0.064,0.8767123287671234,1.7892088342186192,1.0,0.028230000000000005,4.1366666666666685,0.47582928802589003,0.12825651302605212,0.7582594681708301,0.5036622868325413
"frozenset({'writing_cat_μέτριο', 'test preparation course_none'})","frozenset({'parental level of education_high school', 'reading_cat_μέτριο'})",0.308,0.098,0.054,0.17532467532467533,1.789027299231381,1.0,0.023815999999999997,1.093763779527559,0.6373367587240419,0.15340909090909088,0.0857258041293518,0.3631725417439703
"frozenset({'parental level of education_high school', 'reading_cat_μέτριο'})","frozenset({'writing_cat_μέτριο', 'test preparation course_none'})",0.098,0.308,0.054,0.5510204081632653,1.7890272992313807,1.0,0.023815999999999997,1.541272727272727,0.48895458651556206,0.15340909090909088,0.3511855609295741,0.3631725417439703
frozenset({'writing_cat_μέτριο'}),"frozenset({'reading_cat_μέτριο', 'parental level of education_some college', 'lunch_standard'})",0.491,0.074,0.065,0.1323828920570265,1.7889580007706285,1.0,0.028666000000000004,1.0672910798122066,0.8664349403052743,0.13000000000000003,0.06304847954322516,0.5053806352177025
"frozenset({'reading_cat_μέτριο', 'parental level of education_some college', 'lunch_standard'})",frozenset({'writing_cat_μέτριο'}),0.074,0.491,0.065,0.8783783783783784,1.788958000770628,1.0,0.028666000000000004,4.185111111111112,0.4762585147034391,0.13000000000000003,0.761057717835714,0.5053806352177025
"frozenset({'reading_cat_μέτριο', 'lunch_standard'})","frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο'})",0.327,0.154,0.09,0.2752293577981651,1.7872036220660072,1.0,0.039642,1.1672658227848098,0.6544824170381377,0.23017902813299232,0.14329711323660177,0.42982247110687477
"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο'})","frozenset({'reading_cat_μέτριο', 'lunch_standard'})",0.154,0.327,0.09,0.5844155844155844,1.7872036220660072,1.0,0.039642,1.61940625,0.5206461780929865,0.23017902813299232,0.3824897242430674,0.42982247110687477
frozenset({'writing_cat_μέτριο'}),"frozenset({'parental level of education_high school', 'math_cat_μέτριο', 'reading_cat_μέτριο'})",0.491,0.065,0.057,0.11608961303462323,1.7859940466865112,1.0,0.025085000000000003,1.0577995391705068,0.8646124151242547,0.11422845691382764,0.05464129736323339,0.49650634497885004
"frozenset({'parental level of education_high school', 'math_cat_μέτριο', 'reading_cat_μέτριο'})",frozenset({'writing_cat_μέτριο'}),0.065,0.491,0.057,0.8769230769230769,1.785994046686511,1.0,0.025085000000000003,4.135625,0.4706820527253964,0.11422845691382764,0.758198579416654,0.49650634497885004
frozenset({'writing_cat_μέτριο'}),"frozenset({""parental level of education_associate's degree"", 'reading_cat_μέτριο', 'lunch_standard'})",0.491,0.073,0.064,0.13034623217922608,1.7855648243729603,1.0,0.028157,1.0659414519906325,0.8643479862475443,0.12800000000000003,0.061862170635626844,0.5035292804731747
"frozenset({""parental level of education_associate's degree"", 'reading_cat_μέτριο', 'lunch_standard'})",frozenset({'writing_cat_μέτριο'}),0.073,0.491,0.064,0.8767123287671234,1.78556482437296,1.0,0.028157,4.1285555555555575,0.47459884034519956,0.12800000000000003,0.757784535888258,0.5035292804731747
frozenset({'writing_cat_μέτριο'}),"frozenset({'lunch_free/reduced', 'reading_cat_μέτριο', 'gender_female'})",0.491,0.089,0.078,0.15885947046843177,1.7849378704318177,1.0,0.034301000000000005,1.083053268765133,0.8639615132738906,0.1553784860557769,0.07668438031661007,0.5176319824252271
"frozenset({'lunch_free/reduced', 'reading_cat_μέτριο', 'gender_female'})",frozenset({'writing_cat_μέτριο'}),0.089,0.491,0.078,0.8764044943820225,1.7849378704318177,1.0,0.034301000000000005,4.118272727272728,0.4827183427622507,0.1553784860557769,0.757179753206331,0.5176319824252271
"frozenset({'reading_cat_μέτριο', 'race/ethnicity_group D'})","frozenset({'writing_cat_μέτριο', 'lunch_standard'})",0.136,0.338,0.082,0.6029411764705882,1.7838496345283674,1.0,0.036031999999999995,1.667259259259259,0.5085817524841914,0.20918367346938777,0.4002132575084414,0.4227723633832231
"frozenset({'writing_cat_μέτριο', 'lunch_standard'})","frozenset({'reading_cat_μέτριο', 'race/ethnicity_group D'})",0.338,0.136,0.082,0.24260355029585798,1.7838496345283674,1.0,0.036031999999999995,1.14075,0.6637683295261955,0.20918367346938777,0.12338373876835414,0.4227723633832231
"frozenset({'writing_cat_μέτριο', 'gender_female'})","frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",0.264,0.223,0.105,0.3977272727272727,1.783530370974317,1.0,0.046127999999999995,1.2901132075471697,0.5968944099378881,0.27486910994764396,0.22487422487422484,0.43428964533224623
"frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'gender_female'})",0.223,0.264,0.105,0.4708520179372197,1.7835303709743169,1.0,0.046127999999999995,1.3909152542372882,0.5653980511123368,0.27486910994764396,0.28104893741470066,0.43428964533224623
frozenset({'reading_cat_χαμηλό'}),"frozenset({'parental level of education_high school', 'gender_male'})",0.275,0.102,0.05,0.18181818181818182,1.7825311942959003,1.0,0.02195,1.0975555555555556,0.6055172413793104,0.1529051987767584,0.08888438955254101,0.33600713012477723
"frozenset({'parental level of education_high school', 'gender_male'})",frozenset({'reading_cat_χαμηλό'}),0.102,0.275,0.05,0.4901960784313726,1.7825311942959001,1.0,0.02195,1.4221153846153847,0.4888641425389755,0.1529051987767584,0.29682217714672077,0.33600713012477723
frozenset({'writing_cat_μέτριο'}),"frozenset({'gender_female', 'reading_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",0.491,0.12,0.105,0.21384928716904278,1.7820773930753566,1.0,0.046079999999999996,1.1193782383419688,0.8621947796800449,0.2075098814229249,0.10664691723754861,0.5444246435845214
frozenset({'writing_cat_μέτριο'}),"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'gender_female', 'lunch_standard'})",0.491,0.112,0.098,0.19959266802443992,1.7820773930753564,1.0,0.043008000000000005,1.1094351145038168,0.862194779680045,0.19405940594059407,0.09864039191941433,0.53729633401222
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'gender_female', 'lunch_standard'})",frozenset({'writing_cat_μέτριο'}),0.112,0.491,0.098,0.875,1.7820773930753564,1.0,0.043008000000000005,4.072,0.4942084942084942,0.19405940594059407,0.7544204322200393,0.53729633401222
"frozenset({'gender_female', 'reading_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",frozenset({'writing_cat_μέτριο'}),0.12,0.491,0.105,0.875,1.7820773930753564,1.0,0.046079999999999996,4.072,0.4987012987012987,0.2075098814229249,0.7544204322200393,0.5444246435845214
frozenset({'reading_cat_μέτριο'}),"frozenset({'writing_cat_μέτριο', 'gender_female', 'race/ethnicity_group D'})",0.49,0.063,0.055,0.11224489795918367,1.7816650469711695,1.0,0.024130000000000002,1.0554712643678161,0.8602495543672014,0.1104417670682731,0.05255592098098577,0.49263038548752835
"frozenset({'writing_cat_μέτριο', 'gender_female', 'race/ethnicity_group D'})",frozenset({'reading_cat_μέτριο'}),0.063,0.49,0.055,0.873015873015873,1.7816650469711695,1.0,0.024130000000000002,4.01625,0.4682254778306006,0.1104417670682731,0.7510115157173981,0.49263038548752835
"frozenset({'writing_cat_μέτριο', 'lunch_standard'})","frozenset({'parental level of education_high school', 'reading_cat_μέτριο'})",0.338,0.098,0.059,0.17455621301775145,1.7811858471199127,1.0,0.025875999999999996,1.0927455197132616,0.6625019202212095,0.15649867374005302,0.08487385035227434,0.388298514672141
"frozenset({'parental level of education_high school', 'reading_cat_μέτριο'})","frozenset({'writing_cat_μέτριο', 'lunch_standard'})",0.098,0.338,0.059,0.6020408163265305,1.7811858471199127,1.0,0.025875999999999996,1.6634871794871788,0.4862264647299785,0.15649867374005302,0.39885319686787085,0.388298514672141
"frozenset({'test preparation course_completed', 'gender_female'})",frozenset({'reading_cat_υψηλό'}),0.184,0.235,0.077,0.41847826086956524,1.780758556891767,1.0,0.033760000000000005,1.3155140186915888,0.5373058314234785,0.22514619883040937,0.23984086388178463,0.3730689176688252
frozenset({'reading_cat_υψηλό'}),"frozenset({'test preparation course_completed', 'gender_female'})",0.235,0.184,0.077,0.32765957446808514,1.780758556891767,1.0,0.033760000000000005,1.2136708860759495,0.5731262201850438,0.22514619883040937,0.17605340008343764,0.3730689176688252
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})","frozenset({'reading_cat_μέτριο', 'gender_female', 'lunch_standard'})",0.32,0.172,0.098,0.30625,1.7805232558139537,1.0,0.042960000000000005,1.1935135135135138,0.6446578631452583,0.24873096446700507,0.16213768115942032,0.4380087209302326
"frozenset({'reading_cat_μέτριο', 'gender_female', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})",0.172,0.32,0.098,0.5697674418604651,1.7805232558139534,1.0,0.042960000000000005,1.5805405405405404,0.5294291629695357,0.24873096446700507,0.36730506155950754,0.4380087209302326
"frozenset({'writing_cat_μέτριο', 'race/ethnicity_group D', 'lunch_standard'})",frozenset({'reading_cat_μέτριο'}),0.094,0.49,0.082,0.8723404255319149,1.7802865827181937,1.0,0.03594000000000001,3.9950000000000014,0.4837667581973833,0.16334661354581673,0.7496871088861077,0.5198436821537126
frozenset({'reading_cat_μέτριο'}),"frozenset({'writing_cat_μέτριο', 'race/ethnicity_group D', 'lunch_standard'})",0.49,0.094,0.082,0.1673469387755102,1.7802865827181937,1.0,0.03594000000000001,1.0880882352941177,0.8593974175035869,0.16334661354581673,0.08095688606568456,0.5198436821537126
"frozenset({'reading_cat_μέτριο', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', ""parental level of education_associate's degree""})",0.327,0.11,0.064,0.19571865443425077,1.7792604948568251,1.0,0.02803,1.1065779467680608,0.6507708023774145,0.17158176943699732,0.09631309486994467,0.38876841812621626
"frozenset({'writing_cat_μέτριο', ""parental level of education_associate's degree""})","frozenset({'reading_cat_μέτριο', 'lunch_standard'})",0.11,0.327,0.064,0.5818181818181818,1.779260494856825,1.0,0.02803,1.6093478260869565,0.49209971910112354,0.17158176943699732,0.37863028501958657,0.38876841812621626
frozenset({'writing_cat_μέτριο'}),"frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'gender_female'})",0.491,0.173,0.151,0.3075356415478615,1.777662667906714,1.0,0.066057,1.194285294117647,0.8594569276207081,0.2943469785575049,0.1626791312549716,0.590184005745029
"frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'gender_female'})",frozenset({'writing_cat_μέτριο'}),0.173,0.491,0.151,0.8728323699421966,1.777662667906714,1.0,0.066057,4.0025909090909115,0.5289765128886825,0.2943469785575049,0.7501618269984216,0.590184005745029
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})",frozenset({'writing_cat_μέτριο'}),0.228,0.491,0.199,0.8728070175438597,1.7776110336942152,1.0,0.087052,4.001793103448278,0.5666414976436587,0.3826923076923077,0.7501120187502155,0.6390511666130704
frozenset({'writing_cat_μέτριο'}),"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})",0.491,0.228,0.199,0.4052953156822811,1.7776110336942152,1.0,0.087052,1.2981232876712332,0.8594248255027593,0.3826923076923077,0.2296571446661672,0.6390511666130704
"frozenset({'test preparation course_completed', 'lunch_standard'})",frozenset({'math_cat_υψηλό'}),0.227,0.176,0.071,0.31277533039647576,1.777132559070885,1.0,0.031047999999999992,1.199025641025641,0.5657125157152487,0.21385542168674695,0.1659894786364997,0.3580922106527833
frozenset({'math_cat_υψηλό'}),"frozenset({'test preparation course_completed', 'lunch_standard'})",0.176,0.227,0.071,0.4034090909090909,1.7771325590708849,1.0,0.031047999999999992,1.295695238095238,0.5306987556406398,0.21385542168674695,0.2282135716805833,0.3580922106527833
"frozenset({'writing_cat_μέτριο', 'lunch_standard'})","frozenset({'reading_cat_μέτριο', 'test preparation course_none'})",0.338,0.318,0.191,0.5650887573964497,1.777008671058018,1.0,0.08351599999999999,1.5681360544217686,0.6605083753815978,0.410752688172043,0.3623002307865832,0.5828588441070299
"frozenset({'reading_cat_μέτριο', 'test preparation course_none'})","frozenset({'writing_cat_μέτριο', 'lunch_standard'})",0.318,0.338,0.191,0.60062893081761,1.7770086710580177,1.0,0.08351599999999999,1.6576062992125982,0.6411386283029586,0.410752688172043,0.39672043930152573,0.5828588441070299
"frozenset({'math_cat_μέτριο', 'gender_male'})","frozenset({'reading_cat_μέτριο', 'race/ethnicity_group D'})",0.24,0.136,0.058,0.2416666666666667,1.7769607843137256,1.0,0.02536,1.1393406593406594,0.5753176043557169,0.18238993710691825,0.12229938271604941,0.3340686274509804
"frozenset({'reading_cat_μέτριο', 'race/ethnicity_group D'})","frozenset({'math_cat_μέτριο', 'gender_male'})",0.136,0.24,0.058,0.4264705882352941,1.7769607843137254,1.0,0.02536,1.3251282051282052,0.506066411238825,0.18238993710691825,0.24535603715170276,0.3340686274509804
"frozenset({'reading_cat_μέτριο', 'gender_male'})","frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",0.229,0.15,0.061,0.2663755458515284,1.7758369723435228,1.0,0.02665,1.1586309523809524,0.5666475303523207,0.19182389937106917,0.13691240688415107,0.33652110625909754
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})","frozenset({'reading_cat_μέτριο', 'gender_male'})",0.15,0.229,0.061,0.4066666666666667,1.7758369723435226,1.0,0.02665,1.299438202247191,0.5139826422372228,0.19182389937106917,0.23043666234327712,0.33652110625909754
frozenset({'writing_cat_μέτριο'}),"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard', 'gender_male'})",0.491,0.116,0.101,0.20570264765784116,1.7732986867055271,1.0,0.04404400000000001,1.1129333333333333,0.856737147192126,0.19960474308300397,0.10147358332334971,0.5381961514151274
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard', 'gender_male'})",frozenset({'writing_cat_μέτριο'}),0.116,0.491,0.101,0.8706896551724138,1.7732986867055271,1.0,0.04404400000000001,3.9362666666666675,0.493302271403611,0.19960474308300397,0.7459521712621097,0.5381961514151274
"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'gender_female', 'lunch_standard'})",frozenset({'reading_cat_μέτριο'}),0.061,0.49,0.053,0.8688524590163934,1.7731682837069254,1.0,0.02311,3.888749999999999,0.46436393594148734,0.10642570281124498,0.7428479588556733,0.4885078621612579
frozenset({'reading_cat_μέτριο'}),"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'gender_female', 'lunch_standard'})",0.49,0.061,0.053,0.10816326530612244,1.7731682837069254,1.0,0.02311,1.052883295194508,0.854975952645209,0.10642570281124498,0.05022711960183433,0.4885078621612579
"frozenset({'writing_cat_μέτριο', 'gender_male', 'test preparation course_none', 'lunch_standard'})",frozenset({'reading_cat_μέτριο'}),0.099,0.49,0.086,0.8686868686868686,1.7728303442589155,1.0,0.03748999999999999,3.883846153846152,0.4838293369124744,0.1709741550695825,0.7425232719350365,0.5220985363842506
frozenset({'reading_cat_μέτριο'}),"frozenset({'writing_cat_μέτριο', 'gender_male', 'test preparation course_none', 'lunch_standard'})",0.49,0.099,0.086,0.17551020408163265,1.7728303442589155,1.0,0.03748999999999999,1.0927970297029703,0.8547651618787048,0.1709741550695825,0.08491698566218939,0.5220985363842506
"frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο'})","frozenset({'writing_cat_μέτριο', 'gender_female', 'lunch_standard'})",0.169,0.177,0.053,0.3136094674556213,1.7718049008792165,1.0,0.023086999999999996,1.1990258620689656,0.5241922666485025,0.18088737201365188,0.16598963238836126,0.30652224785210447
"frozenset({'writing_cat_μέτριο', 'gender_female', 'lunch_standard'})","frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο'})",0.177,0.169,0.053,0.2994350282485876,1.7718049008792163,1.0,0.023086999999999996,1.1861854838709678,0.5292876957289254,0.18088737201365188,0.15696152617158554,0.30652224785210447
"frozenset({'reading_cat_μέτριο', 'gender_female'})",frozenset({'writing_cat_μέτριο'}),0.261,0.491,0.227,0.8697318007662835,1.7713478630677872,1.0,0.09884899999999999,3.907323529411765,0.5892532473338777,0.43238095238095237,0.7440703354936808,0.6660267965134881
frozenset({'writing_cat_μέτριο'}),"frozenset({'reading_cat_μέτριο', 'gender_female'})",0.491,0.261,0.227,0.4623217922606925,1.7713478630677872,1.0,0.09884899999999999,1.3744280303030303,0.8555169936733509,0.43238095238095237,0.27242461740283147,0.6660267965134881
"frozenset({'writing_cat_μέτριο', 'test preparation course_none'})","frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο', 'gender_female'})",0.308,0.099,0.054,0.17532467532467533,1.7709563164108617,1.0,0.023507999999999998,1.0925511811023623,0.6290944123314065,0.15297450424929177,0.08471107139253643,0.36038961038961037
"frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο', 'gender_female'})","frozenset({'writing_cat_μέτριο', 'test preparation course_none'})",0.099,0.308,0.054,0.5454545454545454,1.7709563164108617,1.0,0.023507999999999998,1.5223999999999998,0.48316685164631884,0.15297450424929177,0.3431424067262217,0.36038961038961037
"frozenset({'reading_cat_μέτριο', 'test preparation course_none'})","frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})",0.318,0.32,0.18,0.5660377358490566,1.7688679245283019,1.0,0.07823999999999999,1.5669565217391301,0.6373411534701857,0.3930131004366812,0.36182019977802443,0.5642688679245282
"frozenset({'parental level of education_high school', 'writing_cat_μέτριο'})","frozenset({'reading_cat_μέτριο', 'test preparation course_none'})",0.096,0.318,0.054,0.5625,1.7688679245283019,1.0,0.023472,1.5588571428571427,0.4808259587020649,0.15,0.3585043988269795,0.36615566037735847
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})","frozenset({'reading_cat_μέτριο', 'test preparation course_none'})",0.32,0.318,0.18,0.5625,1.7688679245283019,1.0,0.07823999999999999,1.5588571428571427,0.6392156862745099,0.3930131004366812,0.3585043988269795,0.5642688679245282
"frozenset({'reading_cat_μέτριο', 'test preparation course_none'})","frozenset({'parental level of education_high school', 'writing_cat_μέτριο'})",0.318,0.096,0.054,0.16981132075471697,1.7688679245283017,1.0,0.023472,1.0889090909090908,0.6373411534701857,0.15,0.08164969110035063,0.36615566037735847
frozenset({'reading_cat_μέτριο'}),"frozenset({'writing_cat_μέτριο', 'parental level of education_some college', 'lunch_standard'})",0.49,0.075,0.065,0.1326530612244898,1.7687074829931975,1.0,0.028250000000000004,1.0664705882352943,0.8521870286576171,0.13000000000000003,0.062327633756205196,0.49965986394557826
"frozenset({'writing_cat_μέτριο', 'parental level of education_some college', 'lunch_standard'})",frozenset({'reading_cat_μέτριο'}),0.075,0.49,0.065,0.8666666666666667,1.7687074829931975,1.0,0.028250000000000004,3.825000000000001,0.4698544698544699,0.13000000000000003,0.738562091503268,0.49965986394557826
"frozenset({'math_cat_χαμηλό', 'reading_cat_χαμηλό'})","frozenset({'lunch_free/reduced', 'gender_male'})",0.218,0.166,0.064,0.29357798165137616,1.7685420581408202,1.0,0.027811999999999996,1.1805974025974026,0.5557065217391304,0.2,0.15297120102083472,0.33956007516303743
"frozenset({'lunch_free/reduced', 'gender_male'})","frozenset({'math_cat_χαμηλό', 'reading_cat_χαμηλό'})",0.166,0.218,0.064,0.38554216867469876,1.76854205814082,1.0,0.027811999999999996,1.2726666666666666,0.5210581534772182,0.2,0.21424829753797794,0.33956007516303743
"frozenset({'gender_female', 'lunch_standard'})","frozenset({'test preparation course_none', 'reading_cat_υψηλό'})",0.329,0.11,0.064,0.1945288753799392,1.768444321635811,1.0,0.02781,1.1049433962264152,0.6475875558867362,0.17066666666666666,0.0949762644718418,0.3881735285990605
"frozenset({'test preparation course_none', 'reading_cat_υψηλό'})","frozenset({'gender_female', 'lunch_standard'})",0.11,0.329,0.064,0.5818181818181818,1.7684443216358108,1.0,0.02781,1.6045652173913043,0.4882373595505618,0.17066666666666666,0.37677821433410097,0.3881735285990605
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'gender_male'})",0.152,0.227,0.061,0.40131578947368424,1.767910966844424,1.0,0.026496,1.2911648351648353,0.5122177544076709,0.19182389937106917,0.22550554912507662,0.3350191282170183
"frozenset({'writing_cat_μέτριο', 'gender_male'})","frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",0.227,0.152,0.061,0.2687224669603524,1.7679109668444237,1.0,0.026496,1.1596144578313252,0.561915466672322,0.19182389937106917,0.13764441858532123,0.3350191282170183
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})","frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο'})",0.228,0.154,0.062,0.2719298245614035,1.7657780815675554,1.0,0.026888000000000002,1.1619759036144577,0.561758315226475,0.19375,0.13939695574633987,0.33726361357940304
"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο'})","frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})",0.154,0.228,0.062,0.4025974025974026,1.7657780815675552,1.0,0.026888000000000002,1.2922608695652176,0.5126210630671852,0.19375,0.22616243859767177,0.33726361357940304
frozenset({'writing_cat_μέτριο'}),"frozenset({'reading_cat_μέτριο', 'parental level of education_some high school', 'lunch_standard'})",0.491,0.06,0.052,0.10590631364562118,1.7651052274270198,1.0,0.022539999999999998,1.0513439635535307,0.8515943781169714,0.10420841683366734,0.04883650387832041,0.48628649015614395
"frozenset({'reading_cat_μέτριο', 'parental level of education_some high school', 'lunch_standard'})",frozenset({'writing_cat_μέτριο'}),0.06,0.491,0.052,0.8666666666666667,1.7651052274270198,1.0,0.022539999999999998,3.817500000000001,0.4611292962356792,0.10420841683366734,0.7380484610347087,0.48628649015614395
frozenset({'reading_cat_μέτριο'}),"frozenset({'writing_cat_μέτριο', 'gender_male', 'test preparation course_none'})",0.49,0.133,0.115,0.23469387755102042,1.764615620684364,1.0,0.04983,1.13288,0.849616368286445,0.22637795275590553,0.11729397641409506,0.5496777658431794
"frozenset({'writing_cat_μέτριο', 'gender_male', 'test preparation course_none'})",frozenset({'reading_cat_μέτριο'}),0.133,0.49,0.115,0.8646616541353384,1.764615620684364,1.0,0.04983,3.7683333333333335,0.4997743342861441,0.22637795275590553,0.7346306943830164,0.5496777658431794
frozenset({'writing_cat_μέτριο'}),"frozenset({'reading_cat_μέτριο', 'gender_female', 'lunch_standard'})",0.491,0.172,0.149,0.3034623217922607,1.7643158243736088,1.0,0.06454800000000001,1.1887368421052633,0.8510963726744112,0.28988326848249024,0.15877092003896218,0.5848706957798513
"frozenset({'reading_cat_μέτριο', 'gender_female', 'lunch_standard'})",frozenset({'writing_cat_μέτριο'}),0.172,0.491,0.149,0.8662790697674418,1.7643158243736086,1.0,0.06454800000000001,3.806434782608695,0.5231981324773854,0.28988326848249024,0.7372869740028327,0.5848706957798513
"frozenset({""parental level of education_associate's degree"", 'reading_cat_μέτριο'})",frozenset({'writing_cat_μέτριο'}),0.112,0.491,0.097,0.8660714285714286,1.76389292988071,1.0,0.042008000000000004,3.8005333333333344,0.48769387944645676,0.191699604743083,0.7368790345214707,0.531813718359034
frozenset({'writing_cat_μέτριο'}),"frozenset({""parental level of education_associate's degree"", 'reading_cat_μέτριο'})",0.491,0.112,0.097,0.1975560081466395,1.7638929298807098,1.0,0.042008000000000004,1.1066192893401017,0.8508294006845848,0.191699604743083,0.09634685602099044,0.531813718359034
frozenset({'reading_cat_μέτριο'}),"frozenset({'writing_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",0.49,0.221,0.191,0.38979591836734695,1.7637824360513437,1.0,0.08271,1.2766220735785954,0.8490914690483524,0.3673076923076923,0.21668282203767258,0.6270246560162527
"frozenset({'writing_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",frozenset({'reading_cat_μέτριο'}),0.221,0.49,0.191,0.8642533936651584,1.7637824360513437,1.0,0.08271,3.757000000000001,0.5558878680547621,0.3673076923076923,0.7338301836571733,0.6270246560162527
frozenset({'writing_cat_μέτριο'}),"frozenset({'reading_cat_μέτριο', 'lunch_standard'})",0.491,0.327,0.283,0.5763747454175152,1.7626139003593737,1.0,0.12244299999999997,1.588668269230769,0.8500211736447131,0.5289719626168222,0.37054196941681306,0.7209090852469838
"frozenset({'reading_cat_μέτριο', 'lunch_standard'})",frozenset({'writing_cat_μέτριο'}),0.327,0.491,0.283,0.8654434250764524,1.7626139003593737,1.0,0.12244299999999997,3.7827954545454503,0.6428837702602659,0.5289719626168222,0.7356452359065864,0.7209090852469838
frozenset({'reading_cat_μέτριο'}),"frozenset({'writing_cat_μέτριο', 'test preparation course_none'})",0.49,0.308,0.266,0.5428571428571429,1.76252319109462,1.0,0.11508000000000002,1.5137500000000002,0.8482972136222912,0.5,0.3393889347646574,0.7032467532467532
"frozenset({'lunch_free/reduced', 'gender_male', 'writing_cat_μέτριο'})",frozenset({'reading_cat_μέτριο'}),0.066,0.49,0.057,0.8636363636363636,1.7625231910946197,1.0,0.02466,3.74,0.4632029753183816,0.11422845691382764,0.732620320855615,0.48998144712430425
"frozenset({'parental level of education_high school', 'math_cat_μέτριο', 'writing_cat_μέτριο'})",frozenset({'reading_cat_μέτριο'}),0.066,0.49,0.057,0.8636363636363636,1.7625231910946197,1.0,0.02466,3.74,0.4632029753183816,0.11422845691382764,0.732620320855615,0.48998144712430425
frozenset({'reading_cat_μέτριο'}),"frozenset({'parental level of education_high school', 'math_cat_μέτριο', 'writing_cat_μέτριο'})",0.49,0.066,0.057,0.1163265306122449,1.7625231910946197,1.0,0.02466,1.0569515011547344,0.848297213622291,0.11422845691382764,0.05388279508805664,0.48998144712430425
"frozenset({'writing_cat_μέτριο', 'test preparation course_none'})",frozenset({'reading_cat_μέτριο'}),0.308,0.49,0.266,0.8636363636363636,1.7625231910946197,1.0,0.11508000000000002,3.74,0.6251901429875267,0.5,0.732620320855615,0.7032467532467532
frozenset({'reading_cat_μέτριο'}),"frozenset({'lunch_free/reduced', 'gender_male', 'writing_cat_μέτριο'})",0.49,0.066,0.057,0.1163265306122449,1.7625231910946197,1.0,0.02466,1.0569515011547344,0.848297213622291,0.11422845691382764,0.05388279508805664,0.48998144712430425
frozenset({'writing_cat_μέτριο'}),"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})",0.491,0.208,0.18,0.3665987780040733,1.7624941250195834,1.0,0.077872,1.2503922829581995,0.8499454267627156,0.34682080924855496,0.2002509823283754,0.6159916966943444
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})",frozenset({'writing_cat_μέτριο'}),0.208,0.491,0.18,0.8653846153846154,1.7624941250195834,1.0,0.077872,3.781142857142858,0.5462401795735129,0.34682080924855496,0.7355296962369655,0.6159916966943444
frozenset({'reading_cat_μέτριο'}),"frozenset({'writing_cat_μέτριο', 'race/ethnicity_group D'})",0.49,0.139,0.12,0.24489795918367346,1.7618558214652766,1.0,0.05188999999999999,1.1402432432432432,0.8478758169934639,0.23575638506876226,0.12299414539334896,0.5541036558508294
"frozenset({'writing_cat_μέτριο', 'race/ethnicity_group D'})",frozenset({'reading_cat_μέτριο'}),0.139,0.49,0.12,0.8633093525179855,1.7618558214652764,1.0,0.05188999999999999,3.7310526315789434,0.5022260936895082,0.23575638506876226,0.7319791225842852,0.5541036558508294
frozenset({'reading_cat_μέτριο'}),"frozenset({'lunch_free/reduced', 'math_cat_μέτριο', 'writing_cat_μέτριο'})",0.49,0.095,0.082,0.1673469387755102,1.7615467239527391,1.0,0.03545,1.0868872549019608,0.8476805356288857,0.16302186878727634,0.07994136881271846,0.5152524167561762
"frozenset({'lunch_free/reduced', 'math_cat_μέτριο', 'writing_cat_μέτριο'})",frozenset({'reading_cat_μέτριο'}),0.095,0.49,0.082,0.8631578947368421,1.7615467239527391,1.0,0.03545,3.7269230769230774,0.47769842339307367,0.16302186878727634,0.7316821465428277,0.5152524167561762
"frozenset({'writing_cat_μέτριο', 'gender_female', 'test preparation course_none'})",frozenset({'reading_cat_μέτριο'}),0.175,0.49,0.151,0.8628571428571429,1.7609329446064141,1.0,0.06525,3.7187500000000004,0.5237808549066828,0.29377431906614787,0.73109243697479,0.5855102040816327
frozenset({'reading_cat_μέτριο'}),"frozenset({'writing_cat_μέτριο', 'gender_female', 'test preparation course_none'})",0.49,0.175,0.151,0.3081632653061224,1.760932944606414,1.0,0.06525,1.1924778761061945,0.8472925594078692,0.29377431906614787,0.16141001855287568,0.5855102040816327
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο'})",frozenset({'writing_cat_μέτριο'}),0.325,0.491,0.281,0.8646153846153847,1.7609274635751215,1.0,0.12142500000000003,3.759659090909093,0.6401739818109925,0.525233644859813,0.7340184373583196,0.7184584051386496
frozenset({'writing_cat_μέτριο'}),"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο'})",0.491,0.325,0.281,0.5723014256619146,1.7609274635751215,1.0,0.12142500000000003,1.5782142857142862,0.8489537086884479,0.525233644859813,0.3663724824620956,0.7184584051386496
"frozenset({'reading_cat_μέτριο', 'lunch_standard', 'gender_male'})",frozenset({'writing_cat_μέτριο'}),0.155,0.491,0.134,0.8645161290322582,1.7607253137113201,1.0,0.057895000000000016,3.7569047619047646,0.5113044246224501,0.26171875,0.733823436212688,0.5687142763287564
frozenset({'writing_cat_μέτριο'}),"frozenset({'reading_cat_μέτριο', 'lunch_standard', 'gender_male'})",0.491,0.155,0.134,0.2729124236252546,1.76072531371132,1.0,0.057895000000000016,1.1621708683473388,0.8488256165146764,0.26171875,0.1395413297340291,0.5687142763287564
"frozenset({'lunch_free/reduced', 'writing_cat_μέτριο', 'test preparation course_none'})",frozenset({'reading_cat_μέτριο'}),0.087,0.49,0.075,0.8620689655172414,1.7593244194229418,1.0,0.03237,3.6975000000000016,0.47272727272727283,0.14940239043824702,0.7295469912102773,0.5075650950035187
frozenset({'reading_cat_μέτριο'}),"frozenset({'lunch_free/reduced', 'writing_cat_μέτριο', 'test preparation course_none'})",0.49,0.087,0.075,0.15306122448979592,1.7593244194229418,1.0,0.03237,1.078,0.8462745098039217,0.14940239043824702,0.07235621521335808,0.5075650950035187
"frozenset({'test preparation course_completed', 'reading_cat_μέτριο', 'gender_female'})",frozenset({'writing_cat_μέτριο'}),0.088,0.491,0.076,0.8636363636363636,1.7589335308276246,1.0,0.032792,3.732666666666667,0.4731071098799631,0.15109343936381708,0.7320950169673156,0.5092112571745973
frozenset({'writing_cat_μέτριο'}),"frozenset({'test preparation course_completed', 'reading_cat_μέτριο', 'gender_female'})",0.491,0.088,0.076,0.15478615071283094,1.7589335308276244,1.0,0.032792,1.0790168674698797,0.8476889670147865,0.15109343936381708,0.07323042841319183,0.5092112571745973
"frozenset({'test preparation course_completed', 'math_cat_μέτριο', 'reading_cat_μέτριο'})",frozenset({'writing_cat_μέτριο'}),0.117,0.491,0.101,0.8632478632478633,1.758142287673856,1.0,0.043553,3.722062500000001,0.4883554040568269,0.19921104536489154,0.7313317549073935,0.5344752554528522
frozenset({'writing_cat_μέτριο'}),"frozenset({'test preparation course_completed', 'math_cat_μέτριο', 'reading_cat_μέτριο'})",0.491,0.117,0.101,0.20570264765784116,1.758142287673856,1.0,0.043553,1.111674358974359,0.8471862903382676,0.19921104536489154,0.10045599961250413,0.5344752554528522
"frozenset({'gender_male', 'reading_cat_χαμηλό', 'lunch_standard'})",frozenset({'math_cat_χαμηλό'}),0.099,0.339,0.059,0.5959595959595959,1.757992908435386,1.0,0.025438999999999996,1.635975,0.4785454955887055,0.15567282321899734,0.3887437155213251,0.3850004469473496
frozenset({'math_cat_χαμηλό'}),"frozenset({'gender_male', 'reading_cat_χαμηλό', 'lunch_standard'})",0.339,0.099,0.059,0.17404129793510323,1.757992908435386,1.0,0.025438999999999996,1.0908535714285714,0.6522987768917151,0.15567282321899734,0.08328667917325552,0.3850004469473496
"frozenset({'reading_cat_μέτριο', 'gender_female'})","frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'test preparation course_none'})",0.261,0.205,0.094,0.36015325670498083,1.7568451546584432,1.0,0.040495,1.2424850299401198,0.582947053234676,0.25268817204301075,0.19516132918865514,0.40934492103541725
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'test preparation course_none'})","frozenset({'reading_cat_μέτριο', 'gender_female'})",0.205,0.261,0.094,0.4585365853658537,1.7568451546584432,1.0,0.040495,1.3648198198198198,0.5418841161514787,0.25268817204301075,0.26730255123931485,0.40934492103541725
frozenset({'reading_cat_μέτριο'}),"frozenset({'writing_cat_μέτριο', 'gender_female', 'test preparation course_none', 'lunch_standard'})",0.49,0.122,0.105,0.21428571428571427,1.756440281030445,1.0,0.045219999999999996,1.1174545454545455,0.8444444444444443,0.20710059171597633,0.10510901399284087,0.5374707259953161
"frozenset({'writing_cat_μέτριο', 'gender_female', 'test preparation course_none', 'lunch_standard'})",frozenset({'reading_cat_μέτριο'}),0.122,0.49,0.105,0.860655737704918,1.756440281030445,1.0,0.045219999999999996,3.659999999999999,0.4905087319665907,0.20710059171597633,0.7267759562841529,0.5374707259953161
"frozenset({'math_cat_χαμηλό', 'gender_female'})","frozenset({'test preparation course_none', 'reading_cat_χαμηλό'})",0.205,0.214,0.077,0.375609756097561,1.755185776156827,1.0,0.03313,1.258828125,0.5412072204525035,0.22514619883040937,0.20561037671445417,0.36771142010485525
"frozenset({'test preparation course_none', 'reading_cat_χαμηλό'})","frozenset({'math_cat_χαμηλό', 'gender_female'})",0.214,0.205,0.077,0.3598130841121495,1.755185776156827,1.0,0.03313,1.2418248175182482,0.547404249694326,0.22514619883040937,0.19473343913477928,0.36771142010485525
"frozenset({'test preparation course_completed', 'writing_cat_υψηλό', 'math_cat_μέτριο'})",frozenset({'gender_female'}),0.055,0.518,0.05,0.9090909090909092,1.7550017550017551,1.0,0.02151,5.302000000000005,0.4552380952380953,0.09560229445506692,0.8113919275745004,0.5028080028080029
frozenset({'gender_female'}),"frozenset({'math_cat_χαμηλό', 'reading_cat_μέτριο', 'test preparation course_none'})",0.518,0.077,0.07,0.13513513513513514,1.7550017550017551,1.0,0.03011400000000001,1.0672187499999999,0.8925311203319505,0.13333333333333336,0.06298497847793623,0.5221130221130221
"frozenset({'math_cat_χαμηλό', 'reading_cat_μέτριο', 'test preparation course_none'})",frozenset({'gender_female'}),0.077,0.518,0.07,0.9090909090909092,1.7550017550017551,1.0,0.03011400000000001,5.302000000000005,0.46608884073672807,0.13333333333333336,0.8113919275745004,0.5221130221130221
frozenset({'gender_female'}),"frozenset({'test preparation course_completed', 'writing_cat_υψηλό', 'math_cat_μέτριο'})",0.518,0.055,0.05,0.09652509652509653,1.755001755001755,1.0,0.02151,1.0459615384615384,0.8925311203319503,0.09560229445506692,0.043941901084758234,0.5028080028080029
frozenset({'reading_cat_μέτριο'}),"frozenset({'writing_cat_μέτριο', 'gender_female'})",0.49,0.264,0.227,0.463265306122449,1.7547928262213977,1.0,0.09764,1.371254752851711,0.8433963893927615,0.4307400379506641,0.27074090505767523,0.6615568954854669
"frozenset({'writing_cat_μέτριο', 'gender_female'})",frozenset({'reading_cat_μέτριο'}),0.264,0.49,0.227,0.8598484848484849,1.7547928262213977,1.0,0.09764,3.6389189189189195,0.5844186937368321,0.4307400379506641,0.7251931075460487,0.6615568954854669
"frozenset({'writing_cat_μέτριο', 'lunch_standard'})","frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'gender_male'})",0.338,0.145,0.086,0.2544378698224852,1.7547439298102427,1.0,0.036989999999999995,1.1467857142857143,0.6497224759362046,0.21662468513853902,0.12799750856431016,0.4237706590491736
"frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'gender_male'})","frozenset({'writing_cat_μέτριο', 'lunch_standard'})",0.145,0.338,0.086,0.593103448275862,1.7547439298102425,1.0,0.036989999999999995,1.6269491525423725,0.5030599755201958,0.21662468513853902,0.3853526409000937,0.4237706590491736
"frozenset({'writing_cat_μέτριο', 'parental level of education_some college'})",frozenset({'reading_cat_μέτριο'}),0.114,0.49,0.098,0.8596491228070176,1.7543859649122808,1.0,0.042140000000000004,3.6337500000000005,0.48532731376975174,0.19367588932806323,0.7248022015823874,0.5298245614035088
frozenset({'reading_cat_μέτριο'}),"frozenset({'writing_cat_μέτριο', 'parental level of education_some college'})",0.49,0.114,0.098,0.2,1.7543859649122808,1.0,0.042140000000000004,1.1075,0.8431372549019608,0.19367588932806323,0.09706546275395035,0.5298245614035088
frozenset({'writing_cat_μέτριο'}),"frozenset({'race/ethnicity_group C', 'math_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})",0.491,0.072,0.062,0.12627291242362526,1.7537904503281287,1.0,0.026648000000000005,1.0621165501165502,0.8444134609290831,0.12375249500998006,0.058483741835803085,0.4936920117673682
"frozenset({'race/ethnicity_group C', 'math_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})",frozenset({'writing_cat_μέτριο'}),0.072,0.491,0.062,0.8611111111111112,1.7537904503281287,1.0,0.026648000000000005,3.6648000000000014,0.46315350389321475,0.12375249500998006,0.7271338135778216,0.4936920117673682
"frozenset({'math_cat_χαμηλό', 'reading_cat_μέτριο'})",frozenset({'gender_female'}),0.12,0.518,0.109,0.9083333333333333,1.7535392535392536,1.0,0.04684,5.258181818181818,0.48832360300250205,0.2060491493383743,0.809820193637621,0.5593790218790219
frozenset({'gender_female'}),"frozenset({'math_cat_χαμηλό', 'reading_cat_μέτριο'})",0.518,0.12,0.109,0.21042471042471042,1.7535392535392536,1.0,0.04684,1.1145232273838632,0.8915451673074727,0.2060491493383743,0.10275535275535276,0.5593790218790219
frozenset({'lunch_free/reduced'}),"frozenset({'gender_female', 'reading_cat_χαμηλό'})",0.355,0.098,0.061,0.17183098591549295,1.7533774073009485,1.0,0.026209999999999997,1.0891496598639456,0.6661583428644046,0.1556122448979592,0.08185253427438242,0.3971399827536648
"frozenset({'gender_female', 'reading_cat_χαμηλό'})",frozenset({'lunch_free/reduced'}),0.098,0.355,0.061,0.6224489795918366,1.7533774073009485,1.0,0.026209999999999997,1.7083783783783781,0.4763549125804223,0.1556122448979592,0.41464958076253744,0.3971399827536648
frozenset({'reading_cat_μέτριο'}),"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'parental level of education_some college'})",0.49,0.078,0.067,0.13673469387755102,1.7530088958660388,1.0,0.028780000000000007,1.0680378250591016,0.8422592917764121,0.13373253493013976,0.0637035725353048,0.497854526425955
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'parental level of education_some college'})",frozenset({'reading_cat_μέτριο'}),0.078,0.49,0.067,0.858974358974359,1.7530088958660388,1.0,0.028780000000000007,3.616363636363638,0.4658917991387963,0.13373253493013976,0.7234791352438412,0.497854526425955
"frozenset({'test preparation course_completed', 'math_cat_μέτριο', 'gender_male'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.086,0.418,0.063,0.7325581395348838,1.7525314342939804,1.0,0.027052000000000007,2.1761739130434794,0.4697995901496997,0.14285714285714285,0.5404779029808999,0.441637921442083
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'test preparation course_completed', 'math_cat_μέτριο', 'gender_male'})",0.418,0.086,0.063,0.1507177033492823,1.7525314342939804,1.0,0.027052000000000007,1.0762028169014086,0.7377952326405935,0.14285714285714285,0.07080711526179684,0.441637921442083
"frozenset({'reading_cat_μέτριο', 'parental level of education_some college'})",frozenset({'writing_cat_μέτριο'}),0.114,0.491,0.098,0.8596491228070176,1.7508128774073677,1.0,0.042026,3.626625,0.4840143732436541,0.19329388560157792,0.7242615379312722,0.5296208954157288
frozenset({'writing_cat_μέτριο'}),"frozenset({'reading_cat_μέτριο', 'parental level of education_some college'})",0.491,0.114,0.098,0.19959266802443992,1.7508128774073677,1.0,0.042026,1.1069363867684479,0.842508319634337,0.19329388560157792,0.096605720117878,0.5296208954157288
frozenset({'lunch_free/reduced'}),"frozenset({'math_cat_χαμηλό', 'gender_female', 'reading_cat_χαμηλό'})",0.355,0.095,0.059,0.16619718309859155,1.749444032616753,1.0,0.025275,1.0853885135135135,0.6641702798581001,0.15089514066496165,0.07867092055092989,0.3936249073387694
"frozenset({'math_cat_χαμηλό', 'gender_female', 'reading_cat_χαμηλό'})",frozenset({'lunch_free/reduced'}),0.095,0.355,0.059,0.6210526315789473,1.749444032616753,1.0,0.025275,1.7020833333333332,0.47335892873864593,0.15089514066496165,0.4124847001223989,0.3936249073387694
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'parental level of education_some college'})",frozenset({'writing_cat_μέτριο'}),0.078,0.491,0.067,0.858974358974359,1.7494386129824013,1.0,0.028702000000000005,3.609272727272729,0.4646291319972804,0.13346613545816735,0.7229358722482496,0.4977152853934932
frozenset({'writing_cat_μέτριο'}),"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'parental level of education_some college'})",0.491,0.078,0.067,0.1364562118126273,1.749438612982401,1.0,0.028702000000000005,1.0676933962264152,0.8416268363487085,0.13346613545816735,0.06340153125013806,0.4977152853934932
"frozenset({'writing_cat_μέτριο', 'gender_female'})","frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο', 'test preparation course_none'})",0.264,0.117,0.054,0.20454545454545453,1.7482517482517481,1.0,0.023111999999999997,1.1100571428571429,0.5815217391304347,0.16513761467889906,0.09914547513641508,0.333041958041958
"frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο', 'test preparation course_none'})","frozenset({'writing_cat_μέτριο', 'gender_female'})",0.117,0.264,0.054,0.4615384615384615,1.7482517482517481,1.0,0.023111999999999997,1.3668571428571425,0.48471121177802934,0.16513761467889906,0.26839464882943137,0.333041958041958
"frozenset({'writing_cat_μέτριο', 'test preparation course_none'})","frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο'})",0.308,0.169,0.091,0.29545454545454547,1.7482517482517481,1.0,0.038947999999999997,1.1794838709677418,0.6184971098265896,0.23575129533678754,0.152171534843015,0.416958041958042
"frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο'})","frozenset({'writing_cat_μέτριο', 'test preparation course_none'})",0.169,0.308,0.091,0.5384615384615384,1.7482517482517481,1.0,0.038947999999999997,1.4993333333333332,0.5150421179302046,0.23575129533678754,0.3330369052912405,0.416958041958042
"frozenset({'math_cat_μέτριο', 'race/ethnicity_group D'})","frozenset({'reading_cat_μέτριο', 'gender_male'})",0.145,0.229,0.058,0.4,1.7467248908296944,1.0,0.024795000000000005,1.2850000000000001,0.5000000000000001,0.18354430379746836,0.2217898832684825,0.32663755458515287
"frozenset({'reading_cat_μέτριο', 'gender_male'})","frozenset({'math_cat_μέτριο', 'race/ethnicity_group D'})",0.229,0.145,0.058,0.25327510917030566,1.7467248908296944,1.0,0.024795000000000005,1.1449999999999998,0.5544747081712064,0.18354430379746836,0.12663755458515283,0.32663755458515287
"frozenset({'gender_female', 'test preparation course_none'})","frozenset({'math_cat_χαμηλό', 'reading_cat_μέτριο'})",0.334,0.12,0.07,0.2095808383233533,1.7465069860279443,1.0,0.02992000000000001,1.1133333333333333,0.641784641784642,0.18229166666666669,0.10179640718562877,0.3964570858283433
"frozenset({'math_cat_χαμηλό', 'reading_cat_μέτριο'})","frozenset({'gender_female', 'test preparation course_none'})",0.12,0.334,0.07,0.5833333333333334,1.746506986027944,1.0,0.02992000000000001,1.5984,0.48571428571428577,0.18229166666666669,0.37437437437437443,0.3964570858283433
"frozenset({'math_cat_μέτριο', 'gender_female'})",frozenset({'writing_cat_υψηλό'}),0.245,0.208,0.089,0.36326530612244895,1.7464678178963893,1.0,0.03804,1.2438461538461538,0.566113550115336,0.2445054945054945,0.1960420531849103,0.39557496075353216
frozenset({'writing_cat_υψηλό'}),"frozenset({'math_cat_μέτριο', 'gender_female'})",0.208,0.245,0.089,0.42788461538461536,1.7464678178963893,1.0,0.03804,1.3196638655462185,0.5396663261831801,0.2445054945054945,0.2422312786551197,0.39557496075353216
"frozenset({'writing_cat_μέτριο', 'gender_male', 'race/ethnicity_group D'})",frozenset({'reading_cat_μέτριο'}),0.076,0.49,0.065,0.855263157894737,1.745435016111708,1.0,0.027760000000000007,3.523636363636366,0.4622044622044623,0.1297405189620759,0.7162022703818371,0.4939581095596134
frozenset({'reading_cat_μέτριο'}),"frozenset({'writing_cat_μέτριο', 'gender_male', 'race/ethnicity_group D'})",0.49,0.076,0.065,0.1326530612244898,1.745435016111708,1.0,0.027760000000000007,1.0653176470588237,0.8374057315233788,0.1297405189620759,0.061312836823040916,0.4939581095596134
frozenset({'writing_cat_μέτριο'}),"frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",0.491,0.223,0.191,0.3890020366598778,1.744403751838017,1.0,0.081507,1.2716900000000002,0.8383855007765971,0.3652007648183557,0.21364483482609756,0.6227521394061721
"frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",frozenset({'writing_cat_μέτριο'}),0.223,0.491,0.191,0.8565022421524664,1.744403751838017,1.0,0.081507,3.5470937500000006,0.5492126382178738,0.3652007648183557,0.7180790612032739,0.6227521394061721
"frozenset({'test preparation course_completed', 'writing_cat_υψηλό'})","frozenset({'math_cat_μέτριο', 'gender_female'})",0.117,0.245,0.05,0.42735042735042733,1.744287458573173,1.0,0.021335000000000003,1.3184328358208957,0.4832389580973953,0.16025641025641027,0.24152374483500308,0.3157160300017443
"frozenset({'math_cat_μέτριο', 'gender_female'})","frozenset({'test preparation course_completed', 'writing_cat_υψηλό'})",0.245,0.117,0.05,0.20408163265306123,1.7442874585731727,1.0,0.021335000000000003,1.1094102564102564,0.5651655629139073,0.16025641025641027,0.09862019553008065,0.3157160300017443
"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'gender_female', 'test preparation course_none'})",frozenset({'reading_cat_μέτριο'}),0.11,0.49,0.094,0.8545454545454545,1.7439703153988868,1.0,0.040100000000000004,3.5062499999999988,0.47932106143915854,0.1857707509881423,0.7147950089126559,0.5231910946196661
frozenset({'reading_cat_μέτριο'}),"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'gender_female', 'test preparation course_none'})",0.49,0.11,0.094,0.19183673469387755,1.7439703153988868,1.0,0.040100000000000004,1.1012626262626262,0.836462244472257,0.1857707509881423,0.09195138729649162,0.5231910946196661
"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'gender_male', 'lunch_standard'})","frozenset({'reading_cat_μέτριο', 'test preparation course_none'})",0.11,0.318,0.061,0.5545454545454546,1.7438536306460835,1.0,0.026019999999999995,1.5310204081632652,0.47927795174065196,0.16621253405994552,0.3468408424420155,0.37318467695826185
"frozenset({'reading_cat_μέτριο', 'test preparation course_none'})","frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'gender_male', 'lunch_standard'})",0.318,0.11,0.061,0.19182389937106917,1.7438536306460832,1.0,0.026019999999999995,1.1012451361867703,0.6254506994856016,0.16621253405994552,0.09193696558547097,0.37318467695826185
"frozenset({'reading_cat_μέτριο', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'parental level of education_some college'})",0.327,0.114,0.065,0.19877675840978593,1.743655775524438,1.0,0.027721999999999997,1.1058091603053435,0.6337181392159102,0.17287234042553193,0.09568482890495025,0.3844760985031386
"frozenset({'writing_cat_μέτριο', 'parental level of education_some college'})","frozenset({'reading_cat_μέτριο', 'lunch_standard'})",0.114,0.327,0.065,0.5701754385964912,1.743655775524438,1.0,0.027721999999999997,1.5657551020408165,0.481368293106442,0.17287234042553193,0.36133051797398397,0.3844760985031386
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'gender_female', 'test preparation course_none'})",0.283,0.15,0.074,0.26148409893992935,1.7432273262661957,1.0,0.03155,1.1509569377990432,0.5946322891929586,0.2061281337047354,0.13115776345874042,0.37740871613663135
"frozenset({'math_cat_μέτριο', 'gender_female', 'test preparation course_none'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})",0.15,0.283,0.074,0.49333333333333335,1.7432273262661957,1.0,0.03155,1.4151315789473689,0.5015898251192369,0.2061281337047354,0.29335192933519294,0.37740871613663135
"frozenset({'reading_cat_μέτριο', 'lunch_standard'})","frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'gender_female'})",0.327,0.093,0.053,0.1620795107033639,1.7427904376705796,1.0,0.022588999999999998,1.0824416058394162,0.6332950180829291,0.14441416893732967,0.07616263583612337,0.3659859919108217
"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'gender_female'})","frozenset({'reading_cat_μέτριο', 'lunch_standard'})",0.093,0.327,0.053,0.5698924731182795,1.7427904376705794,1.0,0.022588999999999998,1.564725,0.4699090928002329,0.14441416893732967,0.36091006406876597,0.3659859919108217
"frozenset({'test preparation course_completed', 'writing_cat_μέτριο', 'gender_female'})",frozenset({'reading_cat_μέτριο'}),0.089,0.49,0.076,0.8539325842696629,1.742719559734006,1.0,0.03239,3.491538461538461,0.46782020913975386,0.15109343936381708,0.7135933024895351,0.5045173125429947
frozenset({'reading_cat_μέτριο'}),"frozenset({'test preparation course_completed', 'writing_cat_μέτριο', 'gender_female'})",0.49,0.089,0.076,0.15510204081632653,1.742719559734006,1.0,0.03239,1.0782367149758454,0.8356553147574819,0.15109343936381708,0.07255986917269652,0.5045173125429947
frozenset({'writing_cat_μέτριο'}),"frozenset({'reading_cat_μέτριο', 'parental level of education_some college', 'test preparation course_none'})",0.491,0.076,0.065,0.1323828920570265,1.7418801586450856,1.0,0.027684000000000007,1.0649859154929577,0.8367538159286687,0.1294820717131474,0.06102044594916287,0.4938230249758817
"frozenset({'reading_cat_μέτριο', 'parental level of education_some college', 'test preparation course_none'})",frozenset({'writing_cat_μέτριο'}),0.076,0.491,0.065,0.855263157894737,1.7418801586450854,1.0,0.027684000000000007,3.5167272727272754,0.46093906093906106,0.1294820717131474,0.7156447109916246,0.4938230249758817
frozenset({'reading_cat_μέτριο'}),"frozenset({'test preparation course_completed', 'writing_cat_μέτριο', 'math_cat_μέτριο', 'lunch_standard'})",0.49,0.075,0.064,0.1306122448979592,1.741496598639456,1.0,0.027250000000000003,1.0639671361502347,0.8348651960784315,0.12774451097804393,0.060121345835631564,0.4919727891156463
"frozenset({'test preparation course_completed', 'writing_cat_μέτριο', 'math_cat_μέτριο', 'lunch_standard'})",frozenset({'reading_cat_μέτριο'}),0.075,0.49,0.064,0.8533333333333334,1.741496598639456,1.0,0.027250000000000003,3.477272727272729,0.4603040540540541,0.12774451097804393,0.7124183006535949,0.4919727891156463
"frozenset({'parental level of education_high school', 'reading_cat_μέτριο', 'lunch_standard'})",frozenset({'writing_cat_μέτριο'}),0.069,0.491,0.059,0.8550724637681159,1.7414917795684641,1.0,0.025120999999999997,3.512099999999998,0.45733583353055757,0.11776447105788419,0.7152700663420744,0.48761769827916995
frozenset({'writing_cat_μέτριο'}),"frozenset({'parental level of education_high school', 'reading_cat_μέτριο', 'lunch_standard'})",0.491,0.069,0.059,0.12016293279022403,1.741491779568464,1.0,0.025120999999999997,1.058150462962963,0.8365022809763244,0.11776447105788419,0.054954815027093463,0.48761769827916995
"frozenset({'gender_female', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'reading_cat_υψηλό'})",0.329,0.103,0.059,0.17933130699088143,1.7410806503969072,1.0,0.025112999999999996,1.0930111111111112,0.6343428730202834,0.1581769436997319,0.08509621738113872,0.37607342048573195
"frozenset({'math_cat_μέτριο', 'reading_cat_υψηλό'})","frozenset({'gender_female', 'lunch_standard'})",0.103,0.329,0.059,0.5728155339805825,1.7410806503969072,1.0,0.025112999999999996,1.5707499999999999,0.4745195850575364,0.1581769436997319,0.3633614515358904,0.37607342048573195
frozenset({'reading_cat_μέτριο'}),"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'gender_female', 'lunch_standard'})",0.49,0.115,0.098,0.2,1.7391304347826086,1.0,0.04165,1.10625,0.8333333333333333,0.19329388560157792,0.096045197740113,0.5260869565217391
"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'gender_female', 'lunch_standard'})",frozenset({'reading_cat_μέτριο'}),0.115,0.49,0.098,0.8521739130434782,1.7391304347826086,1.0,0.04165,3.4499999999999993,0.48022598870056493,0.19329388560157792,0.7101449275362318,0.5260869565217391
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'gender_female'})",frozenset({'reading_cat_μέτριο'}),0.155,0.49,0.132,0.8516129032258065,1.7379855167873604,1.0,0.05605,3.436956521739132,0.5025103102026179,0.2573099415204678,0.7090449082858952,0.5605003291639237
frozenset({'reading_cat_μέτριο'}),"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'gender_female'})",0.49,0.155,0.132,0.2693877551020408,1.7379855167873601,1.0,0.05605,1.1565642458100558,0.8325906120023767,0.2573099415204678,0.1353701243811134,0.5605003291639237
frozenset({'writing_cat_μέτριο'}),"frozenset({'math_cat_χαμηλό', 'reading_cat_μέτριο', 'gender_female'})",0.491,0.109,0.093,0.1894093686354379,1.7377006296829165,1.0,0.039481,1.0991984924623115,0.834041024991022,0.1834319526627219,0.09024620497804477,0.5213101889048749
"frozenset({'math_cat_χαμηλό', 'reading_cat_μέτριο', 'gender_female'})",frozenset({'writing_cat_μέτριο'}),0.109,0.491,0.093,0.8532110091743119,1.7377006296829163,1.0,0.039481,3.4675624999999997,0.47646114671204276,0.1834319526627219,0.7116129846253673,0.5213101889048749
frozenset({'reading_cat_μέτριο'}),frozenset({'writing_cat_μέτριο'}),0.49,0.491,0.418,0.8530612244897959,1.7373955692256537,1.0,0.17740999999999998,3.4640277777777784,0.8322075241579885,0.7424511545293073,0.7113187121606993,0.8521925267051831
frozenset({'writing_cat_μέτριο'}),frozenset({'reading_cat_μέτριο'}),0.491,0.49,0.418,0.8513238289205702,1.7373955692256535,1.0,0.17740999999999998,3.430273972602739,0.8338425094706761,0.7424511545293073,0.7084780959226867,0.8521925267051831
frozenset({'writing_cat_μέτριο'}),"frozenset({'race/ethnicity_group C', 'math_cat_μέτριο', 'reading_cat_μέτριο'})",0.491,0.108,0.092,0.18737270875763748,1.7349324884966433,1.0,0.038972,1.0976741854636591,0.8322371230887503,0.18145956607495067,0.08898285735161153,0.5196122803047447
"frozenset({'race/ethnicity_group C', 'math_cat_μέτριο', 'reading_cat_μέτριο'})",frozenset({'writing_cat_μέτριο'}),0.108,0.491,0.092,0.8518518518518519,1.7349324884966433,1.0,0.038972,3.43575,0.4748976408656658,0.18145956607495067,0.7089427344830096,0.5196122803047447
frozenset({'writing_cat_μέτριο'}),"frozenset({'reading_cat_μέτριο', 'math_cat_μέτριο', 'gender_male'})",0.491,0.175,0.149,0.3034623217922607,1.7340704102414897,1.0,0.063075,1.1844298245614036,0.8316741604145517,0.2882011605415861,0.15571190520274025,0.5774454466104161
"frozenset({'reading_cat_μέτριο', 'math_cat_μέτριο', 'gender_male'})",frozenset({'writing_cat_μέτριο'}),0.175,0.491,0.149,0.8514285714285714,1.7340704102414897,1.0,0.063075,3.4259615384615385,0.5131177547284931,0.2882011605415861,0.7081111422958182,0.5774454466104161
"frozenset({'writing_cat_μέτριο', 'lunch_standard'})","frozenset({'reading_cat_μέτριο', 'gender_male'})",0.338,0.229,0.134,0.3964497041420118,1.7312214154673005,1.0,0.056597999999999996,1.2774411764705884,0.6380258826712359,0.30946882217090066,0.21718508967835512,0.49080127128497975
"frozenset({'reading_cat_μέτριο', 'gender_male'})","frozenset({'writing_cat_μέτριο', 'lunch_standard'})",0.229,0.338,0.134,0.5851528384279476,1.7312214154673005,1.0,0.056597999999999996,1.5957684210526315,0.5478250769498809,0.30946882217090066,0.37334265623557045,0.49080127128497975
frozenset({'math_cat_υψηλό'}),"frozenset({'test preparation course_completed', 'gender_male'})",0.176,0.174,0.053,0.30113636363636365,1.7306687565308256,1.0,0.022376000000000004,1.1819186991869919,0.5123649019967027,0.17845117845117844,0.15391811578252257,0.3028670323928945
"frozenset({'test preparation course_completed', 'gender_male'})",frozenset({'math_cat_υψηλό'}),0.174,0.176,0.053,0.3045977011494253,1.7306687565308256,1.0,0.022376000000000004,1.184925619834711,0.5111243090136599,0.17845117845117844,0.15606517129784625,0.3028670323928945
"frozenset({'math_cat_μέτριο', 'gender_male'})","frozenset({'test preparation course_completed', 'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.24,0.152,0.063,0.2625,1.7269736842105263,1.0,0.026520000000000002,1.1498305084745761,0.5538847117794486,0.19148936170212766,0.13030660377358494,0.33848684210526314
"frozenset({'test preparation course_completed', 'writing_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'math_cat_μέτριο', 'gender_male'})",0.152,0.24,0.063,0.4144736842105263,1.7269736842105263,1.0,0.026520000000000002,1.2979775280898878,0.49640610961365683,0.19148936170212766,0.2295706371191136,0.33848684210526314
"frozenset({'writing_cat_μέτριο', 'gender_female'})","frozenset({'reading_cat_μέτριο', 'lunch_standard'})",0.264,0.327,0.149,0.5643939393939393,1.7259753498285606,1.0,0.06267199999999999,1.544973913043478,0.57149110008754,0.33710407239819007,0.352739880228736,0.5100257158743396
"frozenset({'reading_cat_μέτριο', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'gender_female'})",0.327,0.264,0.149,0.45565749235474,1.7259753498285606,1.0,0.06267199999999999,1.3520898876404495,0.6249887810764182,0.33710407239819007,0.26040420156894023,0.5100257158743396
frozenset({'writing_cat_μέτριο'}),"frozenset({'reading_cat_μέτριο', 'gender_male', 'lunch_standard', 'test preparation course_none', 'math_cat_μέτριο'})",0.491,0.072,0.061,0.12423625254582485,1.7255035075809009,1.0,0.025648000000000004,1.059646511627907,0.8260491481207125,0.12151394422310757,0.056289065243345746,0.48572923738402357
"frozenset({'reading_cat_μέτριο', 'gender_male', 'lunch_standard', 'test preparation course_none', 'math_cat_μέτριο'})",frozenset({'writing_cat_μέτριο'}),0.072,0.491,0.061,0.8472222222222223,1.7255035075809009,1.0,0.025648000000000004,3.331636363636366,0.45308083663086496,0.12151394422310757,0.6998471949356038,0.48572923738402357
"frozenset({'math_cat_μέτριο', 'reading_cat_υψηλό'})",frozenset({'gender_female'}),0.103,0.518,0.092,0.8932038834951457,1.7243318214192,1.0,0.038646,4.513272727272729,0.46830013087101935,0.17391304347826086,0.7784312935583935,0.5354050305506617
frozenset({'gender_female'}),"frozenset({'math_cat_μέτριο', 'reading_cat_υψηλό'})",0.518,0.103,0.092,0.17760617760617758,1.7243318214192,1.0,0.038646,1.0907183098591549,0.8715046003968971,0.17391304347826086,0.08317299621647446,0.5354050305506617
"frozenset({'reading_cat_μέτριο', 'gender_female', 'race/ethnicity_group D'})",frozenset({'writing_cat_μέτριο'}),0.065,0.491,0.055,0.8461538461538461,1.723327588908037,1.0,0.023085,3.3085,0.4489061740398639,0.10978043912175649,0.6977482242708176,0.4790850697164343
frozenset({'writing_cat_μέτριο'}),"frozenset({'reading_cat_μέτριο', 'gender_female', 'race/ethnicity_group D'})",0.491,0.065,0.055,0.1120162932790224,1.723327588908037,1.0,0.023085,1.0529472477064221,0.8246115377746027,0.10978043912175649,0.050284805646013266,0.4790850697164343
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'math_cat_μέτριο', 'gender_male', 'race/ethnicity_group D'})",0.418,0.075,0.054,0.1291866028708134,1.722488038277512,1.0,0.022650000000000003,1.0622252747252747,0.7206949217258496,0.12300683371298406,0.05858011121169017,0.4245933014354067
"frozenset({'math_cat_μέτριο', 'gender_male', 'race/ethnicity_group D'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.075,0.418,0.054,0.72,1.722488038277512,1.0,0.022650000000000003,2.0785714285714287,0.4534534534534535,0.12300683371298406,0.5189003436426116,0.4245933014354067
frozenset({'reading_cat_μέτριο'}),"frozenset({'parental level of education_high school', 'writing_cat_μέτριο'})",0.49,0.096,0.081,0.1653061224489796,1.7219387755102042,1.0,0.033960000000000004,1.0830317848410758,0.8220769789397242,0.1603960396039604,0.07666606465595088,0.5045280612244898
"frozenset({'parental level of education_high school', 'writing_cat_μέτριο', 'test preparation course_none'})",frozenset({'reading_cat_μέτριο'}),0.064,0.49,0.054,0.84375,1.721938775510204,1.0,0.02264,3.2640000000000002,0.44792655903767015,0.108,0.6936274509803921,0.4769770408163265
frozenset({'reading_cat_μέτριο'}),"frozenset({'parental level of education_high school', 'writing_cat_μέτριο', 'test preparation course_none'})",0.49,0.064,0.054,0.11020408163265306,1.721938775510204,1.0,0.02264,1.051926605504587,0.8220769789397241,0.108,0.0493633350776208,0.4769770408163265
"frozenset({'parental level of education_high school', 'writing_cat_μέτριο'})",frozenset({'reading_cat_μέτριο'}),0.096,0.49,0.081,0.84375,1.721938775510204,1.0,0.033960000000000004,3.2640000000000002,0.4637823664372337,0.1603960396039604,0.6936274509803921,0.5045280612244898
"frozenset({'math_cat_μέτριο', 'lunch_free/reduced', 'reading_cat_μέτριο'})",frozenset({'writing_cat_μέτριο'}),0.097,0.491,0.082,0.845360824742268,1.7217124740168392,1.0,0.034373,3.291533333333333,0.4642114361342949,0.16205533596837945,0.6961902254268527,0.5061834673609508
frozenset({'writing_cat_μέτριο'}),"frozenset({'math_cat_μέτριο', 'lunch_free/reduced', 'reading_cat_μέτριο'})",0.491,0.097,0.082,0.16700610997963342,1.7217124740168392,1.0,0.034373,1.084041564792176,0.8235420959317648,0.16205533596837945,0.07752614615684764,0.5061834673609508
frozenset({'math_cat_μέτριο'}),"frozenset({'lunch_free/reduced', 'gender_male', 'writing_cat_μέτριο'})",0.485,0.066,0.055,0.1134020618556701,1.7182130584192439,1.0,0.022989999999999997,1.0534651162790698,0.8116504854368931,0.11088709677419356,0.05075167222234486,0.4733676975945017
"frozenset({'lunch_free/reduced', 'gender_male', 'writing_cat_μέτριο'})",frozenset({'math_cat_μέτριο'}),0.066,0.485,0.055,0.8333333333333333,1.7182130584192439,1.0,0.022989999999999997,3.0899999999999985,0.44753747323340465,0.11088709677419356,0.6763754045307442,0.4733676975945017
frozenset({'reading_cat_μέτριο'}),"frozenset({'writing_cat_μέτριο', 'gender_female', 'lunch_standard'})",0.49,0.177,0.149,0.3040816326530612,1.7179753257235097,1.0,0.062270000000000006,1.1826099706744868,0.8194499276220556,0.2876447876447876,0.15441267637066977,0.5729447711287905
"frozenset({'writing_cat_μέτριο', 'gender_female', 'lunch_standard'})",frozenset({'reading_cat_μέτριο'}),0.177,0.49,0.149,0.8418079096045198,1.7179753257235097,1.0,0.062270000000000006,3.223928571428572,0.5078000766552229,0.2876447876447876,0.6898194305970976,0.5729447711287905
frozenset({'reading_cat_μέτριο'}),"frozenset({'writing_cat_μέτριο', 'gender_male'})",0.49,0.227,0.191,0.38979591836734695,1.717162635979502,1.0,0.07977000000000001,1.2667892976588628,0.8189097628580229,0.36311787072243346,0.2106027404493492,0.6156028049986515
"frozenset({'writing_cat_μέτριο', 'gender_male'})",frozenset({'reading_cat_μέτριο'}),0.227,0.49,0.191,0.8414096916299559,1.717162635979502,1.0,0.07977000000000001,3.2158333333333333,0.5402897529852415,0.36311787072243346,0.6890386110391293,0.6156028049986515
"frozenset({'lunch_free/reduced', 'gender_female'})",frozenset({'math_cat_χαμηλό'}),0.189,0.339,0.11,0.582010582010582,1.7168453746624837,1.0,0.045929,1.581379746835443,0.5148413854948997,0.2631578947368421,0.3676408199857518,0.4532471789108957
frozenset({'math_cat_χαμηλό'}),"frozenset({'lunch_free/reduced', 'gender_female'})",0.339,0.189,0.11,0.3244837758112094,1.7168453746624837,1.0,0.045929,1.2005633187772924,0.6316737725209736,0.2631578947368421,0.16705767670925945,0.4532471789108957
"frozenset({'gender_female', 'test preparation course_none'})","frozenset({'math_cat_χαμηλό', 'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.334,0.096,0.055,0.16467065868263472,1.715319361277445,1.0,0.022935999999999998,1.0822078853046595,0.6261534261534262,0.14666666666666664,0.07596311801176407,0.3687936626746507
"frozenset({'math_cat_χαμηλό', 'writing_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'gender_female', 'test preparation course_none'})",0.096,0.334,0.055,0.5729166666666666,1.7153193612774449,1.0,0.022935999999999998,1.559414634146341,0.46130329847144,0.14666666666666664,0.3587337337337337,0.3687936626746507
frozenset({'writing_cat_μέτριο'}),"frozenset({'test preparation course_completed', 'math_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})",0.491,0.076,0.064,0.13034623217922608,1.715082002358238,1.0,0.026684000000000006,1.0624918032786888,0.8191306483300591,0.12723658051689865,0.05881626859223602,0.4862257476685604
"frozenset({'test preparation course_completed', 'math_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})",frozenset({'writing_cat_μέτριο'}),0.076,0.491,0.064,0.8421052631578948,1.7150820023582378,1.0,0.026684000000000006,3.2236666666666682,0.4512310606060607,0.12723658051689865,0.6897942301726814,0.4862257476685604
frozenset({'math_cat_μέτριο'}),"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'race/ethnicity_group D', 'gender_male'})",0.485,0.065,0.054,0.11134020618556702,1.7129262490087234,1.0,0.022475000000000002,1.0521461716937357,0.8081625314635024,0.10887096774193547,0.04956171784552622,0.4710547184773989
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'race/ethnicity_group D', 'gender_male'})",frozenset({'math_cat_μέτριο'}),0.065,0.485,0.054,0.8307692307692307,1.7129262490087231,1.0,0.022475000000000002,3.043181818181817,0.445137651020004,0.10887096774193547,0.6713965646004479,0.4710547184773989
"frozenset({'writing_cat_μέτριο', 'gender_female'})","frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})",0.264,0.208,0.094,0.356060606060606,1.7118298368298368,1.0,0.039088,1.2299294117647057,0.5649861239592969,0.24867724867724866,0.18694520967248235,0.4039918414918415
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})","frozenset({'writing_cat_μέτριο', 'gender_female'})",0.208,0.264,0.094,0.4519230769230769,1.7118298368298368,1.0,0.039088,1.342877192982456,0.5250376101439931,0.24867724867724866,0.25533026755852845,0.4039918414918415
"frozenset({""parental level of education_associate's degree"", 'reading_cat_μέτριο', 'test preparation course_none'})",frozenset({'writing_cat_μέτριο'}),0.075,0.491,0.063,0.8400000000000001,1.7107942973523425,1.0,0.026175000000000004,3.1812500000000017,0.44916344916344925,0.12524850894632208,0.6856581532416505,0.4841547861507129
frozenset({'writing_cat_μέτριο'}),"frozenset({""parental level of education_associate's degree"", 'reading_cat_μέτριο', 'test preparation course_none'})",0.491,0.075,0.063,0.12830957230142567,1.7107942973523425,1.0,0.026175000000000004,1.061156542056075,0.8162597062400601,0.12524850894632208,0.057631970055595325,0.4841547861507129
"frozenset({'writing_cat_χαμηλό', 'test preparation course_none'})","frozenset({'lunch_free/reduced', 'gender_male'})",0.243,0.166,0.069,0.28395061728395066,1.7105458872527146,1.0,0.028662000000000007,1.1647241379310345,0.5487335592441561,0.20294117647058824,0.14142759866181134,0.3498066339431801
"frozenset({'lunch_free/reduced', 'gender_male'})","frozenset({'writing_cat_χαμηλό', 'test preparation course_none'})",0.166,0.243,0.069,0.41566265060240964,1.7105458872527146,1.0,0.028662000000000007,1.2954845360824743,0.4980711083307268,0.20294117647058824,0.22808804570992028,0.3498066339431801
"frozenset({'race/ethnicity_group E', 'writing_cat_μέτριο'})",frozenset({'reading_cat_μέτριο'}),0.074,0.49,0.062,0.8378378378378378,1.7098731384445671,1.0,0.02574,3.145,0.44833832648226846,0.12350597609561753,0.6820349761526232,0.4821842250413679
frozenset({'reading_cat_μέτριο'}),"frozenset({'race/ethnicity_group E', 'writing_cat_μέτριο'})",0.49,0.074,0.062,0.12653061224489795,1.709873138444567,1.0,0.02574,1.060140186915888,0.8140417457305502,0.12350597609561753,0.056728522942654375,0.4821842250413679
frozenset({'writing_cat_μέτριο'}),"frozenset({'reading_cat_μέτριο', 'parental level of education_some high school'})",0.491,0.081,0.068,0.1384928716904277,1.7097885393879964,1.0,0.028229000000000004,1.0667352245862884,0.8155841904541777,0.13492063492063494,0.0625602521114556,0.48899952226496696
"frozenset({'reading_cat_μέτριο', 'parental level of education_some high school'})",frozenset({'writing_cat_μέτριο'}),0.081,0.491,0.068,0.8395061728395062,1.7097885393879964,1.0,0.028229000000000004,3.1714615384615397,0.45172182039301034,0.13492063492063494,0.6846879623565938,0.48899952226496696
"frozenset({'writing_cat_χαμηλό', 'gender_male', 'test preparation course_none', 'lunch_standard'})",frozenset({'math_cat_χαμηλό'}),0.088,0.339,0.051,0.5795454545454546,1.7095736122284795,1.0,0.021167999999999996,1.5721081081081083,0.45510835913312686,0.13563829787234039,0.36391142896437906,0.3649939662107804
frozenset({'math_cat_χαμηλό'}),"frozenset({'writing_cat_χαμηλό', 'gender_male', 'test preparation course_none', 'lunch_standard'})",0.339,0.088,0.051,0.15044247787610618,1.7095736122284795,1.0,0.021167999999999996,1.0735000000000001,0.6279256029189284,0.13563829787234039,0.06846762925011643,0.3649939662107804
frozenset({'math_cat_μέτριο'}),"frozenset({'test preparation course_completed', 'writing_cat_μέτριο', 'reading_cat_μέτριο', 'gender_male'})",0.485,0.076,0.063,0.12989690721649486,1.709169831795985,1.0,0.026140000000000004,1.0619431279620852,0.8056711357682232,0.12650602409638556,0.05832998616503772,0.47942213781877374
"frozenset({'test preparation course_completed', 'writing_cat_μέτριο', 'reading_cat_μέτριο', 'gender_male'})",frozenset({'math_cat_μέτριο'}),0.076,0.485,0.063,0.8289473684210527,1.709169831795985,1.0,0.026140000000000004,3.010769230769231,0.44904830619116337,0.12650602409638556,0.6678589678078692,0.47942213781877374
"frozenset({'writing_cat_μέτριο', 'lunch_standard'})",frozenset({'reading_cat_μέτριο'}),0.338,0.49,0.283,0.8372781065088756,1.7087308296099502,1.0,0.11737999999999996,3.134181818181815,0.6265412658930534,0.5192660550458714,0.6809374637428933,0.7074145634585194
frozenset({'reading_cat_μέτριο'}),"frozenset({'writing_cat_μέτριο', 'lunch_standard'})",0.49,0.338,0.283,0.5775510204081632,1.7087308296099502,1.0,0.11737999999999996,1.567053140096618,0.8132751333749045,0.5192660550458714,0.3618595474443553,0.7074145634585194
frozenset({'reading_cat_χαμηλό'}),"frozenset({'lunch_free/reduced', 'gender_male'})",0.275,0.166,0.078,0.28363636363636363,1.7086527929901423,1.0,0.03235,1.1642131979695431,0.5720601237842617,0.21487603305785122,0.1410507957270547,0.37675794085432635
"frozenset({'lunch_free/reduced', 'gender_male'})",frozenset({'reading_cat_χαμηλό'}),0.166,0.275,0.078,0.4698795180722891,1.708652792990142,1.0,0.03235,1.3676136363636362,0.49729447211461597,0.21487603305785122,0.2687993352721229,0.37675794085432635
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'gender_male'})","frozenset({'writing_cat_μέτριο', 'lunch_standard'})",0.175,0.338,0.101,0.5771428571428572,1.7075232459847844,1.0,0.041850000000000005,1.5655405405405405,0.5022502250225024,0.24514563106796117,0.3612429866206302,0.4379797125950972
"frozenset({'writing_cat_μέτριο', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'gender_male'})",0.338,0.175,0.101,0.29881656804733725,1.7075232459847844,1.0,0.041850000000000005,1.1765822784810125,0.6259160659268346,0.24514563106796117,0.150080688542227,0.4379797125950972
frozenset({'writing_cat_μέτριο'}),"frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο', 'gender_female'})",0.491,0.099,0.083,0.16904276985743383,1.707502725832665,1.0,0.034391000000000005,1.0842916666666667,0.8140459677610246,0.16370808678500987,0.07773892326019292,0.5037133041206361
"frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο', 'gender_female'})",frozenset({'writing_cat_μέτριο'}),0.099,0.491,0.083,0.8383838383838383,1.7075027258326647,1.0,0.034391000000000005,3.1494374999999994,0.4598772448283701,0.16370808678500987,0.682482983072374,0.5037133041206361
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard', 'gender_male'})","frozenset({'writing_cat_μέτριο', 'test preparation course_none'})",0.116,0.308,0.061,0.5258620689655172,1.7073443797581729,1.0,0.025271999999999996,1.459490909090909,0.4686595949855351,0.16804407713498623,0.3148295794299382,0.36195700850873264
"frozenset({'writing_cat_μέτριο', 'test preparation course_none'})","frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard', 'gender_male'})",0.308,0.116,0.061,0.19805194805194806,1.7073443797581729,1.0,0.025271999999999996,1.1023157894736841,0.5986923149815219,0.16804407713498623,0.09281894576012223,0.36195700850873264
"frozenset({'math_cat_χαμηλό', 'gender_female'})","frozenset({'writing_cat_χαμηλό', 'test preparation course_none'})",0.205,0.243,0.085,0.41463414634146345,1.7063133594298907,1.0,0.03518500000000001,1.2932083333333335,0.5206807251202368,0.23415977961432513,0.22672938750523575,0.3822141925122955
"frozenset({'writing_cat_χαμηλό', 'test preparation course_none'})","frozenset({'math_cat_χαμηλό', 'gender_female'})",0.243,0.205,0.085,0.3497942386831276,1.7063133594298907,1.0,0.03518500000000001,1.2226898734177216,0.5468179345714509,0.23415977961432513,0.18213111784041208,0.3822141925122955
"frozenset({'lunch_free/reduced', 'test preparation course_none'})",frozenset({'reading_cat_χαμηλό'}),0.224,0.275,0.105,0.46875,1.7045454545454544,1.0,0.04339999999999999,1.3647058823529412,0.5326460481099655,0.266497461928934,0.2672413793103448,0.4252840909090909
frozenset({'reading_cat_χαμηλό'}),"frozenset({'lunch_free/reduced', 'test preparation course_none'})",0.275,0.224,0.105,0.3818181818181818,1.7045454545454544,1.0,0.04339999999999999,1.255294117647059,0.5701149425287355,0.266497461928934,0.20337394564198683,0.4252840909090909
"frozenset({'writing_cat_μέτριο', 'gender_male'})","frozenset({'test preparation course_completed', 'math_cat_μέτριο'})",0.227,0.181,0.07,0.30837004405286345,1.7037019008445495,1.0,0.028913000000000008,1.1841592356687896,0.5343374607281465,0.20710059171597633,0.1555189793075256,0.3475551877722881
"frozenset({'test preparation course_completed', 'math_cat_μέτριο'})","frozenset({'writing_cat_μέτριο', 'gender_male'})",0.181,0.227,0.07,0.3867403314917128,1.7037019008445495,1.0,0.028913000000000008,1.2604774774774778,0.5043258328972616,0.20710059171597633,0.20664984669044342,0.3475551877722881
frozenset({'writing_cat_μέτριο'}),"frozenset({'reading_cat_μέτριο', 'test preparation course_none'})",0.491,0.318,0.266,0.5417515274949084,1.7036211556443661,1.0,0.10986200000000002,1.4882755555555556,0.8114244353516404,0.4898710865561695,0.32808141861423523,0.689114757458146
"frozenset({'reading_cat_μέτριο', 'test preparation course_none'})",frozenset({'writing_cat_μέτριο'}),0.318,0.491,0.266,0.8364779874213837,1.703621155644366,1.0,0.10986200000000002,3.11273076923077,0.605593896765374,0.4898710865561695,0.6787386786274728,0.689114757458146
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'writing_cat_μέτριο', 'gender_female', 'lunch_standard'})",0.325,0.177,0.098,0.30153846153846153,1.703607127335941,1.0,0.040475000000000004,1.1783039647577092,0.6118669690098262,0.24257425742574257,0.1513225535096738,0.42760538896132116
"frozenset({'writing_cat_μέτριο', 'gender_female', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο'})",0.177,0.325,0.098,0.5536723163841808,1.7036071273359408,1.0,0.040475000000000004,1.5123417721518988,0.5018349988841223,0.24257425742574257,0.3387738020506382,0.42760538896132116
"frozenset({'writing_cat_χαμηλό', 'gender_female', 'test preparation course_none'})",frozenset({'lunch_free/reduced'}),0.086,0.355,0.052,0.6046511627906977,1.7032427120864726,1.0,0.021470000000000003,1.6314705882352944,0.45173371486281777,0.13367609254498716,0.38705606634216705,0.3755650180150672
frozenset({'lunch_free/reduced'}),"frozenset({'writing_cat_χαμηλό', 'gender_female', 'test preparation course_none'})",0.355,0.086,0.052,0.14647887323943662,1.7032427120864724,1.0,0.021470000000000003,1.070858085808581,0.6401311866428147,0.13367609254498716,0.06616944555737049,0.3755650180150672
"frozenset({'gender_female', 'test preparation course_none'})","frozenset({'math_cat_χαμηλό', 'writing_cat_μέτριο'})",0.334,0.109,0.062,0.18562874251497005,1.703015986375872,1.0,0.025594,1.094095588235294,0.6198295069262811,0.16272965879265092,0.08600307801904607,0.3772180409822557
"frozenset({'math_cat_χαμηλό', 'writing_cat_μέτριο'})","frozenset({'gender_female', 'test preparation course_none'})",0.109,0.334,0.062,0.5688073394495413,1.703015986375872,1.0,0.025594,1.5445531914893615,0.4633069041671192,0.16272965879265092,0.352563572747059,0.3772180409822557
"frozenset({'test preparation course_completed', 'writing_cat_μέτριο', 'math_cat_μέτριο'})","frozenset({'reading_cat_μέτριο', 'lunch_standard'})",0.115,0.327,0.064,0.5565217391304348,1.7019013429065284,1.0,0.026394999999999995,1.5175490196078432,0.4660134180790959,0.1693121693121693,0.34104270301699074,0.37612019678234276
"frozenset({'reading_cat_μέτριο', 'lunch_standard'})","frozenset({'test preparation course_completed', 'writing_cat_μέτριο', 'math_cat_μέτριο'})",0.327,0.115,0.064,0.19571865443425077,1.7019013429065284,1.0,0.026394999999999995,1.100361216730038,0.612811106983655,0.1693121693121693,0.09120751913474662,0.37612019678234276
"frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο'})","frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})",0.169,0.32,0.092,0.5443786982248521,1.7011834319526626,1.0,0.037919999999999995,1.4924675324675323,0.4959974886203108,0.2317380352644836,0.3299686738600766,0.415939349112426
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})","frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο'})",0.32,0.169,0.092,0.2875,1.7011834319526624,1.0,0.037919999999999995,1.1663157894736842,0.6061381074168798,0.2317380352644836,0.14259927797833932,0.415939349112426
"frozenset({'gender_male', 'reading_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",frozenset({'writing_cat_μέτριο'}),0.103,0.491,0.086,0.8349514563106796,1.7005121309789808,1.0,0.035427,3.0839411764705877,0.45924399159990664,0.16929133858267714,0.6757395998245178,0.5050521029007573
frozenset({'writing_cat_μέτριο'}),"frozenset({'gender_male', 'reading_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",0.491,0.103,0.086,0.175152749490835,1.7005121309789808,1.0,0.035427,1.087474074074074,0.8093160323479691,0.16929133858267714,0.08043784781586957,0.5050521029007573
"frozenset({'writing_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})","frozenset({'reading_cat_μέτριο', 'gender_male'})",0.221,0.229,0.086,0.3891402714932126,1.6993024956035485,1.0,0.03539099999999999,1.2621555555555555,0.5282711884646385,0.23626373626373626,0.2077046322869165,0.38234306151079844
"frozenset({'reading_cat_μέτριο', 'gender_male'})","frozenset({'writing_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",0.229,0.221,0.086,0.3755458515283842,1.6993024956035485,1.0,0.03539099999999999,1.2474895104895105,0.5337526015745181,0.23626373626373626,0.1983900533098642,0.38234306151079844
"frozenset({'writing_cat_μέτριο', 'gender_male'})","frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",0.227,0.223,0.086,0.37885462555066074,1.6988996661464606,1.0,0.035378999999999994,1.2509148936170211,0.5321911008153073,0.23626373626373626,0.20058510366880403,0.38225242488295363
"frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'gender_male'})",0.223,0.227,0.086,0.3856502242152466,1.6988996661464606,1.0,0.035378999999999994,1.2582408759124089,0.5294513782885876,0.23626373626373626,0.2052396173547821,0.38225242488295363
"frozenset({'writing_cat_χαμηλό', 'test preparation course_none', 'reading_cat_χαμηλό'})","frozenset({'math_cat_χαμηλό', 'gender_female'})",0.201,0.205,0.07,0.3482587064676617,1.6988229583788377,1.0,0.028795000000000008,1.2198091603053436,0.5148399785446095,0.20833333333333334,0.180199630776933,0.34486106055090404
"frozenset({'math_cat_χαμηλό', 'gender_female'})","frozenset({'writing_cat_χαμηλό', 'test preparation course_none', 'reading_cat_χαμηλό'})",0.205,0.201,0.07,0.34146341463414637,1.6988229583788375,1.0,0.028795000000000008,1.2132962962962963,0.5174303683737647,0.20833333333333334,0.17579901706401296,0.34486106055090404
frozenset({'writing_cat_μέτριο'}),"frozenset({'reading_cat_μέτριο', 'gender_male'})",0.491,0.229,0.191,0.3890020366598778,1.6986988500431346,1.0,0.078561,1.2618700000000003,0.8080827821722092,0.36105860113421556,0.2075253393772734,0.6115315860155284
"frozenset({'reading_cat_μέτριο', 'gender_male'})",frozenset({'writing_cat_μέτριο'}),0.229,0.491,0.191,0.834061135371179,1.6986988500431344,1.0,0.078561,3.0673947368421044,0.5334813698127814,0.36105860113421556,0.6739904427724538,0.6115315860155284
"frozenset({'lunch_free/reduced', 'test preparation course_none'})","frozenset({'math_cat_χαμηλό', 'gender_female'})",0.224,0.205,0.078,0.3482142857142857,1.6986062717770034,1.0,0.032080000000000004,1.2197260273972603,0.5300026434047054,0.22222222222222224,0.1801437556154537,0.36435104529616724
"frozenset({'math_cat_χαμηλό', 'gender_female'})","frozenset({'lunch_free/reduced', 'test preparation course_none'})",0.205,0.224,0.078,0.3804878048780488,1.6986062717770034,1.0,0.032080000000000004,1.2525984251968505,0.5173359135623287,0.22222222222222224,0.20165954236861955,0.36435104529616724
frozenset({'reading_cat_μέτριο'}),"frozenset({'writing_cat_μέτριο', 'gender_male', 'lunch_standard'})",0.49,0.161,0.134,0.27346938775510204,1.6985676258080873,1.0,0.055110000000000006,1.1548033707865166,0.8064091308165057,0.25918762088974856,0.13405171365328014,0.5528837622005324
"frozenset({'writing_cat_μέτριο', 'gender_male', 'lunch_standard'})",frozenset({'reading_cat_μέτριο'}),0.161,0.49,0.134,0.8322981366459627,1.6985676258080873,1.0,0.055110000000000006,3.0411111111111113,0.4901891021649797,0.25918762088974856,0.6711728169528681,0.5528837622005324
"frozenset({'writing_cat_μέτριο', 'lunch_standard'})","frozenset({'race/ethnicity_group C', 'math_cat_μέτριο', 'reading_cat_μέτριο'})",0.338,0.108,0.062,0.1834319526627219,1.6984440061363137,1.0,0.025495999999999998,1.092376811594203,0.6211870188090829,0.16145833333333334,0.08456496935282723,0.37875301336839795
"frozenset({'race/ethnicity_group C', 'math_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'writing_cat_μέτριο', 'lunch_standard'})",0.108,0.338,0.062,0.5740740740740741,1.6984440061363137,1.0,0.025495999999999998,1.5542608695652171,0.4610154780847678,0.16145833333333334,0.3566073626496587,0.37875301336839795
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'test preparation course_completed', 'writing_cat_μέτριο'})",0.325,0.183,0.101,0.3107692307692308,1.6981925178646493,1.0,0.041525000000000006,1.1853794642857143,0.609094242757609,0.24815724815724816,0.15638828735523966,0.4313408995376209
"frozenset({'test preparation course_completed', 'writing_cat_μέτριο'})","frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο'})",0.183,0.325,0.101,0.551912568306011,1.698192517864649,1.0,0.041525000000000006,1.5064024390243906,0.5032296375292366,0.24815724815724816,0.336166767860757,0.4313408995376209
"frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'gender_female'})","frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})",0.173,0.32,0.094,0.5433526011560694,1.697976878612717,1.0,0.03864,1.489113924050633,0.49705420772337855,0.23558897243107768,0.3284597075824551,0.4185513005780347
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})","frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'gender_female'})",0.32,0.173,0.094,0.29375,1.697976878612717,1.0,0.03864,1.1709734513274335,0.6045056320400501,0.23558897243107768,0.14600967351874247,0.4185513005780347
frozenset({'writing_cat_μέτριο'}),"frozenset({""parental level of education_bachelor's degree"", 'reading_cat_μέτριο'})",0.491,0.06,0.05,0.10183299389002037,1.697216564833673,1.0,0.020540000000000003,1.0465759637188208,0.807072691552063,0.09980039920159683,0.04450318498938338,0.46758316361167684
"frozenset({""parental level of education_bachelor's degree"", 'reading_cat_μέτριο'})",frozenset({'writing_cat_μέτριο'}),0.06,0.491,0.05,0.8333333333333334,1.6972165648336728,1.0,0.020540000000000003,3.0540000000000007,0.43702127659574475,0.09980039920159683,0.6725605762933858,0.46758316361167684
frozenset({'writing_cat_μέτριο'}),"frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο', 'lunch_standard'})",0.491,0.108,0.09,0.18329938900203666,1.6972165648336728,1.0,0.036972,1.0921995012468828,0.8070726915520628,0.1768172888015717,0.08441635538344916,0.508316361167685
"frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο', 'lunch_standard'})",frozenset({'writing_cat_μέτριο'}),0.108,0.491,0.09,0.8333333333333333,1.6972165648336726,1.0,0.036972,3.0539999999999985,0.4605381165919282,0.1768172888015717,0.6725605762933856,0.508316361167685
"frozenset({'test preparation course_completed', 'math_cat_χαμηλό'})",frozenset({'lunch_free/reduced'}),0.093,0.355,0.056,0.6021505376344086,1.6961986975617145,1.0,0.022985000000000005,1.6212162162162163,0.4525318947865806,0.14285714285714288,0.3831791281153622,0.37994850825382404
frozenset({'lunch_free/reduced'}),"frozenset({'test preparation course_completed', 'math_cat_χαμηλό'})",0.355,0.093,0.056,0.15774647887323945,1.6961986975617145,1.0,0.022985000000000005,1.0768729096989966,0.6363510520487267,0.14285714285714288,0.07138531298041835,0.37994850825382404
"frozenset({'gender_female', 'lunch_standard'})",frozenset({'writing_cat_υψηλό'}),0.329,0.208,0.116,0.3525835866261398,1.6951133972410568,1.0,0.047568,1.2233239436619718,0.6111310961508813,0.2755344418052256,0.1825550336188634,0.45513794715922373
frozenset({'writing_cat_υψηλό'}),"frozenset({'gender_female', 'lunch_standard'})",0.208,0.329,0.116,0.5576923076923077,1.6951133972410568,1.0,0.047568,1.5170434782608697,0.5177638453500522,0.2755344418052256,0.34082311131491455,0.45513794715922373
"frozenset({'test preparation course_completed', 'writing_cat_μέτριο'})",frozenset({'reading_cat_μέτριο'}),0.183,0.49,0.152,0.8306010928961749,1.6951042712166835,1.0,0.062329999999999997,3.010645161290323,0.5019165109837016,0.29174664107485604,0.6678452801885782,0.570402587264414
frozenset({'reading_cat_μέτριο'}),"frozenset({'test preparation course_completed', 'writing_cat_μέτριο'})",0.49,0.183,0.152,0.31020408163265306,1.6951042712166835,1.0,0.062329999999999997,1.1844082840236685,0.8040505675954591,0.29174664107485604,0.1556965503459646,0.570402587264414
frozenset({'writing_cat_χαμηλό'}),"frozenset({'gender_male', 'test preparation course_none'})",0.301,0.308,0.157,0.521594684385382,1.6934892350174742,1.0,0.064292,1.446472222222222,0.5858414659704947,0.34734513274336287,0.3086628387071995,0.5156674720628209
"frozenset({'gender_male', 'test preparation course_none'})",frozenset({'writing_cat_χαμηλό'}),0.308,0.301,0.157,0.5097402597402597,1.6934892350174742,1.0,0.064292,1.4257748344370862,0.5917676079673061,0.34734513274336287,0.29862698103041446,0.5156674720628209
"frozenset({'writing_cat_μέτριο', 'lunch_standard'})","frozenset({""parental level of education_associate's degree"", 'reading_cat_μέτριο'})",0.338,0.112,0.064,0.1893491124260355,1.6906170752324599,1.0,0.026144,1.0954160583941606,0.617069486404834,0.16580310880829016,0.08710485633562556,0.38038884192730343
"frozenset({""parental level of education_associate's degree"", 'reading_cat_μέτριο'})","frozenset({'writing_cat_μέτριο', 'lunch_standard'})",0.112,0.338,0.064,0.5714285714285714,1.6906170752324596,1.0,0.026144,1.5446666666666664,0.4600225225225225,0.16580310880829016,0.3526111350884764,0.38038884192730343
frozenset({'math_cat_χαμηλό'}),"frozenset({'lunch_free/reduced', 'reading_cat_μέτριο', 'gender_female'})",0.339,0.089,0.051,0.15044247787610618,1.6903649199562494,1.0,0.020828999999999997,1.0723229166666668,0.6178695381329535,0.1352785145888594,0.06744509097267418,0.36173809287063735
"frozenset({'lunch_free/reduced', 'reading_cat_μέτριο', 'gender_female'})",frozenset({'math_cat_χαμηλό'}),0.089,0.339,0.051,0.5730337078651685,1.6903649199562492,1.0,0.020828999999999997,1.5481315789473684,0.44831148705365786,0.1352785145888594,0.3540600724132654,0.36173809287063735
frozenset({'math_cat_υψηλό'}),"frozenset({'gender_male', 'lunch_standard'})",0.176,0.316,0.094,0.5340909090909092,1.690161104718067,1.0,0.038384,1.46809756097561,0.49555876884941125,0.23618090452261306,0.31884635832004266,0.4157796317606445
"frozenset({'gender_male', 'lunch_standard'})",frozenset({'math_cat_υψηλό'}),0.316,0.176,0.094,0.2974683544303797,1.6901611047180667,1.0,0.038384,1.172900900900901,0.59698892621625,0.23618090452261306,0.14741305149317926,0.4157796317606445
frozenset({'lunch_free/reduced'}),"frozenset({'math_cat_χαμηλό', 'gender_female', 'test preparation course_none', 'writing_cat_χαμηλό'})",0.355,0.085,0.051,0.14366197183098592,1.6901408450704225,1.0,0.020824999999999996,1.0685032894736843,0.6330749354005166,0.13110539845758354,0.06411144462402832,0.3718309859154929
"frozenset({'math_cat_χαμηλό', 'gender_female', 'test preparation course_none', 'writing_cat_χαμηλό'})",frozenset({'lunch_free/reduced'}),0.085,0.355,0.051,0.5999999999999999,1.6901408450704223,1.0,0.020824999999999996,1.6124999999999996,0.4462659380692167,0.13110539845758354,0.37984496124030986,0.3718309859154929
"frozenset({'gender_male', 'test preparation course_none'})","frozenset({'math_cat_χαμηλό', 'writing_cat_χαμηλό', 'lunch_standard'})",0.308,0.098,0.051,0.16558441558441558,1.6896368937185262,1.0,0.020815999999999994,1.0809961089494164,0.5898220559900259,0.1436619718309859,0.07492729000489531,0.3429962894248608
"frozenset({'math_cat_χαμηλό', 'writing_cat_χαμηλό', 'lunch_standard'})","frozenset({'gender_male', 'test preparation course_none'})",0.098,0.308,0.051,0.520408163265306,1.6896368937185262,1.0,0.020815999999999994,1.4428936170212763,0.452502065127603,0.1436619718309859,0.30694821281113593,0.3429962894248608
"frozenset({'writing_cat_μέτριο', 'lunch_standard'})","frozenset({'reading_cat_μέτριο', 'gender_female'})",0.338,0.261,0.149,0.4408284023668639,1.6889977102178693,1.0,0.06078199999999999,1.3215978835978834,0.6162128185891846,0.33111111111111113,0.243340192647989,0.5058548142102518
"frozenset({'reading_cat_μέτριο', 'gender_female'})","frozenset({'writing_cat_μέτριο', 'lunch_standard'})",0.261,0.338,0.149,0.5708812260536398,1.688997710217869,1.0,0.06078199999999999,1.5426964285714282,0.5520066115102032,0.33111111111111113,0.35178432938616283,0.5058548142102518
"frozenset({'reading_cat_μέτριο', 'lunch_standard'})","frozenset({'race/ethnicity_group C', 'math_cat_μέτριο', 'test preparation course_none'})",0.327,0.096,0.053,0.1620795107033639,1.688328236493374,1.0,0.021607999999999995,1.0788613138686132,0.6057921444391487,0.14324324324324322,0.07309680387540254,0.3570814220183486
"frozenset({'race/ethnicity_group C', 'math_cat_μέτριο', 'test preparation course_none'})","frozenset({'reading_cat_μέτριο', 'lunch_standard'})",0.096,0.327,0.053,0.5520833333333333,1.6883282364933738,1.0,0.021607999999999995,1.5025116279069766,0.45099348806144585,0.14324324324324322,0.3344477464091133,0.3570814220183486
"frozenset({'reading_cat_μέτριο', 'parental level of education_some college'})","frozenset({'writing_cat_μέτριο', 'lunch_standard'})",0.114,0.338,0.065,0.5701754385964912,1.6869095816464237,1.0,0.026468,1.5401632653061224,0.45959367945823926,0.16795865633074936,0.35071818519107434,0.38124156545209176
"frozenset({'writing_cat_μέτριο', 'lunch_standard'})","frozenset({'reading_cat_μέτριο', 'parental level of education_some college'})",0.338,0.114,0.065,0.1923076923076923,1.6869095816464235,1.0,0.026468,1.096952380952381,0.6151057401812688,0.16795865633074936,0.08838339989581522,0.38124156545209176
frozenset({'writing_cat_μέτριο'}),"frozenset({'lunch_free/reduced', 'reading_cat_μέτριο'})",0.491,0.163,0.135,0.274949083503055,1.6868041932702762,1.0,0.054967,1.1544016853932584,0.7999272356836207,0.26011560693641617,0.1337503984504839,0.5515849711993803
"frozenset({'lunch_free/reduced', 'reading_cat_μέτριο'})",frozenset({'writing_cat_μέτριο'}),0.163,0.491,0.135,0.8282208588957055,1.686804193270276,1.0,0.054967,2.963107142857143,0.48645515288287094,0.26011560693641617,0.6625164221919558,0.5515849711993803
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none', 'gender_male'})","frozenset({'writing_cat_μέτριο', 'lunch_standard'})",0.107,0.338,0.061,0.5700934579439252,1.6866670353370568,1.0,0.024833999999999995,1.539869565217391,0.4558955812971563,0.15885416666666666,0.35059434734731904,0.3752834153624951
"frozenset({'writing_cat_μέτριο', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none', 'gender_male'})",0.338,0.107,0.061,0.18047337278106507,1.6866670353370568,1.0,0.024833999999999995,1.089653429602888,0.6149769699371006,0.15885416666666666,0.08227701319268205,0.3752834153624951
frozenset({'reading_cat_χαμηλό'}),"frozenset({'math_cat_χαμηλό', 'gender_female'})",0.275,0.205,0.095,0.3454545454545454,1.6851441241685143,1.0,0.038625,1.2145833333333333,0.5607985480943739,0.24675324675324675,0.17667238421955397,0.40443458980044344
"frozenset({'math_cat_χαμηλό', 'gender_female'})",frozenset({'reading_cat_χαμηλό'}),0.205,0.275,0.095,0.4634146341463415,1.6851441241685143,1.0,0.038625,1.3511363636363636,0.5114200595829195,0.24675324675324675,0.25988225399495374,0.40443458980044344
frozenset({'math_cat_μέτριο'}),"frozenset({'reading_cat_μέτριο', 'race/ethnicity_group D', 'gender_male'})",0.485,0.071,0.058,0.11958762886597939,1.6843328009292873,1.0,0.02356500000000001,1.0551873536299767,0.7889186474723806,0.11646586345381528,0.0523009998557367,0.4682445186583419
"frozenset({'reading_cat_μέτριο', 'race/ethnicity_group D', 'gender_male'})",frozenset({'math_cat_μέτριο'}),0.071,0.485,0.058,0.8169014084507044,1.6843328009292873,1.0,0.02356500000000001,2.8126923076923096,0.43734456775917757,0.11646586345381528,0.6444687542732124,0.4682445186583419
"frozenset({'reading_cat_μέτριο', 'gender_male'})","frozenset({'lunch_free/reduced', 'math_cat_μέτριο'})",0.229,0.153,0.059,0.2576419213973799,1.6839341267802608,1.0,0.023962999999999998,1.1409588235294117,0.526786695684671,0.18266253869969037,0.12354418110670591,0.32163141821502983
"frozenset({'lunch_free/reduced', 'math_cat_μέτριο'})","frozenset({'reading_cat_μέτριο', 'gender_male'})",0.153,0.229,0.059,0.3856209150326797,1.6839341267802606,1.0,0.023962999999999998,1.2549255319148935,0.47951894022772296,0.18266253869969037,0.20313996761696457,0.32163141821502983
frozenset({'writing_cat_μέτριο'}),"frozenset({'race/ethnicity_group C', 'math_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})",0.491,0.075,0.062,0.12627291242362526,1.6836388323150036,1.0,0.025175000000000003,1.0586829836829836,0.7977374992078079,0.12301587301587301,0.055430175593108394,0.47646978954514596
"frozenset({'race/ethnicity_group C', 'math_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})",frozenset({'writing_cat_μέτριο'}),0.075,0.491,0.062,0.8266666666666667,1.6836388323150033,1.0,0.025175000000000003,2.9365384615384613,0.43897122929380994,0.12301587301587301,0.6594629993451211,0.47646978954514596
frozenset({'writing_cat_μέτριο'}),"frozenset({'parental level of education_high school', 'reading_cat_μέτριο'})",0.491,0.098,0.081,0.164969450101833,1.6833617357329898,1.0,0.032882,1.0802,0.7975454170608067,0.1594488188976378,0.07424551009072394,0.49575003117336547
"frozenset({'parental level of education_high school', 'reading_cat_μέτριο'})",frozenset({'writing_cat_μέτριο'}),0.098,0.491,0.081,0.826530612244898,1.6833617357329898,1.0,0.032882,2.9342352941176477,0.4500561167227834,0.1594488188976378,0.6591957018563811,0.49575003117336547
"frozenset({'test preparation course_completed', 'writing_cat_μέτριο', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο'})",0.117,0.325,0.064,0.547008547008547,1.6831032215647599,1.0,0.025974999999999998,1.4900943396226414,0.4596368912797282,0.1693121693121693,0.32890155112377323,0.3719658119658119
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'test preparation course_completed', 'writing_cat_μέτριο', 'lunch_standard'})",0.325,0.117,0.064,0.19692307692307692,1.6831032215647599,1.0,0.025974999999999998,1.099521072796935,0.601273148148148,0.1693121693121693,0.09051311089816186,0.3719658119658119
"frozenset({'reading_cat_μέτριο', 'test preparation course_none'})","frozenset({'writing_cat_μέτριο', 'gender_male', 'lunch_standard'})",0.318,0.161,0.086,0.27044025157232704,1.6797531153560685,1.0,0.03480199999999999,1.150008620689655,0.5933642501534474,0.2188295165394402,0.13044130103972235,0.40230087112777835
"frozenset({'writing_cat_μέτριο', 'gender_male', 'lunch_standard'})","frozenset({'reading_cat_μέτριο', 'test preparation course_none'})",0.161,0.318,0.086,0.5341614906832297,1.6797531153560683,1.0,0.03480199999999999,1.4640266666666661,0.4823294619840895,0.2188295165394402,0.31695233238010223,0.40230087112777835
"frozenset({'writing_cat_χαμηλό', 'test preparation course_none', 'reading_cat_χαμηλό'})","frozenset({'lunch_free/reduced', 'gender_male'})",0.201,0.166,0.056,0.27860696517412936,1.6783552118923455,1.0,0.022633999999999994,1.156096551724138,0.5058555337028428,0.18006430868167203,0.1350203419354069,0.3079781813822454
"frozenset({'lunch_free/reduced', 'gender_male'})","frozenset({'writing_cat_χαμηλό', 'test preparation course_none', 'reading_cat_χαμηλό'})",0.166,0.201,0.056,0.3373493975903614,1.6783552118923453,1.0,0.022633999999999994,1.2057636363636361,0.48462658444672824,0.18006430868167203,0.17065005956240478,0.3079781813822454
frozenset({'math_cat_χαμηλό'}),"frozenset({'writing_cat_χαμηλό', 'gender_male', 'lunch_standard'})",0.339,0.109,0.062,0.18289085545722714,1.6778977564883224,1.0,0.025048999999999995,1.0904296028880867,0.6112195598067443,0.16062176165803108,0.0829302530384143,0.37584909745338424
"frozenset({'writing_cat_χαμηλό', 'gender_male', 'lunch_standard'})",frozenset({'math_cat_χαμηλό'}),0.109,0.339,0.062,0.5688073394495413,1.6778977564883224,1.0,0.025048999999999995,1.5329574468085108,0.4534412222584265,0.16062176165803108,0.3476661716331941,0.37584909745338424
"frozenset({'math_cat_χαμηλό', 'test preparation course_none'})","frozenset({'lunch_free/reduced', 'gender_female'})",0.246,0.189,0.078,0.3170731707317073,1.6776358239772875,1.0,0.031506,1.1875357142857144,0.53570699857172,0.2184873949579832,0.15792006255450966,0.36488579171505997
"frozenset({'lunch_free/reduced', 'gender_female'})","frozenset({'math_cat_χαμηλό', 'test preparation course_none'})",0.189,0.246,0.078,0.4126984126984127,1.6776358239772873,1.0,0.031506,1.283837837837838,0.4980555819026843,0.2184873949579832,0.2210854279819797,0.36488579171505997
frozenset({'writing_cat_μέτριο'}),"frozenset({'reading_cat_μέτριο', 'race/ethnicity_group B'})",0.491,0.085,0.07,0.14256619144602853,1.6772493111297473,1.0,0.028265000000000005,1.0671377672209028,0.7932921695200674,0.1383399209486166,0.06291387043281806,0.48304780160536726
"frozenset({'reading_cat_μέτριο', 'race/ethnicity_group B'})",frozenset({'writing_cat_μέτριο'}),0.085,0.491,0.07,0.823529411764706,1.6772493111297473,1.0,0.028265000000000005,2.8843333333333345,0.44129586260733805,0.1383399209486166,0.6532994337224085,0.48304780160536726
"frozenset({'gender_female', 'lunch_standard'})","frozenset({'writing_cat_υψηλό', 'reading_cat_υψηλό'})",0.329,0.185,0.102,0.3100303951367781,1.6758399737123142,1.0,0.04113499999999999,1.1812114537444933,0.6010198416177199,0.24757281553398056,0.15341152777518788,0.4306908732440647
"frozenset({'writing_cat_υψηλό', 'reading_cat_υψηλό'})","frozenset({'gender_female', 'lunch_standard'})",0.185,0.329,0.102,0.5513513513513513,1.6758399737123137,1.0,0.04113499999999999,1.495602409638554,0.49482737880428235,0.24757281553398056,0.331373101864905,0.4306908732440647
frozenset({'writing_cat_μέτριο'}),"frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο'})",0.491,0.169,0.139,0.2830957230142567,1.675122621386134,1.0,0.056021,1.1591505681818182,0.7918050628259671,0.2667946257197697,0.13729930567299237,0.5527904650574242
"frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο'})",frozenset({'writing_cat_μέτριο'}),0.169,0.491,0.139,0.8224852071005917,1.6751226213861339,1.0,0.056021,2.8673666666666664,0.4849925114060376,0.2667946257197697,0.6512479510817126,0.5527904650574242
"frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό'})","frozenset({'gender_male', 'test preparation course_none'})",0.252,0.308,0.13,0.5158730158730159,1.6749123891981037,1.0,0.052384,1.429377049180328,0.5387083504730563,0.3023255813953488,0.30039453160840457,0.46897546897546905
"frozenset({'gender_male', 'test preparation course_none'})","frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό'})",0.308,0.252,0.13,0.4220779220779221,1.6749123891981035,1.0,0.052384,1.2942921348314607,0.582303245887061,0.3023255813953488,0.22737690117369266,0.46897546897546905
"frozenset({'writing_cat_μέτριο', 'gender_female'})","frozenset({'test preparation course_completed', 'reading_cat_μέτριο'})",0.264,0.172,0.076,0.28787878787878785,1.6737138830162086,1.0,0.030592,1.1627234042553192,0.5469107551487414,0.2111111111111111,0.1399502269067486,0.36486962649753346
"frozenset({'test preparation course_completed', 'reading_cat_μέτριο'})","frozenset({'writing_cat_μέτριο', 'gender_female'})",0.172,0.264,0.076,0.4418604651162791,1.6737138830162086,1.0,0.030592,1.3186666666666667,0.48614289346554795,0.2111111111111111,0.2416582406471183,0.36486962649753346
"frozenset({'test preparation course_completed', 'gender_female'})","frozenset({'reading_cat_υψηλό', 'lunch_standard'})",0.184,0.182,0.056,0.30434782608695654,1.6722408026755855,1.0,0.022512000000000004,1.1758750000000002,0.49264705882352944,0.1806451612903226,0.14956946954395664,0.3060200668896321
"frozenset({'reading_cat_υψηλό', 'lunch_standard'})","frozenset({'test preparation course_completed', 'gender_female'})",0.182,0.184,0.056,0.3076923076923077,1.6722408026755855,1.0,0.022512000000000004,1.1786666666666668,0.49144254278728616,0.1806451612903226,0.15158371040723984,0.3060200668896321
"frozenset({'test preparation course_completed', 'reading_cat_μέτριο', 'gender_male'})",frozenset({'math_cat_μέτριο'}),0.084,0.485,0.068,0.8095238095238095,1.6691212567501228,1.0,0.027260000000000006,2.7037500000000003,0.4376444901104547,0.1357285429141717,0.6301433194637078,0.4748649975454099
frozenset({'math_cat_μέτριο'}),"frozenset({'test preparation course_completed', 'reading_cat_μέτριο', 'gender_male'})",0.485,0.084,0.068,0.1402061855670103,1.6691212567501226,1.0,0.027260000000000006,1.0653717026378897,0.7784123358081098,0.1357285429141717,0.06136046459280601,0.4748649975454099
"frozenset({'reading_cat_μέτριο', 'gender_female'})","frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'lunch_standard'})",0.261,0.225,0.098,0.37547892720306514,1.6687952320136228,1.0,0.039275,1.240950920245399,0.5423075860926238,0.25257731958762886,0.19416635768137436,0.40551724137931033
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'lunch_standard'})","frozenset({'reading_cat_μέτριο', 'gender_female'})",0.225,0.261,0.098,0.4355555555555556,1.6687952320136228,1.0,0.039275,1.309251968503937,0.5171165240289664,0.25257731958762886,0.23620508194256506,0.40551724137931033
frozenset({'gender_female'}),"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced', 'reading_cat_μέτριο'})",0.518,0.059,0.051,0.09845559845559845,1.66873895687455,1.0,0.020437999999999998,1.0437644539614561,0.8314213652265886,0.09695817490494298,0.041929435128159886,0.48143118905830773
"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced', 'reading_cat_μέτριο'})",frozenset({'gender_female'}),0.059,0.518,0.051,0.864406779661017,1.66873895687455,1.0,0.020437999999999998,3.5547500000000007,0.42587151757621217,0.09695817490494298,0.7186862648568817,0.48143118905830773
"frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο', 'gender_female', 'lunch_standard'})",frozenset({'writing_cat_μέτριο'}),0.065,0.491,0.053,0.8153846153846154,1.660661131129563,1.0,0.021085,2.757083333333333,0.42548683281202704,0.10536779324055666,0.6372978691249811,0.4616637944540185
frozenset({'writing_cat_μέτριο'}),"frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο', 'gender_female', 'lunch_standard'})",0.491,0.065,0.053,0.10794297352342158,1.6606611311295627,1.0,0.021085,1.048139269406393,0.7815917262853541,0.10536779324055666,0.04592831392879313,0.4616637944540185
"frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο'})","frozenset({'writing_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",0.169,0.221,0.062,0.3668639053254438,1.660017671155854,1.0,0.024651,1.2303831775700935,0.47845580528706183,0.18902439024390244,0.18724506460262358,0.32370344587539157
"frozenset({'writing_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})","frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο'})",0.221,0.169,0.062,0.28054298642533937,1.660017671155854,1.0,0.024651,1.1550377358490564,0.5103938051265062,0.18902439024390244,0.134227420487773,0.32370344587539157
frozenset({'math_cat_χαμηλό'}),"frozenset({'lunch_free/reduced', 'test preparation course_none'})",0.339,0.224,0.126,0.3716814159292035,1.65929203539823,1.0,0.050064,1.2350422535211267,0.6011094301563288,0.2883295194508009,0.1903111030015509,0.4670907079646017
"frozenset({'lunch_free/reduced', 'test preparation course_none'})",frozenset({'math_cat_χαμηλό'}),0.224,0.339,0.126,0.5625,1.65929203539823,1.0,0.050064,1.510857142857143,0.5120274914089347,0.2883295194508009,0.3381240544629349,0.4670907079646017
"frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'gender_male'})",0.223,0.165,0.061,0.273542600896861,1.6578339448294606,1.0,0.024204999999999997,1.1494135802469136,0.5106863303584614,0.18654434250764526,0.1299911387986359,0.3216197852969154
"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'gender_male'})","frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",0.165,0.223,0.061,0.3696969696969697,1.6578339448294606,1.0,0.024204999999999997,1.2327403846153846,0.4752135074114067,0.18654434250764526,0.18879918879918878,0.3216197852969154
"frozenset({'test preparation course_completed', 'math_cat_μέτριο', 'gender_male'})",frozenset({'writing_cat_μέτριο'}),0.086,0.491,0.07,0.8139534883720931,1.6577464121631225,1.0,0.027774000000000014,2.735875000000002,0.4341044076273837,0.13806706114398426,0.6344862246995936,0.47825983990906085
frozenset({'writing_cat_μέτριο'}),"frozenset({'test preparation course_completed', 'math_cat_μέτριο', 'gender_male'})",0.491,0.086,0.07,0.14256619144602853,1.6577464121631225,1.0,0.027774000000000014,1.0659714964370548,0.7795116474880722,0.13806706114398426,0.06188861208537039,0.47825983990906085
"frozenset({'math_cat_μέτριο', 'gender_female', 'test preparation course_none'})","frozenset({'writing_cat_μέτριο', 'lunch_standard'})",0.15,0.338,0.084,0.56,1.6568047337278107,1.0,0.0333,1.5045454545454546,0.46638655462184875,0.20792079207920794,0.3353474320241693,0.4042603550295858
"frozenset({'writing_cat_μέτριο', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'gender_female', 'test preparation course_none'})",0.338,0.15,0.084,0.2485207100591716,1.6568047337278107,1.0,0.0333,1.1311023622047243,0.5988347000431593,0.20792079207920794,0.11590671771667246,0.4042603550295858
"frozenset({'math_cat_μέτριο', 'race/ethnicity_group D', 'gender_male'})",frozenset({'writing_cat_μέτριο'}),0.075,0.491,0.061,0.8133333333333334,1.6564833672776647,1.0,0.024175000000000002,2.7267857142857146,0.42844483828090385,0.12079207920792082,0.633267845448592,0.4687847929395791
frozenset({'writing_cat_μέτριο'}),"frozenset({'math_cat_μέτριο', 'race/ethnicity_group D', 'gender_male'})",0.491,0.075,0.061,0.12423625254582485,1.6564833672776647,1.0,0.024175000000000002,1.0562209302325583,0.7786080066990886,0.12079207920792082,0.053228381130621456,0.4687847929395791
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})","frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο', 'test preparation course_none'})",0.32,0.117,0.062,0.19375,1.6559829059829059,1.0,0.02456,1.0951937984496123,0.5825426944971537,0.16533333333333333,0.08691959229898075,0.36183226495726495
"frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο', 'test preparation course_none'})","frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})",0.117,0.32,0.062,0.5299145299145299,1.6559829059829059,1.0,0.02456,1.4465454545454544,0.44861725057538443,0.16533333333333333,0.30869783810960283,0.36183226495726495
frozenset({'math_cat_μέτριο'}),"frozenset({'writing_cat_μέτριο', 'gender_male', 'race/ethnicity_group D'})",0.485,0.076,0.061,0.12577319587628866,1.6549104720564298,1.0,0.02414,1.056933962264151,0.7684227279961803,0.12200000000000001,0.0538670951042085,0.46420238741182857
"frozenset({'writing_cat_μέτριο', 'gender_male', 'race/ethnicity_group D'})",frozenset({'math_cat_μέτριο'}),0.076,0.485,0.061,0.8026315789473685,1.6549104720564298,1.0,0.02414,2.609333333333334,0.42828755943510044,0.12200000000000001,0.6167603474706184,0.46420238741182857
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'lunch_standard'})","frozenset({'test preparation course_completed', 'reading_cat_μέτριο'})",0.225,0.172,0.064,0.28444444444444444,1.65374677002584,1.0,0.025300000000000003,1.1571428571428573,0.5100806451612904,0.1921921921921922,0.13580246913580246,0.3282687338501292
"frozenset({'test preparation course_completed', 'reading_cat_μέτριο'})","frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'lunch_standard'})",0.172,0.225,0.064,0.372093023255814,1.65374677002584,1.0,0.025300000000000003,1.2342592592592594,0.4774305555555556,0.1921921921921922,0.18979744936234064,0.3282687338501292
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'writing_cat_μέτριο', 'gender_female', 'test preparation course_none'})",0.325,0.175,0.094,0.28923076923076924,1.6527472527472529,1.0,0.037125000000000005,1.1607142857142856,0.5851063829787234,0.2315270935960591,0.1384615384615385,0.41318681318681316
"frozenset({'writing_cat_μέτριο', 'gender_female', 'test preparation course_none'})","frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο'})",0.175,0.325,0.094,0.5371428571428571,1.6527472527472526,1.0,0.037125000000000005,1.4583333333333335,0.47872340425531923,0.2315270935960591,0.3142857142857142,0.41318681318681316
"frozenset({'test preparation course_completed', 'writing_cat_μέτριο', 'gender_male'})",frozenset({'reading_cat_μέτριο'}),0.094,0.49,0.076,0.8085106382978723,1.6500217108119843,1.0,0.02994,2.6633333333333327,0.43482049494597425,0.1496062992125984,0.6245306633291614,0.4818063395570994
frozenset({'reading_cat_μέτριο'}),"frozenset({'test preparation course_completed', 'writing_cat_μέτριο', 'gender_male'})",0.49,0.094,0.076,0.15510204081632653,1.6500217108119843,1.0,0.02994,1.0723188405797102,0.7724458204334365,0.1496062992125984,0.06744154615488579,0.4818063395570994
"frozenset({'parental level of education_high school', 'writing_cat_μέτριο', 'lunch_standard'})",frozenset({'reading_cat_μέτριο'}),0.073,0.49,0.059,0.8082191780821918,1.6494268940452894,1.0,0.02323,2.6592857142857147,0.4247344267090853,0.11706349206349205,0.62395917271018,0.46431367067374896
frozenset({'reading_cat_μέτριο'}),"frozenset({'parental level of education_high school', 'writing_cat_μέτριο', 'lunch_standard'})",0.49,0.073,0.059,0.12040816326530612,1.6494268940452894,1.0,0.02323,1.0538979118329468,0.7720172814888668,0.11706349206349205,0.05114149219558374,0.46431367067374896
"frozenset({'lunch_free/reduced', 'writing_cat_μέτριο'})","frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο'})",0.153,0.325,0.082,0.5359477124183006,1.649069884364002,1.0,0.032275000000000005,1.4545774647887324,0.4646960578224437,0.2070707070707071,0.3125151295085935,0.39412770236299643
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'lunch_free/reduced', 'writing_cat_μέτριο'})",0.325,0.153,0.082,0.2523076923076923,1.649069884364002,1.0,0.032275000000000005,1.1328189300411522,0.5831074977416442,0.2070707070707071,0.11724638997366268,0.39412770236299643
"frozenset({'math_cat_μέτριο', 'gender_male'})","frozenset({'test preparation course_completed', 'reading_cat_μέτριο'})",0.24,0.172,0.068,0.2833333333333334,1.6472868217054268,1.0,0.026720000000000008,1.1553488372093026,0.5170278637770899,0.19767441860465118,0.13446054750402583,0.3393410852713179
"frozenset({'test preparation course_completed', 'reading_cat_μέτριο'})","frozenset({'math_cat_μέτριο', 'gender_male'})",0.172,0.24,0.068,0.39534883720930236,1.6472868217054266,1.0,0.026720000000000008,1.256923076923077,0.4745666382495028,0.19767441860465118,0.20440636474908205,0.3393410852713179
"frozenset({'gender_female', 'lunch_standard'})","frozenset({'math_cat_χαμηλό', 'writing_cat_μέτριο'})",0.329,0.109,0.059,0.17933130699088143,1.6452413485401967,1.0,0.023138999999999993,1.0856999999999999,0.5844805375230492,0.15567282321899736,0.07893524914801507,0.3603078553303031
"frozenset({'math_cat_χαμηλό', 'writing_cat_μέτριο'})","frozenset({'gender_female', 'lunch_standard'})",0.109,0.329,0.059,0.5412844036697247,1.6452413485401967,1.0,0.023138999999999993,1.46278,0.4401643554185926,0.15567282321899736,0.31637019920972387,0.3603078553303031
frozenset({'math_cat_μέτριο'}),"frozenset({'lunch_free/reduced', 'reading_cat_μέτριο', 'gender_male'})",0.485,0.074,0.059,0.12164948453608247,1.6439119531903037,1.0,0.02311,1.0542488262910799,0.7605726509791015,0.11800000000000001,0.05145732671283205,0.45947339091668987
"frozenset({'lunch_free/reduced', 'reading_cat_μέτριο', 'gender_male'})",frozenset({'math_cat_μέτριο'}),0.074,0.485,0.059,0.7972972972972973,1.6439119531903037,1.0,0.02311,2.540666666666666,0.42299666874107694,0.11800000000000001,0.6064025190238782,0.45947339091668987
"frozenset({'race/ethnicity_group C', 'gender_female', 'reading_cat_μέτριο', 'test preparation course_none'})",frozenset({'writing_cat_μέτριο'}),0.067,0.491,0.054,0.8059701492537313,1.6414870656898806,1.0,0.021102999999999997,2.623307692307692,0.4188599102854193,0.10714285714285714,0.6188018649385685,0.45797489132747665
frozenset({'writing_cat_μέτριο'}),"frozenset({'race/ethnicity_group C', 'gender_female', 'reading_cat_μέτριο', 'test preparation course_none'})",0.491,0.067,0.054,0.109979633401222,1.6414870656898803,1.0,0.021102999999999997,1.0482906178489704,0.7677726842756312,0.10714285714285714,0.04606605937965915,0.45797489132747665
frozenset({'lunch_free/reduced'}),"frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό', 'math_cat_χαμηλό'})",0.355,0.206,0.12,0.3380281690140845,1.6409134418159443,1.0,0.04687000000000001,1.1994468085106385,0.6055555555555557,0.272108843537415,0.16628232873310392,0.46027622042937233
"frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό', 'math_cat_χαμηλό'})",frozenset({'lunch_free/reduced'}),0.206,0.355,0.12,0.5825242718446602,1.6409134418159441,1.0,0.04687000000000001,1.545,0.4919185558354325,0.272108843537415,0.35275080906148865,0.46027622042937233
"frozenset({'reading_cat_μέτριο', 'gender_male'})","frozenset({'test preparation course_completed', 'math_cat_μέτριο'})",0.229,0.181,0.068,0.29694323144104806,1.6405703394533042,1.0,0.026551000000000005,1.1649130434782609,0.5064278629739835,0.19883040935672514,0.14156682715634686,0.33631691958792737
"frozenset({'test preparation course_completed', 'math_cat_μέτριο'})","frozenset({'reading_cat_μέτριο', 'gender_male'})",0.181,0.229,0.068,0.37569060773480667,1.6405703394533042,1.0,0.026551000000000005,1.2349646017699116,0.47674710910005036,0.19883040935672514,0.19026019161453522,0.33631691958792737
"frozenset({'reading_cat_μέτριο', 'test preparation course_none'})","frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'gender_male'})",0.318,0.165,0.086,0.27044025157232704,1.639031827711073,1.0,0.03352999999999999,1.1445258620689653,0.5716770101616313,0.21662468513853902,0.1262757503860204,0.3958261863922241
"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'gender_male'})","frozenset({'reading_cat_μέτριο', 'test preparation course_none'})",0.165,0.318,0.086,0.5212121212121211,1.6390318277110727,1.0,0.03352999999999999,1.4244303797468352,0.46692661189249407,0.21662468513853902,0.2979649871145471,0.3958261863922241
"frozenset({'race/ethnicity_group C', 'math_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",frozenset({'reading_cat_μέτριο'}),0.066,0.49,0.053,0.803030303030303,1.6388373531230673,1.0,0.020659999999999998,2.5892307692307686,0.41735687446971836,0.10536779324055666,0.6137849079025549,0.4555967841682127
frozenset({'reading_cat_μέτριο'}),"frozenset({'race/ethnicity_group C', 'math_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",0.49,0.066,0.053,0.10816326530612244,1.638837353123067,1.0,0.020659999999999998,1.0472768878718535,0.7643359230484646,0.10536779324055666,0.04514268234060218,0.4555967841682127
"frozenset({'writing_cat_μέτριο', 'test preparation course_none'})","frozenset({'math_cat_χαμηλό', 'reading_cat_μέτριο', 'gender_female'})",0.308,0.109,0.055,0.17857142857142858,1.6382699868938402,1.0,0.021428000000000003,1.084695652173913,0.5630057803468209,0.15193370165745856,0.07808241141574475,0.34157929226736566
"frozenset({'math_cat_χαμηλό', 'reading_cat_μέτριο', 'gender_female'})","frozenset({'writing_cat_μέτριο', 'test preparation course_none'})",0.109,0.308,0.055,0.5045871559633027,1.63826998689384,1.0,0.021428000000000003,1.3968148148148147,0.43726150392817065,0.15193370165745856,0.28408548549610224,0.34157929226736566
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none', 'gender_male'})",frozenset({'writing_cat_μέτριο'}),0.107,0.491,0.086,0.8037383177570093,1.6369415840264956,1.0,0.03346299999999999,2.5934761904761903,0.43572749290346097,0.16796874999999997,0.6144171272239869,0.4894455336239222
frozenset({'writing_cat_μέτριο'}),"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none', 'gender_male'})",0.491,0.107,0.086,0.175152749490835,1.6369415840264954,1.0,0.03346299999999999,1.0826246913580246,0.764449216429844,0.16796874999999997,0.07631886841078948,0.4894455336239222
"frozenset({'writing_cat_υψηλό', 'math_cat_μέτριο'})",frozenset({'test preparation course_completed'}),0.094,0.358,0.055,0.5851063829787234,1.6343753714489482,1.0,0.021348,1.5473846153846154,0.4284166164960867,0.1385390428211587,0.35374826009146954,0.3693688339474623
frozenset({'test preparation course_completed'}),"frozenset({'writing_cat_υψηλό', 'math_cat_μέτριο'})",0.358,0.094,0.055,0.15363128491620112,1.6343753714489482,1.0,0.021348,1.0704554455445545,0.6045879354290569,0.1385390428211587,0.06581819527174516,0.3693688339474623
"frozenset({'math_cat_μέτριο', 'test preparation course_none'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'gender_female', 'lunch_standard'})",0.304,0.149,0.074,0.24342105263157895,1.6336983398092548,1.0,0.028704,1.1248,0.5573159366262815,0.19525065963060687,0.11095305832147939,0.3700326739667962
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'gender_female', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'test preparation course_none'})",0.149,0.304,0.074,0.4966442953020134,1.6336983398092548,1.0,0.028704,1.3827199999999997,0.4558071585098612,0.19525065963060687,0.2767877806063412,0.3700326739667962
"frozenset({'reading_cat_μέτριο', 'test preparation course_none'})","frozenset({'math_cat_χαμηλό', 'writing_cat_μέτριο', 'gender_female'})",0.318,0.106,0.055,0.17295597484276728,1.6316601400261066,1.0,0.021292,1.0809581749049428,0.5676352972540656,0.14905149051490515,0.07489482644604842,0.34591194968553457
"frozenset({'math_cat_χαμηλό', 'writing_cat_μέτριο', 'gender_female'})","frozenset({'reading_cat_μέτριο', 'test preparation course_none'})",0.106,0.318,0.055,0.5188679245283019,1.6316601400261066,1.0,0.021292,1.4174901960784312,0.43302826926988,0.14905149051490515,0.29452774857522274,0.34591194968553457
"frozenset({'reading_cat_μέτριο', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'gender_female', 'test preparation course_none'})",0.327,0.15,0.08,0.24464831804281345,1.6309887869520898,1.0,0.03095,1.1253036437246962,0.5748514115898959,0.2015113350125945,0.11135096240330995,0.3889908256880734
"frozenset({'math_cat_μέτριο', 'gender_female', 'test preparation course_none'})","frozenset({'reading_cat_μέτριο', 'lunch_standard'})",0.15,0.327,0.08,0.5333333333333333,1.6309887869520896,1.0,0.03095,1.4421428571428572,0.45514705882352935,0.2015113350125945,0.30658741951461116,0.3889908256880734
"frozenset({'writing_cat_υψηλό', 'gender_female'})","frozenset({'test preparation course_completed', 'lunch_standard'})",0.154,0.227,0.057,0.37012987012987014,1.630528062246124,1.0,0.022042,1.2272371134020619,0.4570942723238356,0.17592592592592593,0.18516153962467027,0.3106155958578866
"frozenset({'test preparation course_completed', 'lunch_standard'})","frozenset({'writing_cat_υψηλό', 'gender_female'})",0.227,0.154,0.057,0.2511013215859031,1.630528062246124,1.0,0.022042,1.1296588235294116,0.5002610017929688,0.17592592592592593,0.11477697586986181,0.3106155958578866
"frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο'})","frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'lunch_standard'})",0.169,0.225,0.062,0.3668639053254438,1.6305062458908612,1.0,0.023974999999999996,1.2240654205607477,0.4653351966150382,0.18674698795180722,0.18305020041992745,0.32120973044049966
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'lunch_standard'})","frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο'})",0.225,0.169,0.062,0.27555555555555555,1.6305062458908612,1.0,0.023974999999999996,1.147085889570552,0.49895941727367316,0.18674698795180722,0.12822569862281052,0.32120973044049966
"frozenset({'writing_cat_μέτριο', 'test preparation course_none'})","frozenset({'reading_cat_μέτριο', 'gender_male'})",0.308,0.229,0.115,0.3733766233766234,1.630465604264731,1.0,0.04446800000000001,1.2304041450777201,0.5587836139733603,0.2725118483412322,0.18725891488537408,0.4377800147450802
"frozenset({'reading_cat_μέτριο', 'gender_male'})","frozenset({'writing_cat_μέτριο', 'test preparation course_none'})",0.229,0.308,0.115,0.5021834061135371,1.6304656042647308,1.0,0.04446800000000001,1.3900701754385962,0.501528224214741,0.2725118483412322,0.2806118585455738,0.4377800147450802
"frozenset({'math_cat_υψηλό', 'writing_cat_υψηλό', 'reading_cat_υψηλό'})","frozenset({'gender_female', 'lunch_standard'})",0.11,0.329,0.059,0.5363636363636364,1.6302846090080132,1.0,0.022809999999999997,1.4472549019607845,0.4343934488668824,0.15526315789473683,0.30903671589215553,0.3578474716772589
"frozenset({'gender_female', 'lunch_standard'})","frozenset({'math_cat_υψηλό', 'writing_cat_υψηλό', 'reading_cat_υψηλό'})",0.329,0.11,0.059,0.17933130699088143,1.630284609008013,1.0,0.022809999999999997,1.0844814814814814,0.5761701482735102,0.15526315789473683,0.07790034493357463,0.3578474716772589
frozenset({'writing_cat_μέτριο'}),"frozenset({'math_cat_χαμηλό', 'reading_cat_μέτριο'})",0.491,0.12,0.096,0.1955193482688391,1.629327902240326,1.0,0.03708,1.093873417721519,0.7588408644400786,0.18640776699029127,0.0858174412145899,0.49775967413441957
"frozenset({'math_cat_χαμηλό', 'reading_cat_μέτριο'})",frozenset({'writing_cat_μέτριο'}),0.12,0.491,0.096,0.8,1.629327902240326,1.0,0.03708,2.545000000000001,0.4389204545454546,0.18640776699029127,0.607072691552063,0.49775967413441957
"frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο', 'gender_male'})",frozenset({'writing_cat_μέτριο'}),0.07,0.491,0.056,0.7999999999999999,1.6293279022403258,1.0,0.021629999999999996,2.544999999999999,0.41532258064516125,0.11089108910891092,0.6070726915520628,0.4570264765784114
frozenset({'writing_cat_μέτριο'}),"frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο', 'gender_male'})",0.491,0.07,0.056,0.11405295315682282,1.6293279022403258,1.0,0.021629999999999996,1.0497241379310345,0.7588408644400784,0.11089108910891092,0.04736876683529335,0.4570264765784114
"frozenset({'writing_cat_μέτριο', 'gender_female'})","frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})",0.264,0.228,0.098,0.3712121212121212,1.6281233386496543,1.0,0.037808,1.2277590361445783,0.5241792369121562,0.24873096446700507,0.18550792903124508,0.400518341307815
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'gender_female'})",0.228,0.264,0.098,0.4298245614035088,1.6281233386496543,1.0,0.037808,1.2908307692307692,0.4997356455535582,0.24873096446700507,0.22530511060259342,0.400518341307815
"frozenset({'lunch_free/reduced', 'writing_cat_μέτριο'})","frozenset({'reading_cat_μέτριο', 'gender_male'})",0.153,0.229,0.057,0.37254901960784315,1.6268516140080487,1.0,0.021963000000000003,1.22878125,0.4549182874541727,0.1753846153846154,0.1861854988428575,0.3107286582755373
"frozenset({'reading_cat_μέτριο', 'gender_male'})","frozenset({'lunch_free/reduced', 'writing_cat_μέτριο'})",0.229,0.153,0.057,0.24890829694323144,1.6268516140080487,1.0,0.021963000000000003,1.1276918604651163,0.49976107584135443,0.1753846153846154,0.11323293617854951,0.3107286582755373
frozenset({'reading_cat_μέτριο'}),"frozenset({'writing_cat_μέτριο', 'race/ethnicity_group B'})",0.49,0.088,0.07,0.14285714285714288,1.6233766233766238,1.0,0.026880000000000008,1.064,0.7529411764705884,0.1377952755905512,0.06015037593984965,0.4691558441558442
"frozenset({'writing_cat_μέτριο', 'race/ethnicity_group B'})",frozenset({'reading_cat_μέτριο'}),0.088,0.49,0.07,0.7954545454545455,1.6233766233766236,1.0,0.026880000000000008,2.493333333333334,0.42105263157894746,0.1377952755905512,0.5989304812834226,0.4691558441558442
"frozenset({'writing_cat_μέτριο', 'lunch_standard'})","frozenset({'test preparation course_completed', 'math_cat_μέτριο', 'reading_cat_μέτριο'})",0.338,0.117,0.064,0.1893491124260355,1.6183684822738076,1.0,0.024453999999999997,1.0892481751824816,0.5771808912386707,0.16368286445012786,0.08193557466142184,0.3681788297172912
"frozenset({'test preparation course_completed', 'math_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'writing_cat_μέτριο', 'lunch_standard'})",0.117,0.338,0.064,0.547008547008547,1.6183684822738074,1.0,0.024453999999999997,1.461396226415094,0.43272225368063416,0.16368286445012786,0.3157228806775634,0.3681788297172912
"frozenset({'reading_cat_μέτριο', 'test preparation course_none'})","frozenset({'race/ethnicity_group C', 'math_cat_μέτριο', 'lunch_standard'})",0.318,0.103,0.053,0.16666666666666666,1.6181229773462784,1.0,0.020246,1.0764,0.560117302052786,0.14402173913043478,0.07097733184689706,0.34061488673139156
"frozenset({'race/ethnicity_group C', 'math_cat_μέτριο', 'lunch_standard'})","frozenset({'reading_cat_μέτριο', 'test preparation course_none'})",0.103,0.318,0.053,0.5145631067961165,1.6181229773462782,1.0,0.020246,1.40492,0.4258639910813824,0.14402173913043478,0.2882156991145403,0.34061488673139156
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})","frozenset({'math_cat_μέτριο', 'gender_female', 'lunch_standard'})",0.266,0.172,0.074,0.2781954887218045,1.6174156321035147,1.0,0.028247999999999995,1.1471250000000002,0.5200677516753811,0.2032967032967033,0.12825542116159963,0.35421402343066966
"frozenset({'math_cat_μέτριο', 'gender_female', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})",0.172,0.266,0.074,0.43023255813953487,1.6174156321035145,1.0,0.028247999999999995,1.2882448979591836,0.4610262436349392,0.2032967032967033,0.22375007920917556,0.35421402343066966
frozenset({'writing_cat_μέτριο'}),"frozenset({'parental level of education_high school', 'reading_cat_μέτριο', 'test preparation course_none'})",0.491,0.068,0.054,0.109979633401222,1.6173475500179704,1.0,0.020612,1.0471670480549198,0.7499090446045259,0.10693069306930696,0.04504252510860729,0.4520486402300227
"frozenset({'parental level of education_high school', 'reading_cat_μέτριο', 'test preparation course_none'})",frozenset({'writing_cat_μέτριο'}),0.068,0.491,0.054,0.7941176470588235,1.6173475500179704,1.0,0.020612,2.472285714285714,0.4095533301541885,0.10693069306930696,0.5955160060094764,0.4520486402300227
"frozenset({'writing_cat_χαμηλό', 'math_cat_χαμηλό'})",frozenset({'lunch_free/reduced'}),0.23,0.355,0.132,0.5739130434782609,1.6166564605021434,1.0,0.050350000000000006,1.5137755102040817,0.4953758362849272,0.29139072847682124,0.3394000674081564,0.47287201469687695
frozenset({'lunch_free/reduced'}),"frozenset({'writing_cat_χαμηλό', 'math_cat_χαμηλό'})",0.355,0.23,0.132,0.371830985915493,1.6166564605021434,1.0,0.050350000000000006,1.225784753363229,0.5913789053323937,0.29139072847682124,0.18419608560453637,0.47287201469687695
"frozenset({'reading_cat_μέτριο', 'test preparation course_none'})","frozenset({'math_cat_χαμηλό', 'writing_cat_μέτριο'})",0.318,0.109,0.056,0.1761006289308176,1.6156021002827303,1.0,0.021338000000000003,1.0814427480916031,0.5587033933808129,0.1509433962264151,0.07530934784603546,0.34493104841036293
"frozenset({'math_cat_χαμηλό', 'writing_cat_μέτριο'})","frozenset({'reading_cat_μέτριο', 'test preparation course_none'})",0.109,0.318,0.056,0.5137614678899083,1.6156021002827303,1.0,0.021338000000000003,1.4026037735849055,0.42764951098284437,0.1509433962264151,0.2870402754983993,0.34493104841036293
"frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'gender_male'})",frozenset({'writing_cat_μέτριο'}),0.145,0.491,0.115,0.7931034482758622,1.6152819720485991,1.0,0.04380500000000001,2.4601666666666686,0.4455123315535216,0.22072936660268713,0.5935234740193757,0.5136596671114545
frozenset({'writing_cat_μέτριο'}),"frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'gender_male'})",0.491,0.145,0.115,0.23421588594704687,1.6152819720485991,1.0,0.04380500000000001,1.116502659574468,0.7483556846331256,0.22072936660268713,0.10434606543514255,0.5136596671114545
"frozenset({'math_cat_χαμηλό', 'writing_cat_χαμηλό'})","frozenset({'parental level of education_high school', 'test preparation course_none'})",0.23,0.14,0.052,0.2260869565217391,1.6149068322981364,1.0,0.01979999999999999,1.11123595505618,0.4945054945054943,0.16352201257861634,0.10010111223458033,0.29875776397515524
"frozenset({'parental level of education_high school', 'test preparation course_none'})","frozenset({'math_cat_χαμηλό', 'writing_cat_χαμηλό'})",0.14,0.23,0.052,0.3714285714285714,1.6149068322981364,1.0,0.01979999999999999,1.225,0.4427549194991054,0.16352201257861634,0.18367346938775503,0.29875776397515524
"frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'test preparation course_none'})",0.108,0.304,0.053,0.49074074074074076,1.6142787524366473,1.0,0.020168,1.366690909090909,0.42660123529909466,0.14763231197771587,0.26830566198382294,0.33254142300194933
"frozenset({'math_cat_μέτριο', 'test preparation course_none'})","frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο', 'lunch_standard'})",0.304,0.108,0.053,0.17434210526315788,1.614278752436647,1.0,0.020168,1.0803505976095618,0.5467360659292995,0.14763231197771587,0.07437455746990794,0.33254142300194933
"frozenset({'test preparation course_completed', 'math_cat_μέτριο', 'gender_male'})",frozenset({'reading_cat_μέτριο'}),0.086,0.49,0.068,0.7906976744186047,1.613668723303275,1.0,0.025860000000000008,2.4366666666666674,0.41607671514995503,0.13385826771653545,0.5896032831737348,0.4647365923113432
frozenset({'reading_cat_μέτριο'}),"frozenset({'test preparation course_completed', 'math_cat_μέτριο', 'gender_male'})",0.49,0.086,0.068,0.13877551020408163,1.613668723303275,1.0,0.025860000000000008,1.0612796208530806,0.7456747404844293,0.13385826771653545,0.05774125842897335,0.4647365923113432
"frozenset({'math_cat_μέτριο', 'gender_female', 'test preparation course_none', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.11,0.418,0.074,0.6727272727272727,1.6093953892996955,1.0,0.028019999999999996,1.7783333333333335,0.42544791982994223,0.16299559471365638,0.43767572633552015,0.42488038277511964
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'math_cat_μέτριο', 'gender_female', 'test preparation course_none', 'lunch_standard'})",0.418,0.11,0.074,0.17703349282296652,1.6093953892996955,1.0,0.028019999999999996,1.081453488372093,0.6505990526609082,0.16299559471365638,0.07531853126176012,0.42488038277511964
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'gender_male'})",frozenset({'math_cat_μέτριο'}),0.191,0.485,0.149,0.7801047120418848,1.6084633237977006,1.0,0.056365,2.3420238095238095,0.46760023560448316,0.28273244781783685,0.5730188583337569,0.5436606034436229
frozenset({'math_cat_μέτριο'}),"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'gender_male'})",0.485,0.191,0.149,0.30721649484536084,1.6084633237977006,1.0,0.056365,1.1677529761904761,0.7345409526291783,0.28273244781783685,0.14365450537127422,0.5436606034436229
"frozenset({'writing_cat_μέτριο', 'lunch_standard'})","frozenset({'parental level of education_high school', 'math_cat_μέτριο'})",0.338,0.092,0.05,0.14792899408284024,1.6079238487265244,1.0,0.018904,1.0656388888888888,0.571117824773414,0.13157894736842105,0.0615958084612778,0.34570362747620276
"frozenset({'parental level of education_high school', 'math_cat_μέτριο'})","frozenset({'writing_cat_μέτριο', 'lunch_standard'})",0.092,0.338,0.05,0.5434782608695653,1.6079238487265244,1.0,0.018904,1.450095238095238,0.4163876651982379,0.13157894736842105,0.3103901221594642,0.34570362747620276
frozenset({'writing_cat_μέτριο'}),"frozenset({'lunch_free/reduced', 'reading_cat_μέτριο', 'test preparation course_none'})",0.491,0.095,0.075,0.15274949083503056,1.607889377210848,1.0,0.028354999999999998,1.0681610576923077,0.7427635887360837,0.14677103718199608,0.0638115920829067,0.47111158752277843
"frozenset({'lunch_free/reduced', 'reading_cat_μέτριο', 'test preparation course_none'})",frozenset({'writing_cat_μέτριο'}),0.095,0.491,0.075,0.7894736842105263,1.607889377210848,1.0,0.028354999999999998,2.4177500000000003,0.41775322283609573,0.14677103718199608,0.5863923068969084,0.47111158752277843
"frozenset({'math_cat_χαμηλό', 'gender_female'})","frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό'})",0.205,0.252,0.083,0.40487804878048783,1.6066589237320945,1.0,0.03134000000000001,1.2568852459016393,0.4749564294915512,0.22192513368983963,0.20438241815573238,0.3671215640727836
"frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό'})","frozenset({'math_cat_χαμηλό', 'gender_female'})",0.252,0.205,0.083,0.32936507936507936,1.6066589237320945,1.0,0.03134000000000001,1.1854437869822485,0.5047999484569294,0.22192513368983963,0.15643406209443944,0.3671215640727836
"frozenset({'test preparation course_completed', 'writing_cat_μέτριο', 'lunch_standard'})",frozenset({'reading_cat_μέτριο'}),0.117,0.49,0.092,0.7863247863247863,1.604744461887319,1.0,0.03467,2.3868,0.4267812299965532,0.1786407766990291,0.5810289927936987,0.48703994418280133
frozenset({'reading_cat_μέτριο'}),"frozenset({'test preparation course_completed', 'writing_cat_μέτριο', 'lunch_standard'})",0.49,0.117,0.092,0.18775510204081633,1.604744461887319,1.0,0.03467,1.0871105527638192,0.7389173060528559,0.1786407766990291,0.08013035338710796,0.48703994418280133
"frozenset({'lunch_free/reduced', 'gender_male'})","frozenset({'test preparation course_none', 'reading_cat_χαμηλό'})",0.166,0.214,0.057,0.3433734939759036,1.6045490372705777,1.0,0.021476000000000002,1.1970275229357799,0.45176490386638063,0.17647058823529413,0.16459732058003004,0.30486431708140976
"frozenset({'test preparation course_none', 'reading_cat_χαμηλό'})","frozenset({'lunch_free/reduced', 'gender_male'})",0.214,0.166,0.057,0.2663551401869159,1.6045490372705777,1.0,0.021476000000000002,1.1367898089171975,0.47935360028570156,0.17647058823529413,0.12032990430085841,0.30486431708140976
frozenset({'writing_cat_χαμηλό'}),"frozenset({'math_cat_χαμηλό', 'gender_female'})",0.301,0.205,0.099,0.3289036544850499,1.60440807065878,1.0,0.03729500000000001,1.1846287128712873,0.538937298593951,0.24324324324324323,0.15585365344031432,0.4059152418766713
"frozenset({'math_cat_χαμηλό', 'gender_female'})",frozenset({'writing_cat_χαμηλό'}),0.205,0.301,0.099,0.4829268292682927,1.6044080706587798,1.0,0.03729500000000001,1.3518396226415095,0.4738580776316626,0.24324324324324323,0.26026728078439587,0.4059152418766713
frozenset({'math_cat_χαμηλό'}),"frozenset({'race/ethnicity_group C', 'lunch_free/reduced'})",0.339,0.114,0.062,0.18289085545722714,1.6043057496247994,1.0,0.023353999999999993,1.0843104693140795,0.5698599385095894,0.15856777493606136,0.07775491586594484,0.3633752522900171
"frozenset({'race/ethnicity_group C', 'lunch_free/reduced'})",frozenset({'math_cat_χαμηλό'}),0.114,0.339,0.062,0.543859649122807,1.6043057496247994,1.0,0.023353999999999993,1.4491153846153848,0.4251438141702467,0.15856777493606136,0.30992382620696973,0.3633752522900171
frozenset({'gender_male'}),"frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό', 'lunch_standard'})",0.482,0.119,0.092,0.19087136929460582,1.6039610865092928,1.0,0.034642000000000006,1.088825641025641,0.7269179117005204,0.18074656188605107,0.08157930680431989,0.4819903064960424
"frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό', 'lunch_standard'})",frozenset({'gender_male'}),0.119,0.482,0.092,0.773109243697479,1.6039610865092926,1.0,0.034642000000000006,2.283037037037037,0.4274046291269803,0.18074656188605107,0.5619869569449402,0.4819903064960424
"frozenset({'math_cat_χαμηλό', 'reading_cat_μέτριο', 'gender_female', 'test preparation course_none'})",frozenset({'writing_cat_μέτριο'}),0.07,0.491,0.055,0.7857142857142857,1.6002327611288916,1.0,0.020629999999999996,2.3753333333333333,0.4033235581622678,0.10869565217391307,0.579006455234353,0.44886528949665405
frozenset({'writing_cat_μέτριο'}),"frozenset({'math_cat_χαμηλό', 'reading_cat_μέτριο', 'gender_female', 'test preparation course_none'})",0.491,0.07,0.055,0.1120162932790224,1.6002327611288913,1.0,0.020629999999999996,1.0473165137614677,0.7369173066619038,0.10869565217391307,0.04517880997744344,0.44886528949665405
frozenset({'writing_cat_μέτριο'}),"frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",0.491,0.079,0.062,0.12627291242362526,1.5983912965015856,1.0,0.023211000000000002,1.0541048951048952,0.735502883579441,0.12204724409448818,0.05132780936332818,0.45554151950295185
"frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",frozenset({'writing_cat_μέτριο'}),0.079,0.491,0.062,0.7848101265822784,1.5983912965015854,1.0,0.023211000000000002,2.36535294117647,0.40648313544184095,0.12204724409448818,0.5772301111636119,0.45554151950295185
"frozenset({'math_cat_μέτριο', 'gender_female'})",frozenset({'reading_cat_υψηλό'}),0.245,0.235,0.092,0.37551020408163266,1.5979157620495008,1.0,0.034425000000000004,1.225,0.49560898358767647,0.23711340206185566,0.1836734693877551,0.38349978289188014
frozenset({'reading_cat_υψηλό'}),"frozenset({'math_cat_μέτριο', 'gender_female'})",0.235,0.245,0.092,0.39148936170212767,1.5979157620495008,1.0,0.034425000000000004,1.2407342657342657,0.48913043478260876,0.23711340206185566,0.1940256446385797,0.38349978289188014
"frozenset({'math_cat_μέτριο', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none', 'gender_male'})",0.332,0.115,0.061,0.18373493975903613,1.5976951283394445,1.0,0.022819999999999993,1.0842066420664207,0.5600274860115834,0.15803108808290156,0.07766659859778093,0.35708486118386584
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none', 'gender_male'})","frozenset({'math_cat_μέτριο', 'lunch_standard'})",0.115,0.332,0.061,0.5304347826086956,1.5976951283394445,1.0,0.022819999999999993,1.4225925925925922,0.4227100120403815,0.15803108808290156,0.2970580577974485,0.35708486118386584
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'gender_male'})","frozenset({'writing_cat_μέτριο', 'test preparation course_none'})",0.175,0.308,0.086,0.49142857142857144,1.5955473098330242,1.0,0.0321,1.3606741573033707,0.45243128964059204,0.21662468513853902,0.26507018992568127,0.3853246753246753
"frozenset({'writing_cat_μέτριο', 'test preparation course_none'})","frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'gender_male'})",0.308,0.175,0.086,0.2792207792207792,1.5955473098330242,1.0,0.0321,1.1445945945945946,0.5393870143836538,0.21662468513853902,0.12632821723730814,0.3853246753246753
"frozenset({'writing_cat_μέτριο', 'parental level of education_some high school'})",frozenset({'reading_cat_μέτριο'}),0.087,0.49,0.068,0.781609195402299,1.5951208069434675,1.0,0.02537000000000001,2.3352631578947385,0.4086399072224729,0.13359528487229866,0.5717827360829393,0.4601923528031903
frozenset({'reading_cat_μέτριο'}),"frozenset({'writing_cat_μέτριο', 'parental level of education_some high school'})",0.49,0.087,0.068,0.13877551020408163,1.595120806943467,1.0,0.02537000000000001,1.0601184834123223,0.7315455594002309,0.13359528487229866,0.05670921161454725,0.4601923528031903
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'writing_cat_μέτριο', ""parental level of education_associate's degree""})",0.325,0.11,0.057,0.1753846153846154,1.5944055944055944,1.0,0.021249999999999998,1.0792910447761195,0.5523066926575698,0.15079365079365079,0.07346585998271392,0.3467832167832168
"frozenset({'writing_cat_μέτριο', ""parental level of education_associate's degree""})","frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο'})",0.11,0.325,0.057,0.5181818181818182,1.5944055944055944,1.0,0.021249999999999998,1.4009433962264153,0.4188842893751231,0.15079365079365079,0.28619528619528617,0.3467832167832168
frozenset({'reading_cat_μέτριο'}),"frozenset({'race/ethnicity_group C', 'math_cat_μέτριο', 'test preparation course_none'})",0.49,0.096,0.075,0.15306122448979592,1.594387755102041,1.0,0.02796,1.0673734939759036,0.7309803921568627,0.14677103718199608,0.0631208235506592,0.467155612244898
"frozenset({'race/ethnicity_group C', 'math_cat_μέτριο', 'test preparation course_none'})",frozenset({'reading_cat_μέτριο'}),0.096,0.49,0.075,0.78125,1.594387755102041,1.0,0.02796,2.3314285714285714,0.41238938053097346,0.14677103718199608,0.571078431372549,0.467155612244898
"frozenset({'gender_male', 'test preparation course_none'})",frozenset({'reading_cat_χαμηλό'}),0.308,0.275,0.135,0.4383116883116883,1.5938606847697756,1.0,0.0503,1.2907514450867053,0.538428602012417,0.30133928571428575,0.22525750111957007,0.4646103896103896
frozenset({'reading_cat_χαμηλό'}),"frozenset({'gender_male', 'test preparation course_none'})",0.275,0.308,0.135,0.4909090909090909,1.5938606847697756,1.0,0.0503,1.3592857142857144,0.5139208173690932,0.30133928571428575,0.26431949553336836,0.4646103896103896
"frozenset({'test preparation course_completed', 'writing_cat_μέτριο'})","frozenset({'math_cat_μέτριο', 'gender_male'})",0.183,0.24,0.07,0.38251366120218583,1.5938069216757744,1.0,0.026080000000000006,1.2307964601769912,0.4560237803811856,0.198300283286119,0.187517975266034,0.33709016393442626
"frozenset({'math_cat_μέτριο', 'gender_male'})","frozenset({'test preparation course_completed', 'writing_cat_μέτριο'})",0.24,0.183,0.07,0.2916666666666667,1.5938069216757742,1.0,0.026080000000000006,1.1534117647058824,0.4902255639097745,0.198300283286119,0.1330069359445125,0.33709016393442626
"frozenset({'math_cat_μέτριο', 'gender_female', 'lunch_standard'})",frozenset({'writing_cat_υψηλό'}),0.172,0.208,0.057,0.33139534883720934,1.5932468694096604,1.0,0.021224000000000007,1.1845565217391305,0.44969912704466486,0.17647058823529413,0.15580220812778958,0.30271690518783545
frozenset({'writing_cat_υψηλό'}),"frozenset({'math_cat_μέτριο', 'gender_female', 'lunch_standard'})",0.208,0.172,0.057,0.27403846153846156,1.5932468694096604,1.0,0.021224000000000007,1.1405562913907286,0.470139996455786,0.17647058823529413,0.12323485693050915,0.30271690518783545
"frozenset({'writing_cat_μέτριο', 'gender_male'})","frozenset({'reading_cat_μέτριο', 'test preparation course_none'})",0.227,0.318,0.115,0.5066079295154186,1.5931066965893665,1.0,0.042814000000000005,1.3822678571428573,0.48162438832330284,0.26744186046511625,0.2765512162982677,0.4341215748206024
"frozenset({'reading_cat_μέτριο', 'test preparation course_none'})","frozenset({'writing_cat_μέτριο', 'gender_male'})",0.318,0.227,0.115,0.3616352201257862,1.5931066965893665,1.0,0.042814000000000005,1.2109064039408868,0.5458880530409282,0.26744186046511625,0.17417234168924475,0.4341215748206024
"frozenset({'math_cat_μέτριο', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'gender_male'})",0.332,0.191,0.101,0.3042168674698795,1.5927584684286886,1.0,0.037587999999999996,1.1627186147186146,0.5571233770083595,0.23933649289099526,0.1399466841407658,0.4165063394941021
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'gender_male'})","frozenset({'math_cat_μέτριο', 'lunch_standard'})",0.191,0.332,0.101,0.5287958115183247,1.5927584684286886,1.0,0.037587999999999996,1.4176444444444445,0.4600227637102399,0.23933649289099526,0.29460450826096507,0.4165063394941021
"frozenset({'reading_cat_μέτριο', 'gender_female'})","frozenset({'test preparation course_completed', 'writing_cat_μέτριο'})",0.261,0.183,0.076,0.2911877394636015,1.5911898331344345,1.0,0.028236999999999998,1.1526324324324324,0.5027597749448045,0.20652173913043478,0.13242073373757834,0.3532441429558445
"frozenset({'test preparation course_completed', 'writing_cat_μέτριο'})","frozenset({'reading_cat_μέτριο', 'gender_female'})",0.183,0.261,0.076,0.41530054644808745,1.5911898331344345,1.0,0.028236999999999998,1.2638971962616823,0.45476067770405204,0.20652173913043478,0.20879640926669477,0.3532441429558445
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})","frozenset({""parental level of education_associate's degree"", 'reading_cat_μέτριο'})",0.32,0.112,0.057,0.178125,1.5904017857142858,1.0,0.021159999999999998,1.0804562737642585,0.5459236326109391,0.152,0.07446509009009009,0.3435267857142857
"frozenset({""parental level of education_associate's degree"", 'reading_cat_μέτριο'})","frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})",0.112,0.32,0.057,0.5089285714285714,1.5904017857142856,1.0,0.021159999999999998,1.3847272727272726,0.4180496285759443,0.152,0.27783613445378147,0.3435267857142857
frozenset({'writing_cat_χαμηλό'}),"frozenset({'parental level of education_high school', 'test preparation course_none'})",0.301,0.14,0.067,0.22259136212624586,1.589938300901756,1.0,0.02486,1.1062393162393163,0.5308222834326223,0.179144385026738,0.09603646758865797,0.3505813953488372
"frozenset({'parental level of education_high school', 'test preparation course_none'})",frozenset({'writing_cat_χαμηλό'}),0.14,0.301,0.067,0.47857142857142854,1.589938300901756,1.0,0.02486,1.3405479452054796,0.431447414092329,0.179144385026738,0.25403637849989774,0.3505813953488372
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'race/ethnicity_group D'})","frozenset({'math_cat_μέτριο', 'test preparation course_none'})",0.12,0.304,0.058,0.4833333333333334,1.5899122807017547,1.0,0.021520000000000004,1.3470967741935485,0.42163009404388724,0.15846994535519127,0.25766283524904227,0.33706140350877195
"frozenset({'math_cat_μέτριο', 'test preparation course_none'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'race/ethnicity_group D'})",0.304,0.12,0.058,0.19078947368421054,1.5899122807017545,1.0,0.021520000000000004,1.087479674796748,0.5330955212049149,0.15846994535519127,0.08044258373205743,0.33706140350877195
frozenset({'lunch_free/reduced'}),"frozenset({'math_cat_χαμηλό', 'reading_cat_χαμηλό'})",0.355,0.218,0.123,0.34647887323943666,1.5893526295387004,1.0,0.04561,1.1965948275862068,0.5749038885737694,0.2733333333333334,0.16429523432153026,0.4553495283628376
"frozenset({'math_cat_χαμηλό', 'reading_cat_χαμηλό'})",frozenset({'lunch_free/reduced'}),0.218,0.355,0.123,0.5642201834862385,1.5893526295387,1.0,0.04561,1.4801052631578946,0.47418543239140826,0.2733333333333334,0.32437237749804415,0.4553495283628376
"frozenset({'reading_cat_μέτριο', 'gender_male'})","frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο'})",0.229,0.154,0.056,0.2445414847161572,1.5879317189360858,1.0,0.020734000000000002,1.1198497109826588,0.48022049286640733,0.1712538226299694,0.10702303157938203,0.30408892417626043
"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο'})","frozenset({'reading_cat_μέτριο', 'gender_male'})",0.154,0.229,0.056,0.36363636363636365,1.5879317189360858,1.0,0.020734000000000002,1.2115714285714285,0.4376477541371159,0.1712538226299694,0.17462563376960263,0.30408892417626043
"frozenset({'math_cat_μέτριο', 'gender_female', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'test preparation course_none'})",0.172,0.308,0.084,0.4883720930232559,1.5856236786469349,1.0,0.03102400000000001,1.3525454545454547,0.4460547504025765,0.21212121212121215,0.26065331361742183,0.38054968287526436
"frozenset({'writing_cat_μέτριο', 'test preparation course_none'})","frozenset({'math_cat_μέτριο', 'gender_female', 'lunch_standard'})",0.308,0.172,0.084,0.27272727272727276,1.5856236786469349,1.0,0.03102400000000001,1.1385,0.5337186897880541,0.21212121212121215,0.1216512955643391,0.38054968287526436
frozenset({'math_cat_μέτριο'}),"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'race/ethnicity_group D', 'lunch_standard'})",0.485,0.082,0.063,0.12989690721649486,1.5841086245914007,1.0,0.02323,1.055047393364929,0.7159808907381723,0.125,0.052175280192260184,0.44909479507166206
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'race/ethnicity_group D', 'lunch_standard'})",frozenset({'math_cat_μέτριο'}),0.082,0.485,0.063,0.7682926829268293,1.5841086245914007,1.0,0.02323,2.2226315789473685,0.4016668395753363,0.125,0.5500828794695715,0.44909479507166206
frozenset({'writing_cat_μέτριο'}),"frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο', 'test preparation course_none'})",0.491,0.117,0.091,0.18533604887983707,1.584068793844761,1.0,0.03355299999999999,1.0838824999999999,0.7243895593600896,0.1760154738878143,0.07739076883333755,0.48155691332880735
"frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο', 'test preparation course_none'})",frozenset({'writing_cat_μέτριο'}),0.117,0.491,0.091,0.7777777777777777,1.584068793844761,1.0,0.03355299999999999,2.290499999999999,0.41756997249635974,0.1760154738878143,0.5634141017245141,0.48155691332880735
"frozenset({'writing_cat_μέτριο', 'parental level of education_some high school', 'lunch_standard'})",frozenset({'reading_cat_μέτριο'}),0.067,0.49,0.052,0.7761194029850745,1.5839171489491317,1.0,0.01917,2.277999999999999,0.39512738065792724,0.10297029702970299,0.5610184372256364,0.4411209259823332
frozenset({'reading_cat_μέτριο'}),"frozenset({'writing_cat_μέτριο', 'parental level of education_some high school', 'lunch_standard'})",0.49,0.067,0.052,0.10612244897959183,1.5839171489491317,1.0,0.01917,1.0437671232876713,0.7228506787330317,0.10297029702970299,0.04193188529431064,0.4411209259823332
"frozenset({'writing_cat_μέτριο', 'lunch_standard'})","frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο', 'gender_female'})",0.338,0.099,0.053,0.15680473372781065,1.5838861992708146,1.0,0.019537999999999993,1.0685543859649123,0.5568602861540215,0.13802083333333331,0.06415619725617164,0.346079134540673
"frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο', 'gender_female'})","frozenset({'writing_cat_μέτριο', 'lunch_standard'})",0.099,0.338,0.053,0.5353535353535354,1.5838861992708146,1.0,0.019537999999999993,1.4247391304347825,0.4091470692940756,0.13802083333333331,0.2981171228905368,0.346079134540673
"frozenset({'lunch_free/reduced', 'math_cat_μέτριο'})","frozenset({'writing_cat_μέτριο', 'gender_male'})",0.153,0.227,0.055,0.35947712418300654,1.583599666004434,1.0,0.020269000000000002,1.206826530612245,0.435097134270688,0.1692307692307692,0.17138049700259575,0.30088393654084244
"frozenset({'writing_cat_μέτριο', 'gender_male'})","frozenset({'lunch_free/reduced', 'math_cat_μέτριο'})",0.227,0.153,0.055,0.2422907488986784,1.583599666004434,1.0,0.020269000000000002,1.1178430232558139,0.47674938257085736,0.1692307692307692,0.10542001050611383,0.30088393654084244
"frozenset({'writing_cat_μέτριο', 'lunch_standard'})","frozenset({'test preparation course_completed', 'reading_cat_μέτριο'})",0.338,0.172,0.092,0.27218934911242604,1.582496215769919,1.0,0.033864,1.137658536585366,0.5560225929331407,0.2200956937799043,0.12100162936283339,0.40353653502132936
"frozenset({'test preparation course_completed', 'reading_cat_μέτριο'})","frozenset({'writing_cat_μέτριο', 'lunch_standard'})",0.172,0.338,0.092,0.5348837209302326,1.582496215769919,1.0,0.033864,1.4233,0.4445494643982356,0.2200956937799043,0.2974074334293544,0.40353653502132936
"frozenset({'math_cat_μέτριο', 'gender_female'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",0.245,0.191,0.074,0.3020408163265306,1.581365530505396,1.0,0.027204999999999993,1.159093567251462,0.48693395382137095,0.20441988950276244,0.13725688050251006,0.34473768565017626
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'gender_female'})",0.191,0.245,0.074,0.38743455497382195,1.5813655305053957,1.0,0.027204999999999993,1.2325213675213675,0.45443156382587774,0.20441988950276244,0.18865503970042644,0.34473768565017626
"frozenset({'math_cat_μέτριο', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'race/ethnicity_group D'})",0.332,0.12,0.063,0.1897590361445783,1.5813253012048194,1.0,0.02316,1.086096654275093,0.5503279155973767,0.16195372750642673,0.07927163198247536,0.3573795180722892
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'race/ethnicity_group D'})","frozenset({'math_cat_μέτριο', 'lunch_standard'})",0.12,0.332,0.063,0.525,1.5813253012048192,1.0,0.02316,1.4063157894736842,0.4177489177489177,0.16195372750642673,0.2889221556886228,0.3573795180722892
"frozenset({'reading_cat_μέτριο', 'gender_female'})","frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})",0.261,0.32,0.132,0.5057471264367817,1.5804597701149425,1.0,0.048479999999999995,1.3758139534883722,0.496986099151187,0.2939866369710468,0.2731575388776201,0.4591235632183909
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})","frozenset({'reading_cat_μέτριο', 'gender_female'})",0.32,0.261,0.132,0.41250000000000003,1.5804597701149425,1.0,0.048479999999999995,1.257872340425532,0.5401069518716577,0.2939866369710468,0.20500676589986472,0.4591235632183909
"frozenset({'math_cat_μέτριο', 'race/ethnicity_group D', 'gender_male'})",frozenset({'reading_cat_μέτριο'}),0.075,0.49,0.058,0.7733333333333334,1.578231292517007,1.0,0.021250000000000005,2.250000000000001,0.3960857409133272,0.11439842209072981,0.5555555555555558,0.44585034013605446
frozenset({'reading_cat_μέτριο'}),"frozenset({'math_cat_μέτριο', 'race/ethnicity_group D', 'gender_male'})",0.49,0.075,0.058,0.11836734693877551,1.578231292517007,1.0,0.021250000000000005,1.0491898148148149,0.7183908045977012,0.11439842209072981,0.04688361831218975,0.44585034013605446
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'gender_male'})","frozenset({'writing_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",0.175,0.221,0.061,0.3485714285714286,1.5772462831286362,1.0,0.022324999999999998,1.1958333333333333,0.44361649279682064,0.18208955223880596,0.1637630662020906,0.31229476405947
"frozenset({'writing_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'gender_male'})",0.221,0.175,0.061,0.27601809954751133,1.5772462831286362,1.0,0.022324999999999998,1.1395312499999999,0.469812075169932,0.18208955223880596,0.12244618126971073,0.31229476405947
"frozenset({'reading_cat_μέτριο', 'gender_male'})",frozenset({'math_cat_μέτριο'}),0.229,0.485,0.175,0.7641921397379912,1.5756538963669922,1.0,0.06393499999999999,2.1839814814814806,0.4738558458402816,0.32467532467532473,0.5421206596854198,0.5625084410030162
frozenset({'math_cat_μέτριο'}),"frozenset({'reading_cat_μέτριο', 'gender_male'})",0.485,0.229,0.175,0.36082474226804123,1.5756538963669922,1.0,0.06393499999999999,1.2062419354838712,0.7094036061026352,0.32467532467532473,0.17097891344752428,0.5625084410030162
"frozenset({'writing_cat_μέτριο', 'lunch_standard'})","frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο'})",0.338,0.169,0.09,0.2662721893491124,1.5755750849059904,1.0,0.03287799999999999,1.1325725806451612,0.5518294729775092,0.21582733812949637,0.11705437948148302,0.39940828402366857
"frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο'})","frozenset({'writing_cat_μέτριο', 'lunch_standard'})",0.169,0.338,0.09,0.5325443786982248,1.5755750849059904,1.0,0.03287799999999999,1.416177215189873,0.43960422516379183,0.21582733812949637,0.29387368383417645,0.39940828402366857
"frozenset({'math_cat_υψηλό', 'writing_cat_υψηλό'})","frozenset({'gender_female', 'lunch_standard'})",0.114,0.329,0.059,0.5175438596491228,1.5730816402708898,1.0,0.021493999999999992,1.3907999999999998,0.41117955388912253,0.15364583333333331,0.2809893586425078,0.3484375833200021
"frozenset({'gender_female', 'lunch_standard'})","frozenset({'math_cat_υψηλό', 'writing_cat_υψηλό'})",0.329,0.114,0.059,0.17933130699088143,1.5730816402708896,1.0,0.021493999999999992,1.0796074074074073,0.5429285912753541,0.15364583333333331,0.07373736680686391,0.3484375833200021
"frozenset({'writing_cat_υψηλό', 'test preparation course_none', 'reading_cat_υψηλό'})",frozenset({'gender_female'}),0.081,0.518,0.066,0.8148148148148149,1.5730015730015732,1.0,0.024042,2.6028000000000007,0.3963794638440993,0.12382739212007507,0.615798370985093,0.47111397111397113
frozenset({'gender_female'}),"frozenset({'writing_cat_υψηλό', 'test preparation course_none', 'reading_cat_υψηλό'})",0.518,0.081,0.066,0.1274131274131274,1.573001573001573,1.0,0.024042,1.0531902654867256,0.7557525462089778,0.12382739212007507,0.05050394713071535,0.47111397111397113
"frozenset({'lunch_free/reduced', 'reading_cat_μέτριο'})","frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})",0.163,0.32,0.082,0.5030674846625767,1.572085889570552,1.0,0.02984,1.3683950617283949,0.43476993909724043,0.20448877805486287,0.2692168892096716,0.3796587423312883
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})","frozenset({'lunch_free/reduced', 'reading_cat_μέτριο'})",0.32,0.163,0.082,0.25625,1.572085889570552,1.0,0.02984,1.1253781512605041,0.5351506456241033,0.20448877805486287,0.11140979689366783,0.3796587423312883
frozenset({'writing_cat_υψηλό'}),frozenset({'test preparation course_completed'}),0.208,0.358,0.117,0.5625000000000001,1.57122905027933,1.0,0.04253600000000002,1.4674285714285717,0.45903479236812583,0.2605790645879733,0.3185358255451715,0.44465782122905034
frozenset({'test preparation course_completed'}),frozenset({'writing_cat_υψηλό'}),0.358,0.208,0.117,0.3268156424581006,1.5712290502793298,1.0,0.04253600000000002,1.1764979253112033,0.5662859120803049,0.2605790645879733,0.15001975057840983,0.44465782122905034
"frozenset({'writing_cat_υψηλό', 'reading_cat_υψηλό'})",frozenset({'test preparation course_completed'}),0.185,0.358,0.104,0.5621621621621622,1.570285369168051,1.0,0.03777,1.4662962962962964,0.44561113732892876,0.23690205011389523,0.31800959838343024,0.4263324777291258
frozenset({'test preparation course_completed'}),"frozenset({'writing_cat_υψηλό', 'reading_cat_υψηλό'})",0.358,0.185,0.104,0.2905027932960894,1.570285369168051,1.0,0.03777,1.1487007874015747,0.5656901509705248,0.23690205011389523,0.12945128011790114,0.4263324777291258
frozenset({'test preparation course_completed'}),"frozenset({'writing_cat_υψηλό', 'math_cat_μέτριο', 'gender_female'})",0.358,0.089,0.05,0.13966480446927376,1.5692674659468964,1.0,0.01813800000000001,1.0588896103896104,0.5650467289719628,0.12594458438287157,0.05561449447779777,0.3507312786391313
"frozenset({'writing_cat_υψηλό', 'math_cat_μέτριο', 'gender_female'})",frozenset({'test preparation course_completed'}),0.089,0.358,0.05,0.5617977528089888,1.5692674659468961,1.0,0.01813800000000001,1.4650769230769232,0.39819978046103194,0.12594458438287157,0.3174419825685184,0.3507312786391313
"frozenset({'math_cat_μέτριο', 'test preparation course_none'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})",0.304,0.283,0.135,0.4440789473684211,1.5691835596057284,1.0,0.048968000000000025,1.289751479289941,0.5211579395487445,0.29867256637168144,0.22465683036041992,0.4605553747442812
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'test preparation course_none'})",0.283,0.304,0.135,0.4770318021201414,1.5691835596057284,1.0,0.048968000000000025,1.3308648648648649,0.505893899478279,0.29867256637168144,0.24860891109215721,0.4605553747442812
"frozenset({'reading_cat_μέτριο', 'race/ethnicity_group D', 'lunch_standard'})",frozenset({'math_cat_μέτριο'}),0.092,0.485,0.07,0.7608695652173914,1.5688032272523533,1.0,0.025380000000000007,2.1536363636363642,0.3993077407174324,0.13806706114398426,0.5356690586745463,0.45259973106230394
frozenset({'math_cat_μέτριο'}),"frozenset({'reading_cat_μέτριο', 'race/ethnicity_group D', 'lunch_standard'})",0.485,0.092,0.07,0.1443298969072165,1.5688032272523533,1.0,0.025380000000000007,1.0611566265060242,0.7040221914008322,0.13806706114398426,0.05763204505200056,0.45259973106230394
frozenset({'writing_cat_μέτριο'}),"frozenset({'lunch_free/reduced', 'reading_cat_μέτριο', 'gender_male'})",0.491,0.074,0.057,0.11608961303462323,1.5687785545219357,1.0,0.020666000000000004,1.0476175115207373,0.7123013821390413,0.11220472440944884,0.04545314582572703,0.4431799416524468
"frozenset({'lunch_free/reduced', 'reading_cat_μέτριο', 'gender_male'})",frozenset({'writing_cat_μέτριο'}),0.074,0.491,0.057,0.7702702702702703,1.5687785545219355,1.0,0.020666000000000004,2.2156470588235297,0.3915349929900345,0.11220472440944884,0.5486645781341263,0.4431799416524468
"frozenset({'math_cat_μέτριο', 'lunch_standard'})","frozenset({'parental level of education_high school', 'writing_cat_μέτριο'})",0.332,0.096,0.05,0.15060240963855423,1.5687751004016066,1.0,0.018128,1.0642836879432624,0.542754491017964,0.13227513227513227,0.06040089561786972,0.3357178714859438
"frozenset({'parental level of education_high school', 'writing_cat_μέτριο'})","frozenset({'math_cat_μέτριο', 'lunch_standard'})",0.096,0.332,0.05,0.5208333333333334,1.5687751004016064,1.0,0.018128,1.394086956521739,0.4010619469026548,0.13227513227513227,0.282684630738523,0.3357178714859438
"frozenset({'writing_cat_μέτριο', 'lunch_standard'})","frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο', 'test preparation course_none'})",0.338,0.117,0.062,0.1834319526627219,1.567794467202751,1.0,0.022453999999999995,1.0813550724637682,0.5470714355325991,0.15776081424936386,0.07523437447646872,0.3566732412886259
"frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο', 'test preparation course_none'})","frozenset({'writing_cat_μέτριο', 'lunch_standard'})",0.117,0.338,0.062,0.5299145299145299,1.567794467202751,1.0,0.022453999999999995,1.4082545454545452,0.41014868666203913,0.15776081424936386,0.28990110258992435,0.3566732412886259
frozenset({'writing_cat_μέτριο'}),"frozenset({'parental level of education_high school', 'math_cat_μέτριο', 'lunch_standard'})",0.491,0.065,0.05,0.10183299389002037,1.5666614444618518,1.0,0.018085000000000004,1.0410090702947847,0.7106090373280944,0.09881422924901186,0.039393576352962964,0.4355318815603948
"frozenset({'parental level of education_high school', 'math_cat_μέτριο', 'lunch_standard'})",frozenset({'writing_cat_μέτριο'}),0.065,0.491,0.05,0.7692307692307693,1.5666614444618518,1.0,0.018085000000000004,2.205666666666667,0.3868449197860963,0.09881422924901186,0.5466223364062265,0.4355318815603948
"frozenset({'writing_cat_υψηλό', 'reading_cat_υψηλό'})","frozenset({'math_cat_μέτριο', 'gender_female'})",0.185,0.245,0.071,0.38378378378378375,1.5664644236072807,1.0,0.025674999999999996,1.225219298245614,0.443705175840318,0.19777158774373257,0.1838195811705745,0.33678985107556536
"frozenset({'math_cat_μέτριο', 'gender_female'})","frozenset({'writing_cat_υψηλό', 'reading_cat_υψηλό'})",0.245,0.185,0.071,0.2897959183673469,1.5664644236072807,1.0,0.025674999999999996,1.1475574712643677,0.4789665143176942,0.19777158774373257,0.1285839489169901,0.33678985107556536
frozenset({'math_cat_μέτριο'}),"frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο', 'gender_male'})",0.485,0.07,0.053,0.10927835051546392,1.5611192930780557,1.0,0.019049999999999997,1.0440972222222222,0.6979300238138852,0.1055776892430279,0.042234785500498834,0.43321060382916043
"frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο', 'gender_male'})",frozenset({'math_cat_μέτριο'}),0.07,0.485,0.053,0.757142857142857,1.5611192930780557,1.0,0.019049999999999997,2.1205882352941163,0.386488131466829,0.1055776892430279,0.5284327323162272,0.43321060382916043
"frozenset({'writing_cat_μέτριο', 'gender_female', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'test preparation course_none'})",0.177,0.304,0.084,0.47457627118644075,1.5611061552185552,1.0,0.03019200000000001,1.3246451612903227,0.43672973442110763,0.2115869017632242,0.24508084940580568,0.3754460303300625
"frozenset({'math_cat_μέτριο', 'test preparation course_none'})","frozenset({'writing_cat_μέτριο', 'gender_female', 'lunch_standard'})",0.304,0.177,0.084,0.27631578947368424,1.561106155218555,1.0,0.03019200000000001,1.1372363636363636,0.5164203612479477,0.2115869017632242,0.12067532135320079,0.3754460303300625
"frozenset({'reading_cat_μέτριο', 'parental level of education_some college', 'lunch_standard'})",frozenset({'math_cat_μέτριο'}),0.074,0.485,0.056,0.7567567567567568,1.560323209807746,1.0,0.020110000000000003,2.117222222222223,0.38780468991052147,0.1113320079522863,0.527683022828654,0.436110337141265
frozenset({'math_cat_μέτριο'}),"frozenset({'reading_cat_μέτριο', 'parental level of education_some college', 'lunch_standard'})",0.485,0.074,0.056,0.1154639175257732,1.560323209807746,1.0,0.020110000000000003,1.046876456876457,0.6972954230235784,0.1113320079522863,0.0447774487319365,0.436110337141265
"frozenset({'writing_cat_χαμηλό', 'lunch_standard'})",frozenset({'gender_male'}),0.145,0.482,0.109,0.7517241379310345,1.5595936471598228,1.0,0.039110000000000006,2.0863888888888895,0.4196577069585279,0.21042471042471042,0.5207029689788312,0.4889326083846044
frozenset({'gender_male'}),"frozenset({'writing_cat_χαμηλό', 'lunch_standard'})",0.482,0.145,0.109,0.22614107883817428,1.5595936471598226,1.0,0.039110000000000006,1.10485254691689,0.6926782614856011,0.21042471042471042,0.0949018465943559,0.4889326083846044
frozenset({'writing_cat_μέτριο'}),"frozenset({'math_cat_μέτριο', 'gender_female', 'test preparation course_none', 'lunch_standard'})",0.491,0.11,0.084,0.17107942973523424,1.555267543047584,1.0,0.029990000000000003,1.0736855036855038,0.7014220226400973,0.16247582205029015,0.06862857273621825,0.46735789668579897
"frozenset({'math_cat_μέτριο', 'gender_female', 'test preparation course_none', 'lunch_standard'})",frozenset({'writing_cat_μέτριο'}),0.11,0.491,0.084,0.7636363636363637,1.555267543047584,1.0,0.029990000000000003,2.153461538461539,0.40115034777956127,0.16247582205029015,0.5356313627433471,0.46735789668579897
frozenset({'math_cat_μέτριο'}),"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard', 'gender_male'})",0.485,0.134,0.101,0.20824742268041238,1.5540852438836743,1.0,0.03601,1.0937760416666666,0.692300297990964,0.19498069498069498,0.08573605390347849,0.4809893829819972
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard', 'gender_male'})",frozenset({'math_cat_μέτριο'}),0.134,0.485,0.101,0.753731343283582,1.5540852438836743,1.0,0.03601,2.091212121212121,0.41170283310086203,0.19498069498069498,0.5218084335603534,0.4809893829819972
"frozenset({'math_cat_μέτριο', 'race/ethnicity_group D', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.097,0.418,0.063,0.6494845360824743,1.553790756178168,1.0,0.022454000000000002,1.6604117647058827,0.39469844785459407,0.13938053097345132,0.3977397527190279,0.40010111971587825
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'math_cat_μέτριο', 'race/ethnicity_group D', 'lunch_standard'})",0.418,0.097,0.063,0.1507177033492823,1.553790756178168,1.0,0.022454000000000002,1.0632507042253523,0.6123929525991382,0.13938053097345132,0.05948804357617088,0.40010111971587825
"frozenset({'race/ethnicity_group C', 'math_cat_μέτριο'})","frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",0.153,0.223,0.053,0.3464052287581699,1.553386676045605,1.0,0.018881000000000002,1.18881,0.4205965561025596,0.16408668730650153,0.15882268823445292,0.29203669509657376
"frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})","frozenset({'race/ethnicity_group C', 'math_cat_μέτριο'})",0.223,0.153,0.053,0.23766816143497757,1.553386676045605,1.0,0.018881000000000002,1.1110647058823528,0.4584881377334208,0.16408668730650153,0.09996241019477872,0.29203669509657376
"frozenset({'gender_female', 'lunch_standard'})",frozenset({'reading_cat_υψηλό'}),0.329,0.235,0.12,0.364741641337386,1.5520920907973874,1.0,0.042685,1.2042344497607658,0.5301167411823149,0.27027027027027023,0.16959691678089675,0.43768996960486317
frozenset({'reading_cat_υψηλό'}),"frozenset({'gender_female', 'lunch_standard'})",0.235,0.329,0.12,0.5106382978723404,1.5520920907973872,1.0,0.042685,1.3711739130434781,0.4649782135076253,0.27027027027027023,0.27069791039096924,0.43768996960486317
"frozenset({'test preparation course_completed', 'lunch_standard'})","frozenset({'gender_female', 'reading_cat_υψηλό'})",0.227,0.159,0.056,0.24669603524229075,1.551547391460948,1.0,0.019907,1.1164152046783626,0.4598734060247644,0.16969696969696968,0.10427590397418639,0.29944864655196296
"frozenset({'gender_female', 'reading_cat_υψηλό'})","frozenset({'test preparation course_completed', 'lunch_standard'})",0.159,0.227,0.056,0.3522012578616352,1.5515473914609479,1.0,0.019907,1.193271844660194,0.42268982503821984,0.16969696969696968,0.16196799205903648,0.29944864655196296
"frozenset({'writing_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'gender_female'})",0.221,0.245,0.084,0.3800904977375566,1.5513897866839046,1.0,0.029855000000000007,1.217919708029197,0.4562473256311511,0.21989528795811522,0.17892781157292265,0.36147382029734976
"frozenset({'math_cat_μέτριο', 'gender_female'})","frozenset({'writing_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",0.245,0.221,0.084,0.34285714285714286,1.5513897866839044,1.0,0.029855000000000007,1.1854347826086957,0.47075055187637976,0.21989528795811522,0.15642765450210894,0.36147382029734976
frozenset({'gender_male'}),"frozenset({'writing_cat_χαμηλό', 'test preparation course_none', 'reading_cat_χαμηλό', 'lunch_standard'})",0.482,0.099,0.074,0.15352697095435686,1.550777484387443,1.0,0.026281999999999993,1.0644166666666668,0.6856412396952936,0.14595660749506903,0.06051828074845377,0.45050085921455213
"frozenset({'writing_cat_χαμηλό', 'test preparation course_none', 'reading_cat_χαμηλό', 'lunch_standard'})",frozenset({'gender_male'}),0.099,0.482,0.074,0.7474747474747474,1.550777484387443,1.0,0.026281999999999993,2.0512799999999993,0.39418663946965826,0.14595660749506903,0.5124995124995124,0.45050085921455213
"frozenset({'reading_cat_μέτριο', 'race/ethnicity_group D'})","frozenset({'math_cat_μέτριο', 'lunch_standard'})",0.136,0.332,0.07,0.5147058823529412,1.5503189227498229,1.0,0.024848000000000002,1.3764848484848484,0.41084656084656085,0.17587939698492464,0.27351179992955277,0.3627746279234586
"frozenset({'math_cat_μέτριο', 'lunch_standard'})","frozenset({'reading_cat_μέτριο', 'race/ethnicity_group D'})",0.332,0.136,0.07,0.21084337349397592,1.5503189227498229,1.0,0.024848000000000002,1.0948396946564884,0.5313943541488453,0.17587939698492464,0.08662427487728692,0.3627746279234586
"frozenset({'writing_cat_υψηλό', 'test preparation course_none', 'reading_cat_υψηλό', 'lunch_standard'})",frozenset({'gender_female'}),0.066,0.518,0.053,0.803030303030303,1.5502515502515501,1.0,0.018811999999999995,2.4470769230769225,0.3800250494929497,0.09981167608286251,0.5913491764114169,0.45267345267345266
frozenset({'gender_female'}),"frozenset({'writing_cat_υψηλό', 'test preparation course_none', 'reading_cat_υψηλό', 'lunch_standard'})",0.518,0.066,0.053,0.1023166023166023,1.55025155025155,1.0,0.018811999999999995,1.0404559139784946,0.7363970876066701,0.09981167608286251,0.03888287185931724,0.45267345267345266
frozenset({'test preparation course_completed'}),"frozenset({'math_cat_υψηλό', 'writing_cat_υψηλό', 'reading_cat_υψηλό'})",0.358,0.11,0.061,0.17039106145251398,1.549009649568309,1.0,0.02162,1.0727946127946129,0.5520657780501507,0.1498771498771499,0.06785512522754379,0.3624682579989843
"frozenset({'math_cat_υψηλό', 'writing_cat_υψηλό', 'reading_cat_υψηλό'})",frozenset({'test preparation course_completed'}),0.11,0.358,0.061,0.5545454545454546,1.549009649568309,1.0,0.02162,1.4412244897959186,0.3982317185485357,0.1498771498771499,0.3061455678278109,0.3624682579989843
frozenset({'gender_female'}),"frozenset({'writing_cat_υψηλό', 'test preparation course_none'})",0.518,0.091,0.073,0.14092664092664092,1.548644405787263,1.0,0.025861999999999996,1.0581168539325845,0.7350082419143977,0.13619402985074625,0.05492479749905492,0.4715622215622215
"frozenset({'writing_cat_υψηλό', 'test preparation course_none'})",frozenset({'gender_female'}),0.091,0.518,0.073,0.8021978021978021,1.5486444057872628,1.0,0.025861999999999996,2.4367777777777766,0.38974034389740336,0.13619402985074625,0.5896219962610002,0.4715622215622215
"frozenset({'reading_cat_μέτριο', 'race/ethnicity_group D'})","frozenset({'math_cat_μέτριο', 'test preparation course_none'})",0.136,0.304,0.064,0.47058823529411764,1.5479876160990713,1.0,0.022656000000000003,1.3146666666666667,0.40972222222222227,0.1702127659574468,0.2393509127789047,0.34055727554179566
"frozenset({'math_cat_μέτριο', 'test preparation course_none'})","frozenset({'reading_cat_μέτριο', 'race/ethnicity_group D'})",0.304,0.136,0.064,0.2105263157894737,1.5479876160990713,1.0,0.022656000000000003,1.0944,0.5086206896551725,0.1702127659574468,0.08625730994152048,0.34055727554179566
frozenset({'lunch_free/reduced'}),"frozenset({'math_cat_χαμηλό', 'test preparation course_none', 'reading_cat_χαμηλό', 'writing_cat_χαμηλό'})",0.355,0.162,0.089,0.2507042253521127,1.5475569466179795,1.0,0.03149,1.1183834586466166,0.5485584879365909,0.20794392523364483,0.10585229755622037,0.4000434707007477
"frozenset({'math_cat_χαμηλό', 'test preparation course_none', 'reading_cat_χαμηλό', 'writing_cat_χαμηλό'})",frozenset({'lunch_free/reduced'}),0.162,0.355,0.089,0.5493827160493827,1.5475569466179795,1.0,0.03149,1.4313698630136986,0.42221983856694645,0.20794392523364483,0.3013685520145469,0.4000434707007477
frozenset({'reading_cat_μέτριο'}),"frozenset({'race/ethnicity_group C', 'math_cat_μέτριο', 'gender_male'})",0.49,0.07,0.053,0.10816326530612244,1.5451895043731776,1.0,0.018699999999999994,1.04279176201373,0.691823899371069,0.10453648915187376,0.04103576914636821,0.4326530612244897
"frozenset({'race/ethnicity_group C', 'math_cat_μέτριο', 'gender_male'})",frozenset({'reading_cat_μέτριο'}),0.07,0.49,0.053,0.757142857142857,1.5451895043731776,1.0,0.018699999999999994,2.0999999999999988,0.3793872996551024,0.10453648915187376,0.5238095238095235,0.4326530612244897
"frozenset({'race/ethnicity_group C', 'math_cat_μέτριο', 'test preparation course_none'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.096,0.418,0.062,0.6458333333333334,1.545055821371611,1.0,0.021872000000000003,1.6432941176470592,0.39023693976591495,0.13716814159292035,0.3914662084765178,0.397079346092504
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'race/ethnicity_group C', 'math_cat_μέτριο', 'test preparation course_none'})",0.418,0.096,0.062,0.14832535885167464,1.5450558213716108,1.0,0.021872000000000003,1.061438202247191,0.6061412260281566,0.13716814159292035,0.05788203412795866,0.397079346092504
frozenset({'math_cat_μέτριο'}),"frozenset({'reading_cat_μέτριο', 'lunch_standard', 'gender_male'})",0.485,0.155,0.116,0.23917525773195877,1.543066178915863,1.0,0.040825000000000014,1.1106368563685638,0.6833779712085707,0.22137404580152673,0.09961568962361986,0.4937811772530762
"frozenset({'reading_cat_μέτριο', 'lunch_standard', 'gender_male'})",frozenset({'math_cat_μέτριο'}),0.155,0.485,0.116,0.7483870967741936,1.543066178915863,1.0,0.040825000000000014,2.046794871794872,0.4164966333401348,0.22137404580152673,0.5114312558722205,0.4937811772530762
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none', 'gender_male'})",frozenset({'math_cat_μέτριο'}),0.115,0.485,0.086,0.7478260869565216,1.5419094576423127,1.0,0.030224999999999995,2.042241379310344,0.3971225857311785,0.16731517509727625,0.5103419164204304,0.46257283729269383
frozenset({'math_cat_μέτριο'}),"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none', 'gender_male'})",0.485,0.115,0.086,0.17731958762886596,1.5419094576423127,1.0,0.030224999999999995,1.0757518796992482,0.682433958004064,0.16731517509727625,0.07041761313996153,0.46257283729269383
"frozenset({'reading_cat_μέτριο', 'test preparation course_none'})","frozenset({'lunch_free/reduced', 'writing_cat_μέτριο'})",0.318,0.153,0.075,0.23584905660377356,1.5414971019854482,1.0,0.026345999999999994,1.1084197530864197,0.5150733137829911,0.1893939393939394,0.09781470673408922,0.36302256751757306
"frozenset({'race/ethnicity_group C', 'math_cat_μέτριο'})","frozenset({'reading_cat_μέτριο', 'test preparation course_none'})",0.153,0.318,0.075,0.49019607843137253,1.5414971019854482,1.0,0.026345999999999994,1.3377692307692308,0.4147343565525383,0.1893939393939394,0.2524869185210741,0.36302256751757306
"frozenset({'reading_cat_μέτριο', 'test preparation course_none'})","frozenset({'race/ethnicity_group C', 'math_cat_μέτριο'})",0.318,0.153,0.075,0.23584905660377356,1.5414971019854482,1.0,0.026345999999999994,1.1084197530864197,0.5150733137829911,0.1893939393939394,0.09781470673408922,0.36302256751757306
"frozenset({'lunch_free/reduced', 'writing_cat_μέτριο'})","frozenset({'reading_cat_μέτριο', 'test preparation course_none'})",0.153,0.318,0.075,0.49019607843137253,1.5414971019854482,1.0,0.026345999999999994,1.3377692307692308,0.4147343565525383,0.1893939393939394,0.2524869185210741,0.36302256751757306
"frozenset({'writing_cat_μέτριο', 'gender_male'})","frozenset({'lunch_free/reduced', 'reading_cat_μέτριο'})",0.227,0.163,0.057,0.2511013215859031,1.5404989054349882,1.0,0.019999000000000003,1.117641176470588,0.4538934658768526,0.17117117117117117,0.10525844872867753,0.3003972865598227
"frozenset({'lunch_free/reduced', 'reading_cat_μέτριο'})","frozenset({'writing_cat_μέτριο', 'gender_male'})",0.163,0.227,0.057,0.3496932515337423,1.540498905434988,1.0,0.019999000000000003,1.1886698113207548,0.4191871554633298,0.17117117117117117,0.15872348193239624,0.3003972865598227
"frozenset({'writing_cat_μέτριο', 'gender_female'})","frozenset({'reading_cat_μέτριο', 'math_cat_μέτριο'})",0.264,0.325,0.132,0.5,1.5384615384615383,1.0,0.046200000000000005,1.35,0.4755434782608696,0.2888402625820569,0.25925925925925924,0.45307692307692304
"frozenset({'reading_cat_μέτριο', 'math_cat_μέτριο'})","frozenset({'writing_cat_μέτριο', 'gender_female'})",0.325,0.264,0.132,0.40615384615384614,1.5384615384615383,1.0,0.046200000000000005,1.2393782383419687,0.5185185185185185,0.2888402625820569,0.19314381270903008,0.45307692307692304
"frozenset({'reading_cat_μέτριο', 'lunch_standard'})","frozenset({'test preparation course_completed', 'writing_cat_μέτριο'})",0.327,0.183,0.092,0.28134556574923547,1.5374074631105763,1.0,0.032159,1.1368468085106382,0.5193972478842303,0.2200956937799043,0.12037400948498835,0.392038903093197
"frozenset({'test preparation course_completed', 'writing_cat_μέτριο'})","frozenset({'reading_cat_μέτριο', 'lunch_standard'})",0.183,0.327,0.092,0.5027322404371585,1.5374074631105763,1.0,0.032159,1.3533956043956046,0.427851098930339,0.2200956937799043,0.2611177421057332,0.392038903093197
"frozenset({'reading_cat_μέτριο', 'gender_male'})","frozenset({'math_cat_μέτριο', 'test preparation course_none'})",0.229,0.304,0.107,0.4672489082969432,1.5370029878188922,1.0,0.037384,1.3064262295081965,0.4531558723347516,0.25117370892018775,0.23455302916227475,0.40961129625373477
"frozenset({'math_cat_μέτριο', 'test preparation course_none'})","frozenset({'reading_cat_μέτριο', 'gender_male'})",0.304,0.229,0.107,0.3519736842105263,1.5370029878188922,1.0,0.037384,1.189766497461929,0.5019873240949619,0.25117370892018775,0.15949894190729744,0.40961129625373477
frozenset({'math_cat_μέτριο'}),"frozenset({'test preparation course_completed', 'writing_cat_μέτριο', 'gender_male'})",0.485,0.094,0.07,0.1443298969072165,1.5354244351831543,1.0,0.024410000000000008,1.0588192771084337,0.677115117891817,0.13752455795677804,0.0555517625907467,0.4445053739855232
"frozenset({'writing_cat_μέτριο', 'race/ethnicity_group D', 'lunch_standard'})",frozenset({'math_cat_μέτριο'}),0.094,0.485,0.07,0.7446808510638299,1.5354244351831543,1.0,0.024410000000000008,2.017083333333334,0.3848943550930307,0.13752455795677804,0.5042346622598638,0.4445053739855232
frozenset({'math_cat_μέτριο'}),"frozenset({'writing_cat_μέτριο', 'race/ethnicity_group D', 'lunch_standard'})",0.485,0.094,0.07,0.1443298969072165,1.5354244351831543,1.0,0.024410000000000008,1.0588192771084337,0.677115117891817,0.13752455795677804,0.0555517625907467,0.4445053739855232
"frozenset({'test preparation course_completed', 'writing_cat_μέτριο', 'gender_male'})",frozenset({'math_cat_μέτριο'}),0.094,0.485,0.07,0.7446808510638299,1.5354244351831543,1.0,0.024410000000000008,2.017083333333334,0.3848943550930307,0.13752455795677804,0.5042346622598638,0.4445053739855232
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'race/ethnicity_group D'})",0.283,0.145,0.063,0.22261484098939932,1.5352747654441334,1.0,0.021965000000000005,1.0998409090909091,0.48626331053109306,0.17260273972602744,0.09077759180046706,0.3285487998050445
"frozenset({'math_cat_μέτριο', 'race/ethnicity_group D'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})",0.145,0.283,0.063,0.4344827586206897,1.5352747654441332,1.0,0.021965000000000005,1.2678658536585368,0.4077787060243202,0.17260273972602744,0.2112730245755784,0.3285487998050445
"frozenset({'math_cat_υψηλό', 'reading_cat_υψηλό'})",frozenset({'test preparation course_completed'}),0.131,0.358,0.072,0.5496183206106869,1.535246705616444,1.0,0.025101999999999992,1.4254576271186439,0.401195499296765,0.1726618705035971,0.29847090437801704,0.37536781952322057
frozenset({'test preparation course_completed'}),"frozenset({'math_cat_υψηλό', 'reading_cat_υψηλό'})",0.358,0.131,0.072,0.20111731843575417,1.535246705616444,1.0,0.025101999999999992,1.0877692307692308,0.5430512287988922,0.1726618705035971,0.08068736298705888,0.37536781952322057
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})","frozenset({'test preparation course_completed', 'writing_cat_μέτριο'})",0.228,0.183,0.064,0.2807017543859649,1.5338893682293164,1.0,0.022276000000000004,1.135829268292683,0.45085816062176176,0.1844380403458213,0.11958599068049561,0.3152142651711245
"frozenset({'test preparation course_completed', 'writing_cat_μέτριο'})","frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})",0.183,0.228,0.064,0.34972677595628415,1.5338893682293164,1.0,0.022276000000000004,1.1871932773109244,0.4260250917992657,0.1844380403458213,0.1576771709278292,0.3152142651711245
"frozenset({'lunch_free/reduced', 'test preparation course_none'})","frozenset({'writing_cat_χαμηλό', 'gender_male'})",0.224,0.201,0.069,0.3080357142857143,1.5325159914712154,1.0,0.023976000000000004,1.1546838709677418,0.44778126400717166,0.19382022471910113,0.1339620954764885,0.32565964818763327
"frozenset({'writing_cat_χαμηλό', 'gender_male'})","frozenset({'lunch_free/reduced', 'test preparation course_none'})",0.201,0.224,0.069,0.34328358208955223,1.5325159914712152,1.0,0.023976000000000004,1.1816363636363636,0.43489144038744093,0.19382022471910113,0.15371595630096935,0.32565964818763327
"frozenset({'reading_cat_μέτριο', 'race/ethnicity_group D'})","frozenset({'writing_cat_μέτριο', 'gender_female'})",0.136,0.264,0.055,0.4044117647058823,1.5318627450980389,1.0,0.019095999999999995,1.2357530864197528,0.40185185185185174,0.15942028985507245,0.19077685421994878,0.3063725490196078
"frozenset({'writing_cat_μέτριο', 'gender_female'})","frozenset({'reading_cat_μέτριο', 'race/ethnicity_group D'})",0.264,0.136,0.055,0.20833333333333331,1.5318627450980389,1.0,0.019095999999999995,1.0913684210526315,0.47173913043478244,0.15942028985507245,0.0837191358024691,0.3063725490196078
"frozenset({'parental level of education_high school', 'test preparation course_none'})","frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό'})",0.14,0.252,0.054,0.3857142857142857,1.530612244897959,1.0,0.018719999999999994,1.217674418604651,0.4031007751937983,0.15976331360946744,0.17876241405653165,0.3
"frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό'})","frozenset({'parental level of education_high school', 'test preparation course_none'})",0.252,0.14,0.054,0.21428571428571427,1.5306122448979589,1.0,0.018719999999999994,1.0945454545454545,0.4634581105169339,0.15976331360946744,0.08637873754152821,0.3
"frozenset({'math_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'gender_female'})",0.213,0.227,0.074,0.3474178403755869,1.5304750677338628,1.0,0.025648999999999998,1.1845251798561152,0.4404169099213572,0.20218579234972678,0.155779871119776,0.3367045149014498
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'gender_female'})","frozenset({'math_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",0.227,0.213,0.074,0.32599118942731276,1.5304750677338628,1.0,0.025648999999999998,1.167640522875817,0.44839341281773365,0.20218579234972678,0.14357203230916488,0.3367045149014498
"frozenset({'reading_cat_μέτριο', 'gender_female', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'test preparation course_none'})",0.172,0.304,0.08,0.4651162790697675,1.5299877600979195,1.0,0.027712000000000007,1.3012173913043479,0.4183574879227054,0.20202020202020204,0.2314889067094361,0.3641370869033048
"frozenset({'math_cat_μέτριο', 'test preparation course_none'})","frozenset({'reading_cat_μέτριο', 'gender_female', 'lunch_standard'})",0.304,0.172,0.08,0.2631578947368421,1.5299877600979193,1.0,0.027712000000000007,1.1237142857142857,0.4977011494252875,0.20202020202020204,0.11009407576913297,0.3641370869033048
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'race/ethnicity_group D'})",frozenset({'math_cat_μέτριο'}),0.12,0.485,0.089,0.7416666666666667,1.5292096219931273,1.0,0.0308,1.9935483870967745,0.39325842696629215,0.17248062015503873,0.4983818770226538,0.462585910652921
frozenset({'math_cat_μέτριο'}),"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'race/ethnicity_group D'})",0.485,0.12,0.089,0.18350515463917524,1.529209621993127,1.0,0.0308,1.0777777777777777,0.6719755645249263,0.17248062015503873,0.07216494845360824,0.462585910652921
"frozenset({'math_cat_μέτριο', 'test preparation course_none'})","frozenset({'reading_cat_μέτριο', 'lunch_standard'})",0.304,0.327,0.152,0.5,1.529051987767584,1.0,0.052592,1.346,0.4971264367816093,0.3173277661795407,0.25705794947994054,0.48241590214067276
"frozenset({'reading_cat_μέτριο', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'test preparation course_none'})",0.327,0.304,0.152,0.4648318042813455,1.5290519877675839,1.0,0.052592,1.300525714285714,0.5141158989598812,0.3173277661795407,0.23108017856515164,0.48241590214067276
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})","frozenset({'math_cat_μέτριο', 'lunch_standard'})",0.266,0.332,0.135,0.5075187969924813,1.528671075278558,1.0,0.04668799999999999,1.3563969465648855,0.4711676253910585,0.2915766738660907,0.2627526901085049,0.4570726515082888
"frozenset({'math_cat_μέτριο', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})",0.332,0.266,0.135,0.4066265060240964,1.5286710752785577,1.0,0.04668799999999999,1.236994923857868,0.5177201153249057,0.2915766738660907,0.1915892452644365,0.4570726515082888
"frozenset({'reading_cat_μέτριο', 'lunch_standard', 'gender_male'})","frozenset({'math_cat_μέτριο', 'test preparation course_none'})",0.155,0.304,0.072,0.46451612903225803,1.5280135823429541,1.0,0.02488,1.299759036144578,0.40894148586456286,0.18604651162790697,0.23062662217278454,0.35067911714770794
"frozenset({'math_cat_μέτριο', 'test preparation course_none'})","frozenset({'reading_cat_μέτριο', 'lunch_standard', 'gender_male'})",0.304,0.155,0.072,0.23684210526315788,1.5280135823429541,1.0,0.02488,1.1072413793103448,0.4964878671775224,0.18604651162790697,0.09685456244160696,0.35067911714770794
"frozenset({'math_cat_χαμηλό', 'gender_female'})","frozenset({'lunch_free/reduced', 'reading_cat_μέτριο'})",0.205,0.163,0.051,0.24878048780487805,1.52626066137962,1.0,0.017584999999999996,1.1141883116883116,0.43371562461462565,0.1608832807570978,0.1024856485123991,0.28083196169385005
"frozenset({'lunch_free/reduced', 'reading_cat_μέτριο'})","frozenset({'math_cat_χαμηλό', 'gender_female'})",0.163,0.205,0.051,0.31288343558282206,1.52626066137962,1.0,0.017584999999999996,1.1570089285714287,0.41195211656944736,0.1608832807570978,0.13570243469537366,0.28083196169385005
"frozenset({'math_cat_μέτριο', 'lunch_standard'})","frozenset({'reading_cat_μέτριο', 'gender_male'})",0.332,0.229,0.116,0.3493975903614458,1.5257536697006366,1.0,0.039971999999999994,1.1850555555555555,0.5158476151145983,0.2606741573033708,0.15615770474895693,0.42797390435102856
"frozenset({'reading_cat_μέτριο', 'gender_male'})","frozenset({'math_cat_μέτριο', 'lunch_standard'})",0.229,0.332,0.116,0.5065502183406113,1.5257536697006364,1.0,0.039971999999999994,1.353734513274336,0.44693412048839387,0.2606741573033708,0.2613027220667834,0.42797390435102856
frozenset({'writing_cat_χαμηλό'}),"frozenset({'test preparation course_none', 'race/ethnicity_group B'})",0.301,0.122,0.056,0.18604651162790697,1.5249714067861229,1.0,0.019278000000000003,1.0786857142857142,0.4924892703862661,0.15258855585831063,0.07294591301583939,0.32253145253526494
"frozenset({'test preparation course_none', 'race/ethnicity_group B'})",frozenset({'writing_cat_χαμηλό'}),0.122,0.301,0.056,0.45901639344262296,1.5249714067861229,1.0,0.019278000000000003,1.2920909090909094,0.3920842824601367,0.15258855585831063,0.22606064870189263,0.32253145253526494
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})","frozenset({'race/ethnicity_group C', 'math_cat_μέτριο'})",0.266,0.153,0.062,0.23308270676691728,1.5234163840974986,1.0,0.021302,1.1044215686274508,0.4680935220181067,0.1736694677871148,0.0945486502561007,0.319155732468426
"frozenset({'race/ethnicity_group C', 'math_cat_μέτριο'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})",0.153,0.266,0.062,0.40522875816993464,1.5234163840974986,1.0,0.021302,1.234087912087912,0.40564420916327076,0.1736694677871148,0.18968495663478832,0.319155732468426
"frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'gender_male'})",frozenset({'math_cat_μέτριο'}),0.145,0.485,0.107,0.7379310344827587,1.5215072875933169,1.0,0.036675,1.9651315789473687,0.40088539104771276,0.2045889101338432,0.49112822229661873,0.4792747955918948
frozenset({'math_cat_μέτριο'}),"frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'gender_male'})",0.485,0.145,0.107,0.22061855670103092,1.5215072875933169,1.0,0.036675,1.0970238095238094,0.6655475909627075,0.2045889101338432,0.08844275637547477,0.4792747955918948
"frozenset({'math_cat_υψηλό', 'writing_cat_υψηλό'})",frozenset({'test preparation course_completed'}),0.114,0.358,0.062,0.543859649122807,1.5191610310692936,1.0,0.021188,1.4074615384615385,0.38571324546712293,0.15121951219512195,0.28950101109471504,0.35852200333235323
frozenset({'test preparation course_completed'}),"frozenset({'math_cat_υψηλό', 'writing_cat_υψηλό'})",0.358,0.114,0.062,0.17318435754189945,1.5191610310692933,1.0,0.021188,1.071581081081081,0.5323083107225404,0.15121951219512195,0.06679950061162467,0.35852200333235323
frozenset({'gender_female'}),"frozenset({'writing_cat_υψηλό', 'test preparation course_none', 'lunch_standard'})",0.518,0.075,0.059,0.1138996138996139,1.5186615186615187,1.0,0.020149999999999994,1.0438997821350764,0.7085589703917292,0.11048689138576778,0.04205363664823124,0.45028314028314026
"frozenset({'writing_cat_υψηλό', 'test preparation course_none', 'lunch_standard'})",frozenset({'gender_female'}),0.075,0.518,0.059,0.7866666666666666,1.5186615186615184,1.0,0.020149999999999994,2.2593749999999995,0.36921667430141997,0.11048689138576778,0.557399723374827,0.45028314028314026
"frozenset({'writing_cat_μέτριο', 'race/ethnicity_group D'})","frozenset({'math_cat_μέτριο', 'lunch_standard'})",0.139,0.332,0.07,0.5035971223021583,1.5168588021149345,1.0,0.023851999999999998,1.3456811594202895,0.3957524473203915,0.1745635910224439,0.25688191961400936,0.35722024789806706
"frozenset({'math_cat_μέτριο', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'race/ethnicity_group D'})",0.332,0.139,0.07,0.21084337349397592,1.5168588021149345,1.0,0.023851999999999998,1.0910381679389312,0.5100940975192473,0.1745635910224439,0.08344178106152834,0.35722024789806706
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'math_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",0.418,0.213,0.135,0.3229665071770335,1.516274681582317,1.0,0.04596600000000002,1.1624240282685514,0.585032455135548,0.27217741935483875,0.1397287257649727,0.47838466203922103
"frozenset({'math_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.213,0.418,0.135,0.6338028169014085,1.516274681582317,1.0,0.04596600000000002,1.5893076923076928,0.43264153607228595,0.27217741935483875,0.37079521804365717,0.47838466203922103
"frozenset({'reading_cat_μέτριο', 'race/ethnicity_group D'})",frozenset({'math_cat_μέτριο'}),0.136,0.485,0.1,0.7352941176470588,1.5160703456640388,1.0,0.03404,1.9455555555555553,0.3939814814814815,0.19193857965451055,0.48600799543118206,0.47073984232868404
frozenset({'math_cat_μέτριο'}),"frozenset({'reading_cat_μέτριο', 'race/ethnicity_group D'})",0.485,0.136,0.1,0.2061855670103093,1.5160703456640388,1.0,0.03404,1.0884155844155845,0.6609708737864077,0.19193857965451055,0.08123329515082094,0.47073984232868404
"frozenset({'reading_cat_μέτριο', 'gender_female'})","frozenset({'writing_cat_μέτριο', 'race/ethnicity_group D'})",0.261,0.139,0.055,0.210727969348659,1.5160285564651725,1.0,0.018720999999999995,1.090878640776699,0.4605978595153154,0.15942028985507245,0.08330774604954587,0.30320571129303453
"frozenset({'writing_cat_μέτριο', 'race/ethnicity_group D'})","frozenset({'reading_cat_μέτριο', 'gender_female'})",0.139,0.261,0.055,0.39568345323741005,1.5160285564651725,1.0,0.018720999999999995,1.2228690476190476,0.3953331221623903,0.15942028985507245,0.18225095160677948,0.30320571129303453
"frozenset({'writing_cat_μέτριο', 'test preparation course_none'})","frozenset({'math_cat_χαμηλό', 'reading_cat_μέτριο'})",0.308,0.12,0.056,0.18181818181818182,1.5151515151515154,1.0,0.01904,1.0755555555555556,0.4913294797687862,0.15053763440860216,0.07024793388429754,0.3242424242424242
"frozenset({'math_cat_χαμηλό', 'reading_cat_μέτριο'})","frozenset({'writing_cat_μέτριο', 'test preparation course_none'})",0.12,0.308,0.056,0.4666666666666667,1.5151515151515151,1.0,0.01904,1.2974999999999999,0.38636363636363635,0.15053763440860216,0.2292870905587669,0.3242424242424242
frozenset({'lunch_free/reduced'}),"frozenset({'math_cat_χαμηλό', 'gender_male'})",0.355,0.134,0.072,0.2028169014084507,1.5135589657347066,1.0,0.024429999999999993,1.0863250883392226,0.5260551248923341,0.1726618705035971,0.07946524412061282,0.3700651671221358
"frozenset({'math_cat_χαμηλό', 'gender_male'})",frozenset({'lunch_free/reduced'}),0.134,0.355,0.072,0.5373134328358208,1.5135589657347066,1.0,0.024429999999999993,1.394032258064516,0.3918078008724659,0.1726618705035971,0.2826564850167764,0.3700651671221358
"frozenset({'writing_cat_υψηλό', 'reading_cat_υψηλό', 'lunch_standard'})",frozenset({'test preparation course_completed'}),0.144,0.358,0.078,0.5416666666666667,1.5130353817504658,1.0,0.026448000000000006,1.400727272727273,0.3961179007907981,0.1839622641509434,0.286085150571132,0.3797718808193669
frozenset({'test preparation course_completed'}),"frozenset({'writing_cat_υψηλό', 'reading_cat_υψηλό', 'lunch_standard'})",0.358,0.144,0.078,0.21787709497206706,1.5130353817504658,1.0,0.026448000000000006,1.0944571428571428,0.5281572010543975,0.1839622641509434,0.08630501749073256,0.3797718808193669
"frozenset({'reading_cat_μέτριο', 'gender_male'})","frozenset({'race/ethnicity_group C', 'math_cat_μέτριο'})",0.229,0.153,0.053,0.23144104803493448,1.512686588463624,1.0,0.017963,1.1020625,0.4395908278883098,0.16109422492401215,0.09261044632223671,0.2889231383965522
"frozenset({'race/ethnicity_group C', 'math_cat_μέτριο'})","frozenset({'reading_cat_μέτριο', 'gender_male'})",0.153,0.229,0.053,0.3464052287581699,1.512686588463624,1.0,0.017963,1.17963,0.4001470227885322,0.16109422492401215,0.1522765612946432,0.2889231383965522
frozenset({'math_cat_χαμηλό'}),frozenset({'lunch_free/reduced'}),0.339,0.355,0.182,0.5368731563421828,1.51231875025967,1.0,0.06165499999999999,1.3927070063694265,0.5125018702930956,0.35546875,0.2819738858018338,0.5247746063401055
frozenset({'lunch_free/reduced'}),frozenset({'math_cat_χαμηλό'}),0.355,0.339,0.182,0.5126760563380282,1.51231875025967,1.0,0.06165499999999999,1.3563872832369943,0.5252150949825367,0.35546875,0.26274743772772785,0.5247746063401055
"frozenset({'writing_cat_μέτριο', 'parental level of education_some college', 'lunch_standard'})",frozenset({'math_cat_μέτριο'}),0.075,0.485,0.055,0.7333333333333334,1.5120274914089349,1.0,0.018625000000000003,1.9312500000000006,0.3660933660933661,0.10891089108910894,0.4822006472491911,0.42336769759450177
frozenset({'math_cat_μέτριο'}),"frozenset({'writing_cat_μέτριο', 'parental level of education_some college', 'lunch_standard'})",0.485,0.075,0.055,0.1134020618556701,1.5120274914089347,1.0,0.018625000000000003,1.0433139534883722,0.6575463371579877,0.10891089108910894,0.041515742546670384,0.42336769759450177
frozenset({'lunch_free/reduced'}),"frozenset({'math_cat_χαμηλό', 'gender_female'})",0.355,0.205,0.11,0.3098591549295775,1.5115080728272074,1.0,0.03722500000000001,1.1519387755102042,0.5246652572233969,0.24444444444444446,0.13189830808751887,0.42322226039161803
"frozenset({'math_cat_χαμηλό', 'gender_female'})",frozenset({'lunch_free/reduced'}),0.205,0.355,0.11,0.5365853658536586,1.5115080728272072,1.0,0.03722500000000001,1.391842105263158,0.4256718124642654,0.24444444444444446,0.2815276989979203,0.42322226039161803
frozenset({'gender_male'}),"frozenset({'reading_cat_χαμηλό', 'lunch_standard'})",0.482,0.136,0.099,0.20539419087136931,1.5102514034659507,1.0,0.033448000000000006,1.087331592689295,0.6522366522366523,0.19075144508670522,0.08031735054556632,0.4666676836709788
"frozenset({'reading_cat_χαμηλό', 'lunch_standard'})",frozenset({'gender_male'}),0.136,0.482,0.099,0.7279411764705882,1.5102514034659507,1.0,0.033448000000000006,1.904,0.3910400299289189,0.19075144508670522,0.47478991596638653,0.4666676836709788
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'math_cat_μέτριο', 'gender_male', 'lunch_standard'})",0.418,0.16,0.101,0.24162679425837322,1.5101674641148326,1.0,0.03412000000000001,1.1076340694006308,0.5804497975570754,0.2117400419287212,0.09717475506949193,0.4364383971291866
"frozenset({'math_cat_μέτριο', 'gender_male', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.16,0.418,0.101,0.63125,1.5101674641148326,1.0,0.03412000000000001,1.578305084745763,0.4021687883074023,0.2117400419287212,0.3664089347079037,0.4364383971291866
frozenset({'lunch_free/reduced'}),"frozenset({'writing_cat_χαμηλό', 'test preparation course_none', 'math_cat_χαμηλό'})",0.355,0.183,0.098,0.27605633802816903,1.5085045793889018,1.0,0.03303500000000001,1.1285408560311285,0.522623002689448,0.22272727272727272,0.11390004654610654,0.40578773185561456
"frozenset({'writing_cat_χαμηλό', 'test preparation course_none', 'math_cat_χαμηλό'})",frozenset({'lunch_free/reduced'}),0.183,0.355,0.098,0.5355191256830601,1.5085045793889018,1.0,0.03303500000000001,1.3886470588235296,0.4125971073864063,0.22272727272727272,0.2798746134621088,0.40578773185561456
"frozenset({'lunch_free/reduced', 'reading_cat_μέτριο'})","frozenset({'math_cat_μέτριο', 'gender_male'})",0.163,0.24,0.059,0.36196319018404904,1.5081799591002043,1.0,0.019879999999999995,1.191153846153846,0.4025676852358098,0.17151162790697672,0.16047788182111716,0.30389826175869117
"frozenset({'math_cat_μέτριο', 'gender_male'})","frozenset({'lunch_free/reduced', 'reading_cat_μέτριο'})",0.24,0.163,0.059,0.24583333333333332,1.5081799591002043,1.0,0.019879999999999995,1.1098342541436463,0.44335414808206947,0.17151162790697672,0.09896455595380324,0.30389826175869117
"frozenset({'test preparation course_completed', 'reading_cat_μέτριο', 'lunch_standard'})",frozenset({'math_cat_μέτριο'}),0.104,0.485,0.076,0.7307692307692308,1.5067406819984142,1.0,0.02556,1.9128571428571435,0.37535244360902253,0.14814814814814814,0.47722180731889485,0.44373513084853294
frozenset({'math_cat_μέτριο'}),"frozenset({'test preparation course_completed', 'reading_cat_μέτριο', 'lunch_standard'})",0.485,0.104,0.076,0.15670103092783505,1.506740681998414,1.0,0.02556,1.0624938875305623,0.6530403679100664,0.14814814814814814,0.058818114874815905,0.44373513084853294
frozenset({'writing_cat_χαμηλό'}),"frozenset({'race/ethnicity_group C', 'gender_male'})",0.301,0.139,0.063,0.20930232558139536,1.5057721264848585,1.0,0.021161,1.0889117647058824,0.48052773803846754,0.16710875331564987,0.08165194608756719,0.3312698678266689
"frozenset({'race/ethnicity_group C', 'gender_male'})",frozenset({'writing_cat_χαμηλό'}),0.139,0.301,0.063,0.4532374100719424,1.5057721264848585,1.0,0.021161,1.2784342105263158,0.39011485352948766,0.16710875331564987,0.21779314745628384,0.3312698678266689
frozenset({'reading_cat_μέτριο'}),"frozenset({'lunch_free/reduced', 'math_cat_μέτριο', 'gender_male'})",0.49,0.08,0.059,0.12040816326530612,1.5051020408163265,1.0,0.019799999999999998,1.045939675174014,0.6580259222333,0.11545988258317028,0.04392191659272404,0.42895408163265303
"frozenset({'lunch_free/reduced', 'math_cat_μέτριο', 'gender_male'})",frozenset({'reading_cat_μέτριο'}),0.08,0.49,0.059,0.7374999999999999,1.5051020408163265,1.0,0.019799999999999998,1.9428571428571424,0.36477523949889457,0.11545988258317028,0.4852941176470587,0.42895408163265303
"frozenset({'math_cat_μέτριο', 'race/ethnicity_group D'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})",0.145,0.266,0.058,0.4,1.5037593984962405,1.0,0.019430000000000003,1.2233333333333334,0.39181286549707606,0.16430594900849857,0.18256130790190736,0.3090225563909774
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})","frozenset({'math_cat_μέτριο', 'race/ethnicity_group D'})",0.266,0.145,0.058,0.21804511278195488,1.5037593984962405,1.0,0.019430000000000003,1.0934134615384616,0.45640326975476847,0.16430594900849857,0.08543288044673086,0.3090225563909774
"frozenset({'math_cat_μέτριο', 'test preparation course_none'})","frozenset({'reading_cat_μέτριο', 'parental level of education_some college'})",0.304,0.114,0.052,0.17105263157894737,1.500461680517082,1.0,0.017344,1.068825396825397,0.4792219274977896,0.14207650273224043,0.06439348936675775,0.31359649122807015
"frozenset({'reading_cat_μέτριο', 'parental level of education_some college'})","frozenset({'math_cat_μέτριο', 'test preparation course_none'})",0.114,0.304,0.052,0.45614035087719296,1.500461680517082,1.0,0.017344,1.2797419354838708,0.3764542455287376,0.14207650273224043,0.21859245815688647,0.31359649122807015
"frozenset({'math_cat_χαμηλό', 'test preparation course_none', 'reading_cat_χαμηλό'})",frozenset({'lunch_free/reduced'}),0.169,0.355,0.09,0.5325443786982248,1.5001250104175348,1.0,0.030004999999999997,1.3798101265822784,0.4011899986629228,0.20737327188940088,0.2752626026329067,0.3930327527293941
frozenset({'lunch_free/reduced'}),"frozenset({'math_cat_χαμηλό', 'test preparation course_none', 'reading_cat_χαμηλό'})",0.355,0.169,0.09,0.2535211267605634,1.5001250104175348,1.0,0.030004999999999997,1.1132264150943396,0.516881998277347,0.20737327188940088,0.1017101405060931,0.3930327527293941
"frozenset({'writing_cat_υψηλό', 'lunch_standard'})",frozenset({'test preparation course_completed'}),0.162,0.358,0.087,0.537037037037037,1.500103455410718,1.0,0.029003999999999995,1.38672,0.3978273393136367,0.20092378752886833,0.2788738894657897,0.3900268984067866
frozenset({'test preparation course_completed'}),"frozenset({'writing_cat_υψηλό', 'lunch_standard'})",0.358,0.162,0.087,0.2430167597765363,1.500103455410718,1.0,0.029003999999999995,1.1070258302583025,0.5192824148673326,0.20092378752886833,0.09667871095052065,0.3900268984067866
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'gender_male'})","frozenset({'math_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",0.191,0.213,0.061,0.3193717277486911,1.4993977828577048,1.0,0.020317,1.1562846153846156,0.4117003384060468,0.17784256559766762,0.13516102636428348,0.30287835213725633
"frozenset({'math_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'gender_male'})",0.213,0.191,0.061,0.2863849765258216,1.4993977828577048,1.0,0.020317,1.1336644736842105,0.4232091153373467,0.17784256559766762,0.11790479175008854,0.30287835213725633
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'math_cat_μέτριο', 'gender_female', 'test preparation course_none'})",0.418,0.15,0.094,0.22488038277511962,1.499202551834131,1.0,0.03130000000000001,1.0966049382716048,0.5721283907289612,0.19831223628691985,0.08809456797072898,0.42577352472089314
"frozenset({'math_cat_μέτριο', 'gender_female', 'test preparation course_none'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.15,0.418,0.094,0.6266666666666667,1.499202551834131,1.0,0.03130000000000001,1.5589285714285719,0.39173967459324166,0.19831223628691985,0.3585337915234823,0.42577352472089314
frozenset({'math_cat_μέτριο'}),"frozenset({'writing_cat_μέτριο', 'gender_male'})",0.485,0.227,0.165,0.3402061855670103,1.4987056632907942,1.0,0.05490500000000001,1.1715781250000001,0.646131215063254,0.30164533820840955,0.14645043410997452,0.5335392161315228
"frozenset({'writing_cat_μέτριο', 'gender_male'})",frozenset({'math_cat_μέτριο'}),0.227,0.485,0.165,0.7268722466960352,1.4987056632907942,1.0,0.05490500000000001,1.8855645161290322,0.4304755184444706,0.30164533820840955,0.46965484795346646,0.5335392161315228
"frozenset({'race/ethnicity_group C', 'writing_cat_υψηλό'})",frozenset({'gender_female'}),0.067,0.518,0.052,0.7761194029850745,1.498300005762692,1.0,0.017293999999999997,2.1529333333333325,0.3564597246269271,0.09756097560975611,0.5355174335789928,0.43825275168558747
frozenset({'gender_female'}),"frozenset({'race/ethnicity_group C', 'writing_cat_υψηλό'})",0.518,0.067,0.052,0.10038610038610038,1.498300005762692,1.0,0.017293999999999997,1.0371115879828328,0.6899936163421639,0.09756097560975611,0.0357836017000004,0.43825275168558747
"frozenset({'lunch_free/reduced', 'writing_cat_μέτριο'})","frozenset({'math_cat_μέτριο', 'gender_male'})",0.153,0.24,0.055,0.35947712418300654,1.4978213507625273,1.0,0.018280000000000005,1.186530612244898,0.39240098744230983,0.16272189349112426,0.15720674234606125,0.2943218954248366
"frozenset({'math_cat_μέτριο', 'gender_male'})","frozenset({'lunch_free/reduced', 'writing_cat_μέτριο'})",0.24,0.153,0.055,0.22916666666666669,1.4978213507625273,1.0,0.018280000000000005,1.0988108108108108,0.4373205741626795,0.16272189349112426,0.08992522628886268,0.2943218954248366
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard', 'gender_male'})","frozenset({'math_cat_μέτριο', 'test preparation course_none'})",0.134,0.304,0.061,0.4552238805970149,1.4974469756480755,1.0,0.020263999999999997,1.2775890410958903,0.383598985348124,0.16180371352785144,0.21727569051295248,0.3279408876669285
"frozenset({'math_cat_μέτριο', 'test preparation course_none'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard', 'gender_male'})",0.304,0.134,0.061,0.20065789473684212,1.4974469756480755,1.0,0.020263999999999997,1.0833909465020577,0.47729413981533825,0.16180371352785144,0.07697216482314331,0.3279408876669285
"frozenset({'writing_cat_χαμηλό', 'test preparation course_none', 'lunch_standard'})",frozenset({'gender_male'}),0.122,0.482,0.088,0.7213114754098361,1.4964968369498675,1.0,0.029196,1.8587058823529414,0.37787326568647756,0.17054263565891473,0.4619912652699539,0.45194204475885996
frozenset({'gender_male'}),"frozenset({'writing_cat_χαμηλό', 'test preparation course_none', 'lunch_standard'})",0.482,0.122,0.088,0.1825726141078838,1.4964968369498672,1.0,0.029196,1.0741015228426396,0.6404878904878905,0.17054263565891473,0.0689893099178631,0.45194204475885996
"frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'gender_male'})","frozenset({'math_cat_μέτριο', 'lunch_standard'})",0.145,0.332,0.072,0.496551724137931,1.495637723307021,1.0,0.023859999999999992,1.326849315068493,0.3875893437296945,0.17777777777777778,0.24633491637414826,0.3567095970087245
"frozenset({'math_cat_μέτριο', 'lunch_standard'})","frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'gender_male'})",0.332,0.145,0.072,0.21686746987951805,1.495637723307021,1.0,0.023859999999999992,1.0917692307692308,0.4960911510312707,0.17777777777777778,0.0840555203269217,0.3567095970087245
"frozenset({'math_cat_υψηλό', 'writing_cat_υψηλό', 'reading_cat_υψηλό', 'lunch_standard'})",frozenset({'test preparation course_completed'}),0.099,0.358,0.053,0.5353535353535354,1.495400936741719,1.0,0.017557999999999997,1.381695652173913,0.36768370573576525,0.1311881188118812,0.27625161270021087,0.34169911404548275
frozenset({'test preparation course_completed'}),"frozenset({'math_cat_υψηλό', 'writing_cat_υψηλό', 'reading_cat_υψηλό', 'lunch_standard'})",0.358,0.099,0.053,0.14804469273743018,1.495400936741719,1.0,0.017557999999999997,1.0575672131147542,0.5160171633456767,0.1311881188118812,0.05443362124021107,0.34169911404548275
"frozenset({'writing_cat_μέτριο', 'gender_male', 'test preparation course_none'})","frozenset({'math_cat_μέτριο', 'lunch_standard'})",0.133,0.332,0.066,0.49624060150375937,1.4947006069390343,1.0,0.021843999999999995,1.3260298507462684,0.38174128831568266,0.16541353383458646,0.2458691639277835,0.3475178911133255
"frozenset({'math_cat_μέτριο', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'gender_male', 'test preparation course_none'})",0.332,0.133,0.066,0.19879518072289157,1.4947006069390343,1.0,0.021843999999999995,1.0821203007518798,0.4954636182181092,0.16541353383458646,0.0758883284001056,0.3475178911133255
"frozenset({'math_cat_χαμηλό', 'gender_female', 'test preparation course_none'})",frozenset({'lunch_free/reduced'}),0.147,0.355,0.078,0.5306122448979592,1.4946823799942515,1.0,0.025815000000000005,1.3741304347826089,0.38799711425737227,0.1839622641509434,0.2722670463534252,0.3751652773785571
frozenset({'lunch_free/reduced'}),"frozenset({'math_cat_χαμηλό', 'gender_female', 'test preparation course_none'})",0.355,0.147,0.078,0.21971830985915494,1.4946823799942515,1.0,0.025815000000000005,1.0931949458483754,0.513118664281455,0.1839622641509434,0.08525007017485926,0.3751652773785571
"frozenset({'writing_cat_μέτριο', 'test preparation course_none'})","frozenset({'lunch_free/reduced', 'reading_cat_μέτριο'})",0.308,0.163,0.075,0.2435064935064935,1.493904868137997,1.0,0.024796,1.106420600858369,0.4777649325626204,0.1893939393939394,0.0961845800555478,0.35181459644649826
"frozenset({'lunch_free/reduced', 'reading_cat_μέτριο'})","frozenset({'writing_cat_μέτριο', 'test preparation course_none'})",0.163,0.308,0.075,0.460122699386503,1.493904868137997,1.0,0.024796,1.2817727272727273,0.39499800876144964,0.1893939393939394,0.21983049044292347,0.35181459644649826
"frozenset({'writing_cat_μέτριο', 'gender_female'})","frozenset({'math_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",0.264,0.213,0.084,0.3181818181818182,1.4938113529662826,1.0,0.027768,1.1542666666666666,0.44914596273291924,0.2137404580152672,0.1336490701166686,0.3562740076824584
"frozenset({'math_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'gender_female'})",0.213,0.264,0.084,0.3943661971830986,1.4938113529662826,1.0,0.027768,1.2152558139534884,0.42003993465238704,0.2137404580152672,0.17712798530312313,0.3562740076824584
frozenset({'writing_cat_μέτριο'}),"frozenset({'math_cat_μέτριο', 'test preparation course_none', 'gender_female'})",0.491,0.15,0.11,0.2240325865580448,1.4935505770536321,1.0,0.03635000000000001,1.0954068241469817,0.6492230755492053,0.2071563088512241,0.08709716065652331,0.4786829599456891
"frozenset({'math_cat_μέτριο', 'test preparation course_none', 'gender_female'})",frozenset({'writing_cat_μέτριο'}),0.15,0.491,0.11,0.7333333333333334,1.4935505770536321,1.0,0.03635000000000001,1.9087500000000004,0.3887700534759359,0.2071563088512241,0.4760969220694173,0.4786829599456891
frozenset({'test preparation course_completed'}),"frozenset({'math_cat_υψηλό', 'reading_cat_υψηλό', 'lunch_standard'})",0.358,0.118,0.063,0.17597765363128492,1.4913360477227537,1.0,0.020756000000000004,1.0703593220338983,0.5131780645799338,0.15254237288135594,0.0657343011692573,0.35493797935801535
"frozenset({'math_cat_υψηλό', 'reading_cat_υψηλό', 'lunch_standard'})",frozenset({'test preparation course_completed'}),0.118,0.358,0.063,0.5338983050847458,1.4913360477227537,1.0,0.020756000000000004,1.3773818181818183,0.3735377748983192,0.15254237288135594,0.2739848988858969,0.35493797935801535
"frozenset({'math_cat_μέτριο', 'test preparation course_none'})","frozenset({'writing_cat_μέτριο', 'race/ethnicity_group D'})",0.304,0.139,0.063,0.20723684210526316,1.4909125331313895,1.0,0.020744,1.0860746887966806,0.47308885239919723,0.16578947368421051,0.07925301057521852,0.3302371260886028
"frozenset({'writing_cat_μέτριο', 'race/ethnicity_group D'})","frozenset({'math_cat_μέτριο', 'test preparation course_none'})",0.139,0.304,0.063,0.4532374100719424,1.4909125331313895,1.0,0.020744,1.2729473684210524,0.3824272256328005,0.16578947368421051,0.21442156619531957,0.3302371260886028
frozenset({'reading_cat_μέτριο'}),"frozenset({'math_cat_μέτριο', 'gender_male'})",0.49,0.24,0.175,0.35714285714285715,1.4880952380952381,1.0,0.05739999999999999,1.1822222222222223,0.6431372549019607,0.3153153153153153,0.15413533834586468,0.5431547619047619
"frozenset({'math_cat_μέτριο', 'gender_male'})",frozenset({'reading_cat_μέτριο'}),0.24,0.49,0.175,0.7291666666666666,1.4880952380952381,1.0,0.05739999999999999,1.8830769230769229,0.43157894736842106,0.3153153153153153,0.468954248366013,0.5431547619047619
"frozenset({'writing_cat_χαμηλό', 'gender_male', 'reading_cat_χαμηλό'})","frozenset({'lunch_free/reduced', 'test preparation course_none'})",0.168,0.224,0.056,0.3333333333333333,1.488095238095238,1.0,0.018367999999999995,1.164,0.3942307692307691,0.16666666666666666,0.14089347079037798,0.29166666666666663
"frozenset({'lunch_free/reduced', 'test preparation course_none'})","frozenset({'writing_cat_χαμηλό', 'gender_male', 'reading_cat_χαμηλό'})",0.224,0.168,0.056,0.25,1.488095238095238,1.0,0.018367999999999995,1.1093333333333333,0.4226804123711339,0.16666666666666666,0.0985576923076923,0.29166666666666663
"frozenset({'math_cat_μέτριο', 'gender_male'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})",0.24,0.283,0.101,0.4208333333333334,1.487043580683157,1.0,0.03308000000000001,1.2379856115107917,0.43095362167795737,0.23933649289099532,0.19223616922361703,0.38886189634864554
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'gender_male'})",0.283,0.24,0.101,0.35689045936395764,1.4870435806831568,1.0,0.03308000000000001,1.181758241758242,0.45679881795710964,0.23933649289099532,0.15380323600520743,0.38886189634864554
frozenset({'lunch_free/reduced'}),"frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό'})",0.355,0.252,0.133,0.3746478873239437,1.4866979655712051,1.0,0.04354000000000001,1.1961261261261262,0.5075479396164831,0.2805907172995781,0.1639677638020638,0.45121283255086075
"frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό'})",frozenset({'lunch_free/reduced'}),0.252,0.355,0.133,0.5277777777777778,1.4866979655712051,1.0,0.04354000000000001,1.3658823529411765,0.43765831691528295,0.2805907172995781,0.26787252368647724,0.45121283255086075
frozenset({'writing_cat_χαμηλό'}),"frozenset({'race/ethnicity_group C', 'lunch_free/reduced'})",0.301,0.114,0.051,0.16943521594684385,1.4862738240951214,1.0,0.016686,1.066744,0.46806362029790455,0.1401098901098901,0.06256796382262285,0.3084018184997377
"frozenset({'race/ethnicity_group C', 'lunch_free/reduced'})",frozenset({'writing_cat_χαμηλό'}),0.114,0.301,0.051,0.4473684210526315,1.4862738240951214,1.0,0.016686,1.264857142857143,0.36927366883548,0.1401098901098901,0.20939688276485197,0.3084018184997377
frozenset({'reading_cat_υψηλό'}),frozenset({'test preparation course_completed'}),0.235,0.358,0.125,0.5319148936170213,1.4857957922263165,1.0,0.040870000000000004,1.3715454545454546,0.42739869281045756,0.2670940170940171,0.27089547292370914,0.4405384523951028
frozenset({'test preparation course_completed'}),frozenset({'reading_cat_υψηλό'}),0.358,0.235,0.125,0.34916201117318435,1.4857957922263165,1.0,0.040870000000000004,1.1754077253218884,0.5092834890965733,0.2670940170940171,0.14923138715448936,0.4405384523951028
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'math_cat_μέτριο', 'gender_male'})",0.418,0.24,0.149,0.35645933014354064,1.4852472089314195,1.0,0.04868,1.1809665427509293,0.5613598099587167,0.2927308447937132,0.15323596071518505,0.48864633173843697
"frozenset({'math_cat_μέτριο', 'gender_male'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.24,0.418,0.149,0.6208333333333333,1.4852472089314195,1.0,0.04868,1.5349450549450552,0.4298834334157542,0.2927308447937132,0.3485108820160367,0.48864633173843697
"frozenset({'test preparation course_none', 'reading_cat_χαμηλό', 'lunch_standard'})",frozenset({'gender_male'}),0.109,0.482,0.078,0.7155963302752294,1.4846396893676959,1.0,0.025462,1.8213548387096776,0.36637025525914413,0.15204678362573099,0.45095816655449694,0.4387110282081541
frozenset({'gender_male'}),"frozenset({'test preparation course_none', 'reading_cat_χαμηλό', 'lunch_standard'})",0.482,0.109,0.078,0.16182572614107885,1.4846396893676959,1.0,0.025462,1.0630247524752474,0.6301851301851301,0.15204678362573099,0.059288132593803416,0.4387110282081541
frozenset({'reading_cat_μέτριο'}),"frozenset({'math_cat_μέτριο', 'gender_female', 'test preparation course_none', 'lunch_standard'})",0.49,0.11,0.08,0.163265306122449,1.4842300556586272,1.0,0.026100000000000005,1.063658536585366,0.6397058823529412,0.15384615384615385,0.05984865856454943,0.4452690166975881
"frozenset({'math_cat_μέτριο', 'gender_female', 'test preparation course_none', 'lunch_standard'})",frozenset({'reading_cat_μέτριο'}),0.11,0.49,0.08,0.7272727272727273,1.4842300556586272,1.0,0.026100000000000005,1.87,0.36657303370786526,0.15384615384615385,0.46524064171123,0.4452690166975881
"frozenset({'math_cat_χαμηλό', 'gender_male', 'writing_cat_χαμηλό'})",frozenset({'lunch_free/reduced'}),0.131,0.355,0.069,0.5267175572519084,1.4837114288786153,1.0,0.022495000000000008,1.3628225806451613,0.3751605210053202,0.1654676258992806,0.2662287709331913,0.3605418772175035
frozenset({'lunch_free/reduced'}),"frozenset({'math_cat_χαμηλό', 'gender_male', 'writing_cat_χαμηλό'})",0.355,0.131,0.069,0.1943661971830986,1.4837114288786153,1.0,0.022495000000000008,1.0786538461538462,0.5054488259746097,0.1654676258992806,0.07291852380103407,0.3605418772175035
"frozenset({'writing_cat_μέτριο', 'race/ethnicity_group D'})",frozenset({'math_cat_μέτριο'}),0.139,0.485,0.1,0.7194244604316546,1.4833494029518652,1.0,0.032585,1.8355128205128204,0.3784552845528455,0.19083969465648856,0.45519312705175663,0.462805013720982
frozenset({'math_cat_μέτριο'}),"frozenset({'writing_cat_μέτριο', 'race/ethnicity_group D'})",0.485,0.139,0.1,0.2061855670103093,1.4833494029518652,1.0,0.032585,1.0846363636363636,0.6327184466019418,0.19083969465648856,0.0780320174335764,0.462805013720982
"frozenset({'parental level of education_high school', 'math_cat_μέτριο'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.092,0.418,0.057,0.6195652173913044,1.4822134387351782,1.0,0.018544000000000005,1.529828571428572,0.3582966226138033,0.12582781456953643,0.346331988644853,0.3779644268774704
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'parental level of education_high school', 'math_cat_μέτριο'})",0.418,0.092,0.057,0.13636363636363638,1.4822134387351782,1.0,0.018544000000000005,1.0513684210526315,0.5589919816723942,0.12582781456953643,0.048858630356427735,0.3779644268774704
"frozenset({'math_cat_χαμηλό', 'reading_cat_μέτριο', 'test preparation course_none'})",frozenset({'writing_cat_μέτριο'}),0.077,0.491,0.056,0.7272727272727273,1.4812071838548417,1.0,0.018193,1.8663333333333334,0.35197724810400866,0.10937500000000003,0.4641900339346312,0.42066284021477507
frozenset({'writing_cat_μέτριο'}),"frozenset({'math_cat_χαμηλό', 'reading_cat_μέτριο', 'test preparation course_none'})",0.491,0.077,0.056,0.11405295315682282,1.4812071838548417,1.0,0.018193,1.0418229885057473,0.6382612966601179,0.10937500000000003,0.04014404459027391,0.42066284021477507
"frozenset({'math_cat_μέτριο', 'test preparation course_none'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'gender_male'})",0.304,0.191,0.086,0.28289473684210525,1.4811242766602368,1.0,0.027935999999999996,1.1281467889908257,0.466720128307939,0.2102689486552567,0.11359052761694098,0.36657825847340864
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'gender_male'})","frozenset({'math_cat_μέτριο', 'test preparation course_none'})",0.191,0.304,0.086,0.450261780104712,1.4811242766602368,1.0,0.027935999999999996,1.2660571428571425,0.4015293069250008,0.2102689486552567,0.21014623578263222,0.36657825847340864
"frozenset({'gender_female', 'lunch_standard'})","frozenset({'test preparation course_completed', 'writing_cat_υψηλό'})",0.329,0.117,0.057,0.17325227963525835,1.480788714831268,1.0,0.018506999999999996,1.0680404411764706,0.4838810887128401,0.14652956298200515,0.06370586595159496,0.33021588340737273
"frozenset({'test preparation course_completed', 'writing_cat_υψηλό'})","frozenset({'gender_female', 'lunch_standard'})",0.117,0.329,0.057,0.48717948717948717,1.480788714831268,1.0,0.018506999999999996,1.3084500000000001,0.36770578768552176,0.14652956298200515,0.2357369406549734,0.33021588340737273
frozenset({'reading_cat_χαμηλό'}),"frozenset({'parental level of education_high school', 'test preparation course_none'})",0.275,0.14,0.057,0.20727272727272728,1.4805194805194803,1.0,0.018499999999999996,1.0848623853211008,0.44767090139140947,0.15921787709497207,0.07822410147991542,0.3072077922077922
"frozenset({'parental level of education_high school', 'test preparation course_none'})",frozenset({'reading_cat_χαμηλό'}),0.14,0.275,0.057,0.40714285714285714,1.4805194805194803,1.0,0.018499999999999996,1.2228915662650601,0.3773969808241533,0.15921787709497207,0.18226600985221672,0.3072077922077922
"frozenset({'race/ethnicity_group C', 'math_cat_χαμηλό'})",frozenset({'lunch_free/reduced'}),0.118,0.355,0.062,0.5254237288135594,1.4800668417283362,1.0,0.020110000000000003,1.359107142857143,0.36774925023772953,0.15085158150851583,0.2642228353698595,0.3500358080687515
frozenset({'lunch_free/reduced'}),"frozenset({'race/ethnicity_group C', 'math_cat_χαμηλό'})",0.355,0.118,0.062,0.17464788732394368,1.4800668417283362,1.0,0.020110000000000003,1.0686348122866893,0.5028757189297325,0.15085158150851583,0.06422662961898377,0.3500358080687515
"frozenset({'reading_cat_μέτριο', 'parental level of education_some college'})","frozenset({'math_cat_μέτριο', 'lunch_standard'})",0.114,0.332,0.056,0.49122807017543857,1.479602621010357,1.0,0.018151999999999995,1.312965517241379,0.3658497258948725,0.14358974358974358,0.2383653745141296,0.32995138448530964
"frozenset({'math_cat_μέτριο', 'lunch_standard'})","frozenset({'reading_cat_μέτριο', 'parental level of education_some college'})",0.332,0.114,0.056,0.1686746987951807,1.479602621010357,1.0,0.018151999999999995,1.065768115942029,0.4852437981180495,0.14358974358974358,0.06170959231961705,0.32995138448530964
frozenset({'reading_cat_μέτριο'}),"frozenset({'math_cat_μέτριο', 'lunch_standard', 'gender_male'})",0.49,0.16,0.116,0.23673469387755103,1.4795918367346939,1.0,0.03760000000000001,1.1005347593582886,0.6355645706558486,0.21722846441947566,0.0913508260447036,0.48086734693877553
"frozenset({'math_cat_μέτριο', 'lunch_standard', 'gender_male'})",frozenset({'reading_cat_μέτριο'}),0.16,0.49,0.116,0.725,1.4795918367346939,1.0,0.03760000000000001,1.8545454545454545,0.3858784893267653,0.21722846441947566,0.46078431372549017,0.48086734693877553
"frozenset({'reading_cat_μέτριο', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'gender_male'})",0.327,0.24,0.116,0.3547400611620795,1.4780835881753314,1.0,0.03752000000000001,1.17781990521327,0.4806066506122868,0.2572062084257207,0.15097376468694673,0.4190366972477064
"frozenset({'math_cat_μέτριο', 'gender_male'})","frozenset({'reading_cat_μέτριο', 'lunch_standard'})",0.24,0.327,0.116,0.4833333333333334,1.4780835881753314,1.0,0.03752000000000001,1.3025806451612905,0.42558983666061717,0.2572062084257207,0.2322932144626053,0.4190366972477064
frozenset({'math_cat_υψηλό'}),"frozenset({'writing_cat_μέτριο', 'gender_male'})",0.176,0.227,0.059,0.3352272727272727,1.476772126551862,1.0,0.019047999999999995,1.1628034188034189,0.39180516702320217,0.17151162790697672,0.14000940844407853,0.29756958350020024
"frozenset({'writing_cat_μέτριο', 'gender_male'})",frozenset({'math_cat_υψηλό'}),0.227,0.176,0.059,0.2599118942731277,1.476772126551862,1.0,0.019047999999999995,1.1133809523809526,0.41765518451114947,0.17151162790697672,0.10183482314699967,0.29756958350020024
frozenset({'math_cat_μέτριο'}),"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none', 'race/ethnicity_group D'})",0.485,0.081,0.058,0.11958762886597939,1.4763904798269059,1.0,0.018715000000000002,1.0438290398126464,0.6265483762972883,0.11417322834645673,0.04198871476167506,0.41781850579101437
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none', 'race/ethnicity_group D'})",frozenset({'math_cat_μέτριο'}),0.081,0.485,0.058,0.7160493827160493,1.4763904798269059,1.0,0.018715000000000002,1.813695652173913,0.35111252861055875,0.11417322834645673,0.44863957808941624,0.41781850579101437
"frozenset({'math_cat_μέτριο', 'race/ethnicity_group D'})","frozenset({'reading_cat_μέτριο', 'lunch_standard'})",0.145,0.327,0.07,0.48275862068965525,1.476326057154909,1.0,0.022585000000000008,1.3011333333333337,0.3773600668337511,0.17412935323383089,0.23143925808269722,0.3484129494885585
"frozenset({'reading_cat_μέτριο', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'race/ethnicity_group D'})",0.327,0.145,0.07,0.21406727828746178,1.4763260571549088,1.0,0.022585000000000008,1.0878793774319067,0.4794098917427299,0.17412935323383089,0.08078044244147578,0.3484129494885585
"frozenset({'math_cat_μέτριο', 'test preparation course_none', 'race/ethnicity_group D'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.094,0.418,0.058,0.6170212765957447,1.4761274559706812,1.0,0.018708000000000002,1.519666666666667,0.3560173555606303,0.1277533039647577,0.3419609563500768,0.3778886287284944
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'math_cat_μέτριο', 'test preparation course_none', 'race/ethnicity_group D'})",0.418,0.094,0.058,0.13875598086124402,1.4761274559706812,1.0,0.018708000000000002,1.0519666666666667,0.5542125844294347,0.1277533039647577,0.04939953737444152,0.3778886287284944
"frozenset({'math_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})","frozenset({'reading_cat_μέτριο', 'gender_male'})",0.213,0.229,0.072,0.3380281690140845,1.4761055415462205,1.0,0.023222999999999994,1.1647021276595746,0.40983693350275296,0.19459459459459458,0.14141137355912384,0.3262193246817147
"frozenset({'reading_cat_μέτριο', 'gender_male'})","frozenset({'math_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",0.229,0.213,0.072,0.31441048034934493,1.4761055415462203,1.0,0.023222999999999994,1.1479171974522293,0.41834198011240803,0.19459459459459458,0.1288570271275031,0.3262193246817147
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none', 'gender_female'})","frozenset({'math_cat_μέτριο', 'lunch_standard'})",0.151,0.332,0.074,0.4900662251655629,1.476103087848081,1.0,0.023867999999999993,1.309974025974026,0.37990640817495935,0.18092909535452323,0.23662608557719,0.35647889571531155
"frozenset({'math_cat_μέτριο', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none', 'gender_female'})",0.332,0.151,0.074,0.2228915662650602,1.476103087848081,1.0,0.023867999999999993,1.0925116279069766,0.48284512056967144,0.18092909535452323,0.08467793435224996,0.35647889571531155
"frozenset({'lunch_free/reduced', 'gender_female'})","frozenset({'math_cat_χαμηλό', 'test preparation course_none', 'writing_cat_χαμηλό'})",0.189,0.183,0.051,0.2698412698412698,1.4745424581490154,1.0,0.016412999999999997,1.1189347826086955,0.396823094219192,0.1588785046728972,0.10629286394280274,0.2742648972157169
"frozenset({'math_cat_χαμηλό', 'test preparation course_none', 'writing_cat_χαμηλό'})","frozenset({'lunch_free/reduced', 'gender_female'})",0.183,0.189,0.051,0.2786885245901639,1.4745424581490154,1.0,0.016412999999999997,1.124340909090909,0.3939088487292101,0.1588785046728972,0.1105900426512502,0.2742648972157169
"frozenset({'reading_cat_υψηλό', 'lunch_standard'})",frozenset({'test preparation course_completed'}),0.182,0.358,0.096,0.5274725274725275,1.4733869482472834,1.0,0.03084400000000001,1.3586511627906976,0.39277709861450705,0.2162162162162162,0.26397589948991823,0.39781447602676656
frozenset({'test preparation course_completed'}),"frozenset({'reading_cat_υψηλό', 'lunch_standard'})",0.358,0.182,0.096,0.2681564245810056,1.4733869482472834,1.0,0.03084400000000001,1.1177251908396946,0.5004543094496368,0.2162162162162162,0.10532570242176721,0.39781447602676656
"frozenset({'math_cat_μέτριο', 'race/ethnicity_group D', 'lunch_standard'})",frozenset({'reading_cat_μέτριο'}),0.097,0.49,0.07,0.7216494845360826,1.472754050073638,1.0,0.022470000000000004,1.8322222222222229,0.3554817275747509,0.13539651837524183,0.45421467556094625,0.43225331369661274
"frozenset({'writing_cat_μέτριο', 'gender_male', 'test preparation course_none'})",frozenset({'math_cat_μέτριο'}),0.133,0.485,0.095,0.7142857142857143,1.4727540500736378,1.0,0.030494999999999994,1.8025000000000002,0.3702422145328719,0.18164435946462715,0.44521497919556174,0.45508100147275404
frozenset({'reading_cat_μέτριο'}),"frozenset({'math_cat_μέτριο', 'race/ethnicity_group D', 'lunch_standard'})",0.49,0.097,0.07,0.14285714285714288,1.4727540500736378,1.0,0.022470000000000004,1.0535,0.6294117647058824,0.13539651837524183,0.05078310393925014,0.43225331369661274
frozenset({'math_cat_μέτριο'}),"frozenset({'writing_cat_μέτριο', 'gender_male', 'test preparation course_none'})",0.485,0.133,0.095,0.1958762886597938,1.4727540500736376,1.0,0.030494999999999994,1.0781923076923077,0.6233009708737862,0.18164435946462715,0.07252167088788213,0.45508100147275404
"frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο'})","frozenset({'math_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",0.169,0.213,0.053,0.3136094674556213,1.472344917632025,1.0,0.017002999999999997,1.1465775862068968,0.386054537610971,0.16109422492401215,0.1278392216716916,0.28121787926771674
"frozenset({'math_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})","frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο'})",0.213,0.169,0.053,0.24882629107981222,1.472344917632025,1.0,0.017002999999999997,1.10626875,0.40763827287765814,0.16109422492401215,0.0960605187482698,0.28121787926771674
"frozenset({'math_cat_μέτριο', 'race/ethnicity_group D', 'lunch_standard'})",frozenset({'writing_cat_μέτριο'}),0.097,0.491,0.07,0.7216494845360826,1.469754550989985,1.0,0.022373000000000004,1.8286296296296303,0.35394716025945266,0.13513513513513514,0.45314240576833514,0.43210783799105557
frozenset({'writing_cat_μέτριο'}),"frozenset({'math_cat_μέτριο', 'race/ethnicity_group D', 'lunch_standard'})",0.491,0.097,0.07,0.14256619144602853,1.4697545509899848,1.0,0.022373000000000004,1.053142517814727,0.6279259051361213,0.13513513513513514,0.05046089861132727,0.43210783799105557
frozenset({'test preparation course_completed'}),"frozenset({'writing_cat_υψηλό', 'gender_female'})",0.358,0.154,0.081,0.22625698324022347,1.469201189871581,1.0,0.02586800000000001,1.0933862815884476,0.4974424060613055,0.18793503480278423,0.08541014567402302,0.37611550460712473
"frozenset({'writing_cat_υψηλό', 'gender_female'})",frozenset({'test preparation course_completed'}),0.154,0.358,0.081,0.525974025974026,1.469201189871581,1.0,0.02586800000000001,1.3543561643835618,0.377491754954324,0.18793503480278423,0.261641785006271,0.37611550460712473
"frozenset({'math_cat_χαμηλό', 'reading_cat_μέτριο'})","frozenset({'gender_female', 'lunch_standard'})",0.12,0.329,0.058,0.4833333333333334,1.469098277608916,1.0,0.018520000000000002,1.2987096774193552,0.36285266457680254,0.1483375959079284,0.2300049677098858,0.32981256332320164
"frozenset({'gender_female', 'lunch_standard'})","frozenset({'math_cat_χαμηλό', 'reading_cat_μέτριο'})",0.329,0.12,0.058,0.1762917933130699,1.4690982776089159,1.0,0.018520000000000002,1.0683394833948339,0.47587234698597053,0.1483375959079284,0.06396794694667035,0.32981256332320164
"frozenset({'math_cat_μέτριο', 'race/ethnicity_group D'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.145,0.418,0.089,0.6137931034482759,1.4684045537040094,1.0,0.028390000000000006,1.506964285714286,0.3730862737367765,0.1877637130801688,0.3364142670932575,0.4133558818676786
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'math_cat_μέτριο', 'race/ethnicity_group D'})",0.418,0.145,0.089,0.21291866028708134,1.4684045537040094,1.0,0.028390000000000006,1.08629179331307,0.5480906598710376,0.1877637130801688,0.07943702957553374,0.4133558818676786
"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'math_cat_μέτριο', 'test preparation course_none'})",0.139,0.304,0.062,0.44604316546762585,1.4672472548277167,1.0,0.019743999999999998,1.256415584415584,0.36986250046832264,0.16272965879265092,0.2040850078557843,0.3249952669443392
"frozenset({'math_cat_μέτριο', 'test preparation course_none'})","frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.304,0.139,0.062,0.20394736842105263,1.4672472548277167,1.0,0.019743999999999998,1.0815867768595042,0.4575454208379681,0.16272965879265092,0.07543248364814473,0.3249952669443392
"frozenset({'writing_cat_μέτριο', 'test preparation course_none'})","frozenset({'math_cat_μέτριο', 'lunch_standard'})",0.308,0.332,0.15,0.487012987012987,1.466906587388515,1.0,0.047743999999999995,1.3021772151898732,0.45996146435452795,0.30612244897959184,0.23205536977992067,0.4694101079643248
"frozenset({'math_cat_μέτριο', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'test preparation course_none'})",0.332,0.308,0.15,0.4518072289156626,1.466906587388515,1.0,0.047743999999999995,1.26232967032967,0.4764870259481038,0.30612244897959184,0.20781391461800958,0.4694101079643248
frozenset({'lunch_free/reduced'}),"frozenset({'race/ethnicity_group C', 'writing_cat_χαμηλό'})",0.355,0.098,0.051,0.14366197183098592,1.465938488071285,1.0,0.016209999999999995,1.0533223684210526,0.49278005775953776,0.12686567164179105,0.050623028637456664,0.332035067548146
"frozenset({'race/ethnicity_group C', 'writing_cat_χαμηλό'})",frozenset({'lunch_free/reduced'}),0.098,0.355,0.051,0.520408163265306,1.4659384880712847,1.0,0.016209999999999995,1.3448936170212764,0.35237598365288453,0.12686567164179105,0.25644676475241246,0.332035067548146
"frozenset({'math_cat_χαμηλό', 'gender_male', 'reading_cat_χαμηλό', 'writing_cat_χαμηλό'})",frozenset({'lunch_free/reduced'}),0.123,0.355,0.064,0.5203252032520326,1.4657047978930495,1.0,0.020335000000000006,1.3446610169491526,0.3622968928164197,0.15458937198067635,0.2563181445767947,0.3503034466964388
frozenset({'lunch_free/reduced'}),"frozenset({'math_cat_χαμηλό', 'gender_male', 'reading_cat_χαμηλό'})",0.355,0.123,0.064,0.18028169014084508,1.4657047978930495,1.0,0.020335000000000006,1.0698797250859107,0.4926114341085272,0.15458937198067635,0.06531549616972074,0.3503034466964388
"frozenset({'math_cat_χαμηλό', 'gender_male', 'reading_cat_χαμηλό'})",frozenset({'lunch_free/reduced'}),0.123,0.355,0.064,0.5203252032520326,1.4657047978930495,1.0,0.020335000000000006,1.3446610169491526,0.3622968928164197,0.15458937198067635,0.2563181445767947,0.3503034466964388
frozenset({'lunch_free/reduced'}),"frozenset({'math_cat_χαμηλό', 'gender_male', 'reading_cat_χαμηλό', 'writing_cat_χαμηλό'})",0.355,0.123,0.064,0.18028169014084508,1.4657047978930495,1.0,0.020335000000000006,1.0698797250859107,0.4926114341085272,0.15458937198067635,0.06531549616972074,0.3503034466964388
"frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'gender_female'})",0.223,0.245,0.08,0.35874439461883406,1.4642628351789146,1.0,0.025365,1.1773776223776222,0.40805984555984554,0.2061855670103093,0.150654827309714,0.34263750343186605
"frozenset({'math_cat_μέτριο', 'gender_female'})","frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",0.245,0.223,0.08,0.326530612244898,1.4642628351789146,1.0,0.025365,1.1537272727272727,0.4199503311258278,0.2061855670103093,0.1332440312032149,0.34263750343186605
"frozenset({'math_cat_χαμηλό', 'writing_cat_χαμηλό'})",frozenset({'parental level of education_high school'}),0.23,0.196,0.066,0.28695652173913044,1.4640638864241349,1.0,0.02092,1.1275609756097562,0.4116489571035025,0.18333333333333332,0.11313000216309756,0.3118456078083407
frozenset({'parental level of education_high school'}),"frozenset({'math_cat_χαμηλό', 'writing_cat_χαμηλό'})",0.196,0.23,0.066,0.336734693877551,1.4640638864241347,1.0,0.02092,1.160923076923077,0.3942409166289763,0.18333333333333332,0.13861648555526104,0.3118456078083407
"frozenset({'reading_cat_μέτριο', 'test preparation course_none'})","frozenset({'math_cat_μέτριο', 'gender_female', 'lunch_standard'})",0.318,0.172,0.08,0.25157232704402516,1.4626298083954952,1.0,0.025304000000000007,1.1063193277310925,0.4637829912023462,0.1951219512195122,0.09610184425606903,0.35834430305689635
"frozenset({'math_cat_μέτριο', 'gender_female', 'lunch_standard'})","frozenset({'reading_cat_μέτριο', 'test preparation course_none'})",0.172,0.318,0.08,0.4651162790697675,1.4626298083954952,1.0,0.025304000000000007,1.2750434782608695,0.38200483091787446,0.1951219512195122,0.21571301916388197,0.35834430305689635
frozenset({'math_cat_μέτριο'}),"frozenset({'reading_cat_μέτριο', 'writing_cat_μέτριο', 'lunch_standard', 'test preparation course_none', 'gender_male'})",0.485,0.086,0.061,0.12577319587628866,1.46247902181731,1.0,0.01929,1.045495283018868,0.6140378799936337,0.1196078431372549,0.04351553159331364,0.417537760728842
"frozenset({'reading_cat_μέτριο', 'writing_cat_μέτριο', 'lunch_standard', 'test preparation course_none', 'gender_male'})",frozenset({'math_cat_μέτριο'}),0.086,0.485,0.061,0.7093023255813954,1.46247902181731,1.0,0.01929,1.7716000000000003,0.34598414463536253,0.1196078431372549,0.43553849627455415,0.417537760728842
"frozenset({'parental level of education_high school', 'math_cat_μέτριο'})",frozenset({'writing_cat_μέτριο'}),0.092,0.491,0.066,0.7173913043478262,1.4610820862481184,1.0,0.020828000000000006,1.8010769230769237,0.3475503938059005,0.12765957446808512,0.44477662936704554,0.42590542814132654
frozenset({'writing_cat_μέτριο'}),"frozenset({'parental level of education_high school', 'math_cat_μέτριο'})",0.491,0.092,0.066,0.13441955193482688,1.4610820862481182,1.0,0.020828000000000006,1.0490070588235294,0.6199916651783058,0.12765957446808512,0.046717568210161764,0.42590542814132654
"frozenset({'test preparation course_completed', 'math_cat_μέτριο'})",frozenset({'writing_cat_υψηλό'}),0.181,0.208,0.055,0.30386740331491713,1.4609009774755632,1.0,0.017352000000000006,1.1377142857142857,0.3852147852147854,0.16467065868263472,0.1210447011551984,0.284145240118997
frozenset({'writing_cat_υψηλό'}),"frozenset({'test preparation course_completed', 'math_cat_μέτριο'})",0.208,0.181,0.055,0.2644230769230769,1.4609009774755632,1.0,0.017352000000000006,1.113411764705882,0.39834710743801666,0.16467065868263472,0.10185967878275573,0.284145240118997
frozenset({'lunch_free/reduced'}),frozenset({'writing_cat_χαμηλό'}),0.355,0.301,0.156,0.4394366197183099,1.459922324645548,1.0,0.04914500000000001,1.2469597989949752,0.48842178493341293,0.31200000000000006,0.19804952749400556,0.4788545224837397
frozenset({'writing_cat_χαμηλό'}),frozenset({'lunch_free/reduced'}),0.301,0.355,0.156,0.5182724252491695,1.459922324645548,1.0,0.04914500000000001,1.3389310344827587,0.45068962987417927,0.31200000000000006,0.2531355430219682,0.4788545224837397
"frozenset({'math_cat_μέτριο', 'test preparation course_none'})","frozenset({'writing_cat_μέτριο', 'lunch_standard'})",0.304,0.338,0.15,0.4934210526315789,1.4598255995017126,1.0,0.047247999999999984,1.3068051948051946,0.452567049808429,0.3048780487804878,0.23477500397519474,0.46860401744004976
"frozenset({'writing_cat_μέτριο', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'test preparation course_none'})",0.338,0.304,0.15,0.44378698224852065,1.4598255995017126,1.0,0.047247999999999984,1.25131914893617,0.4758106747230613,0.3048780487804878,0.20084336529959865,0.46860401744004976
"frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο'})","frozenset({'math_cat_μέτριο', 'test preparation course_none'})",0.169,0.304,0.075,0.44378698224852065,1.4598255995017126,1.0,0.023623999999999992,1.25131914893617,0.37904532691536297,0.1884422110552764,0.20084336529959865,0.34524875428215507
"frozenset({'math_cat_μέτριο', 'test preparation course_none'})","frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο'})",0.304,0.169,0.075,0.24671052631578946,1.4598255995017126,1.0,0.023623999999999992,1.1031615720524017,0.452567049808429,0.1884422110552764,0.09351447210082967,0.34524875428215507
"frozenset({'writing_cat_μέτριο', 'gender_male'})","frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο'})",0.227,0.169,0.056,0.24669603524229075,1.4597398535046788,1.0,0.017637,1.1031403508771929,0.4074339308815376,0.16470588235294117,0.09349703398590944,0.2890284909939264
"frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο'})","frozenset({'writing_cat_μέτριο', 'gender_male'})",0.169,0.227,0.056,0.3313609467455621,1.4597398535046788,1.0,0.017637,1.156079646017699,0.3789969056214543,0.16470588235294117,0.13500769307317217,0.2890284909939264
"frozenset({'math_cat_μέτριο', 'gender_female', 'lunch_standard'})",frozenset({'reading_cat_υψηλό'}),0.172,0.235,0.059,0.3430232558139535,1.4596734289955469,1.0,0.01858,1.1644247787610618,0.3803324326537296,0.1695402298850575,0.14120687034503726,0.29704354280059375
frozenset({'reading_cat_υψηλό'}),"frozenset({'math_cat_μέτριο', 'gender_female', 'lunch_standard'})",0.235,0.172,0.059,0.251063829787234,1.4596734289955466,1.0,0.01858,1.105568181818182,0.41165392710756615,0.1695402298850575,0.09548771713434061,0.29704354280059375
"frozenset({'writing_cat_μέτριο', 'gender_male'})","frozenset({'math_cat_μέτριο', 'lunch_standard'})",0.227,0.332,0.11,0.4845814977973568,1.4595828246908338,1.0,0.034636,1.296034188034188,0.4073385863812772,0.24498886414253895,0.2284154158643066,0.40795339950108805
"frozenset({'math_cat_μέτριο', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'gender_male'})",0.332,0.227,0.11,0.3313253012048193,1.4595828246908338,1.0,0.034636,1.1560180180180182,0.47136635819270556,0.24498886414253895,0.13496157982512197,0.40795339950108805
"frozenset({'writing_cat_μέτριο', 'test preparation course_none'})","frozenset({'math_cat_μέτριο', 'gender_female'})",0.308,0.245,0.11,0.35714285714285715,1.457725947521866,1.0,0.03454,1.1744444444444446,0.4537572254335261,0.24830699774266368,0.14853358561967836,0.4030612244897959
"frozenset({'math_cat_μέτριο', 'gender_female'})","frozenset({'writing_cat_μέτριο', 'test preparation course_none'})",0.245,0.308,0.11,0.4489795918367347,1.4577259475218658,1.0,0.03454,1.2558518518518518,0.41589403973509936,0.24830699774266368,0.20372773386811374,0.4030612244897959
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",frozenset({'math_cat_μέτριο'}),0.191,0.485,0.135,0.7068062827225131,1.4573325416959033,1.0,0.042365000000000014,1.7565178571428572,0.38790459186009263,0.2495378927911276,0.430691811111676,0.4925783990932153
frozenset({'math_cat_μέτριο'}),"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",0.485,0.191,0.135,0.27835051546391754,1.4573325416959033,1.0,0.042365000000000014,1.121042857142857,0.6093491549802231,0.2495378927911276,0.10797344309507731,0.4925783990932153
frozenset({'reading_cat_μέτριο'}),"frozenset({'math_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",0.49,0.213,0.152,0.31020408163265306,1.4563571907636295,1.0,0.047630000000000006,1.1409171597633136,0.614422084623323,0.2758620689655173,0.12351217488265955,0.5119095525534157
"frozenset({'math_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",frozenset({'reading_cat_μέτριο'}),0.213,0.49,0.152,0.7136150234741784,1.4563571907636295,1.0,0.047630000000000006,1.7808196721311473,0.3981642479769946,0.2758620689655173,0.4384608303415262,0.5119095525534157
"frozenset({'math_cat_μέτριο', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'parental level of education_some college'})",0.332,0.114,0.055,0.16566265060240964,1.4531811456351722,1.0,0.017151999999999994,1.0619205776173286,0.4668481219379422,0.14066496163682865,0.05830998939323887,0.3240593954766434
"frozenset({'writing_cat_μέτριο', 'parental level of education_some college'})","frozenset({'math_cat_μέτριο', 'lunch_standard'})",0.114,0.332,0.055,0.4824561403508772,1.4531811456351722,1.0,0.017151999999999994,1.2907118644067794,0.3519802996100963,0.14066496163682865,0.22523374304023533,0.3240593954766434
frozenset({'math_cat_μέτριο'}),"frozenset({'reading_cat_μέτριο', 'writing_cat_μέτριο', 'lunch_standard', 'test preparation course_none', 'gender_female'})",0.485,0.105,0.074,0.15257731958762885,1.4531173294059891,1.0,0.023075,1.0561435523114355,0.6054841249016006,0.14341085271317827,0.053159016299026654,0.4286696121747668
"frozenset({'reading_cat_μέτριο', 'writing_cat_μέτριο', 'lunch_standard', 'test preparation course_none', 'gender_female'})",frozenset({'math_cat_μέτριο'}),0.105,0.485,0.074,0.7047619047619047,1.4531173294059891,1.0,0.023075,1.744354838709677,0.34840706628416124,0.14341085271317827,0.426722145168747,0.4286696121747668
frozenset({'writing_cat_χαμηλό'}),"frozenset({'lunch_free/reduced', 'gender_female', 'test preparation course_none'})",0.301,0.119,0.052,0.17275747508305647,1.4517434880929116,1.0,0.016181,1.064983935742972,0.44516892263673385,0.14130434782608695,0.06101870043479736,0.30486613249951144
"frozenset({'lunch_free/reduced', 'gender_female', 'test preparation course_none'})",frozenset({'writing_cat_χαμηλό'}),0.119,0.301,0.052,0.4369747899159664,1.4517434880929116,1.0,0.016181,1.2415074626865672,0.3532044005937309,0.14130434782608695,0.19452759644630385,0.30486613249951144
"frozenset({'math_cat_υψηλό', 'writing_cat_υψηλό', 'lunch_standard'})",frozenset({'test preparation course_completed'}),0.102,0.358,0.053,0.5196078431372549,1.4514185562493156,1.0,0.016484000000000006,1.3364081632653062,0.3463461780896753,0.13022113022113022,0.25172561236332547,0.33382626793734255
frozenset({'test preparation course_completed'}),"frozenset({'math_cat_υψηλό', 'writing_cat_υψηλό', 'lunch_standard'})",0.358,0.102,0.053,0.14804469273743018,1.4514185562493156,1.0,0.016484000000000006,1.0540459016393442,0.48445306530300375,0.13022113022113022,0.05127471351606925,0.33382626793734255
"frozenset({'math_cat_μέτριο', 'parental level of education_some college'})","frozenset({'reading_cat_μέτριο', 'lunch_standard'})",0.118,0.327,0.056,0.4745762711864407,1.4513035816099102,1.0,0.017414,1.2808709677419357,0.35256721736313573,0.14395886889460155,0.21928123504671718,0.32291504690820505
"frozenset({'reading_cat_μέτριο', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'parental level of education_some college'})",0.327,0.118,0.056,0.1712538226299694,1.4513035816099102,1.0,0.017414,1.0642583025830257,0.462056888134154,0.14395886889460155,0.06037848370744832,0.32291504690820505
frozenset({'math_cat_μέτριο'}),"frozenset({'parental level of education_high school', 'reading_cat_μέτριο', 'writing_cat_μέτριο'})",0.485,0.081,0.057,0.1175257731958763,1.4509354715540284,1.0,0.017715,1.041390186915888,0.6034747061829331,0.1119842829076621,0.03974512861357595,0.41061473844979
"frozenset({'parental level of education_high school', 'reading_cat_μέτριο', 'writing_cat_μέτριο'})",frozenset({'math_cat_μέτριο'}),0.081,0.485,0.057,0.7037037037037037,1.4509354715540284,1.0,0.017715,1.7381250000000001,0.33818223469446196,0.1119842829076621,0.4246673858324344,0.41061473844979
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({""parental level of education_associate's degree"", 'math_cat_μέτριο'})",0.418,0.094,0.057,0.13636363636363638,1.4506769825918764,1.0,0.017708,1.0490526315789475,0.5337915234822451,0.12527472527472527,0.04675898053381499,0.3713733075435203
"frozenset({""parental level of education_associate's degree"", 'math_cat_μέτριο'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.094,0.418,0.057,0.6063829787234043,1.4506769825918764,1.0,0.017708,1.4785945945945949,0.3428991905813098,0.12527472527472527,0.32368209402646786,0.3713733075435203
"frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'race/ethnicity_group D'})",frozenset({'math_cat_μέτριο'}),0.091,0.485,0.064,0.7032967032967034,1.4500962954571204,1.0,0.019865,1.735740740740741,0.3414638338833883,0.125,0.42387709378000654,0.41762773309165063
frozenset({'math_cat_μέτριο'}),"frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'race/ethnicity_group D'})",0.485,0.091,0.064,0.13195876288659794,1.4500962954571204,1.0,0.019865,1.047185273159145,0.6027002427184466,0.125,0.04505914508976671,0.41762773309165063
"frozenset({'math_cat_χαμηλό', 'reading_cat_χαμηλό', 'writing_cat_χαμηλό'})","frozenset({'gender_male', 'test preparation course_none'})",0.206,0.308,0.092,0.44660194174757284,1.4500063043752365,1.0,0.028552000000000008,1.2504561403508772,0.39086627970649446,0.21800947867298576,0.20029182333464285,0.37265162022443576
"frozenset({'gender_male', 'test preparation course_none'})","frozenset({'math_cat_χαμηλό', 'reading_cat_χαμηλό', 'writing_cat_χαμηλό'})",0.308,0.206,0.092,0.2987012987012987,1.4500063043752365,1.0,0.028552000000000008,1.1321851851851852,0.44847951746670034,0.21800947867298576,0.11675226536687493,0.37265162022443576
frozenset({'math_cat_μέτριο'}),"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})",0.485,0.283,0.199,0.4103092783505155,1.449856107245638,1.0,0.06174500000000002,1.2158916083916087,0.6024784114748502,0.3497363796133568,0.17755826827129081,0.556744745182325
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})",frozenset({'math_cat_μέτριο'}),0.283,0.485,0.199,0.7031802120141344,1.449856107245638,1.0,0.06174500000000002,1.7350595238095246,0.4327425131234976,0.3497363796133568,0.42365089711482407,0.556744745182325
frozenset({'parental level of education_high school'}),"frozenset({'math_cat_χαμηλό', 'test preparation course_none', 'writing_cat_χαμηλό'})",0.196,0.183,0.052,0.26530612244897955,1.449760231961637,1.0,0.016131999999999994,1.1120277777777776,0.38585916570991186,0.15902140672782875,0.10074188794244744,0.2747295639567302
"frozenset({'math_cat_χαμηλό', 'test preparation course_none', 'writing_cat_χαμηλό'})",frozenset({'parental level of education_high school'}),0.183,0.196,0.052,0.28415300546448086,1.449760231961637,1.0,0.016131999999999994,1.1231450381679389,0.37971942378307116,0.15902140672782875,0.10964304162248861,0.2747295639567302
"frozenset({'lunch_free/reduced', 'gender_female'})","frozenset({'writing_cat_χαμηλό', 'math_cat_χαμηλό'})",0.189,0.23,0.063,0.3333333333333333,1.4492753623188404,1.0,0.01953,1.1549999999999998,0.38224414303329224,0.17696629213483145,0.13419913419913415,0.30362318840579705
"frozenset({'writing_cat_χαμηλό', 'math_cat_χαμηλό'})","frozenset({'lunch_free/reduced', 'gender_female'})",0.23,0.189,0.063,0.27391304347826084,1.4492753623188404,1.0,0.01953,1.116946107784431,0.4025974025974025,0.17696629213483145,0.10470165656998871,0.30362318840579705
"frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό'})","frozenset({'gender_male', 'test preparation course_none', 'lunch_standard'})",0.252,0.203,0.074,0.2936507936507936,1.4465556337477516,1.0,0.02284399999999999,1.1283370786516853,0.4127041479982655,0.1942257217847769,0.11374001712772096,0.3290914066776135
"frozenset({'gender_male', 'test preparation course_none', 'lunch_standard'})","frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό'})",0.203,0.252,0.074,0.36453201970443344,1.4465556337477516,1.0,0.02284399999999999,1.1770852713178295,0.3873308691376444,0.1942257217847769,0.15044387661020514,0.3290914066776135
"frozenset({'writing_cat_μέτριο', 'gender_female', 'test preparation course_none'})","frozenset({'math_cat_μέτριο', 'lunch_standard'})",0.175,0.332,0.084,0.48000000000000004,1.4457831325301205,1.0,0.025900000000000006,1.2846153846153845,0.3737373737373738,0.19858156028368795,0.22155688622754496,0.36650602409638555
"frozenset({'math_cat_μέτριο', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'gender_female', 'test preparation course_none'})",0.332,0.175,0.084,0.25301204819277107,1.4457831325301205,1.0,0.025900000000000006,1.1044354838709678,0.4615768463073854,0.19858156028368795,0.0945600584154801,0.36650602409638555
"frozenset({'math_cat_υψηλό', 'test preparation course_none', 'reading_cat_υψηλό'})",frozenset({'lunch_standard'}),0.059,0.645,0.055,0.9322033898305085,1.4452765733806334,1.0,0.016945,5.236250000000005,0.32740797990532317,0.08474576271186442,0.8090236333253762,0.508737353829983
frozenset({'lunch_standard'}),"frozenset({'math_cat_υψηλό', 'test preparation course_none', 'reading_cat_υψηλό'})",0.645,0.059,0.055,0.08527131782945736,1.4452765733806334,1.0,0.016945,1.028720338983051,0.8678617157490397,0.08474576271186442,0.0279185099144074,0.508737353829983
"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'gender_male', 'race/ethnicity_group D'})",0.281,0.133,0.054,0.19217081850533807,1.444893372220587,1.0,0.016626999999999996,1.0732466960352423,0.42824396023283356,0.15,0.06824777220915579,0.2990929280496615
"frozenset({'gender_male', 'race/ethnicity_group D'})","frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.133,0.281,0.054,0.40601503759398494,1.4448933722205868,1.0,0.016626999999999996,1.2104683544303796,0.35514118501431063,0.15,0.1738734876133309,0.2990929280496615
"frozenset({'math_cat_χαμηλό', 'test preparation course_none'})",frozenset({'lunch_free/reduced'}),0.246,0.355,0.126,0.5121951219512195,1.4428031604259706,1.0,0.03867000000000001,1.3222500000000001,0.40703549324238986,0.26526315789473687,0.24371336736623184,0.4335623497080041
frozenset({'lunch_free/reduced'}),"frozenset({'math_cat_χαμηλό', 'test preparation course_none'})",0.355,0.246,0.126,0.35492957746478876,1.4428031604259706,1.0,0.03867000000000001,1.1688646288209608,0.4758213362864526,0.26526315789473687,0.14446893562969332,0.4335623497080041
"frozenset({'writing_cat_μέτριο', 'parental level of education_some college'})","frozenset({'math_cat_μέτριο', 'test preparation course_none'})",0.114,0.304,0.05,0.4385964912280702,1.44275161588181,1.0,0.015344000000000003,1.23975,0.3463656884875847,0.13586956521739132,0.1933857632587216,0.30153508771929827
"frozenset({'math_cat_μέτριο', 'test preparation course_none'})","frozenset({'writing_cat_μέτριο', 'parental level of education_some college'})",0.304,0.114,0.05,0.16447368421052633,1.44275161588181,1.0,0.015344000000000003,1.0604094488188978,0.44091954022988517,0.13586956521739132,0.056968040869668536,0.30153508771929827
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})","frozenset({'math_cat_μέτριο', 'gender_female'})",0.266,0.245,0.094,0.3533834586466165,1.4423814638637409,1.0,0.028829999999999995,1.1676162790697673,0.4178503101629079,0.22541966426858512,0.14355424986306825,0.3685284640171858
"frozenset({'math_cat_μέτριο', 'gender_female'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})",0.245,0.266,0.094,0.3836734693877551,1.4423814638637409,1.0,0.028829999999999995,1.1909271523178806,0.4062279836550654,0.22541966426858512,0.16031807818495242,0.3685284640171858
"frozenset({'parental level of education_high school', 'math_cat_μέτριο'})",frozenset({'reading_cat_μέτριο'}),0.092,0.49,0.065,0.7065217391304348,1.4418811002661935,1.0,0.019920000000000007,1.737777777777778,0.33751270755676055,0.12572533849129597,0.42455242966751927,0.4195874001774623
frozenset({'reading_cat_μέτριο'}),"frozenset({'parental level of education_high school', 'math_cat_μέτριο'})",0.49,0.092,0.065,0.1326530612244898,1.4418811002661935,1.0,0.019920000000000007,1.0468705882352942,0.6009049773755658,0.12572533849129597,0.04477209385957027,0.4195874001774623
"frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'lunch_standard', 'gender_male'})",frozenset({'math_cat_μέτριο'}),0.103,0.485,0.072,0.6990291262135923,1.4412971674507058,1.0,0.022045000000000002,1.7111290322580648,0.3413384119905859,0.13953488372093023,0.4155905363370724,0.4237413672305075
frozenset({'math_cat_μέτριο'}),"frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'lunch_standard', 'gender_male'})",0.485,0.103,0.072,0.14845360824742268,1.4412971674507058,1.0,0.022045000000000002,1.0533777239709443,0.5945253505933119,0.13953488372093023,0.050672918893447806,0.4237413672305075
frozenset({'parental level of education_high school'}),"frozenset({'gender_male', 'reading_cat_χαμηλό'})",0.196,0.177,0.05,0.25510204081632654,1.4412544678888506,1.0,0.015308000000000002,1.104849315068493,0.3807960199004975,0.15479876160990713,0.09489919904778439,0.2687939582612706
"frozenset({'gender_male', 'reading_cat_χαμηλό'})",frozenset({'parental level of education_high school'}),0.177,0.196,0.05,0.2824858757062147,1.4412544678888504,1.0,0.015308000000000002,1.1205354330708663,0.3720048602673148,0.15479876160990713,0.10756949714703319,0.2687939582612706
"frozenset({'race/ethnicity_group C', 'math_cat_μέτριο'})",frozenset({'reading_cat_μέτριο'}),0.153,0.49,0.108,0.7058823529411765,1.440576230492197,1.0,0.033030000000000004,1.7340000000000002,0.3610783156237702,0.20186915887850465,0.42329873125720885,0.4631452581032413
frozenset({'reading_cat_μέτριο'}),"frozenset({'race/ethnicity_group C', 'math_cat_μέτριο'})",0.49,0.153,0.108,0.22040816326530613,1.440576230492197,1.0,0.033030000000000004,1.0864659685863873,0.5996732026143792,0.20186915887850465,0.07958460834156568,0.4631452581032413
"frozenset({'gender_male', 'test preparation course_none', 'lunch_standard'})",frozenset({'writing_cat_χαμηλό'}),0.203,0.301,0.088,0.4334975369458128,1.4401911526438964,1.0,0.02689699999999999,1.233886956521739,0.3834977757499714,0.2115384615384615,0.1895529856163273,0.36292817046626186
frozenset({'writing_cat_χαμηλό'}),"frozenset({'gender_male', 'test preparation course_none', 'lunch_standard'})",0.301,0.203,0.088,0.29235880398671094,1.4401911526438962,1.0,0.02689699999999999,1.1262769953051641,0.437264273637664,0.2115384615384615,0.11211895104982551,0.36292817046626186
"frozenset({'writing_cat_μέτριο', 'gender_male'})","frozenset({'math_cat_υψηλό', 'lunch_standard'})",0.227,0.156,0.051,0.22466960352422904,1.4401897661809555,1.0,0.015587999999999998,1.0885681818181818,0.39540369834867967,0.1536144578313253,0.0813620894836837,0.27579634022365296
"frozenset({'math_cat_υψηλό', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'gender_male'})",0.156,0.227,0.051,0.3269230769230769,1.4401897661809555,1.0,0.015587999999999998,1.1484571428571428,0.3621410649567884,0.1536144578313253,0.1292665936909145,0.27579634022365296
"frozenset({'race/ethnicity_group C', 'math_cat_μέτριο', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.103,0.418,0.062,0.6019417475728156,1.4400520276861617,1.0,0.018946000000000005,1.4620975609756102,0.34066961556442626,0.13507625272331153,0.316051112668068,0.3751335532122451
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'race/ethnicity_group C', 'math_cat_μέτριο', 'lunch_standard'})",0.418,0.103,0.062,0.14832535885167464,1.4400520276861617,1.0,0.018946000000000005,1.0532191011235956,0.5250526549163065,0.13507625272331153,0.05052994297845557,0.3751335532122451
"frozenset({'reading_cat_μέτριο', 'test preparation course_none'})","frozenset({'math_cat_μέτριο', 'lunch_standard'})",0.318,0.332,0.152,0.4779874213836478,1.439721148745927,1.0,0.04642399999999999,1.2796626506024096,0.44783145547152337,0.30522088353413657,0.21854404398749672,0.4679093733424263
"frozenset({'math_cat_μέτριο', 'lunch_standard'})","frozenset({'reading_cat_μέτριο', 'test preparation course_none'})",0.332,0.318,0.152,0.4578313253012048,1.439721148745927,1.0,0.04642399999999999,1.2579111111111108,0.4572171446580523,0.30522088353413657,0.205031268770095,0.4679093733424263
"frozenset({'race/ethnicity_group C', 'math_cat_μέτριο'})","frozenset({'reading_cat_μέτριο', 'lunch_standard'})",0.153,0.327,0.072,0.47058823529411764,1.4391077531930203,1.0,0.021968999999999995,1.2712222222222223,0.3602420306965761,0.17647058823529413,0.21335547591993703,0.3453858607663249
"frozenset({'reading_cat_μέτριο', 'lunch_standard'})","frozenset({'race/ethnicity_group C', 'math_cat_μέτριο'})",0.327,0.153,0.072,0.2201834862385321,1.4391077531930203,1.0,0.021968999999999995,1.0861529411764705,0.45338038632986616,0.17647058823529413,0.07931934620842042,0.3453858607663249
"frozenset({'test preparation course_none', 'reading_cat_υψηλό'})",frozenset({'gender_female'}),0.11,0.518,0.082,0.7454545454545455,1.4391014391014392,1.0,0.02502,1.8935714285714287,0.3428336530556317,0.15018315018315018,0.4718973972086006,0.4518778518778519
frozenset({'gender_female'}),"frozenset({'test preparation course_none', 'reading_cat_υψηλό'})",0.518,0.11,0.082,0.1583011583011583,1.439101439101439,1.0,0.02502,1.0573853211009174,0.6330330938164153,0.15018315018315018,0.05427096438332392,0.4518778518778519
"frozenset({'math_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})","frozenset({'reading_cat_μέτριο', 'gender_female'})",0.213,0.261,0.08,0.37558685446009393,1.439030093716835,1.0,0.024406999999999998,1.1835112781954888,0.3876588310038119,0.20304568527918784,0.15505663661717717,0.34105013221088987
"frozenset({'reading_cat_μέτριο', 'gender_female'})","frozenset({'math_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",0.261,0.213,0.08,0.3065134099616858,1.4390300937168348,1.0,0.024406999999999998,1.1348453038674033,0.4128382949932341,0.20304568527918784,0.11882263019273927,0.34105013221088987
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'race/ethnicity_group C', 'math_cat_μέτριο'})",0.418,0.153,0.092,0.22009569377990432,1.4385339462738844,1.0,0.028046,1.0860306748466257,0.5237935156133273,0.19206680584551147,0.07921569513566036,0.410701441661194
"frozenset({'race/ethnicity_group C', 'math_cat_μέτριο'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.153,0.418,0.092,0.6013071895424836,1.4385339462738844,1.0,0.028046,1.4597704918032788,0.35991478876854377,0.19206680584551147,0.3149608067740268,0.410701441661194
"frozenset({'math_cat_χαμηλό', 'reading_cat_χαμηλό', 'writing_cat_χαμηλό'})","frozenset({'lunch_free/reduced', 'gender_female'})",0.206,0.189,0.056,0.2718446601941748,1.4383315354189143,1.0,0.017066000000000005,1.1137733333333335,0.38381612090680106,0.16519174041297935,0.10215124561550532,0.28407047824523557
"frozenset({'lunch_free/reduced', 'gender_female'})","frozenset({'math_cat_χαμηλό', 'reading_cat_χαμηλό', 'writing_cat_χαμηλό'})",0.189,0.206,0.056,0.2962962962962963,1.4383315354189141,1.0,0.017066000000000005,1.1283157894736842,0.37577065351418015,0.16519174041297935,0.11372329508349659,0.28407047824523557
"frozenset({'writing_cat_υψηλό', 'gender_female', 'reading_cat_υψηλό'})",frozenset({'test preparation course_completed'}),0.136,0.358,0.07,0.5147058823529412,1.437725928360171,1.0,0.021312000000000005,1.322909090909091,0.3523809523809524,0.16509433962264153,0.2440901594282574,0.3551183043049623
frozenset({'test preparation course_completed'}),"frozenset({'writing_cat_υψηλό', 'gender_female', 'reading_cat_υψηλό'})",0.358,0.136,0.07,0.19553072625698326,1.4377259283601709,1.0,0.021312000000000005,1.0739999999999998,0.47423230974632846,0.16509433962264153,0.06890130353817506,0.3551183043049623
"frozenset({'gender_male', 'reading_cat_χαμηλό'})","frozenset({'lunch_free/reduced', 'test preparation course_none'})",0.177,0.224,0.057,0.3220338983050848,1.4376513317191284,1.0,0.017352000000000006,1.1446,0.3698919230031337,0.16569767441860464,0.12633234317665562,0.28824909200968524
"frozenset({'lunch_free/reduced', 'test preparation course_none'})","frozenset({'gender_male', 'reading_cat_χαμηλό'})",0.224,0.177,0.057,0.2544642857142857,1.4376513317191284,1.0,0.017352000000000006,1.1039041916167665,0.3922951709169833,0.16569767441860464,0.09412428397847596,0.28824909200968524
"frozenset({'reading_cat_μέτριο', 'lunch_standard'})",frozenset({'math_cat_μέτριο'}),0.327,0.485,0.228,0.6972477064220184,1.4376241369526153,1.0,0.069405,1.7010606060606064,0.4523148510205677,0.39041095890410954,0.41213146878061824,0.5836753996027618
frozenset({'math_cat_μέτριο'}),"frozenset({'reading_cat_μέτριο', 'lunch_standard'})",0.485,0.327,0.228,0.4701030927835052,1.4376241369526153,1.0,0.069405,1.2700583657587552,0.5910832907511496,0.39041095890410954,0.21263461037667933,0.5836753996027618
"frozenset({'test preparation course_none', 'reading_cat_υψηλό', 'lunch_standard'})",frozenset({'gender_female'}),0.086,0.518,0.064,0.744186046511628,1.4366525994432973,1.0,0.019452000000000004,1.8841818181818188,0.33253555798687096,0.11851851851851851,0.46926565666312864,0.43386908503187577
frozenset({'gender_female'}),"frozenset({'test preparation course_none', 'reading_cat_υψηλό', 'lunch_standard'})",0.518,0.086,0.064,0.12355212355212356,1.4366525994432973,1.0,0.019452000000000004,1.0428458149779736,0.6305757261410789,0.11851851851851851,0.04108547434586823,0.43386908503187577
"frozenset({'math_cat_μέτριο', 'gender_female'})","frozenset({'writing_cat_υψηλό', 'lunch_standard'})",0.245,0.162,0.057,0.2326530612244898,1.436130007558579,1.0,0.01731,1.0920744680851064,0.40223074241896123,0.16285714285714284,0.08431152890750573,0.29225245653817084
"frozenset({'writing_cat_υψηλό', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'gender_female'})",0.162,0.245,0.057,0.35185185185185186,1.436130007558579,1.0,0.01731,1.1648571428571428,0.3623916593392789,0.16285714285714284,0.14152563159185677,0.29225245653817084
"frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced'})","frozenset({'gender_male', 'test preparation course_none'})",0.156,0.308,0.069,0.44230769230769235,1.4360639360639362,1.0,0.020952000000000005,1.2408275862068965,0.3597774572429426,0.1746835443037975,0.19408626056024908,0.3331668331668332
"frozenset({'gender_male', 'test preparation course_none'})","frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced'})",0.308,0.156,0.069,0.22402597402597405,1.4360639360639362,1.0,0.020952000000000005,1.0876652719665272,0.4388037195275196,0.1746835443037975,0.08059949529143845,0.3331668331668332
"frozenset({'test preparation course_completed', 'writing_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})",frozenset({'math_cat_μέτριο'}),0.092,0.485,0.064,0.6956521739130435,1.4343343792021515,1.0,0.01938,1.6921428571428572,0.3334939427312775,0.12475633528265109,0.4090333474039679,0.4138054683998207
frozenset({'math_cat_μέτριο'}),"frozenset({'test preparation course_completed', 'writing_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})",0.485,0.092,0.064,0.13195876288659794,1.4343343792021515,1.0,0.01938,1.0460332541567696,0.5879854368932038,0.12475633528265109,0.04400744811299333,0.4138054683998207
"frozenset({'math_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",frozenset({'writing_cat_μέτριο'}),0.213,0.491,0.15,0.704225352112676,1.4342675195777517,1.0,0.045417,1.7209047619047617,0.38472681067344344,0.27075812274368233,0.4189103184924873,0.5048621668913685
frozenset({'writing_cat_μέτριο'}),"frozenset({'math_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",0.491,0.213,0.15,0.3054989816700611,1.4342675195777517,1.0,0.045417,1.1331876832844574,0.594852652259332,0.27075812274368233,0.1175336488819074,0.5048621668913685
"frozenset({'math_cat_μέτριο', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.332,0.418,0.199,0.5993975903614458,1.4339655271804923,1.0,0.060224,1.452812030075188,0.4530436614208769,0.3611615245009075,0.31167970852482096,0.5377370726926847
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'math_cat_μέτριο', 'lunch_standard'})",0.418,0.332,0.199,0.4760765550239235,1.4339655271804923,1.0,0.060224,1.274995433789954,0.5199882574383946,0.3611615245009075,0.21568346560467588,0.5377370726926847
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})","frozenset({'gender_male', 'race/ethnicity_group D'})",0.32,0.133,0.061,0.190625,1.4332706766917291,1.0,0.018439999999999998,1.0711969111969113,0.4445515911282546,0.15561224489795916,0.06646482122260666,0.3246358082706767
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})","frozenset({'math_cat_μέτριο', 'gender_male', 'lunch_standard'})",0.266,0.16,0.061,0.22932330827067668,1.4332706766917291,1.0,0.018439999999999998,1.0899512195121952,0.41184616071827396,0.16712328767123286,0.08252774794128176,0.30528665413533834
"frozenset({'gender_male', 'race/ethnicity_group D'})","frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})",0.133,0.32,0.061,0.45864661654135336,1.4332706766917291,1.0,0.018439999999999998,1.256111111111111,0.34866791461039576,0.15561224489795916,0.20389208314904905,0.3246358082706767
"frozenset({'math_cat_μέτριο', 'gender_male', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})",0.16,0.266,0.061,0.38125,1.4332706766917291,1.0,0.018439999999999998,1.1862626262626261,0.3598750975800156,0.16712328767123286,0.1570163487738419,0.30528665413533834
"frozenset({'math_cat_υψηλό', 'test preparation course_none'})",frozenset({'lunch_standard'}),0.092,0.645,0.085,0.923913043478261,1.43242332322211,1.0,0.025660000000000002,4.665714285714292,0.3324695516973309,0.1303680981595092,0.7856705450091859,0.5278479946073475
frozenset({'lunch_standard'}),"frozenset({'math_cat_υψηλό', 'test preparation course_none'})",0.645,0.092,0.085,0.13178294573643412,1.43242332322211,1.0,0.025660000000000002,1.0458214285714287,0.8503728251864127,0.1303680981595092,0.04381381689034595,0.5278479946073475
"frozenset({'lunch_free/reduced', 'gender_female'})","frozenset({'math_cat_χαμηλό', 'reading_cat_χαμηλό'})",0.189,0.218,0.059,0.31216931216931215,1.4319693218775786,1.0,0.017797999999999994,1.1369076923076922,0.37196179648477495,0.16954022988505746,0.12042111530602577,0.29140575700208726
"frozenset({'math_cat_χαμηλό', 'reading_cat_χαμηλό'})","frozenset({'lunch_free/reduced', 'gender_female'})",0.218,0.189,0.059,0.2706422018348624,1.4319693218775786,1.0,0.017797999999999994,1.1119371069182389,0.3857557761498113,0.16954022988505746,0.10066855959909048,0.29140575700208726
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})","frozenset({'race/ethnicity_group C', 'math_cat_μέτριο'})",0.283,0.153,0.062,0.21908127208480568,1.4319037391163771,1.0,0.018701000000000002,1.0846199095022624,0.4206820533585279,0.16577540106951874,0.07801803079670093,0.31215501512737015
"frozenset({'race/ethnicity_group C', 'math_cat_μέτριο'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})",0.153,0.283,0.062,0.40522875816993464,1.431903739116377,1.0,0.018701000000000002,1.2055054945054946,0.3561145599268767,0.16577540106951874,0.17047246606685448,0.31215501512737015
"frozenset({'math_cat_μέτριο', 'test preparation course_none'})","frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο'})",0.304,0.154,0.067,0.22039473684210528,1.4311346548188655,1.0,0.020184000000000007,1.0851645569620254,0.4328358208955225,0.17135549872122766,0.0784807764091079,0.3277298359535202
"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο'})","frozenset({'math_cat_μέτριο', 'test preparation course_none'})",0.154,0.304,0.067,0.4350649350649351,1.4311346548188655,1.0,0.020184000000000007,1.2320000000000002,0.3560918810204299,0.17135549872122766,0.18831168831168837,0.3277298359535202
"frozenset({'reading_cat_μέτριο', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'gender_male', 'test preparation course_none'})",0.327,0.154,0.072,0.2201834862385321,1.4297628976528058,1.0,0.021641999999999995,1.084870588235294,0.4466319960376423,0.17603911980440098,0.07823107120393866,0.3438579768854998
"frozenset({'math_cat_μέτριο', 'gender_male', 'test preparation course_none'})","frozenset({'reading_cat_μέτριο', 'lunch_standard'})",0.154,0.327,0.072,0.4675324675324675,1.4297628976528058,1.0,0.021641999999999995,1.2639268292682926,0.35529944838455474,0.17603911980440098,0.20881495918643017,0.3438579768854998
frozenset({'lunch_free/reduced'}),"frozenset({'writing_cat_χαμηλό', 'test preparation course_none', 'reading_cat_χαμηλό'})",0.355,0.201,0.102,0.28732394366197184,1.429472356527223,1.0,0.03064499999999999,1.1211264822134388,0.46580027359781107,0.22466960352422902,0.10803997955190467,0.397393315114568
"frozenset({'writing_cat_χαμηλό', 'test preparation course_none', 'reading_cat_χαμηλό'})",frozenset({'lunch_free/reduced'}),0.201,0.355,0.102,0.5074626865671641,1.429472356527223,1.0,0.03064499999999999,1.3095454545454543,0.3760214974600603,0.22466960352422902,0.2363762582436653,0.397393315114568
frozenset({'writing_cat_υψηλό'}),frozenset({'gender_female'}),0.208,0.518,0.154,0.7403846153846154,1.4293139293139294,1.0,0.046256000000000005,1.8565925925925928,0.379247015610652,0.2692307692307693,0.46137887009256306,0.5188409563409564
frozenset({'gender_female'}),frozenset({'writing_cat_υψηλό'}),0.518,0.208,0.154,0.29729729729729726,1.4293139293139292,1.0,0.046256000000000005,1.127076923076923,0.6231610712938515,0.2692307692307693,0.1127491127491127,0.5188409563409564
"frozenset({'math_cat_μέτριο', 'parental level of education_some college', 'lunch_standard'})",frozenset({'reading_cat_μέτριο'}),0.08,0.49,0.056,0.7,1.4285714285714286,1.0,0.016800000000000002,1.6999999999999997,0.32608695652173914,0.10894941634241248,0.41176470588235287,0.40714285714285714
frozenset({'reading_cat_μέτριο'}),"frozenset({'math_cat_μέτριο', 'parental level of education_some college', 'lunch_standard'})",0.49,0.08,0.056,0.1142857142857143,1.4285714285714286,1.0,0.016800000000000002,1.038709677419355,0.5882352941176471,0.10894941634241248,0.03726708074534162,0.40714285714285714
"frozenset({'math_cat_μέτριο', 'race/ethnicity_group D'})","frozenset({'writing_cat_μέτριο', 'lunch_standard'})",0.145,0.338,0.07,0.48275862068965525,1.4282799428688024,1.0,0.02099000000000001,1.2798666666666667,0.3507101086048455,0.16949152542372883,0.21866861131367862,0.34492960620281576
"frozenset({'writing_cat_μέτριο', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'race/ethnicity_group D'})",0.338,0.145,0.07,0.20710059171597633,1.4282799428688024,1.0,0.02099000000000001,1.078320895522388,0.4529564091497628,0.16949152542372883,0.07263227101283783,0.34492960620281576
frozenset({'math_cat_μέτριο'}),"frozenset({'writing_cat_μέτριο', 'test preparation course_none', 'race/ethnicity_group D'})",0.485,0.091,0.063,0.12989690721649486,1.427438540840603,1.0,0.018865,1.0447037914691943,0.581445523193096,0.12280701754385967,0.04279087702584693,0.41110229976209356
"frozenset({'writing_cat_μέτριο', 'test preparation course_none', 'race/ethnicity_group D'})",frozenset({'math_cat_μέτριο'}),0.091,0.485,0.063,0.6923076923076923,1.4274385408406027,1.0,0.018865,1.6737499999999998,0.3294218310719961,0.12280701754385967,0.40253920836445106,0.41110229976209356
"frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό'})","frozenset({'race/ethnicity_group C', 'gender_male'})",0.252,0.139,0.05,0.19841269841269843,1.427429484983442,1.0,0.014972,1.074118811881188,0.4003208556149732,0.1466275659824047,0.06900429548513173,0.27906246431426285
"frozenset({'race/ethnicity_group C', 'gender_male'})","frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό'})",0.139,0.252,0.05,0.3597122302158273,1.4274294849834417,1.0,0.014972,1.1682247191011235,0.34778164924506383,0.1466275659824047,0.14400030777517023,0.27906246431426285
frozenset({'reading_cat_μέτριο'}),"frozenset({'math_cat_μέτριο', 'gender_male', 'test preparation course_none', 'lunch_standard'})",0.49,0.103,0.072,0.14693877551020407,1.426590053497127,1.0,0.02153,1.0515071770334927,0.5863289760348585,0.13819577735124758,0.048984142151844004,0.42298395086189816
"frozenset({'math_cat_μέτριο', 'gender_male', 'test preparation course_none', 'lunch_standard'})",frozenset({'reading_cat_μέτριο'}),0.103,0.49,0.072,0.6990291262135923,1.426590053497127,1.0,0.02153,1.6945161290322581,0.3333643007556051,0.13819577735124758,0.40986103179135736,0.42298395086189816
frozenset({'reading_cat_μέτριο'}),"frozenset({'race/ethnicity_group C', 'math_cat_μέτριο', 'lunch_standard'})",0.49,0.103,0.072,0.14693877551020407,1.426590053497127,1.0,0.02153,1.0515071770334927,0.5863289760348585,0.13819577735124758,0.048984142151844004,0.42298395086189816
"frozenset({'race/ethnicity_group C', 'math_cat_μέτριο', 'lunch_standard'})",frozenset({'reading_cat_μέτριο'}),0.103,0.49,0.072,0.6990291262135923,1.426590053497127,1.0,0.02153,1.6945161290322581,0.3333643007556051,0.13819577735124758,0.40986103179135736,0.42298395086189816
frozenset({'math_cat_μέτριο'}),"frozenset({'reading_cat_μέτριο', 'parental level of education_some high school'})",0.485,0.081,0.056,0.1154639175257732,1.4254804632811506,1.0,0.016715,1.038962703962704,0.5795769764216366,0.10980392156862748,0.03750154246547682,0.4034109711085656
"frozenset({'reading_cat_μέτριο', 'parental level of education_some high school'})",frozenset({'math_cat_μέτριο'}),0.081,0.485,0.056,0.691358024691358,1.4254804632811506,1.0,0.016715,1.6685999999999999,0.324790144567076,0.10980392156862748,0.4006951935754524,0.4034109711085656
frozenset({'reading_cat_χαμηλό'}),frozenset({'lunch_free/reduced'}),0.275,0.355,0.139,0.5054545454545455,1.4238156209987198,1.0,0.04137500000000001,1.3042279411764708,0.4105680972463409,0.2830957230142567,0.233262861169838,0.4485019206145967
frozenset({'lunch_free/reduced'}),frozenset({'reading_cat_χαμηλό'}),0.355,0.275,0.139,0.3915492957746479,1.4238156209987196,1.0,0.04137500000000001,1.191550925925926,0.46149127209860025,0.2830957230142567,0.16075764934434195,0.4485019206145967
"frozenset({'math_cat_χαμηλό', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'gender_female'})",0.157,0.264,0.059,0.375796178343949,1.4234703725149582,1.0,0.017551999999999998,1.1791020408163264,0.3528962341918491,0.1629834254143646,0.1518969814455829,0.29964051341439873
"frozenset({'writing_cat_μέτριο', 'gender_female'})","frozenset({'math_cat_χαμηλό', 'lunch_standard'})",0.264,0.157,0.059,0.22348484848484845,1.4234703725149582,1.0,0.017551999999999998,1.0856195121951218,0.40420044215180545,0.1629834254143646,0.07886696142923898,0.29964051341439873
frozenset({'gender_male'}),"frozenset({'math_cat_χαμηλό', 'lunch_standard', 'reading_cat_χαμηλό', 'writing_cat_χαμηλό'})",0.482,0.086,0.059,0.12240663900414937,1.4233330116761556,1.0,0.017548,1.04148463356974,0.574177082651659,0.11591355599214147,0.039832208976093415,0.4042265753160282
"frozenset({'math_cat_χαμηλό', 'lunch_standard', 'reading_cat_χαμηλό', 'writing_cat_χαμηλό'})",frozenset({'gender_male'}),0.086,0.482,0.059,0.686046511627907,1.4233330116761556,1.0,0.017548,1.649925925925926,0.32540889366910214,0.11591355599214147,0.39391218460985905,0.4042265753160282
"frozenset({'writing_cat_μέτριο', 'test preparation course_none'})","frozenset({'race/ethnicity_group C', 'math_cat_μέτριο'})",0.308,0.153,0.067,0.21753246753246755,1.4217808335455395,1.0,0.019876000000000005,1.0824730290456432,0.4286946769044949,0.17005076142131983,0.07618945399346819,0.32772048213224686
"frozenset({'race/ethnicity_group C', 'math_cat_μέτριο'})","frozenset({'writing_cat_μέτριο', 'test preparation course_none'})",0.153,0.308,0.067,0.4379084967320262,1.4217808335455395,1.0,0.019876000000000005,1.2311162790697674,0.350244057164003,0.17005076142131983,0.1877290415202691,0.32772048213224686
"frozenset({'writing_cat_χαμηλό', 'gender_male'})",frozenset({'parental level of education_high school'}),0.201,0.196,0.056,0.27860696517412936,1.4214641080312722,1.0,0.016604,1.1145103448275864,0.3710888610763455,0.16422287390029325,0.10274498155986238,0.2821606254442075
frozenset({'parental level of education_high school'}),"frozenset({'writing_cat_χαμηλό', 'gender_male'})",0.196,0.201,0.056,0.2857142857142857,1.421464108031272,1.0,0.016604,1.1185999999999998,0.3687810945273632,0.16422287390029325,0.10602538887895581,0.2821606254442075
"frozenset({'race/ethnicity_group C', 'math_cat_μέτριο', 'test preparation course_none'})",frozenset({'writing_cat_μέτριο'}),0.096,0.491,0.067,0.6979166666666667,1.4214188730482011,1.0,0.019864000000000007,1.6849655172413798,0.32796196011094975,0.12884615384615386,0.40651604453176177,0.41718643923964704
frozenset({'writing_cat_μέτριο'}),"frozenset({'race/ethnicity_group C', 'math_cat_μέτριο', 'test preparation course_none'})",0.491,0.096,0.067,0.1364562118126273,1.421418873048201,1.0,0.019864000000000007,1.0468490566037736,0.5824707503738675,0.12884615384615386,0.04475244669538417,0.41718643923964704
"frozenset({'parental level of education_high school', 'reading_cat_χαμηλό'})",frozenset({'gender_male'}),0.073,0.482,0.05,0.6849315068493151,1.4210197237537658,1.0,0.014814000000000008,1.6440869565217395,0.31961165048543705,0.09900990099009904,0.3917596657322686,0.39433297334167
frozenset({'gender_male'}),"frozenset({'parental level of education_high school', 'reading_cat_χαμηλό'})",0.482,0.073,0.05,0.1037344398340249,1.4210197237537658,1.0,0.014814000000000008,1.0342916666666666,0.5719691119691122,0.09900990099009904,0.03315473552753496,0.39433297334167
"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})",frozenset({'math_cat_μέτριο'}),0.09,0.485,0.062,0.6888888888888889,1.420389461626575,1.0,0.018350000000000005,1.655357142857143,0.3252392768521801,0.12085769980506825,0.3959007551240561,0.4083619702176403
frozenset({'math_cat_μέτριο'}),"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})",0.485,0.09,0.062,0.12783505154639174,1.4203894616265749,1.0,0.018350000000000005,1.0433806146572104,0.5746946445349203,0.12085769980506825,0.04157697972130961,0.4083619702176403
frozenset({'math_cat_μέτριο'}),"frozenset({'writing_cat_μέτριο', 'gender_female', 'test preparation course_none', 'lunch_standard'})",0.485,0.122,0.084,0.17319587628865982,1.4196383302349165,1.0,0.024830000000000005,1.061920199501247,0.5739713361072585,0.16061185468451245,0.05830965408731187,0.4308602332262972
"frozenset({'writing_cat_μέτριο', 'gender_female', 'test preparation course_none', 'lunch_standard'})",frozenset({'math_cat_μέτριο'}),0.122,0.485,0.084,0.6885245901639345,1.4196383302349165,1.0,0.024830000000000005,1.6534210526315796,0.3366688360993601,0.16061185468451245,0.3951933789590962,0.4308602332262972
frozenset({'gender_female'}),"frozenset({'writing_cat_υψηλό', 'reading_cat_υψηλό'})",0.518,0.185,0.136,0.2625482625482626,1.4191797975581761,1.0,0.04017000000000001,1.105157068062827,0.6127959482548208,0.23985890652557318,0.09515124239050624,0.4988416988416989
"frozenset({'writing_cat_υψηλό', 'reading_cat_υψηλό'})",frozenset({'gender_female'}),0.185,0.518,0.136,0.7351351351351352,1.419179797558176,1.0,0.04017000000000001,1.819795918367347,0.36241429086972227,0.23985890652557318,0.4504878322305709,0.4988416988416989
frozenset({'reading_cat_μέτριο'}),"frozenset({'math_cat_μέτριο', 'test preparation course_none', 'gender_male'})",0.49,0.154,0.107,0.21836734693877552,1.4179697853167241,1.0,0.03154,1.082349869451697,0.5779732453729155,0.19925512104283052,0.07608433444299707,0.4565862708719851
"frozenset({'math_cat_μέτριο', 'test preparation course_none', 'gender_male'})",frozenset({'reading_cat_μέτριο'}),0.154,0.49,0.107,0.6948051948051948,1.4179697853167241,1.0,0.03154,1.6710638297872338,0.3484235876361548,0.19925512104283052,0.40157881334351914,0.4565862708719851
frozenset({'math_cat_μέτριο'}),"frozenset({'parental level of education_high school', 'writing_cat_μέτριο'})",0.485,0.096,0.066,0.13608247422680414,1.4175257731958764,1.0,0.019440000000000006,1.0463961813842482,0.5719329214474848,0.12815533980582527,0.04433902016239396,0.41179123711340204
"frozenset({'parental level of education_high school', 'writing_cat_μέτριο'})",frozenset({'math_cat_μέτριο'}),0.096,0.485,0.066,0.6875,1.4175257731958764,1.0,0.019440000000000006,1.6480000000000001,0.32582461786001615,0.12815533980582527,0.3932038834951456,0.41179123711340204
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'math_cat_μέτριο', 'gender_male', 'test preparation course_none', 'lunch_standard'})",0.418,0.103,0.061,0.145933014354067,1.4168253820783205,1.0,0.017946000000000004,1.0502689075630254,0.5054926483015042,0.1326086956521739,0.047862892256484955,0.3690830120314025
"frozenset({'math_cat_μέτριο', 'gender_male', 'test preparation course_none', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.103,0.418,0.061,0.5922330097087379,1.4168253820783205,1.0,0.017946000000000004,1.4272857142857147,0.32797850759361813,0.1326086956521739,0.2993694324892404,0.3690830120314025
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'math_cat_μέτριο', 'test preparation course_none'})",0.418,0.304,0.18,0.430622009569378,1.4165197683203226,1.0,0.052928,1.2223865546218486,0.5052310042000764,0.3321033210332103,0.1819281746686466,0.5113636363636364
"frozenset({'math_cat_μέτριο', 'test preparation course_none'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.304,0.418,0.18,0.5921052631578947,1.4165197683203223,1.0,0.052928,1.4268387096774193,0.42247765006385707,0.3321033210332103,0.2991499366974136,0.5113636363636364
"frozenset({'math_cat_χαμηλό', 'lunch_standard'})","frozenset({'reading_cat_μέτριο', 'gender_female'})",0.157,0.261,0.058,0.3694267515923567,1.4154281670205238,1.0,0.017023000000000003,1.171949494949495,0.3481613285883749,0.1611111111111111,0.14672090878532706,0.29582448690728946
"frozenset({'reading_cat_μέτριο', 'gender_female'})","frozenset({'math_cat_χαμηλό', 'lunch_standard'})",0.261,0.157,0.058,0.22222222222222224,1.4154281670205238,1.0,0.017023000000000003,1.0838571428571429,0.3971583220568336,0.1611111111111111,0.07736918413074999,0.29582448690728946
"frozenset({'math_cat_μέτριο', 'gender_male', 'lunch_standard'})","frozenset({'reading_cat_μέτριο', 'test preparation course_none'})",0.16,0.318,0.072,0.44999999999999996,1.4150943396226414,1.0,0.021119999999999993,1.2399999999999998,0.34920634920634913,0.17733990147783252,0.19354838709677413,0.3382075471698113
"frozenset({'reading_cat_μέτριο', 'test preparation course_none'})","frozenset({'math_cat_μέτριο', 'gender_male', 'lunch_standard'})",0.318,0.16,0.072,0.22641509433962262,1.4150943396226414,1.0,0.021119999999999993,1.0858536585365852,0.4301075268817203,0.17733990147783252,0.07906558849955074,0.3382075471698113
frozenset({'lunch_standard'}),"frozenset({'math_cat_υψηλό', 'gender_female'})",0.645,0.068,0.062,0.09612403100775194,1.4135886912904696,1.0,0.018139999999999996,1.0311149228130359,0.8241708314402544,0.09523809523809523,0.030175998935356153,0.5039443684450524
"frozenset({'math_cat_υψηλό', 'gender_female'})",frozenset({'lunch_standard'}),0.068,0.645,0.062,0.9117647058823529,1.4135886912904696,1.0,0.018139999999999996,4.0233333333333325,0.3139277308597535,0.09523809523809523,0.7514498757249378,0.5039443684450524
"frozenset({'math_cat_μέτριο', 'gender_female'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})",0.245,0.283,0.098,0.4,1.413427561837456,1.0,0.02866500000000001,1.1950000000000003,0.3874172185430465,0.22790697674418603,0.16317991631799167,0.37314487632508836
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'gender_female'})",0.283,0.245,0.098,0.3462897526501767,1.4134275618374559,1.0,0.02866500000000001,1.154945945945946,0.4079497907949792,0.22790697674418603,0.134158612781691,0.37314487632508836
frozenset({'math_cat_μέτριο'}),"frozenset({'parental level of education_high school', 'writing_cat_μέτριο', 'lunch_standard'})",0.485,0.073,0.05,0.10309278350515465,1.4122299110295158,1.0,0.014595000000000004,1.033551724137931,0.5667961165048545,0.09842519685039372,0.03246254962799854,0.3940121451772349
"frozenset({'writing_cat_μέτριο', 'parental level of education_some college', 'test preparation course_none'})",frozenset({'math_cat_μέτριο'}),0.073,0.485,0.05,0.6849315068493151,1.4122299110295158,1.0,0.014595000000000004,1.6345652173913048,0.31488673139158585,0.09842519685039372,0.38821651815401,0.3940121451772349
frozenset({'math_cat_μέτριο'}),"frozenset({'writing_cat_μέτριο', 'parental level of education_some college', 'test preparation course_none'})",0.485,0.073,0.05,0.10309278350515465,1.4122299110295158,1.0,0.014595000000000004,1.033551724137931,0.5667961165048545,0.09842519685039372,0.03246254962799854,0.3940121451772349
"frozenset({'parental level of education_high school', 'writing_cat_μέτριο', 'lunch_standard'})",frozenset({'math_cat_μέτριο'}),0.073,0.485,0.05,0.6849315068493151,1.4122299110295158,1.0,0.014595000000000004,1.6345652173913048,0.31488673139158585,0.09842519685039372,0.38821651815401,0.3940121451772349
"frozenset({'math_cat_χαμηλό', 'reading_cat_χαμηλό', 'writing_cat_χαμηλό'})",frozenset({'parental level of education_high school'}),0.206,0.196,0.057,0.27669902912621364,1.4117297404398654,1.0,0.016624,1.111570469798658,0.36731627557558877,0.16521739130434782,0.10037192677389754,0.28375767782841294
frozenset({'parental level of education_high school'}),"frozenset({'math_cat_χαμηλό', 'reading_cat_χαμηλό', 'writing_cat_χαμηλό'})",0.196,0.206,0.057,0.29081632653061223,1.4117297404398652,1.0,0.016624,1.1195971223021584,0.36274766518285756,0.16521739130434782,0.10682156993779879,0.28375767782841294
frozenset({'race/ethnicity_group D'}),"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'gender_male'})",0.262,0.165,0.061,0.23282442748091603,1.4110571362479758,1.0,0.017769999999999994,1.0884079601990049,0.3947309964902927,0.16666666666666663,0.08122685925858207,0.3012606985889429
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'gender_male'})",frozenset({'race/ethnicity_group D'}),0.165,0.262,0.061,0.3696969696969697,1.4110571362479758,1.0,0.017769999999999994,1.1708653846153845,0.3488760184548934,0.16666666666666663,0.1459308532479264,0.3012606985889429
frozenset({'math_cat_μέτριο'}),"frozenset({'reading_cat_μέτριο', 'parental level of education_some college'})",0.485,0.114,0.078,0.16082474226804125,1.410743353228432,1.0,0.02271,1.0557985257985258,0.5653472740851381,0.14971209213051823,0.052849596239324205,0.42251763429191536
frozenset({'math_cat_μέτριο'}),"frozenset({'writing_cat_μέτριο', 'parental level of education_some college'})",0.485,0.114,0.078,0.16082474226804125,1.410743353228432,1.0,0.02271,1.0557985257985258,0.5653472740851381,0.14971209213051823,0.052849596239324205,0.42251763429191536
"frozenset({'writing_cat_μέτριο', 'parental level of education_some college'})",frozenset({'math_cat_μέτριο'}),0.114,0.485,0.078,0.6842105263157895,1.410743353228432,1.0,0.02271,1.6308333333333334,0.32861607918041325,0.14971209213051823,0.3868165559529893,0.42251763429191536
frozenset({'math_cat_μέτριο'}),"frozenset({'reading_cat_μέτριο', 'parental level of education_some college', 'test preparation course_none'})",0.485,0.076,0.052,0.10721649484536082,1.410743353228432,1.0,0.01514,1.0349653579676674,0.5653472740851382,0.10216110019646367,0.033784085330477075,0.39571351058057513
"frozenset({'reading_cat_μέτριο', 'parental level of education_some college'})",frozenset({'math_cat_μέτριο'}),0.114,0.485,0.078,0.6842105263157895,1.410743353228432,1.0,0.02271,1.6308333333333334,0.32861607918041325,0.14971209213051823,0.3868165559529893,0.42251763429191536
"frozenset({'reading_cat_μέτριο', 'parental level of education_some college', 'test preparation course_none'})",frozenset({'math_cat_μέτριο'}),0.076,0.485,0.052,0.6842105263157895,1.410743353228432,1.0,0.01514,1.6308333333333334,0.3151015651015651,0.10216110019646367,0.3868165559529893,0.39571351058057513
"frozenset({'writing_cat_μέτριο', 'test preparation course_none'})","frozenset({'math_cat_μέτριο', 'race/ethnicity_group D'})",0.308,0.145,0.063,0.20454545454545456,1.4106583072100316,1.0,0.018340000000000002,1.074857142857143,0.42068079640333983,0.16153846153846155,0.06964380648591177,0.3195141065830721
"frozenset({'math_cat_μέτριο', 'race/ethnicity_group D'})","frozenset({'writing_cat_μέτριο', 'test preparation course_none'})",0.145,0.308,0.063,0.4344827586206897,1.4106583072100314,1.0,0.018340000000000002,1.2236585365853658,0.3404808317089019,0.16153846153846155,0.18277855292007184,0.3195141065830721
frozenset({'test preparation course_completed'}),"frozenset({'math_cat_μέτριο', 'reading_cat_υψηλό'})",0.358,0.103,0.052,0.1452513966480447,1.410207734447036,1.0,0.015126,1.0494313725490196,0.4530913012221424,0.1271393643031785,0.04710300629659388,0.32505288279004174
"frozenset({'math_cat_μέτριο', 'reading_cat_υψηλό'})",frozenset({'test preparation course_completed'}),0.103,0.358,0.052,0.5048543689320388,1.4102077344470358,1.0,0.015126,1.2965882352941176,0.32428608181116547,0.1271393643031785,0.228745122947101,0.32505288279004174
frozenset({'math_cat_μέτριο'}),"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'parental level of education_some college'})",0.485,0.098,0.067,0.13814432989690723,1.409636019356196,1.0,0.01947,1.046578947368421,0.5642660483987828,0.1298449612403101,0.04450590897661554,0.4109088996423311
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'parental level of education_some college'})",frozenset({'math_cat_μέτριο'}),0.098,0.485,0.067,0.6836734693877551,1.409636019356196,1.0,0.01947,1.6280645161290321,0.3221696396068438,0.1298449612403101,0.38577372696651474,0.4109088996423311
frozenset({'lunch_standard'}),"frozenset({'math_cat_υψηλό', 'gender_female', 'reading_cat_υψηλό'})",0.645,0.066,0.06,0.09302325581395349,1.409443269908386,1.0,0.017429999999999994,1.0297948717948717,0.8183098591549294,0.09216589861751151,0.02893282207061401,0.5010570824524313
"frozenset({'math_cat_υψηλό', 'gender_female', 'reading_cat_υψηλό'})",frozenset({'lunch_standard'}),0.066,0.645,0.06,0.9090909090909091,1.409443269908386,1.0,0.017429999999999994,3.9049999999999985,0.3110278372591006,0.09216589861751151,0.7439180537772087,0.5010570824524313
frozenset({'math_cat_μέτριο'}),"frozenset({'writing_cat_μέτριο', 'gender_male', 'lunch_standard'})",0.485,0.161,0.11,0.2268041237113402,1.4087212652878274,1.0,0.031915,1.0851066666666667,0.5633715798764343,0.2052238805970149,0.07843161348193112,0.4550169686879682
"frozenset({'writing_cat_μέτριο', 'gender_male', 'lunch_standard'})",frozenset({'math_cat_μέτριο'}),0.161,0.485,0.11,0.6832298136645962,1.4087212652878274,1.0,0.031915,1.62578431372549,0.34581211398851447,0.2052238805970149,0.3849122595429053,0.4550169686879682
"frozenset({'race/ethnicity_group C', 'gender_female', 'test preparation course_none'})",frozenset({'math_cat_χαμηλό'}),0.111,0.339,0.053,0.4774774774774775,1.408488134151851,1.0,0.015370999999999996,1.2650172413793104,0.3262304476091431,0.13350125944584382,0.20949694020798404,0.31690983018416646
frozenset({'math_cat_χαμηλό'}),"frozenset({'race/ethnicity_group C', 'gender_female', 'test preparation course_none'})",0.339,0.111,0.053,0.15634218289085544,1.4084881341518507,1.0,0.015370999999999996,1.0537447552447552,0.4387577426997401,0.13350125944584382,0.051003580304674286,0.31690983018416646
"frozenset({'math_cat_μέτριο', 'race/ethnicity_group D'})",frozenset({'reading_cat_μέτριο'}),0.145,0.49,0.1,0.6896551724137931,1.4074595355383535,1.0,0.028950000000000017,1.6433333333333335,0.33859649122807034,0.18691588785046728,0.39148073022312385,0.4468684025334272
frozenset({'reading_cat_μέτριο'}),"frozenset({'math_cat_μέτριο', 'race/ethnicity_group D'})",0.49,0.145,0.1,0.20408163265306123,1.4074595355383535,1.0,0.028950000000000017,1.0742307692307693,0.5676470588235297,0.18691588785046728,0.0691013247404225,0.4468684025334272
frozenset({'lunch_standard'}),"frozenset({'math_cat_υψηλό', 'writing_cat_υψηλό', 'gender_female'})",0.645,0.065,0.059,0.09147286821705426,1.4072748956469887,1.0,0.017074999999999993,1.0291382252559726,0.815230365242301,0.09062980030721965,0.028313228039630222,0.4995825879546809
frozenset({'lunch_standard'}),"frozenset({'math_cat_υψηλό', 'writing_cat_υψηλό', 'gender_female', 'reading_cat_υψηλό'})",0.645,0.065,0.059,0.09147286821705426,1.4072748956469887,1.0,0.017074999999999993,1.0291382252559726,0.815230365242301,0.09062980030721965,0.028313228039630222,0.4995825879546809
"frozenset({'math_cat_υψηλό', 'writing_cat_υψηλό', 'gender_female', 'reading_cat_υψηλό'})",frozenset({'lunch_standard'}),0.065,0.645,0.059,0.9076923076923076,1.4072748956469885,1.0,0.017074999999999993,3.8458333333333283,0.30952596755188966,0.09062980030721965,0.7399783315276269,0.4995825879546809
"frozenset({'math_cat_υψηλό', 'writing_cat_υψηλό', 'gender_female'})",frozenset({'lunch_standard'}),0.065,0.645,0.059,0.9076923076923076,1.4072748956469885,1.0,0.017074999999999993,3.8458333333333283,0.30952596755188966,0.09062980030721965,0.7399783315276269,0.4995825879546809
frozenset({'writing_cat_χαμηλό'}),frozenset({'parental level of education_high school'}),0.301,0.196,0.083,0.27574750830564787,1.4068750423757543,1.0,0.024004000000000004,1.1101100917431195,0.41374080011031256,0.20048309178743964,0.09918844316622868,0.34960844803037494
frozenset({'parental level of education_high school'}),frozenset({'writing_cat_χαμηλό'}),0.196,0.301,0.083,0.42346938775510207,1.4068750423757543,1.0,0.024004000000000004,1.2124247787610622,0.3597074866630702,0.20048309178743964,0.1752065633120201,0.34960844803037494
"frozenset({'writing_cat_χαμηλό', 'test preparation course_none'})",frozenset({'parental level of education_high school'}),0.243,0.196,0.067,0.2757201646090535,1.4067355337196608,1.0,0.019372,1.110068181818182,0.38194759360397484,0.18010752688172044,0.09915443359335013,0.3087784496514655
frozenset({'parental level of education_high school'}),"frozenset({'writing_cat_χαμηλό', 'test preparation course_none'})",0.196,0.243,0.067,0.34183673469387754,1.4067355337196608,1.0,0.019372,1.150170542635659,0.3596198113908071,0.18010752688172044,0.1305637182217669,0.3087784496514655
"frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",frozenset({'math_cat_μέτριο'}),0.223,0.485,0.152,0.6816143497757847,1.4053904119088345,1.0,0.043844999999999995,1.6175352112676056,0.37124060150375937,0.2733812949640288,0.3817754364578344,0.4975082058157274
frozenset({'math_cat_μέτριο'}),"frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",0.485,0.223,0.152,0.3134020618556701,1.4053904119088345,1.0,0.043844999999999995,1.1316666666666666,0.5601047521716913,0.2733812949640288,0.11634756995581737,0.4975082058157274
"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})",frozenset({'math_cat_μέτριο'}),0.091,0.485,0.062,0.6813186813186813,1.4047807862240853,1.0,0.017865,1.6160344827586208,0.31699137655701054,0.12062256809338522,0.3812013229488958,0.40457686643253654
frozenset({'math_cat_μέτριο'}),"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})",0.485,0.091,0.062,0.12783505154639174,1.404780786224085,1.0,0.017865,1.0422340425531915,0.5595051675540244,0.12062256809338522,0.04052260896192711,0.40457686643253654
frozenset({'writing_cat_μέτριο'}),"frozenset({'math_cat_μέτριο', 'race/ethnicity_group D'})",0.491,0.145,0.1,0.20366598778004075,1.404593019172695,1.0,0.02880500000000001,1.0736700767263427,0.5659135559921417,0.1865671641791045,0.06861519038601259,0.44666058009691695
"frozenset({'math_cat_μέτριο', 'race/ethnicity_group D'})",frozenset({'writing_cat_μέτριο'}),0.145,0.491,0.1,0.6896551724137931,1.4045930191726947,1.0,0.02880500000000001,1.6401111111111113,0.33690058479532176,0.1865671641791045,0.39028521102906316,0.44666058009691695
"frozenset({'writing_cat_χαμηλό', 'test preparation course_none'})",frozenset({'lunch_free/reduced'}),0.243,0.355,0.121,0.49794238683127573,1.4026546107923261,1.0,0.034735,1.2847131147540987,0.3792154764894047,0.25366876310272535,0.221616103614381,0.4193937286269055
frozenset({'lunch_free/reduced'}),"frozenset({'writing_cat_χαμηλό', 'test preparation course_none'})",0.355,0.243,0.121,0.3408450704225352,1.402654610792326,1.0,0.034735,1.148440170940171,0.4450637452751618,0.25366876310272535,0.12925372578934638,0.4193937286269055
"frozenset({'test preparation course_completed', 'reading_cat_μέτριο'})",frozenset({'math_cat_μέτριο'}),0.172,0.485,0.117,0.680232558139535,1.402541356988732,1.0,0.03358000000000001,1.6105454545454552,0.34662867996201335,0.21666666666666667,0.3790923459020097,0.46073483577079843
frozenset({'math_cat_μέτριο'}),"frozenset({'test preparation course_completed', 'reading_cat_μέτριο'})",0.485,0.172,0.117,0.2412371134020619,1.402541356988732,1.0,0.03358000000000001,1.09125,0.5572981495311594,0.21666666666666667,0.08361970217640326,0.46073483577079843
"frozenset({'math_cat_μέτριο', 'gender_male'})","frozenset({'reading_cat_μέτριο', 'test preparation course_none'})",0.24,0.318,0.107,0.44583333333333336,1.4019916142557654,1.0,0.03068,1.230676691729323,0.37727496310870634,0.23725055432372502,0.18743890518084072,0.3911556603773585
"frozenset({'reading_cat_μέτριο', 'test preparation course_none'})","frozenset({'math_cat_μέτριο', 'gender_male'})",0.318,0.24,0.107,0.33647798742138363,1.4019916142557651,1.0,0.03068,1.1454028436018957,0.42042371255515665,0.23725055432372502,0.12694472029129425,0.3911556603773585
"frozenset({'math_cat_μέτριο', 'lunch_standard'})",frozenset({'reading_cat_μέτριο'}),0.332,0.49,0.228,0.6867469879518072,1.4015244652077699,1.0,0.06531999999999999,1.628076923076923,0.42887908393738833,0.3838383838383838,0.38577840774864164,0.5760265552003934
frozenset({'reading_cat_μέτριο'}),"frozenset({'math_cat_μέτριο', 'lunch_standard'})",0.49,0.332,0.228,0.4653061224489796,1.4015244652077699,1.0,0.06531999999999999,1.2493129770992366,0.5617475060199517,0.3838383838383838,0.19956006354637668,0.5760265552003934
"frozenset({'math_cat_υψηλό', 'reading_cat_υψηλό'})","frozenset({'gender_male', 'lunch_standard'})",0.131,0.316,0.058,0.44274809160305345,1.4011015557058653,1.0,0.016604,1.2274520547945207,0.3294313717709615,0.14910025706940874,0.18530422748984424,0.3131461977002609
"frozenset({'gender_male', 'lunch_standard'})","frozenset({'math_cat_υψηλό', 'reading_cat_υψηλό'})",0.316,0.131,0.058,0.18354430379746836,1.4011015557058653,1.0,0.016604,1.0643565891472868,0.4185319620891309,0.14910025706940874,0.0604652517807461,0.3131461977002609
"frozenset({'math_cat_μέτριο', 'gender_male'})",frozenset({'writing_cat_μέτριο'}),0.24,0.491,0.165,0.6875000000000001,1.4002036659877803,1.0,0.04716000000000001,1.6288000000000007,0.37607655502392345,0.2915194346289753,0.38605108055009846,0.5117744399185337
frozenset({'writing_cat_μέτριο'}),"frozenset({'math_cat_μέτριο', 'gender_male'})",0.491,0.24,0.165,0.3360488798370672,1.4002036659877801,1.0,0.04716000000000001,1.1446625766871166,0.5615288444365066,0.2915194346289753,0.12638010504877267,0.5117744399185337
"frozenset({'math_cat_μέτριο', 'parental level of education_some college', 'lunch_standard'})",frozenset({'writing_cat_μέτριο'}),0.08,0.491,0.055,0.6875,1.4002036659877801,1.0,0.015719999999999998,1.6288,0.31067193675889326,0.10658914728682173,0.38605108055009824,0.3997581466395112
frozenset({'writing_cat_μέτριο'}),"frozenset({'math_cat_μέτριο', 'parental level of education_some college', 'lunch_standard'})",0.491,0.08,0.055,0.1120162932790224,1.4002036659877801,1.0,0.015719999999999998,1.0360550458715596,0.5615288444365064,0.10658914728682173,0.03480031878154609,0.3997581466395112
frozenset({'writing_cat_μέτριο'}),"frozenset({'lunch_free/reduced', 'math_cat_μέτριο', 'gender_male'})",0.491,0.08,0.055,0.1120162932790224,1.4002036659877801,1.0,0.015719999999999998,1.0360550458715596,0.5615288444365064,0.10658914728682173,0.03480031878154609,0.3997581466395112
"frozenset({'lunch_free/reduced', 'math_cat_μέτριο', 'gender_male'})",frozenset({'writing_cat_μέτριο'}),0.08,0.491,0.055,0.6875,1.4002036659877801,1.0,0.015719999999999998,1.6288,0.31067193675889326,0.10658914728682173,0.38605108055009824,0.3997581466395112
frozenset({'writing_cat_μέτριο'}),"frozenset({'math_cat_μέτριο', 'lunch_standard', 'gender_male'})",0.491,0.16,0.11,0.2240325865580448,1.4002036659877801,1.0,0.031439999999999996,1.0825196850393701,0.5615288444365064,0.2033271719038817,0.0762292697119581,0.4557662932790224
"frozenset({'math_cat_μέτριο', 'lunch_standard', 'gender_male'})",frozenset({'writing_cat_μέτριο'}),0.16,0.491,0.11,0.6875,1.4002036659877801,1.0,0.031439999999999996,1.6288,0.3402597402597402,0.2033271719038817,0.38605108055009824,0.4557662932790224
"frozenset({'writing_cat_χαμηλό', 'parental level of education_high school'})",frozenset({'gender_male'}),0.083,0.482,0.056,0.6746987951807228,1.3997900314952756,1.0,0.015994,1.5923703703703702,0.31145817105468143,0.11001964636542243,0.37200539610178157,0.3954406838974154
frozenset({'gender_male'}),"frozenset({'writing_cat_χαμηλό', 'parental level of education_high school'})",0.482,0.083,0.056,0.11618257261410789,1.3997900314952756,1.0,0.015994,1.0375446009389673,0.551365140650855,0.11001964636542243,0.03618601157481775,0.3954406838974154
"frozenset({'math_cat_μέτριο', 'gender_male', 'test preparation course_none'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})",0.154,0.283,0.061,0.3961038961038961,1.3996604102611172,1.0,0.017418000000000003,1.1872903225806453,0.33751889315195915,0.16223404255319152,0.15774601967070592,0.3058257996420541
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'gender_male', 'test preparation course_none'})",0.283,0.154,0.061,0.21554770318021202,1.399660410261117,1.0,0.017418000000000003,1.0784594594594594,0.39824404966047056,0.16223404255319152,0.07275142219883218,0.3058257996420541
frozenset({'math_cat_μέτριο'}),"frozenset({'writing_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",0.485,0.221,0.15,0.30927835051546393,1.3994495498437283,1.0,0.04281499999999999,1.1278059701492538,0.5542394822006471,0.26978417266187055,0.11332265791458783,0.494005691094836
"frozenset({'writing_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",frozenset({'math_cat_μέτριο'}),0.221,0.485,0.15,0.6787330316742081,1.399449549843728,1.0,0.04281499999999999,1.6030281690140842,0.36640992725716726,0.26978417266187055,0.3761806440275885,0.494005691094836
"frozenset({'math_cat_μέτριο', 'gender_female'})","frozenset({'reading_cat_μέτριο', 'lunch_standard'})",0.245,0.327,0.112,0.4571428571428572,1.397990388816077,1.0,0.031885,1.2397368421052632,0.37706953642384106,0.24347826086956517,0.19337720229250693,0.399825251201398
"frozenset({'reading_cat_μέτριο', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'gender_female'})",0.327,0.245,0.112,0.3425076452599388,1.3979903888160767,1.0,0.031885,1.1483023255813953,0.4230126300148587,0.24347826086956517,0.12914919901978653,0.399825251201398
frozenset({'reading_cat_χαμηλό'}),"frozenset({'gender_male', 'test preparation course_none', 'lunch_standard'})",0.275,0.203,0.078,0.28363636363636363,1.3972234661889833,1.0,0.022174999999999993,1.1125634517766498,0.39213085764809896,0.19499999999999998,0.10117486027147256,0.33393640841916705
"frozenset({'gender_male', 'test preparation course_none', 'lunch_standard'})",frozenset({'reading_cat_χαμηλό'}),0.203,0.275,0.078,0.3842364532019704,1.3972234661889833,1.0,0.022174999999999993,1.1774,0.35670623813660196,0.19499999999999998,0.15067096993375229,0.33393640841916705
"frozenset({'math_cat_υψηλό', 'gender_male'})","frozenset({'writing_cat_μέτριο', 'lunch_standard'})",0.108,0.338,0.051,0.4722222222222222,1.3971071663379355,1.0,0.014495999999999995,1.254315789473684,0.31864943286731723,0.1291139240506329,0.2027526015441423,0.3115548980933596
"frozenset({'writing_cat_μέτριο', 'lunch_standard'})","frozenset({'math_cat_υψηλό', 'gender_male'})",0.338,0.108,0.051,0.15088757396449703,1.3971071663379355,1.0,0.014495999999999995,1.0505087108013937,0.4293584503287719,0.1291139240506329,0.048080239870512374,0.3115548980933596
"frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό'})",frozenset({'parental level of education_high school'}),0.252,0.196,0.069,0.27380952380952384,1.3969873663751216,1.0,0.019608,1.1071475409836067,0.3799116484538479,0.1820580474934037,0.09677801468846246,0.3129251700680272
frozenset({'parental level of education_high school'}),"frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό'})",0.196,0.252,0.069,0.3520408163265306,1.3969873663751216,1.0,0.019608,1.1543937007874017,0.35345014060134106,0.1820580474934037,0.13374440685365055,0.3129251700680272
frozenset({'lunch_standard'}),"frozenset({'math_cat_υψηλό', 'reading_cat_υψηλό'})",0.645,0.131,0.118,0.18294573643410852,1.396532339191668,1.0,0.03350499999999999,1.0635768500948766,0.7998328956791596,0.17933130699088143,0.05977645159276009,0.5418545476063672
"frozenset({'math_cat_υψηλό', 'reading_cat_υψηλό'})",frozenset({'lunch_standard'}),0.131,0.645,0.118,0.9007633587786259,1.396532339191668,1.0,0.03350499999999999,3.57730769230769,0.3267441633672056,0.17933130699088143,0.720460165573594,0.5418545476063672
"frozenset({'math_cat_μέτριο', 'test preparation course_none'})",frozenset({'reading_cat_μέτριο'}),0.304,0.49,0.208,0.6842105263157895,1.3963480128893664,1.0,0.05904000000000001,1.6150000000000002,0.4078249336870028,0.3549488054607508,0.3808049535603716,0.5543501611170785
frozenset({'reading_cat_μέτριο'}),"frozenset({'math_cat_μέτριο', 'test preparation course_none'})",0.49,0.304,0.208,0.42448979591836733,1.3963480128893662,1.0,0.05904000000000001,1.2093617021276595,0.5565610859728508,0.3549488054607508,0.17311752287121746,0.5543501611170785
frozenset({'math_cat_μέτριο'}),"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'test preparation course_none'})",0.485,0.099,0.067,0.13814432989690723,1.3953972716859315,1.0,0.018985000000000002,1.0454186602870814,0.5502101144761629,0.1295938104448743,0.04344542718857627,0.40745600333229204
"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'test preparation course_none'})",frozenset({'math_cat_μέτριο'}),0.099,0.485,0.067,0.6767676767676768,1.3953972716859315,1.0,0.018985000000000002,1.5932812500000002,0.3144930177083506,0.1295938104448743,0.3723644209081103,0.40745600333229204
"frozenset({'math_cat_υψηλό', 'writing_cat_υψηλό', 'reading_cat_υψηλό'})",frozenset({'lunch_standard'}),0.11,0.645,0.099,0.9,1.3953488372093024,1.0,0.028050000000000005,3.5500000000000007,0.3183520599250937,0.15091463414634146,0.7183098591549296,0.5267441860465116
frozenset({'lunch_standard'}),"frozenset({'math_cat_υψηλό', 'writing_cat_υψηλό', 'reading_cat_υψηλό'})",0.645,0.11,0.099,0.15348837209302327,1.3953488372093024,1.0,0.028050000000000005,1.0513736263736264,0.7981220657276996,0.15091463414634146,0.04886333943036322,0.5267441860465116
frozenset({'math_cat_μέτριο'}),"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})",0.485,0.266,0.18,0.3711340206185567,1.3952406790171303,1.0,0.05098999999999998,1.1671803278688524,0.5500539374325779,0.3152364273204904,0.1432343605157448,0.5239128749709325
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})",frozenset({'math_cat_μέτριο'}),0.266,0.485,0.18,0.6766917293233082,1.3952406790171303,1.0,0.05098999999999998,1.592906976744186,0.3859370269452012,0.3152364273204904,0.3722169501423461,0.5239128749709325
"frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'gender_female'})","frozenset({'math_cat_μέτριο', 'lunch_standard'})",0.173,0.332,0.08,0.46242774566473993,1.3928546556166865,1.0,0.022564,1.2426236559139783,0.34105199516324064,0.18823529411764706,0.19525111626458072,0.35169580054321337
"frozenset({'math_cat_μέτριο', 'lunch_standard'})","frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'gender_female'})",0.332,0.173,0.08,0.24096385542168675,1.3928546556166865,1.0,0.022564,1.0895396825396826,0.42223053892215573,0.18823529411764706,0.08218120365379294,0.35169580054321337
"frozenset({'gender_female', 'lunch_standard'})","frozenset({'math_cat_υψηλό', 'reading_cat_υψηλό'})",0.329,0.131,0.06,0.182370820668693,1.3921436692266642,1.0,0.016900999999999992,1.062828996282528,0.4197963238946843,0.15,0.05911486843347871,0.32019304392213277
"frozenset({'math_cat_υψηλό', 'reading_cat_υψηλό'})","frozenset({'gender_female', 'lunch_standard'})",0.131,0.329,0.06,0.4580152671755725,1.392143669226664,1.0,0.016900999999999992,1.2380422535211268,0.3241465285769082,0.15,0.19227312544794706,0.32019304392213277
frozenset({'test preparation course_none'}),"frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό', 'race/ethnicity_group D'})",0.642,0.056,0.05,0.07788161993769471,1.3907432131731199,1.0,0.014047999999999998,1.0237297297297296,0.7848044692737429,0.07716049382716049,0.023179682137388465,0.4853693813974188
"frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό', 'race/ethnicity_group D'})",frozenset({'test preparation course_none'}),0.056,0.642,0.05,0.8928571428571429,1.3907432131731197,1.0,0.014047999999999998,3.341333333333335,0.29762711864406777,0.07716049382716049,0.7007182761372707,0.4853693813974188
frozenset({'reading_cat_μέτριο'}),"frozenset({""parental level of education_associate's degree"", 'math_cat_μέτριο'})",0.49,0.094,0.064,0.1306122448979592,1.389491966999566,1.0,0.017940000000000005,1.0421126760563382,0.5496323529411765,0.12307692307692307,0.040410866333288296,0.4057316543638732
frozenset({'reading_cat_μέτριο'}),"frozenset({'math_cat_μέτριο', 'test preparation course_none', 'race/ethnicity_group D'})",0.49,0.094,0.064,0.1306122448979592,1.389491966999566,1.0,0.017940000000000005,1.0421126760563382,0.5496323529411765,0.12307692307692307,0.040410866333288296,0.4057316543638732
"frozenset({'math_cat_μέτριο', 'test preparation course_none', 'race/ethnicity_group D'})",frozenset({'reading_cat_μέτριο'}),0.094,0.49,0.064,0.6808510638297872,1.3894919669995658,1.0,0.017940000000000005,1.5979999999999999,0.3093956953642385,0.12307692307692307,0.3742177722152691,0.4057316543638732
"frozenset({""parental level of education_associate's degree"", 'math_cat_μέτριο'})",frozenset({'reading_cat_μέτριο'}),0.094,0.49,0.064,0.6808510638297872,1.3894919669995658,1.0,0.017940000000000005,1.5979999999999999,0.3093956953642385,0.12307692307692307,0.3742177722152691,0.4057316543638732
frozenset({'writing_cat_υψηλό'}),"frozenset({'race/ethnicity_group C', 'gender_female'})",0.208,0.18,0.052,0.25,1.3888888888888888,1.0,0.014560000000000003,1.0933333333333335,0.35353535353535365,0.15476190476190474,0.08536585365853659,0.26944444444444443
"frozenset({'race/ethnicity_group C', 'gender_female'})",frozenset({'writing_cat_υψηλό'}),0.18,0.208,0.052,0.28888888888888886,1.3888888888888888,1.0,0.014560000000000003,1.11375,0.34146341463414637,0.15476190476190474,0.10213243546576878,0.26944444444444443
"frozenset({'writing_cat_μέτριο', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'gender_female'})",0.338,0.245,0.115,0.34023668639053256,1.3887211689409493,1.0,0.032189999999999996,1.1443497757847534,0.4228293708130829,0.24572649572649574,0.12614130647752658,0.4048122207462867
"frozenset({'math_cat_μέτριο', 'gender_female'})","frozenset({'writing_cat_μέτριο', 'lunch_standard'})",0.245,0.338,0.115,0.46938775510204084,1.388721168940949,1.0,0.032189999999999996,1.2476153846153846,0.3707457529513389,0.24572649572649574,0.19847092915716139,0.4048122207462867
"frozenset({'math_cat_μέτριο', 'race/ethnicity_group D'})","frozenset({'reading_cat_μέτριο', 'test preparation course_none'})",0.145,0.318,0.064,0.44137931034482764,1.3879852526566907,1.0,0.017890000000000003,1.220864197530864,0.32693713450292405,0.16040100250626568,0.180908079684498,0.3213185859900239
"frozenset({'reading_cat_μέτριο', 'test preparation course_none'})","frozenset({'math_cat_μέτριο', 'race/ethnicity_group D'})",0.318,0.145,0.064,0.20125786163522014,1.3879852526566907,1.0,0.017890000000000003,1.0704330708661416,0.4098698680351907,0.16040100250626568,0.06579866857920486,0.3213185859900239
frozenset({'test preparation course_none'}),"frozenset({'writing_cat_χαμηλό', 'race/ethnicity_group D'})",0.642,0.064,0.057,0.08878504672897196,1.387266355140187,1.0,0.015912000000000003,1.0272,0.7797706556895032,0.08782742681047767,0.026479750778816195,0.48970502336448596
"frozenset({'writing_cat_χαμηλό', 'race/ethnicity_group D'})",frozenset({'test preparation course_none'}),0.064,0.642,0.057,0.890625,1.387266355140187,1.0,0.015912000000000003,3.273142857142857,0.29824561403508776,0.08782742681047767,0.6944832402234636,0.48970502336448596
frozenset({'lunch_standard'}),"frozenset({'math_cat_υψηλό', 'writing_cat_υψηλό'})",0.645,0.114,0.102,0.15813953488372093,1.38718890248878,1.0,0.028469999999999995,1.0524309392265192,0.7862468931234464,0.1552511415525114,0.04981888813061052,0.526438188494492
"frozenset({'math_cat_υψηλό', 'writing_cat_υψηλό'})",frozenset({'lunch_standard'}),0.114,0.645,0.102,0.894736842105263,1.3871889024887798,1.0,0.028469999999999995,3.3724999999999965,0.3150312043553313,0.1552511415525114,0.7034840622683466,0.526438188494492
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})",frozenset({'math_cat_μέτριο'}),0.418,0.485,0.281,0.6722488038277513,1.3860800078922708,1.0,0.07827000000000003,1.5713138686131394,0.4785926550977731,0.45176848874598075,0.3635899103451482,0.6258151235633602
frozenset({'math_cat_μέτριο'}),"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.485,0.418,0.281,0.5793814432989691,1.3860800078922706,1.0,0.07827000000000003,1.3836764705882356,0.5408561655668039,0.45176848874598075,0.27728770326283353,0.6258151235633602
"frozenset({'reading_cat_μέτριο', 'test preparation course_none'})","frozenset({'math_cat_μέτριο', 'parental level of education_some college'})",0.318,0.118,0.052,0.16352201257861634,1.385779767615393,1.0,0.014475999999999996,1.054421052631579,0.40818858560794036,0.13541666666666666,0.05161225915942897,0.30209998934015564
"frozenset({'math_cat_μέτριο', 'parental level of education_some college'})","frozenset({'reading_cat_μέτριο', 'test preparation course_none'})",0.118,0.318,0.052,0.4406779661016949,1.3857797676153927,1.0,0.014475999999999996,1.2193333333333332,0.31562881562881556,0.13541666666666666,0.17987971569163477,0.30209998934015564
frozenset({'writing_cat_χαμηλό'}),frozenset({'gender_male'}),0.301,0.482,0.201,0.6677740863787376,1.385423415723522,1.0,0.05591800000000002,1.5591800000000002,0.39799571527199495,0.34536082474226815,0.35863723239138534,0.5423932672557589
frozenset({'gender_male'}),frozenset({'writing_cat_χαμηλό'}),0.482,0.301,0.201,0.4170124481327801,1.385423415723522,1.0,0.05591800000000002,1.1989964412811391,0.5370637161681939,0.34536082474226815,0.16596916757193148,0.5423932672557589
frozenset({'lunch_free/reduced'}),"frozenset({'math_cat_χαμηλό', 'reading_cat_μέτριο'})",0.355,0.12,0.059,0.16619718309859155,1.3849765258215962,1.0,0.016399999999999998,1.0554054054054054,0.43095519642622515,0.14182692307692307,0.05249679897567222,0.3289319248826291
"frozenset({'math_cat_χαμηλό', 'reading_cat_μέτριο'})",frozenset({'lunch_free/reduced'}),0.12,0.355,0.059,0.49166666666666664,1.3849765258215962,1.0,0.016399999999999998,1.2688524590163937,0.3158705701078582,0.14182692307692307,0.2118863049095607,0.3289319248826291
frozenset({'lunch_standard'}),"frozenset({'math_cat_υψηλό', 'gender_male', 'reading_cat_υψηλό'})",0.645,0.065,0.058,0.08992248062015504,1.383422778771616,1.0,0.016075,1.0273850085178877,0.7807187955318114,0.08895705521472394,0.026655059486796827,0.4911150864639237
"frozenset({'math_cat_υψηλό', 'gender_male', 'reading_cat_υψηλό'})",frozenset({'lunch_standard'}),0.065,0.645,0.058,0.8923076923076924,1.383422778771616,1.0,0.016075,3.2964285714285726,0.2964226442928268,0.08895705521472394,0.696641386782232,0.4911150864639237
"frozenset({'gender_male', 'test preparation course_none'})","frozenset({'math_cat_χαμηλό', 'writing_cat_χαμηλό'})",0.308,0.23,0.098,0.3181818181818182,1.383399209486166,1.0,0.027160000000000004,1.1293333333333333,0.4004954582989266,0.22272727272727272,0.11452184179456905,0.3721343873517786
"frozenset({'math_cat_χαμηλό', 'writing_cat_χαμηλό'})","frozenset({'gender_male', 'test preparation course_none'})",0.23,0.308,0.098,0.4260869565217391,1.383399209486166,1.0,0.027160000000000004,1.2057575757575756,0.3599257884972171,0.22272727272727272,0.17064589092736868,0.3721343873517786
"frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",frozenset({'math_cat_μέτριο'}),0.079,0.485,0.053,0.6708860759493671,1.3832702596894169,1.0,0.014684999999999997,1.5648076923076926,0.30084198881445506,0.10371819960861059,0.3609438367948876,0.39008221323241554
frozenset({'math_cat_μέτριο'}),"frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",0.485,0.079,0.053,0.10927835051546392,1.3832702596894166,1.0,0.014684999999999997,1.0339930555555557,0.5380106246565304,0.10371819960861059,0.032875516303435305,0.39008221323241554
"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο', 'gender_male'})",frozenset({'race/ethnicity_group D'}),0.149,0.262,0.054,0.36241610738255037,1.3832675854295815,1.0,0.014961999999999996,1.1574947368421054,0.32558645602123854,0.15126050420168066,0.1360651861552173,0.284261488805779
frozenset({'race/ethnicity_group D'}),"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο', 'gender_male'})",0.262,0.149,0.054,0.20610687022900762,1.3832675854295813,1.0,0.014961999999999996,1.0719326923076924,0.3754391247616179,0.15126050420168066,0.06710560543949191,0.284261488805779
"frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό'})",frozenset({'gender_male'}),0.252,0.482,0.168,0.6666666666666667,1.3831258644536655,1.0,0.04653600000000001,1.5540000000000005,0.37032085561497335,0.2968197879858658,0.35649935649935666,0.5076071922544952
frozenset({'gender_male'}),"frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό'})",0.482,0.252,0.168,0.3485477178423237,1.3831258644536655,1.0,0.04653600000000001,1.148203821656051,0.5347490347490348,0.2968197879858658,0.12907448909401562,0.5076071922544952
frozenset({'gender_female'}),"frozenset({'writing_cat_υψηλό', 'lunch_standard'})",0.518,0.162,0.116,0.22393822393822393,1.382334715668049,1.0,0.032084,1.0798109452736318,0.5738303047646302,0.20567375886524822,0.07391196174012403,0.46999380332713664
"frozenset({'writing_cat_υψηλό', 'lunch_standard'})",frozenset({'gender_female'}),0.162,0.518,0.116,0.7160493827160493,1.3823347156680488,1.0,0.032084,1.6974782608695649,0.3300551394946918,0.20567375886524822,0.4108908355104758,0.46999380332713664
frozenset({'lunch_free/reduced'}),"frozenset({'test preparation course_none', 'reading_cat_χαμηλό'})",0.355,0.214,0.105,0.29577464788732394,1.3821245228379624,1.0,0.02903,1.1161200000000002,0.42864525655223334,0.22629310344827586,0.10403899222305844,0.39321442674740026
"frozenset({'test preparation course_none', 'reading_cat_χαμηλό'})",frozenset({'lunch_free/reduced'}),0.214,0.355,0.105,0.49065420560747663,1.3821245228379624,1.0,0.02903,1.266330275229358,0.3517508784684357,0.22629310344827586,0.21031659784104906,0.39321442674740026
"frozenset({""parental level of education_associate's degree""})",frozenset({'math_cat_υψηλό'}),0.222,0.176,0.054,0.24324324324324323,1.382063882063882,1.0,0.014928000000000004,1.088857142857143,0.3553270494144531,0.15697674418604649,0.08160587772238256,0.27503071253071254
frozenset({'math_cat_υψηλό'}),"frozenset({""parental level of education_associate's degree""})",0.176,0.222,0.054,0.3068181818181818,1.382063882063882,1.0,0.014928000000000004,1.122360655737705,0.3354908306364618,0.15697674418604649,0.10902079925216172,0.27503071253071254
"frozenset({'math_cat_χαμηλό', 'reading_cat_χαμηλό'})",frozenset({'parental level of education_high school'}),0.218,0.196,0.059,0.2706422018348624,1.3808275603819509,1.0,0.016271999999999995,1.1023396226415096,0.35268108717326274,0.16619718309859152,0.09283855949609747,0.28583130499906384
frozenset({'parental level of education_high school'}),"frozenset({'math_cat_χαμηλό', 'reading_cat_χαμηλό'})",0.196,0.218,0.059,0.30102040816326525,1.3808275603819506,1.0,0.016271999999999995,1.1187737226277372,0.3430306096635466,0.16619718309859152,0.10616420481235965,0.28583130499906384
"frozenset({'math_cat_μέτριο', 'lunch_standard'})",frozenset({'writing_cat_μέτριο'}),0.332,0.491,0.225,0.677710843373494,1.3802664834490712,1.0,0.06198799999999999,1.5793271028037383,0.4124284763805721,0.3762541806020067,0.36681894572395674,0.5679796579392928
frozenset({'writing_cat_μέτριο'}),"frozenset({'math_cat_μέτριο', 'lunch_standard'})",0.491,0.332,0.225,0.45824847250509165,1.3802664834490712,1.0,0.06198799999999999,1.2330375939849625,0.541261733246016,0.3762541806020067,0.18899471931899947,0.5679796579392928
"frozenset({'writing_cat_μέτριο', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'parental level of education_some college'})",0.338,0.118,0.055,0.16272189349112426,1.3789990973824091,1.0,0.015115999999999997,1.0534134275618374,0.41516067014556435,0.1371571072319202,0.05070509466113862,0.31441179420318927
"frozenset({'math_cat_μέτριο', 'parental level of education_some college'})","frozenset({'writing_cat_μέτριο', 'lunch_standard'})",0.118,0.338,0.055,0.46610169491525427,1.378999097382409,1.0,0.015115999999999997,1.239936507936508,0.31160585446299727,0.1371571072319202,0.1935070920170004,0.31441179420318927
"frozenset({'math_cat_μέτριο', 'gender_female'})","frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο'})",0.245,0.154,0.052,0.21224489795918366,1.3782136231115822,1.0,0.014269999999999998,1.0739378238341968,0.36347427407030053,0.14985590778097982,0.06884739711487431,0.27495361781076066
"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο'})","frozenset({'math_cat_μέτριο', 'gender_female'})",0.154,0.245,0.052,0.33766233766233766,1.3782136231115822,1.0,0.014269999999999998,1.1399019607843137,0.32437715948354245,0.14985590778097982,0.1227315730626989,0.27495361781076066
"frozenset({'reading_cat_μέτριο', 'gender_female'})","frozenset({'race/ethnicity_group C', 'math_cat_μέτριο'})",0.261,0.153,0.055,0.210727969348659,1.3773069892069216,1.0,0.015066999999999997,1.073140776699029,0.37069750276786806,0.1532033426183844,0.0681558079677202,0.2851025467658328
"frozenset({'race/ethnicity_group C', 'math_cat_μέτριο'})","frozenset({'reading_cat_μέτριο', 'gender_female'})",0.153,0.261,0.055,0.35947712418300654,1.3773069892069216,1.0,0.015066999999999997,1.1537448979591838,0.3234302887195448,0.1532033426183844,0.13325727223681533,0.2851025467658328
"frozenset({'writing_cat_μέτριο', 'gender_male'})","frozenset({'math_cat_μέτριο', 'test preparation course_none'})",0.227,0.304,0.095,0.4185022026431718,1.3766519823788546,1.0,0.025992,1.1969090909090907,0.3539456662354463,0.2178899082568807,0.16451465897007442,0.36550110132158586
"frozenset({'math_cat_μέτριο', 'test preparation course_none'})","frozenset({'writing_cat_μέτριο', 'gender_male'})",0.304,0.227,0.095,0.3125,1.3766519823788546,1.0,0.025992,1.1243636363636365,0.3931034482758621,0.2178899082568807,0.11060802069857696,0.36550110132158586
"frozenset({'math_cat_μέτριο', 'parental level of education_some college'})","frozenset({'writing_cat_μέτριο', 'test preparation course_none'})",0.118,0.308,0.05,0.42372881355932207,1.3757429011666302,1.0,0.013656000000000008,1.2008235294117646,0.30965986394557843,0.13297872340425532,0.16723816988341342,0.29303323794849223
"frozenset({'writing_cat_μέτριο', 'test preparation course_none'})","frozenset({'math_cat_μέτριο', 'parental level of education_some college'})",0.308,0.118,0.05,0.16233766233766234,1.37574290116663,1.0,0.013656000000000008,1.0529302325581396,0.39468208092485574,0.13297872340425532,0.05026945843272374,0.29303323794849223
"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'lunch_standard'})",frozenset({'math_cat_μέτριο'}),0.102,0.485,0.068,0.6666666666666667,1.3745704467353954,1.0,0.018530000000000012,1.5450000000000004,0.30345211581291776,0.13102119460500966,0.3527508090614888,0.4034364261168385
"frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'gender_female', 'lunch_standard'})",frozenset({'math_cat_μέτριο'}),0.12,0.485,0.08,0.6666666666666667,1.3745704467353954,1.0,0.021800000000000007,1.5450000000000004,0.309659090909091,0.15238095238095237,0.3527508090614888,0.41580756013745707
frozenset({'math_cat_μέτριο'}),"frozenset({'writing_cat_μέτριο', 'gender_male', 'test preparation course_none', 'lunch_standard'})",0.485,0.099,0.066,0.13608247422680414,1.3745704467353952,1.0,0.017985,1.0429236276849643,0.529126213592233,0.1274131274131274,0.041157019119649425,0.4013745704467354
"frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο', 'lunch_standard'})",frozenset({'math_cat_μέτριο'}),0.108,0.485,0.072,0.6666666666666666,1.3745704467353952,1.0,0.01962,1.545,0.3054932735426009,0.13819577735124758,0.3527508090614886,0.40756013745704467
frozenset({'math_cat_μέτριο'}),"frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'gender_female', 'lunch_standard'})",0.485,0.12,0.08,0.16494845360824742,1.3745704467353952,1.0,0.021800000000000007,1.0538271604938272,0.5291262135922332,0.15238095238095237,0.05107778819119026,0.41580756013745707
frozenset({'math_cat_μέτριο'}),"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'lunch_standard'})",0.485,0.102,0.068,0.1402061855670103,1.3745704467353952,1.0,0.018530000000000012,1.0444364508393285,0.5291262135922333,0.13102119460500966,0.04254586366036783,0.4034364261168385
frozenset({'math_cat_μέτριο'}),"frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο', 'lunch_standard'})",0.485,0.108,0.072,0.14845360824742268,1.3745704467353952,1.0,0.01962,1.0475060532687652,0.529126213592233,0.13819577735124758,0.04535157875271601,0.40756013745704467
"frozenset({'writing_cat_μέτριο', 'gender_male', 'test preparation course_none', 'lunch_standard'})",frozenset({'math_cat_μέτριο'}),0.099,0.485,0.066,0.6666666666666666,1.3745704467353952,1.0,0.017985,1.545,0.30244173140954494,0.1274131274131274,0.3527508090614886,0.4013745704467354
frozenset({'math_cat_υψηλό'}),frozenset({'lunch_standard'}),0.176,0.645,0.156,0.8863636363636365,1.3742071881606766,1.0,0.042480000000000004,3.1240000000000028,0.33047050037341297,0.23458646616541357,0.6798975672215112,0.5641120507399577
frozenset({'lunch_standard'}),frozenset({'math_cat_υψηλό'}),0.645,0.176,0.156,0.24186046511627907,1.3742071881606766,1.0,0.042480000000000004,1.0868711656441719,0.7670639219934996,0.23458646616541357,0.07992774892752315,0.5641120507399577
"frozenset({'math_cat_μέτριο', 'test preparation course_none', 'gender_female'})",frozenset({'reading_cat_μέτριο'}),0.15,0.49,0.101,0.6733333333333335,1.3741496598639458,1.0,0.02750000000000001,1.5612244897959189,0.3203261502620851,0.18738404452690166,0.35947712418300676,0.43972789115646266
frozenset({'reading_cat_μέτριο'}),"frozenset({'math_cat_μέτριο', 'test preparation course_none', 'gender_female'})",0.49,0.15,0.101,0.20612244897959187,1.3741496598639458,1.0,0.02750000000000001,1.070694087403599,0.5338769171034752,0.18738404452690166,0.06602641056422573,0.43972789115646266
frozenset({'writing_cat_μέτριο'}),"frozenset({'math_cat_μέτριο', 'test preparation course_none'})",0.491,0.304,0.205,0.41751527494908347,1.3734055097009326,1.0,0.05573600000000001,1.1948811188811188,0.5341511332598592,0.3474576271186441,0.16309665940960272,0.5459286901061207
"frozenset({'math_cat_μέτριο', 'test preparation course_none'})",frozenset({'writing_cat_μέτριο'}),0.304,0.491,0.205,0.6743421052631579,1.3734055097009326,1.0,0.05573600000000001,1.5629898989898987,0.39063638912251203,0.3474576271186441,0.3602005997311549,0.5459286901061207
frozenset({'test preparation course_completed'}),"frozenset({'writing_cat_υψηλό', 'gender_female', 'lunch_standard'})",0.358,0.116,0.057,0.15921787709497207,1.3725679059911384,1.0,0.015472,1.0514019933554817,0.42280155216702187,0.1366906474820144,0.04888900123865617,0.32529859371989983
"frozenset({'writing_cat_υψηλό', 'gender_female', 'lunch_standard'})",frozenset({'test preparation course_completed'}),0.116,0.358,0.057,0.49137931034482757,1.3725679059911384,1.0,0.015472,1.2622372881355934,0.3070572358498055,0.1366906474820144,0.20775593511655385,0.32529859371989983
frozenset({'math_cat_μέτριο'}),"frozenset({'writing_cat_μέτριο', 'lunch_standard'})",0.485,0.338,0.225,0.4639175257731959,1.3725370585005794,1.0,0.061069999999999985,1.2348846153846154,0.5270334412081983,0.3762541806020067,0.1902077428598125,0.5647989995729885
"frozenset({'writing_cat_μέτριο', 'lunch_standard'})",frozenset({'math_cat_μέτριο'}),0.338,0.485,0.225,0.665680473372781,1.3725370585005794,1.0,0.061069999999999985,1.540442477876106,0.4100033568311513,0.3762541806020067,0.3508358706267593,0.5647989995729885
frozenset({'math_cat_μέτριο'}),"frozenset({'writing_cat_μέτριο', 'test preparation course_none'})",0.485,0.308,0.205,0.422680412371134,1.3723390012049805,1.0,0.05562,1.1986428571428571,0.526829268292683,0.3486394557823129,0.16572313926464455,0.5441324139777748
"frozenset({'writing_cat_μέτριο', 'test preparation course_none'})",frozenset({'math_cat_μέτριο'}),0.308,0.485,0.205,0.6655844155844155,1.3723390012049805,1.0,0.05562,1.5399999999999996,0.39207669533342737,0.3486394557823129,0.3506493506493505,0.5441324139777748
"frozenset({'race/ethnicity_group D', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'gender_male'})",0.167,0.227,0.052,0.311377245508982,1.3717059273523435,1.0,0.014090999999999992,1.1225304347826088,0.3253070458952811,0.15204678362573099,0.10915555693270632,0.2702260676884117
"frozenset({'writing_cat_μέτριο', 'gender_male'})","frozenset({'race/ethnicity_group D', 'lunch_standard'})",0.227,0.167,0.052,0.22907488986784139,1.3717059273523435,1.0,0.014090999999999992,1.08052,0.3505572693800376,0.15204678362573099,0.07451967571169434,0.2702260676884117
"frozenset({'math_cat_υψηλό', 'gender_male'})",frozenset({'test preparation course_completed'}),0.108,0.358,0.053,0.49074074074074076,1.3707841920132424,1.0,0.014336000000000002,1.2606545454545455,0.3032405448853541,0.12832929782082325,0.2067612784123688,0.3193927167390855
frozenset({'test preparation course_completed'}),"frozenset({'math_cat_υψηλό', 'gender_male'})",0.358,0.108,0.053,0.14804469273743018,1.3707841920132424,1.0,0.014336000000000002,1.0470032786885246,0.4213248692176571,0.12832929782082325,0.04489315329308316,0.3193927167390855
"frozenset({'writing_cat_χαμηλό', 'test preparation course_none', 'reading_cat_χαμηλό'})",frozenset({'parental level of education_high school'}),0.201,0.196,0.054,0.26865671641791045,1.370697532744441,1.0,0.014603999999999999,1.0993469387755104,0.33847865387289666,0.1574344023323615,0.09036905027103288,0.27208346024977154
frozenset({'parental level of education_high school'}),"frozenset({'writing_cat_χαμηλό', 'test preparation course_none', 'reading_cat_χαμηλό'})",0.196,0.201,0.054,0.2755102040816326,1.3706975327444408,1.0,0.014603999999999999,1.102845070422535,0.33637368711995574,0.1574344023323615,0.09325432300579803,0.27208346024977154
"frozenset({'math_cat_μέτριο', 'test preparation course_none'})","frozenset({'writing_cat_μέτριο', 'gender_female'})",0.304,0.264,0.11,0.3618421052631579,1.3706140350877194,1.0,0.029743999999999993,1.153319587628866,0.38850574712643676,0.24017467248908292,0.13293764302059496,0.38925438596491224
"frozenset({'writing_cat_μέτριο', 'gender_female'})","frozenset({'math_cat_μέτριο', 'test preparation course_none'})",0.264,0.304,0.11,0.41666666666666663,1.3706140350877192,1.0,0.029743999999999993,1.193142857142857,0.36739130434782596,0.24017467248908292,0.1618773946360153,0.38925438596491224
"frozenset({'gender_male', 'test preparation course_none'})","frozenset({'math_cat_χαμηλό', 'reading_cat_χαμηλό'})",0.308,0.218,0.092,0.2987012987012987,1.370189443583939,1.0,0.024856000000000003,1.115074074074074,0.390424729831616,0.21198156682027647,0.10319859168964027,0.3603598236625759
"frozenset({'math_cat_χαμηλό', 'reading_cat_χαμηλό'})","frozenset({'gender_male', 'test preparation course_none'})",0.218,0.308,0.092,0.4220183486238532,1.370189443583939,1.0,0.024856000000000003,1.197269841269841,0.34549093739575226,0.21198156682027647,0.1647663997454526,0.3603598236625759
"frozenset({'test preparation course_completed', 'writing_cat_μέτριο', 'reading_cat_μέτριο'})",frozenset({'math_cat_μέτριο'}),0.152,0.485,0.101,0.6644736842105263,1.3700488334237657,1.0,0.027280000000000013,1.5349019607843137,0.31851298337380923,0.1884328358208955,0.3484925907000511,0.43636055344546937
frozenset({'math_cat_μέτριο'}),"frozenset({'test preparation course_completed', 'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.485,0.152,0.101,0.20824742268041238,1.3700488334237657,1.0,0.027280000000000013,1.0710416666666667,0.5244640968951266,0.1884328358208955,0.06632950787784479,0.43636055344546937
frozenset({'reading_cat_μέτριο'}),frozenset({'math_cat_μέτριο'}),0.49,0.485,0.325,0.663265306122449,1.367557332211235,1.0,0.08735000000000001,1.5293939393939397,0.5269984917043741,0.5000000000000001,0.3461462254804836,0.6666841994529771
frozenset({'math_cat_μέτριο'}),frozenset({'reading_cat_μέτριο'}),0.485,0.49,0.325,0.6701030927835052,1.367557332211235,1.0,0.08735000000000001,1.5459375000000004,0.5218820014936519,0.5000000000000001,0.3531433191833436,0.6666841994529771
"frozenset({'parental level of education_high school', 'reading_cat_μέτριο'})",frozenset({'math_cat_μέτριο'}),0.098,0.485,0.065,0.6632653061224489,1.3675573322112349,1.0,0.01747,1.5293939393939393,0.29797032236056625,0.12548262548262548,0.3461462254804834,0.39864296233957497
frozenset({'math_cat_μέτριο'}),"frozenset({'parental level of education_high school', 'reading_cat_μέτριο'})",0.485,0.098,0.065,0.13402061855670103,1.3675573322112349,1.0,0.01747,1.041595238095238,0.5218820014936519,0.12548262548262548,0.03993416691430268,0.39864296233957497
frozenset({'gender_female'}),"frozenset({'writing_cat_υψηλό', 'reading_cat_υψηλό', 'lunch_standard'})",0.518,0.144,0.102,0.1969111969111969,1.3674388674388676,1.0,0.027408000000000002,1.0658846153846155,0.5574810837197951,0.1821428571428571,0.061812145924295465,0.4526222651222651
"frozenset({'writing_cat_υψηλό', 'reading_cat_υψηλό', 'lunch_standard'})",frozenset({'gender_female'}),0.144,0.518,0.102,0.7083333333333334,1.3674388674388676,1.0,0.027408000000000002,1.6525714285714288,0.3139087410665201,0.1821428571428571,0.3948824343015215,0.4526222651222651
"frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced', 'reading_cat_χαμηλό'})","frozenset({'gender_male', 'test preparation course_none'})",0.133,0.308,0.056,0.42105263157894735,1.367053998632946,1.0,0.015036,1.195272727272727,0.3096885813148789,0.14545454545454545,0.1633708548828719,0.3014354066985646
"frozenset({'gender_male', 'test preparation course_none'})","frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced', 'reading_cat_χαμηλό'})",0.308,0.133,0.056,0.18181818181818182,1.367053998632946,1.0,0.015036,1.0596666666666668,0.38800578034682087,0.14545454545454545,0.056307014784523435,0.3014354066985646
"frozenset({'math_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'gender_male'})",0.213,0.227,0.066,0.3098591549295775,1.3650183036545265,1.0,0.017649000000000005,1.120061224489796,0.3397828347002427,0.17647058823529413,0.10719166226336027,0.3003040268039958
"frozenset({'writing_cat_μέτριο', 'gender_male'})","frozenset({'math_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",0.227,0.213,0.066,0.2907488986784141,1.3650183036545263,1.0,0.017649000000000005,1.1096211180124225,0.3459367282135718,0.17647058823529413,0.09879148497892513,0.3003040268039958
frozenset({'writing_cat_μέτριο'}),"frozenset({'math_cat_μέτριο', 'test preparation course_none', 'race/ethnicity_group D'})",0.491,0.094,0.063,0.12830957230142567,1.3649954500151666,1.0,0.016846,1.0393598130841122,0.525337574453488,0.12068965517241378,0.03786928510091134,0.39926116912943627
"frozenset({'math_cat_μέτριο', 'test preparation course_none', 'race/ethnicity_group D'})",frozenset({'writing_cat_μέτριο'}),0.094,0.491,0.063,0.6702127659574468,1.3649954500151666,1.0,0.016846,1.5434193548387098,0.2951399838817057,0.12068965517241378,0.35208794883584843,0.39926116912943627
"frozenset({""parental level of education_associate's degree"", 'math_cat_μέτριο'})",frozenset({'writing_cat_μέτριο'}),0.094,0.491,0.063,0.6702127659574468,1.3649954500151666,1.0,0.016846,1.5434193548387098,0.2951399838817057,0.12068965517241378,0.35208794883584843,0.39926116912943627
frozenset({'writing_cat_μέτριο'}),"frozenset({""parental level of education_associate's degree"", 'math_cat_μέτριο'})",0.491,0.094,0.063,0.12830957230142567,1.3649954500151666,1.0,0.016846,1.0393598130841122,0.525337574453488,0.12068965517241378,0.03786928510091134,0.39926116912943627
"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'reading_cat_μέτριο'})",frozenset({'math_cat_μέτριο'}),0.139,0.485,0.092,0.6618705035971222,1.364681450715716,1.0,0.024584999999999996,1.5230851063829782,0.31036964096349035,0.17293233082706766,0.34343787106237317,0.42578061262330336
frozenset({'math_cat_μέτριο'}),"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.485,0.139,0.092,0.18969072164948453,1.364681450715716,1.0,0.024584999999999996,1.062557251908397,0.5188898269311946,0.17293233082706766,0.058874241172455884,0.42578061262330336
"frozenset({'math_cat_μέτριο', 'lunch_standard'})","frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο', 'test preparation course_none'})",0.332,0.117,0.053,0.15963855421686746,1.3644320873236533,1.0,0.014155999999999995,1.0507383512544803,0.39984182578239735,0.13383838383838384,0.04828828337131082,0.3063150036041602
"frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο', 'test preparation course_none'})","frozenset({'math_cat_μέτριο', 'lunch_standard'})",0.117,0.332,0.053,0.45299145299145294,1.3644320873236533,1.0,0.014155999999999995,1.2211874999999996,0.30248509583538097,0.13383838383838384,0.18112492962792356,0.3063150036041602
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'math_cat_μέτριο', 'gender_female', 'lunch_standard'})",0.418,0.172,0.098,0.23444976076555027,1.3630800044508737,1.0,0.026104000000000016,1.0815750000000002,0.4576758538466935,0.1991869918699187,0.07542241638351482,0.4021086013130077
"frozenset({'math_cat_μέτριο', 'gender_female', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.172,0.418,0.098,0.5697674418604651,1.3630800044508735,1.0,0.026104000000000016,1.3527567567567569,0.321699694370502,0.1991869918699187,0.2607688004475346,0.4021086013130077
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'gender_female'})","frozenset({'math_cat_μέτριο', 'test preparation course_none'})",0.227,0.304,0.094,0.41409691629955947,1.3621609088801299,1.0,0.024992,1.1879097744360902,0.3439486939528226,0.21510297482837526,0.1581852245683326,0.3616537213076745
"frozenset({'math_cat_μέτριο', 'test preparation course_none'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'gender_female'})",0.304,0.227,0.094,0.3092105263157895,1.3621609088801299,1.0,0.024992,1.119009523809524,0.38200048911714357,0.21510297482837526,0.1063525566827807,0.3616537213076745
frozenset({'writing_cat_μέτριο'}),"frozenset({'math_cat_μέτριο', 'gender_female', 'lunch_standard'})",0.491,0.172,0.115,0.23421588594704687,1.3617202671339936,1.0,0.03054800000000002,1.081244680851064,0.5218758008029387,0.20985401459854014,0.07513995887324502,0.45141026855491884
"frozenset({'math_cat_μέτριο', 'gender_female', 'lunch_standard'})",frozenset({'writing_cat_μέτριο'}),0.172,0.491,0.115,0.6686046511627908,1.3617202671339934,1.0,0.03054800000000002,1.535929824561404,0.32081495484142003,0.20985401459854014,0.3489285877461508,0.45141026855491884
"frozenset({'gender_female', 'lunch_standard'})","frozenset({'test preparation course_completed', 'reading_cat_υψηλό'})",0.329,0.125,0.056,0.1702127659574468,1.3617021276595744,1.0,0.014875,1.0544871794871795,0.39586438152011916,0.1407035175879397,0.051671732522796346,0.3091063829787234
"frozenset({'test preparation course_completed', 'reading_cat_υψηλό'})","frozenset({'gender_female', 'lunch_standard'})",0.125,0.329,0.056,0.448,1.3617021276595744,1.0,0.014875,1.2155797101449275,0.30357142857142855,0.1407035175879397,0.1773472429210134,0.3091063829787234
frozenset({'reading_cat_μέτριο'}),"frozenset({'math_cat_μέτριο', 'parental level of education_some college', 'test preparation course_none'})",0.49,0.078,0.052,0.10612244897959183,1.3605442176870748,1.0,0.01378,1.0314611872146118,0.5196078431372549,0.10077519379844962,0.030501571561379427,0.38639455782312926
"frozenset({'math_cat_μέτριο', 'parental level of education_some college', 'test preparation course_none'})",frozenset({'reading_cat_μέτριο'}),0.078,0.49,0.052,0.6666666666666666,1.3605442176870748,1.0,0.01378,1.5299999999999998,0.2874186550976139,0.10077519379844962,0.34640522875816987,0.38639455782312926
frozenset({'parental level of education_high school'}),"frozenset({'test preparation course_none', 'reading_cat_χαμηλό'})",0.196,0.214,0.057,0.29081632653061223,1.35895479687202,1.0,0.015056,1.10831654676259,0.3285327747228768,0.16147308781869688,0.09773069533156772,0.2785857333587641
"frozenset({'test preparation course_none', 'reading_cat_χαμηλό'})",frozenset({'parental level of education_high school'}),0.214,0.196,0.057,0.2663551401869159,1.35895479687202,1.0,0.015056,1.0958980891719747,0.3360564260524084,0.16147308781869688,0.08750639326730834,0.2785857333587641
"frozenset({'math_cat_μέτριο', 'parental level of education_some college'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.118,0.418,0.067,0.5677966101694916,1.3583650960992621,1.0,0.01767600000000001,1.3465882352941179,0.2991166615900093,0.14285714285714285,0.2573824917001573,0.36404184575460224
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'math_cat_μέτριο', 'parental level of education_some college'})",0.418,0.118,0.067,0.16028708133971292,1.3583650960992621,1.0,0.01767600000000001,1.0503589743589743,0.4533005077704264,0.14285714285714285,0.047944536666341186,0.36404184575460224
frozenset({'test preparation course_none'}),"frozenset({'writing_cat_χαμηλό', 'parental level of education_some college'})",0.642,0.062,0.054,0.08411214953271028,1.3566475731082304,1.0,0.014196,1.024142857142857,0.734326505276226,0.08307692307692309,0.023573720184126096,0.4775399457340971
"frozenset({'writing_cat_χαμηλό', 'parental level of education_some college'})",frozenset({'test preparation course_none'}),0.062,0.642,0.054,0.8709677419354839,1.3566475731082304,1.0,0.014196,2.7744999999999997,0.28026533996683256,0.08307692307692309,0.6395746981438097,0.4775399457340971
"frozenset({'test preparation course_completed', 'math_cat_υψηλό', 'reading_cat_υψηλό'})",frozenset({'lunch_standard'}),0.072,0.645,0.063,0.8750000000000001,1.356589147286822,1.0,0.016560000000000005,2.8400000000000025,0.2832512315270937,0.09633027522935782,0.6478873239436622,0.48633720930232566
frozenset({'lunch_standard'}),"frozenset({'test preparation course_completed', 'math_cat_υψηλό', 'reading_cat_υψηλό'})",0.645,0.072,0.063,0.09767441860465116,1.356589147286822,1.0,0.016560000000000005,1.0284536082474227,0.740442655935614,0.09633027522935782,0.027666399358460308,0.48633720930232566
frozenset({'math_cat_μέτριο'}),"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'gender_female', 'lunch_standard'})",0.485,0.149,0.098,0.2020618556701031,1.3561198367121015,1.0,0.025735000000000008,1.0664987080103359,0.5099068753715079,0.1828358208955224,0.06235235683913408,0.42988998823773616
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'gender_female', 'lunch_standard'})",frozenset({'math_cat_μέτριο'}),0.149,0.485,0.098,0.6577181208053692,1.3561198367121015,1.0,0.025735000000000008,1.5046078431372552,0.30858054149979625,0.1828358208955224,0.3353749918550858,0.42988998823773616
"frozenset({'writing_cat_μέτριο', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'gender_male'})",0.338,0.24,0.11,0.3254437869822485,1.356015779092702,1.0,0.028880000000000003,1.1266666666666667,0.3965943422136776,0.235042735042735,0.11242603550295859,0.39188856015779094
"frozenset({'math_cat_μέτριο', 'gender_male'})","frozenset({'writing_cat_μέτριο', 'lunch_standard'})",0.24,0.338,0.11,0.45833333333333337,1.356015779092702,1.0,0.028880000000000003,1.2221538461538461,0.34545454545454546,0.235042735042735,0.18177240684793558,0.39188856015779094
frozenset({'reading_cat_χαμηλό'}),frozenset({'parental level of education_high school'}),0.275,0.196,0.073,0.26545454545454544,1.354359925788497,1.0,0.019099999999999992,1.0945544554455444,0.36088804912612177,0.18341708542713567,0.08638625056535501,0.3189517625231911
frozenset({'parental level of education_high school'}),frozenset({'reading_cat_χαμηλό'}),0.196,0.275,0.073,0.3724489795918367,1.354359925788497,1.0,0.019099999999999992,1.1552845528455282,0.32542765623935105,0.18341708542713567,0.13441238564391267,0.3189517625231911
"frozenset({'gender_female', 'reading_cat_υψηλό'})",frozenset({'test preparation course_completed'}),0.159,0.358,0.077,0.48427672955974843,1.3527282948596326,1.0,0.020078,1.2448536585365855,0.310051423012184,0.175,0.19669272517094774,0.349680264221215
frozenset({'test preparation course_completed'}),"frozenset({'gender_female', 'reading_cat_υψηλό'})",0.358,0.159,0.077,0.21508379888268156,1.3527282948596324,1.0,0.020078,1.0714519572953736,0.4061577052231258,0.175,0.06668703791044181,0.349680264221215
"frozenset({'race/ethnicity_group C', 'math_cat_μέτριο', 'gender_female'})",frozenset({'reading_cat_μέτριο'}),0.083,0.49,0.055,0.6626506024096386,1.352348168182936,1.0,0.014329999999999996,1.5117857142857145,0.28412808565480313,0.1061776061776062,0.33853059296007565,0.38744775018441113
frozenset({'reading_cat_μέτριο'}),"frozenset({'race/ethnicity_group C', 'math_cat_μέτριο', 'gender_female'})",0.49,0.083,0.055,0.11224489795918367,1.3523481681829357,1.0,0.014329999999999996,1.0329425287356322,0.5108734402852048,0.1061776061776062,0.03189192798166158,0.38744775018441113
frozenset({'test preparation course_none'}),"frozenset({'writing_cat_χαμηλό', 'math_cat_χαμηλό', 'lunch_standard'})",0.642,0.098,0.085,0.132398753894081,1.351007692796745,1.0,0.022084000000000006,1.0396481149012566,0.7257311863292805,0.1297709923664122,0.038136090791664086,0.4998728463347957
"frozenset({'writing_cat_χαμηλό', 'math_cat_χαμηλό', 'lunch_standard'})",frozenset({'test preparation course_none'}),0.098,0.642,0.085,0.8673469387755103,1.351007692796745,1.0,0.022084000000000006,2.698769230769232,0.28803965044998053,0.1297709923664122,0.6294607228366209,0.4998728463347957
"frozenset({'math_cat_υψηλό', 'writing_cat_μέτριο'})",frozenset({'lunch_standard'}),0.062,0.645,0.054,0.8709677419354839,1.350337584396099,1.0,0.014010000000000002,2.7512499999999998,0.27659322435441847,0.08269525267993874,0.6365288505224898,0.477344336084021
frozenset({'lunch_standard'}),"frozenset({'math_cat_υψηλό', 'writing_cat_μέτριο'})",0.645,0.062,0.054,0.08372093023255814,1.350337584396099,1.0,0.014010000000000002,1.0237055837563451,0.7308294209702662,0.08269525267993874,0.02315664203897456,0.477344336084021
frozenset({'lunch_standard'}),"frozenset({'math_cat_υψηλό', 'gender_male'})",0.645,0.108,0.094,0.14573643410852713,1.3494114269308068,1.0,0.02434,1.044174228675136,0.7293976625711718,0.14264036418816387,0.042305419404178396,0.5080534022394487
"frozenset({'math_cat_υψηλό', 'gender_male'})",frozenset({'lunch_standard'}),0.108,0.645,0.094,0.8703703703703703,1.3494114269308066,1.0,0.02434,2.738571428571428,0.29028718633718154,0.14264036418816387,0.6348461137193531,0.5080534022394487
frozenset({'reading_cat_μέτριο'}),"frozenset({'math_cat_μέτριο', 'parental level of education_some college'})",0.49,0.118,0.078,0.15918367346938775,1.349014181943964,1.0,0.020180000000000003,1.048980582524272,0.5072900955253897,0.1471698113207547,0.04669350733490675,0.4101003113109651
"frozenset({'math_cat_μέτριο', 'parental level of education_some college'})",frozenset({'reading_cat_μέτριο'}),0.118,0.49,0.078,0.6610169491525424,1.349014181943964,1.0,0.020180000000000003,1.5045000000000002,0.293331007616722,0.1471698113207547,0.33532735127949487,0.4101003113109651
"frozenset({'reading_cat_μέτριο', 'test preparation course_none'})",frozenset({'math_cat_μέτριο'}),0.318,0.485,0.208,0.6540880503144654,1.3486351552875575,1.0,0.053769999999999984,1.4888181818181818,0.37904635686893745,0.3495798319327731,0.32832631129022405,0.5414770148479543
frozenset({'math_cat_μέτριο'}),"frozenset({'reading_cat_μέτριο', 'test preparation course_none'})",0.485,0.318,0.208,0.4288659793814433,1.3486351552875575,1.0,0.053769999999999984,1.194115523465704,0.501960418222554,0.3495798319327731,0.16256008706956493,0.5414770148479543
"frozenset({'math_cat_μέτριο', 'test preparation course_none'})","frozenset({'writing_cat_μέτριο', 'gender_male', 'lunch_standard'})",0.304,0.161,0.066,0.21710526315789475,1.3484798953906505,1.0,0.017056,1.0716638655462183,0.37129919888540586,0.16541353383458648,0.0668715889843799,0.3135215756783263
"frozenset({'writing_cat_μέτριο', 'gender_male', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'test preparation course_none'})",0.161,0.304,0.066,0.40993788819875776,1.3484798953906505,1.0,0.017056,1.179536842105263,0.30801459168562867,0.16541353383458648,0.1522096094809738,0.3135215756783263
"frozenset({'race/ethnicity_group C', 'writing_cat_χαμηλό', 'reading_cat_χαμηλό'})",frozenset({'gender_male'}),0.077,0.482,0.05,0.6493506493506493,1.3472005173249988,1.0,0.012886000000000002,1.4772592592592593,0.2792199349945829,0.0982318271119843,0.32307075164218024,0.3765425445923371
frozenset({'gender_male'}),"frozenset({'race/ethnicity_group C', 'writing_cat_χαμηλό', 'reading_cat_χαμηλό'})",0.482,0.077,0.05,0.1037344398340249,1.3472005173249988,1.0,0.012886000000000002,1.0298287037037037,0.49752895752895754,0.0982318271119843,0.02896472354715591,0.3765425445923371
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})","frozenset({'math_cat_μέτριο', 'gender_male'})",0.266,0.24,0.086,0.3233082706766917,1.3471177944862156,1.0,0.022159999999999985,1.123111111111111,0.35105506621887056,0.20476190476190473,0.10961614562722595,0.3408208020050125
"frozenset({'math_cat_μέτριο', 'gender_male'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})",0.24,0.266,0.086,0.35833333333333334,1.3471177944862154,1.0,0.022159999999999985,1.143896103896104,0.33904528763769864,0.20476190476190473,0.12579473206176203,0.3408208020050125
"frozenset({'test preparation course_completed', 'math_cat_υψηλό', 'writing_cat_υψηλό', 'reading_cat_υψηλό'})",frozenset({'lunch_standard'}),0.061,0.645,0.053,0.8688524590163934,1.3470580759944084,1.0,0.013655,2.7068749999999993,0.27437860429601946,0.08116385911179173,0.6305703070884321,0.47551150082602617
frozenset({'lunch_standard'}),"frozenset({'test preparation course_completed', 'math_cat_υψηλό', 'writing_cat_υψηλό', 'reading_cat_υψηλό'})",0.645,0.061,0.053,0.08217054263565891,1.3470580759944084,1.0,0.013655,1.0230658783783784,0.7257507307998938,0.08116385911179173,0.02254583880261865,0.47551150082602617
frozenset({'writing_cat_μέτριο'}),"frozenset({'math_cat_μέτριο', 'parental level of education_some college'})",0.491,0.118,0.078,0.15885947046843177,1.3462666988850152,1.0,0.020062000000000003,1.0485762711864406,0.5053145937232382,0.14689265536723162,0.0463259302363172,0.40993820981048706
"frozenset({'math_cat_μέτριο', 'parental level of education_some college'})",frozenset({'writing_cat_μέτριο'}),0.118,0.491,0.078,0.6610169491525424,1.3462666988850152,1.0,0.020062000000000003,1.5015500000000002,0.2916157916157917,0.14689265536723162,0.33402151110519135,0.40993820981048706
"frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'gender_male'})",0.223,0.24,0.072,0.32286995515695066,1.345291479820628,1.0,0.018479999999999996,1.122384105960265,0.33033033033033027,0.18414322250639387,0.10903941468019825,0.3114349775784753
"frozenset({'math_cat_μέτριο', 'gender_male'})","frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",0.24,0.223,0.072,0.3,1.3452914798206277,1.0,0.018479999999999996,1.11,0.337719298245614,0.18414322250639387,0.09909909909909907,0.3114349775784753
"frozenset({'race/ethnicity_group C', 'math_cat_χαμηλό'})","frozenset({'gender_female', 'test preparation course_none'})",0.118,0.334,0.053,0.4491525423728814,1.3447680909367705,1.0,0.013587999999999996,1.2090461538461537,0.2906772772001882,0.13283208020050125,0.17290171527459666,0.30391758855171014
"frozenset({'gender_female', 'test preparation course_none'})","frozenset({'race/ethnicity_group C', 'math_cat_χαμηλό'})",0.334,0.118,0.053,0.1586826347305389,1.3447680909367703,1.0,0.013587999999999996,1.048355871886121,0.38495098872457356,0.13283208020050125,0.04612543620242506,0.30391758855171014
frozenset({'writing_cat_μέτριο'}),"frozenset({'race/ethnicity_group C', 'math_cat_μέτριο', 'lunch_standard'})",0.491,0.103,0.068,0.1384928716904277,1.344590987285706,1.0,0.017427000000000012,1.0411985815602838,0.5034958973766327,0.12927756653992395,0.03956841882990826,0.39934352322385464
"frozenset({'race/ethnicity_group C', 'math_cat_μέτριο', 'lunch_standard'})",frozenset({'writing_cat_μέτριο'}),0.103,0.491,0.068,0.6601941747572816,1.3445909872857058,1.0,0.017427000000000012,1.497914285714286,0.28570725949242587,0.12927756653992395,0.33240505846224283,0.39934352322385464
frozenset({'writing_cat_μέτριο'}),frozenset({'math_cat_μέτριο'}),0.491,0.485,0.32,0.6517311608961304,1.3437755894765575,1.0,0.08186500000000002,1.4787426900584797,0.5026092829076623,0.48780487804878053,0.32374982698277743,0.65576248766456
frozenset({'math_cat_μέτριο'}),frozenset({'writing_cat_μέτριο'}),0.485,0.491,0.32,0.6597938144329897,1.3437755894765575,1.0,0.08186500000000002,1.496151515151515,0.49675364077669915,0.48780487804878053,0.331618495939076,0.65576248766456
"frozenset({'math_cat_μέτριο', 'lunch_standard'})","frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.332,0.139,0.062,0.18674698795180722,1.3435035104446562,1.0,0.01585199999999999,1.0587111111111112,0.3827506277767045,0.1515892420537897,0.05545527055959025,0.31639507670971656
"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'math_cat_μέτριο', 'lunch_standard'})",0.139,0.332,0.062,0.44604316546762585,1.343503510444656,1.0,0.01585199999999999,1.2058701298701295,0.29695402944812843,0.1515892420537897,0.17072330159824228,0.31639507670971656
frozenset({'math_cat_μέτριο'}),"frozenset({'reading_cat_μέτριο', 'gender_female', 'lunch_standard'})",0.485,0.172,0.112,0.2309278350515464,1.3426036921601536,1.0,0.028580000000000008,1.0766219839142093,0.49549237170596405,0.2055045871559633,0.07116888291249567,0.44104531287461046
"frozenset({'reading_cat_μέτριο', 'gender_female', 'lunch_standard'})",frozenset({'math_cat_μέτριο'}),0.172,0.485,0.112,0.6511627906976745,1.3426036921601536,1.0,0.028580000000000008,1.4763333333333335,0.3081866804692892,0.2055045871559633,0.322646195529465,0.44104531287461046
"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'race/ethnicity_group D', 'lunch_standard'})",0.281,0.167,0.063,0.22419928825622773,1.342510708121124,1.0,0.01607299999999999,1.073729357798165,0.3548358610945535,0.1636363636363636,0.06866661255249426,0.3007223986191318
"frozenset({'race/ethnicity_group D', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.167,0.281,0.063,0.3772455089820359,1.342510708121124,1.0,0.01607299999999999,1.154548076923077,0.30627489090874427,0.1636363636363636,0.13386023502369385,0.3007223986191318
frozenset({'gender_male'}),"frozenset({'writing_cat_χαμηλό', 'test preparation course_none', 'reading_cat_χαμηλό'})",0.482,0.201,0.13,0.26970954356846477,1.3418385252162426,1.0,0.03311800000000001,1.0940852272727273,0.49180279180279185,0.23508137432188064,0.08599442248869182,0.4582378563613468
"frozenset({'writing_cat_χαμηλό', 'test preparation course_none', 'reading_cat_χαμηλό'})",frozenset({'gender_male'}),0.201,0.482,0.13,0.6467661691542288,1.3418385252162424,1.0,0.03311800000000001,1.4664507042253518,0.318840858765765,0.23508137432188064,0.31808140763364634,0.4582378563613468
"frozenset({'reading_cat_μέτριο', 'math_cat_μέτριο'})","frozenset({'gender_male', 'race/ethnicity_group D'})",0.325,0.133,0.058,0.17846153846153848,1.3418160786581839,1.0,0.014774999999999996,1.0553370786516854,0.3773946360153255,0.145,0.052435453819536876,0.3072758820127241
"frozenset({'gender_male', 'race/ethnicity_group D'})","frozenset({'reading_cat_μέτριο', 'math_cat_μέτριο'})",0.133,0.325,0.058,0.43609022556390975,1.3418160786581839,1.0,0.014774999999999996,1.197,0.2938193532991289,0.145,0.16457811194653293,0.3072758820127241
"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο', 'gender_female'})","frozenset({'test preparation course_none', 'lunch_standard'})",0.132,0.418,0.074,0.5606060606060606,1.3411628244164129,1.0,0.018823999999999994,1.324551724137931,0.2930626479013575,0.15546218487394955,0.24502759554305936,0.36881977671451355
"frozenset({'test preparation course_none', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο', 'gender_female'})",0.418,0.132,0.074,0.17703349282296652,1.3411628244164129,1.0,0.018823999999999994,1.0547209302325582,0.437076251509241,0.15546218487394955,0.05188190417392455,0.36881977671451355
"frozenset({'writing_cat_χαμηλό', 'test preparation course_none'})",frozenset({'gender_male'}),0.243,0.482,0.157,0.6460905349794239,1.3404367945631201,1.0,0.03987400000000001,1.4636511627906978,0.33550135045309604,0.2764084507042254,0.31677709455487246,0.485908338029131
frozenset({'gender_male'}),"frozenset({'writing_cat_χαμηλό', 'test preparation course_none'})",0.482,0.243,0.157,0.3257261410788382,1.3404367945631201,1.0,0.03987400000000001,1.1226892307692307,0.4902983055849298,0.2764084507042254,0.10928156020982588,0.485908338029131
"frozenset({'math_cat_υψηλό', 'gender_male', 'writing_cat_μέτριο'})",frozenset({'lunch_standard'}),0.059,0.645,0.051,0.864406779661017,1.3401655498620417,1.0,0.012944999999999998,2.6181250000000005,0.2697380758892292,0.0781010719754977,0.618047266650752,0.4717382735514387
frozenset({'lunch_standard'}),"frozenset({'math_cat_υψηλό', 'gender_male', 'writing_cat_μέτριο'})",0.645,0.059,0.051,0.07906976744186046,1.3401655498620417,1.0,0.012944999999999998,1.0217929292929293,0.7149958574979287,0.0781010719754977,0.021328126930776264,0.4717382735514387
"frozenset({'writing_cat_μέτριο', 'test preparation course_none'})","frozenset({'parental level of education_high school', 'lunch_standard'})",0.308,0.126,0.052,0.16883116883116883,1.3399299113584828,1.0,0.013191999999999995,1.05153125,0.366607381058248,0.13612565445026178,0.04900591399447235,0.2907647907647908
"frozenset({'parental level of education_high school', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'test preparation course_none'})",0.126,0.308,0.052,0.4126984126984127,1.3399299113584828,1.0,0.013191999999999995,1.17827027027027,0.2902657982749515,0.13612565445026178,0.1512982842462611,0.2907647907647908
frozenset({'math_cat_μέτριο'}),"frozenset({'writing_cat_μέτριο', 'gender_female', 'lunch_standard'})",0.485,0.177,0.115,0.2371134020618557,1.3396237404624616,1.0,0.029155000000000014,1.0787972972972972,0.49227522161249493,0.21023765996343696,0.0730418008041989,0.44341545809307475
"frozenset({'writing_cat_μέτριο', 'gender_female', 'lunch_standard'})",frozenset({'math_cat_μέτριο'}),0.177,0.485,0.115,0.6497175141242938,1.3396237404624616,1.0,0.029155000000000014,1.470241935483871,0.30804585556553454,0.21023765996343696,0.3198398332510559,0.44341545809307475
frozenset({'test preparation course_none'}),"frozenset({'writing_cat_χαμηλό', 'gender_female'})",0.642,0.1,0.086,0.13395638629283488,1.3395638629283486,1.0,0.021799999999999986,1.0392086330935253,0.7080680784721316,0.13109756097560973,0.037729318103149855,0.4969781931464174
"frozenset({'writing_cat_χαμηλό', 'gender_female'})",frozenset({'test preparation course_none'}),0.1,0.642,0.086,0.8599999999999999,1.3395638629283486,1.0,0.021799999999999986,2.5571428571428547,0.2816537467700257,0.13109756097560973,0.6089385474860332,0.4969781931464174
"frozenset({'math_cat_μέτριο', 'gender_male', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'test preparation course_none'})",0.16,0.308,0.066,0.41250000000000003,1.3392857142857144,1.0,0.016720000000000006,1.177872340425532,0.3015873015873017,0.16417910447761197,0.1510115606936417,0.31339285714285714
"frozenset({'writing_cat_μέτριο', 'test preparation course_none'})","frozenset({'math_cat_μέτριο', 'gender_male', 'lunch_standard'})",0.308,0.16,0.066,0.2142857142857143,1.3392857142857144,1.0,0.016720000000000006,1.069090909090909,0.3660886319845859,0.16417910447761197,0.06462585034013607,0.31339285714285714
frozenset({'math_cat_μέτριο'}),"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο'})",0.485,0.154,0.1,0.2061855670103093,1.3388673182487616,1.0,0.025310000000000013,1.0657402597402599,0.4914563106796119,0.18552875695732837,0.06168506738807246,0.42776810818047933
"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο'})",frozenset({'math_cat_μέτριο'}),0.154,0.485,0.1,0.6493506493506493,1.3388673182487616,1.0,0.025310000000000013,1.4687037037037036,0.2991725768321514,0.18552875695732837,0.3191274744672803,0.42776810818047933
frozenset({'test preparation course_none'}),"frozenset({'math_cat_χαμηλό', 'gender_female', 'writing_cat_χαμηλό'})",0.642,0.099,0.085,0.132398753894081,1.3373611504452627,1.0,0.021442000000000003,1.0384955116696588,0.7046335852776865,0.12957317073170732,0.03706853928310878,0.49549230623996976
"frozenset({'math_cat_χαμηλό', 'gender_female', 'writing_cat_χαμηλό'})",frozenset({'test preparation course_none'}),0.099,0.642,0.085,0.8585858585858586,1.3373611504452625,1.0,0.021442000000000003,2.531571428571428,0.2799764967030097,0.12957317073170732,0.6049884318040741,0.49549230623996976
"frozenset({'test preparation course_completed', 'writing_cat_υψηλό'})",frozenset({'gender_female'}),0.117,0.518,0.081,0.6923076923076923,1.3365013365013365,1.0,0.020393999999999995,1.5664999999999998,0.2851390461809487,0.14620938628158844,0.3616342164060006,0.42433917433917434
frozenset({'gender_female'}),"frozenset({'test preparation course_completed', 'writing_cat_υψηλό'})",0.518,0.117,0.081,0.15637065637065636,1.3365013365013363,1.0,0.020393999999999995,1.0466681922196797,0.5223605348086675,0.14620938628158844,0.04458737980821784,0.42433917433917434
"frozenset({'math_cat_μέτριο', 'gender_male', 'test preparation course_none'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.154,0.418,0.086,0.5584415584415584,1.3359845895731064,1.0,0.021627999999999994,1.3180588235294117,0.29726757930617403,0.17695473251028807,0.24130851965903505,0.38209159261790837
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'math_cat_μέτριο', 'gender_male', 'test preparation course_none'})",0.418,0.154,0.086,0.20574162679425836,1.3359845895731062,1.0,0.021627999999999994,1.065144578313253,0.43211060497083026,0.17695473251028807,0.06116031535964346,0.38209159261790837
frozenset({'reading_cat_χαμηλό'}),frozenset({'gender_male'}),0.275,0.482,0.177,0.6436363636363636,1.3353451527725386,1.0,0.04444999999999999,1.4535714285714283,0.3463861289694135,0.3051724137931034,0.3120393120393119,0.5054281403244059
frozenset({'gender_male'}),frozenset({'reading_cat_χαμηλό'}),0.482,0.275,0.177,0.3672199170124481,1.3353451527725384,1.0,0.04444999999999999,1.1457377049180328,0.484806840739044,0.3051724137931034,0.12719988553441114,0.5054281403244059
"frozenset({'test preparation course_completed', 'math_cat_μέτριο'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.181,0.418,0.101,0.5580110497237569,1.3349546644109018,1.0,0.025342000000000017,1.3167750000000003,0.30636250438230656,0.2028112449799197,0.24056881395834523,0.39981892199106506
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'test preparation course_completed', 'math_cat_μέτριο'})",0.418,0.181,0.101,0.24162679425837322,1.3349546644109018,1.0,0.025342000000000017,1.079943217665615,0.4311183695689159,0.2028112449799197,0.07402538981486352,0.39981892199106506
frozenset({'reading_cat_χαμηλό'}),"frozenset({'race/ethnicity_group C', 'gender_male'})",0.275,0.139,0.051,0.18545454545454543,1.3342053629823412,1.0,0.012774999999999988,1.0570312499999999,0.3455037187288706,0.14049586776859502,0.05395417590539537,0.2761805101373446
"frozenset({'race/ethnicity_group C', 'gender_male'})",frozenset({'reading_cat_χαμηλό'}),0.139,0.275,0.051,0.3669064748201438,1.3342053629823412,1.0,0.012774999999999988,1.1451704545454544,0.2909293798820338,0.14049586776859502,0.12676755147606042,0.2761805101373446
frozenset({'gender_male'}),"frozenset({'race/ethnicity_group C', 'writing_cat_χαμηλό'})",0.482,0.098,0.063,0.13070539419087138,1.3337285121517486,1.0,0.015764,1.0376229116945108,0.483054483054483,0.12185686653771763,0.03625875187458023,0.38678126852400707
"frozenset({'race/ethnicity_group C', 'writing_cat_χαμηλό'})",frozenset({'gender_male'}),0.098,0.482,0.063,0.6428571428571428,1.3337285121517486,1.0,0.015764,1.4503999999999997,0.27740822862774084,0.12185686653771763,0.310535024820739,0.38678126852400707
frozenset({'math_cat_υψηλό'}),frozenset({'test preparation course_completed'}),0.176,0.358,0.084,0.47727272727272735,1.3331640426612497,1.0,0.02099200000000001,1.2281739130434783,0.30328247803975966,0.18666666666666668,0.18578306428773733,0.35595479939055363
frozenset({'test preparation course_completed'}),frozenset({'math_cat_υψηλό'}),0.358,0.176,0.084,0.23463687150837992,1.3331640426612497,1.0,0.02099200000000001,1.0766131386861315,0.38925975374573524,0.18666666666666668,0.07116125183055816,0.35595479939055363
"frozenset({'gender_female', 'test preparation course_none', 'lunch_standard'})","frozenset({'writing_cat_υψηλό', 'reading_cat_υψηλό'})",0.215,0.185,0.053,0.24651162790697675,1.332495285983658,1.0,0.013225,1.0816358024691357,0.31787044826342986,0.15273775216138327,0.07547439006991014,0.2664990571967316
"frozenset({'writing_cat_υψηλό', 'reading_cat_υψηλό'})","frozenset({'gender_female', 'test preparation course_none', 'lunch_standard'})",0.185,0.215,0.053,0.2864864864864865,1.332495285983658,1.0,0.013225,1.1001893939393939,0.30616969556661655,0.15273775216138327,0.09106558788087453,0.2664990571967316
"frozenset({'gender_male', 'test preparation course_none'})","frozenset({'lunch_free/reduced', 'reading_cat_χαμηλό'})",0.308,0.139,0.057,0.18506493506493507,1.3314024105391011,1.0,0.014188,1.0565258964143427,0.3596998276036913,0.14615384615384616,0.053501666742084855,0.2975684387554891
"frozenset({'lunch_free/reduced', 'reading_cat_χαμηλό'})","frozenset({'gender_male', 'test preparation course_none'})",0.139,0.308,0.057,0.41007194244604317,1.3314024105391011,1.0,0.014188,1.1730243902439024,0.2890967255537217,0.14615384615384616,0.14750280700295257,0.2975684387554891
"frozenset({'race/ethnicity_group C', 'math_cat_μέτριο'})",frozenset({'writing_cat_μέτριο'}),0.153,0.491,0.1,0.6535947712418301,1.331150246928371,1.0,0.02487700000000001,1.469377358490566,0.2937072018890202,0.1838235294117647,0.31943962915880175,0.4286303795109354
frozenset({'writing_cat_μέτριο'}),"frozenset({'race/ethnicity_group C', 'math_cat_μέτριο'})",0.491,0.153,0.1,0.20366598778004075,1.331150246928371,1.0,0.02487700000000001,1.063624040920716,0.4887426326129668,0.1838235294117647,0.05981816739083914,0.4286303795109354
"frozenset({'math_cat_μέτριο', 'lunch_standard'})","frozenset({'test preparation course_completed', 'reading_cat_μέτριο'})",0.332,0.172,0.076,0.2289156626506024,1.3309050154104791,1.0,0.018896000000000003,1.0738125,0.3722029624960606,0.17757009345794392,0.06873872300797391,0.3353880638834407
"frozenset({'test preparation course_completed', 'reading_cat_μέτριο'})","frozenset({'math_cat_μέτριο', 'lunch_standard'})",0.172,0.332,0.076,0.4418604651162791,1.3309050154104791,1.0,0.018896000000000003,1.1968333333333332,0.30027968471904404,0.17757009345794392,0.16446177412616628,0.3353880638834407
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'gender_male'})",0.191,0.24,0.061,0.3193717277486911,1.330715532286213,1.0,0.01516,1.1166153846153848,0.30719974062291033,0.16486486486486487,0.10443648387985673,0.2867691972076789
"frozenset({'math_cat_μέτριο', 'gender_male'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",0.24,0.191,0.061,0.25416666666666665,1.3307155322862128,1.0,0.01516,1.0846927374301676,0.3270060396893874,0.16486486486486487,0.07807993407498968,0.2867691972076789
"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο'})","frozenset({'math_cat_μέτριο', 'lunch_standard'})",0.154,0.332,0.068,0.4415584415584416,1.3299953058989205,1.0,0.016872000000000005,1.1961860465116279,0.2932832707551106,0.1626794258373206,0.16400964305155927,0.32318885933343766
"frozenset({'math_cat_μέτριο', 'lunch_standard'})","frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο'})",0.332,0.154,0.068,0.20481927710843373,1.3299953058989202,1.0,0.016872000000000005,1.063909090909091,0.37143360338147247,0.1626794258373206,0.060070067504058786,0.32318885933343766
"frozenset({'test preparation course_none', 'race/ethnicity_group B'})",frozenset({'math_cat_χαμηλό'}),0.122,0.339,0.055,0.45081967213114754,1.3298515402098747,1.0,0.013642000000000001,1.2036119402985075,0.2825015531165873,0.1354679802955665,0.1691674313633094,0.30653078001837614
frozenset({'math_cat_χαμηλό'}),"frozenset({'test preparation course_none', 'race/ethnicity_group B'})",0.339,0.122,0.055,0.1622418879056047,1.3298515402098747,1.0,0.013642000000000001,1.0480352112676057,0.37524412047861366,0.1354679802955665,0.045833585313900575,0.30653078001837614
"frozenset({'test preparation course_completed', 'lunch_standard'})",frozenset({'parental level of education_some high school'}),0.227,0.179,0.054,0.23788546255506607,1.3289690645534418,1.0,0.013366999999999997,1.077265895953757,0.3202290259211345,0.15340909090909088,0.07172407132163956,0.26978072010434867
frozenset({'parental level of education_some high school'}),"frozenset({'test preparation course_completed', 'lunch_standard'})",0.179,0.227,0.054,0.3016759776536313,1.3289690645534418,1.0,0.013366999999999997,1.1069360000000001,0.30150674425948476,0.15340909090909088,0.09660540446782832,0.26978072010434867
frozenset({'reading_cat_μέτριο'}),"frozenset({'math_cat_μέτριο', 'gender_female', 'lunch_standard'})",0.49,0.172,0.112,0.2285714285714286,1.32890365448505,1.0,0.02772000000000001,1.0733333333333335,0.48529411764705893,0.20363636363636367,0.06832298136645966,0.43986710963455156
"frozenset({'math_cat_μέτριο', 'gender_female', 'lunch_standard'})",frozenset({'reading_cat_μέτριο'}),0.172,0.49,0.112,0.6511627906976745,1.3289036544850499,1.0,0.02772000000000001,1.4620000000000002,0.2989130434782609,0.20363636363636367,0.31600547195622447,0.43986710963455156
"frozenset({'math_cat_μέτριο', 'gender_female'})","frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο'})",0.245,0.169,0.055,0.22448979591836735,1.328341987682647,1.0,0.013594999999999996,1.0715526315789474,0.32739313666465975,0.1532033426183844,0.06677472433016526,0.27496679145030795
"frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο'})","frozenset({'math_cat_μέτριο', 'gender_female'})",0.169,0.245,0.055,0.3254437869822485,1.328341987682647,1.0,0.013594999999999996,1.1192543859649122,0.29745104474346346,0.1532033426183844,0.10654806222814374,0.27496679145030795
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})","frozenset({'race/ethnicity_group D', 'lunch_standard'})",0.266,0.167,0.059,0.22180451127819548,1.3281707262167393,1.0,0.014577999999999994,1.0704251207729467,0.33662771902276806,0.15775401069518713,0.06579173022592494,0.2875489622259241
"frozenset({'race/ethnicity_group D', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})",0.167,0.266,0.059,0.35329341317365265,1.3281707262167393,1.0,0.014577999999999994,1.1349814814814814,0.29662034305247514,0.15775401069518713,0.11892835582241504,0.2875489622259241
frozenset({'math_cat_μέτριο'}),"frozenset({'writing_cat_μέτριο', 'parental level of education_some high school'})",0.485,0.087,0.056,0.1154639175257732,1.3271714658134852,1.0,0.013805000000000005,1.0321794871794872,0.4786754507628295,0.10852713178294576,0.031176251397341957,0.3795710392226568
"frozenset({'writing_cat_μέτριο', 'parental level of education_some high school'})",frozenset({'math_cat_μέτριο'}),0.087,0.485,0.056,0.6436781609195403,1.3271714658134852,1.0,0.013805000000000005,1.4453225806451617,0.27000860585197944,0.10852713178294576,0.3081129338243502,0.3795710392226568
"frozenset({'test preparation course_completed', 'math_cat_υψηλό', 'writing_cat_υψηλό'})",frozenset({'lunch_standard'}),0.062,0.645,0.053,0.8548387096774194,1.3253313328332084,1.0,0.01301,2.445555555555556,0.2616969063040593,0.08103975535168195,0.591094956837801,0.46850462615653915
frozenset({'lunch_standard'}),"frozenset({'test preparation course_completed', 'math_cat_υψηλό', 'writing_cat_υψηλό'})",0.645,0.062,0.053,0.08217054263565891,1.3253313328332084,1.0,0.01301,1.0219763513513513,0.6914695721498805,0.08103975535168195,0.021503776797077736,0.46850462615653915
"frozenset({'reading_cat_υψηλό', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'gender_female'})",0.182,0.245,0.059,0.3241758241758242,1.3231666292890782,1.0,0.01441,1.1171544715447155,0.29857859185280344,0.16032608695652173,0.10486864129248237,0.2824960753532182
"frozenset({'math_cat_μέτριο', 'gender_female'})","frozenset({'reading_cat_υψηλό', 'lunch_standard'})",0.245,0.182,0.059,0.24081632653061225,1.3231666292890782,1.0,0.01441,1.07747311827957,0.3234930968683354,0.16032608695652173,0.07190259967067512,0.2824960753532182
"frozenset({'math_cat_χαμηλό', 'lunch_standard', 'reading_cat_χαμηλό', 'writing_cat_χαμηλό'})",frozenset({'test preparation course_none'}),0.086,0.642,0.073,0.8488372093023256,1.3221763384771428,1.0,0.017787999999999998,2.368307692307693,0.26659872305986027,0.1114503816793893,0.5777575678835911,0.48127218720567994
frozenset({'test preparation course_none'}),"frozenset({'math_cat_χαμηλό', 'lunch_standard', 'reading_cat_χαμηλό', 'writing_cat_χαμηλό'})",0.642,0.086,0.073,0.11370716510903425,1.3221763384771426,1.0,0.017787999999999998,1.0312618629173989,0.6806459018902579,0.1114503816793893,0.030314185020825227,0.48127218720567994
frozenset({'math_cat_μέτριο'}),"frozenset({'test preparation course_completed', 'writing_cat_μέτριο', 'lunch_standard'})",0.485,0.117,0.075,0.15463917525773196,1.3217023526301876,1.0,0.018254999999999993,1.0445243902439025,0.4726213592233008,0.14231499051233396,0.04262647254556281,0.39783240814168647
"frozenset({'test preparation course_completed', 'writing_cat_μέτριο', 'lunch_standard'})",frozenset({'math_cat_μέτριο'}),0.117,0.485,0.075,0.641025641025641,1.3217023526301876,1.0,0.018254999999999993,1.4346428571428569,0.27565118912797276,0.14231499051233396,0.3029624097585262,0.39783240814168647
"frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο', 'test preparation course_none'})",frozenset({'math_cat_μέτριο'}),0.117,0.485,0.075,0.641025641025641,1.3217023526301876,1.0,0.018254999999999993,1.4346428571428569,0.27565118912797276,0.14231499051233396,0.3029624097585262,0.39783240814168647
frozenset({'math_cat_μέτριο'}),"frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο', 'test preparation course_none'})",0.485,0.117,0.075,0.15463917525773196,1.3217023526301876,1.0,0.018254999999999993,1.0445243902439025,0.4726213592233008,0.14231499051233396,0.04262647254556281,0.39783240814168647
frozenset({'writing_cat_υψηλό'}),"frozenset({'gender_female', 'test preparation course_none', 'lunch_standard'})",0.208,0.215,0.059,0.28365384615384615,1.319320214669052,1.0,0.014280000000000001,1.0958389261744967,0.3055983564458141,0.1620879120879121,0.08745712885840273,0.2790362254025045
"frozenset({'gender_female', 'test preparation course_none', 'lunch_standard'})",frozenset({'writing_cat_υψηλό'}),0.215,0.208,0.059,0.2744186046511628,1.319320214669052,1.0,0.014280000000000001,1.0915384615384616,0.3083234373313182,0.1620879120879121,0.083861874559549,0.2790362254025045
frozenset({'reading_cat_μέτριο'}),"frozenset({'test preparation course_completed', 'math_cat_μέτριο'})",0.49,0.181,0.117,0.23877551020408166,1.3192017138347054,1.0,0.028310000000000016,1.0758981233243967,0.47444276856041584,0.2111913357400722,0.07054396850315223,0.4425921749915436
"frozenset({'test preparation course_completed', 'math_cat_μέτριο'})",frozenset({'reading_cat_μέτριο'}),0.181,0.49,0.117,0.6464088397790055,1.3192017138347052,1.0,0.028310000000000016,1.44234375,0.2954405518508084,0.2111913357400722,0.3066839995666775,0.4425921749915436
frozenset({'lunch_free/reduced'}),"frozenset({'math_cat_χαμηλό', 'reading_cat_μέτριο', 'gender_female'})",0.355,0.109,0.051,0.14366197183098592,1.3179997415686782,1.0,0.012304999999999996,1.0404769736842105,0.3740690074479403,0.1234866828087167,0.038902325287301816,0.3057759400439333
"frozenset({'math_cat_χαμηλό', 'reading_cat_μέτριο', 'gender_female'})",frozenset({'lunch_free/reduced'}),0.109,0.355,0.051,0.4678899082568807,1.3179997415686782,1.0,0.012304999999999996,1.2121551724137931,0.2707906956272969,0.1234866828087167,0.17502311357655925,0.3057759400439333
"frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο'})",frozenset({'math_cat_μέτριο'}),0.169,0.485,0.108,0.6390532544378698,1.3176355761605563,1.0,0.02603499999999999,1.4268032786885243,0.29009002986138954,0.1978021978021978,0.2991325328890675,0.4308668334045019
frozenset({'math_cat_μέτριο'}),"frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο'})",0.485,0.169,0.108,0.22268041237113403,1.3176355761605563,1.0,0.02603499999999999,1.0690583554376658,0.46808701905789263,0.1978021978021978,0.06459736747428883,0.4308668334045019
"frozenset({'writing_cat_χαμηλό', 'gender_female', 'reading_cat_χαμηλό'})",frozenset({'test preparation course_none'}),0.084,0.642,0.071,0.8452380952380951,1.3165702418038865,1.0,0.01707199999999999,2.3132307692307674,0.2625007688049694,0.10839694656488548,0.5677041766427238,0.4779149977748108
frozenset({'test preparation course_none'}),"frozenset({'writing_cat_χαμηλό', 'gender_female', 'reading_cat_χαμηλό'})",0.642,0.084,0.071,0.11059190031152646,1.3165702418038865,1.0,0.01707199999999999,1.0298984238178635,0.6716500118026593,0.10839694656488548,0.02903045885537823,0.4779149977748108
"frozenset({'writing_cat_χαμηλό', 'math_cat_μέτριο'})",frozenset({'test preparation course_none'}),0.071,0.642,0.06,0.8450704225352114,1.316309069369488,1.0,0.014418,2.310727272727274,0.25866523143164694,0.09188361408882081,0.5672358171374619,0.4692641832302225
frozenset({'test preparation course_none'}),"frozenset({'writing_cat_χαμηλό', 'math_cat_μέτριο'})",0.642,0.071,0.06,0.09345794392523364,1.316309069369488,1.0,0.014418,1.0247731958762887,0.6712290502793297,0.09188361408882081,0.024174320694546444,0.4692641832302225
"frozenset({'gender_female', 'test preparation course_none'})","frozenset({'math_cat_χαμηλό', 'lunch_standard'})",0.334,0.157,0.069,0.2065868263473054,1.3158396582630918,1.0,0.016562,1.062498113207547,0.36040388214301255,0.16350710900473936,0.058821858063232986,0.323038636103589
"frozenset({'math_cat_χαμηλό', 'lunch_standard'})","frozenset({'gender_female', 'test preparation course_none'})",0.157,0.334,0.069,0.4394904458598726,1.3158396582630916,1.0,0.016562,1.1882045454545453,0.2847318926539103,0.16350710900473936,0.15839406285266158,0.323038636103589
"frozenset({'writing_cat_μέτριο', 'lunch_standard'})","frozenset({'race/ethnicity_group C', 'math_cat_μέτριο'})",0.338,0.153,0.068,0.20118343195266272,1.314924391847469,1.0,0.016286000000000002,1.0603185185185184,0.36178247734138974,0.1607565011820331,0.05688716877528067,0.3228139381985536
"frozenset({'race/ethnicity_group C', 'math_cat_μέτριο'})","frozenset({'writing_cat_μέτριο', 'lunch_standard'})",0.153,0.338,0.068,0.4444444444444445,1.3149243918474687,1.0,0.016286000000000002,1.1915999999999998,0.282762691853601,0.1607565011820331,0.16079221215172879,0.3228139381985536
"frozenset({'parental level of education_high school', 'lunch_standard'})","frozenset({'gender_male', 'test preparation course_none'})",0.126,0.308,0.051,0.4047619047619047,1.3141620284477427,1.0,0.012191999999999995,1.1625599999999998,0.27352268138376623,0.13315926892950392,0.13982934214148082,0.28517316017316013
"frozenset({'gender_male', 'test preparation course_none'})","frozenset({'parental level of education_high school', 'lunch_standard'})",0.308,0.126,0.051,0.16558441558441558,1.3141620284477427,1.0,0.012191999999999995,1.0474396887159534,0.3454607276436585,0.13315926892950392,0.04529109334601325,0.28517316017316013
frozenset({'test preparation course_none'}),"frozenset({'math_cat_χαμηλό', 'gender_female', 'reading_cat_χαμηλό', 'writing_cat_χαμηλό'})",0.642,0.083,0.07,0.1090342679127726,1.3136658784671396,1.0,0.016714,1.0292202797202799,0.6669592976855546,0.1068702290076336,0.02839069565187851,0.47620388094433813
"frozenset({'math_cat_χαμηλό', 'gender_female', 'reading_cat_χαμηλό', 'writing_cat_χαμηλό'})",frozenset({'test preparation course_none'}),0.083,0.642,0.07,0.8433734939759037,1.3136658784671396,1.0,0.016714,2.285692307692308,0.26038323726437135,0.1068702290076336,0.5624957932287812,0.47620388094433813
frozenset({'writing_cat_υψηλό'}),"frozenset({'race/ethnicity_group C', 'lunch_standard'})",0.208,0.205,0.056,0.2692307692307693,1.3133208255159476,1.0,0.013360000000000004,1.0878947368421055,0.3012265512265513,0.1568627450980392,0.080793420416062,0.2712007504690432
"frozenset({'race/ethnicity_group C', 'lunch_standard'})",frozenset({'writing_cat_υψηλό'}),0.205,0.208,0.056,0.2731707317073171,1.3133208255159476,1.0,0.013360000000000004,1.0896644295302014,0.30008984725965865,0.1568627450980392,0.08228627740822866,0.2712007504690432
frozenset({'test preparation course_none'}),"frozenset({'writing_cat_χαμηλό', 'math_cat_μέτριο', 'gender_male'})",0.642,0.07,0.059,0.09190031152647975,1.3128615932354248,1.0,0.01405999999999999,1.0241166380789022,0.6656566613010128,0.0903522205206738,0.023548722071483594,0.4673787271918112
"frozenset({'writing_cat_χαμηλό', 'math_cat_μέτριο', 'gender_male'})",frozenset({'test preparation course_none'}),0.07,0.642,0.059,0.8428571428571427,1.3128615932354248,1.0,0.01405999999999999,2.2781818181818165,0.25624202660834683,0.0903522205206738,0.5610534716679965,0.4673787271918112
frozenset({'gender_male'}),"frozenset({'math_cat_χαμηλό', 'writing_cat_χαμηλό', 'lunch_standard'})",0.482,0.098,0.062,0.1286307053941909,1.3125582183080702,1.0,0.014764,1.035152380952381,0.45970855648275005,0.11969111969111969,0.03395865343036683,0.3806418833093403
"frozenset({'math_cat_χαμηλό', 'writing_cat_χαμηλό', 'lunch_standard'})",frozenset({'gender_male'}),0.098,0.482,0.062,0.6326530612244897,1.31255821830807,1.0,0.014764,1.4101111111111109,0.26400114441027106,0.11969111969111969,0.29083602552990295,0.3806418833093403
"frozenset({'writing_cat_μέτριο', 'gender_female'})","frozenset({'math_cat_μέτριο', 'lunch_standard'})",0.264,0.332,0.115,0.4356060606060606,1.3120664476086161,1.0,0.027352,1.1835704697986575,0.32315689981096407,0.23908523908523904,0.15509889312284522,0.39099580138736767
"frozenset({'math_cat_μέτριο', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'gender_female'})",0.332,0.264,0.115,0.3463855421686747,1.3120664476086161,1.0,0.027352,1.1260460829493086,0.35605311116896643,0.23908523908523904,0.11193687794656884,0.39099580138736767
frozenset({'test preparation course_none'}),"frozenset({'writing_cat_χαμηλό', 'lunch_standard'})",0.642,0.145,0.122,0.19003115264797507,1.310559673434311,1.0,0.028910000000000005,1.0555961538461538,0.6619195897060172,0.18345864661654135,0.05266801479295331,0.5157052314964014
"frozenset({'writing_cat_χαμηλό', 'lunch_standard'})",frozenset({'test preparation course_none'}),0.145,0.642,0.122,0.8413793103448276,1.3105596734343108,1.0,0.028910000000000005,2.2569565217391307,0.2771546352219347,0.18345864661654135,0.5569254478905798,0.5157052314964014
frozenset({'lunch_standard'}),"frozenset({'test preparation course_completed', 'math_cat_υψηλό'})",0.645,0.084,0.071,0.11007751937984495,1.3104466592838684,1.0,0.016819999999999988,1.0293031358885016,0.6673279111287439,0.10790273556231002,0.028468907619918056,0.47765780730897
"frozenset({'test preparation course_completed', 'math_cat_υψηλό'})",frozenset({'lunch_standard'}),0.084,0.645,0.071,0.8452380952380951,1.3104466592838684,1.0,0.016819999999999988,2.293846153846152,0.25862599175841056,0.10790273556231002,0.5640509725016764,0.47765780730897
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})","frozenset({'race/ethnicity_group D', 'lunch_standard'})",0.32,0.167,0.07,0.21875000000000003,1.3098802395209581,1.0,0.016560000000000005,1.0662399999999999,0.34789915966386564,0.16786570743405277,0.062124849939976017,0.31895583832335334
"frozenset({'race/ethnicity_group D', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})",0.167,0.32,0.07,0.4191616766467066,1.3098802395209581,1.0,0.016560000000000005,1.1707216494845358,0.2839993140113189,0.16786570743405277,0.1458259950686862,0.31895583832335334
"frozenset({'test preparation course_completed', 'math_cat_υψηλό'})",frozenset({'gender_male'}),0.084,0.482,0.053,0.6309523809523809,1.3090298360007904,1.0,0.012511999999999995,1.4036129032258065,0.2577243140809095,0.10331384015594544,0.2875528589814304,0.3704554435882237
frozenset({'gender_male'}),"frozenset({'test preparation course_completed', 'math_cat_υψηλό'})",0.482,0.084,0.053,0.10995850622406639,1.3090298360007901,1.0,0.012511999999999995,1.0291655011655012,0.4557441538573612,0.10331384015594544,0.02833898059395893,0.3704554435882237
"frozenset({'test preparation course_none', 'reading_cat_χαμηλό'})",frozenset({'gender_male'}),0.214,0.482,0.135,0.6308411214953271,1.3087990072517162,1.0,0.03185200000000002,1.4031898734177217,0.3001790594665914,0.24064171122994657,0.2873380723848014,0.45546205452359717
frozenset({'gender_male'}),"frozenset({'test preparation course_none', 'reading_cat_χαμηλό'})",0.482,0.214,0.135,0.2800829875518672,1.308799007251716,1.0,0.03185200000000002,1.091792507204611,0.45548405548405574,0.24064171122994657,0.0840750477759125,0.45546205452359717
"frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο'})","frozenset({'math_cat_μέτριο', 'gender_male'})",0.169,0.24,0.053,0.3136094674556213,1.3067061143984222,1.0,0.01244,1.1072413793103448,0.28245124083282247,0.148876404494382,0.09685456244160698,0.2672214003944773
"frozenset({'math_cat_μέτριο', 'gender_male'})","frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο'})",0.24,0.169,0.053,0.22083333333333333,1.306706114398422,1.0,0.01244,1.0665240641711229,0.30883813306852037,0.148876404494382,0.062374649017248276,0.2672214003944773
"frozenset({'parental level of education_high school', 'test preparation course_none'})",frozenset({'math_cat_χαμηλό'}),0.14,0.339,0.062,0.44285714285714284,1.306363253265908,1.0,0.01453999999999999,1.1864102564102563,0.27269317329332315,0.1486810551558753,0.15712124486708443,0.312873999157185
frozenset({'math_cat_χαμηλό'}),"frozenset({'parental level of education_high school', 'test preparation course_none'})",0.339,0.14,0.062,0.18289085545722714,1.306363253265908,1.0,0.01453999999999999,1.052490974729242,0.35478990776438407,0.1486810551558753,0.049873087740961775,0.312873999157185
frozenset({'reading_cat_υψηλό'}),frozenset({'gender_female'}),0.235,0.518,0.159,0.6765957446808512,1.3061693912757744,1.0,0.03727000000000001,1.4903947368421055,0.3064085172853209,0.2676767676767677,0.3290368146905625,0.4917727758153291
frozenset({'gender_female'}),frozenset({'reading_cat_υψηλό'}),0.518,0.235,0.159,0.30694980694980695,1.3061693912757744,1.0,0.03727000000000001,1.103816155988858,0.4863122732847936,0.2676767676767677,0.09405203522850583,0.4917727758153291
frozenset({'writing_cat_μέτριο'}),"frozenset({'math_cat_μέτριο', 'parental level of education_some college', 'test preparation course_none'})",0.491,0.078,0.05,0.10183299389002037,1.30555120371821,1.0,0.011702000000000004,1.0265351473922903,0.4598035363457762,0.09633911368015416,0.02584923415403511,0.3714293174578307
"frozenset({'math_cat_μέτριο', 'parental level of education_some college', 'test preparation course_none'})",frozenset({'writing_cat_μέτριο'}),0.078,0.491,0.05,0.6410256410256411,1.30555120371821,1.0,0.011702000000000004,1.4179285714285716,0.2538394793926248,0.09633911368015416,0.29474585663190783,0.3714293174578307
"frozenset({'math_cat_μέτριο', 'gender_male', 'test preparation course_none', 'lunch_standard'})",frozenset({'writing_cat_μέτριο'}),0.103,0.491,0.066,0.6407766990291263,1.3050441935420087,1.0,0.01542700000000001,1.4169459459459461,0.26058241275632593,0.125,0.2942567760886567,0.3875981254819766
frozenset({'writing_cat_μέτριο'}),"frozenset({'math_cat_μέτριο', 'gender_male', 'test preparation course_none', 'lunch_standard'})",0.491,0.103,0.066,0.13441955193482688,1.3050441935420085,1.0,0.01542700000000001,1.0362988235294117,0.45921890813835836,0.125,0.03502737116480143,0.3875981254819766
"frozenset({'gender_female', 'reading_cat_υψηλό', 'lunch_standard'})",frozenset({'test preparation course_completed'}),0.12,0.358,0.056,0.4666666666666667,1.3035381750465551,1.0,0.013040000000000003,1.20375,0.26461038961038963,0.13270142180094788,0.16926272066458986,0.3115456238361266
frozenset({'test preparation course_completed'}),"frozenset({'gender_female', 'reading_cat_υψηλό', 'lunch_standard'})",0.358,0.12,0.056,0.1564245810055866,1.303538175046555,1.0,0.013040000000000003,1.04317880794702,0.3627058299955497,0.13270142180094788,0.041391569324530225,0.3115456238361266
"frozenset({'lunch_free/reduced', 'writing_cat_μέτριο', 'test preparation course_none'})",frozenset({'math_cat_μέτριο'}),0.087,0.485,0.055,0.632183908045977,1.30347197535253,1.0,0.012805000000000004,1.40015625,0.255003485014438,0.10638297872340427,0.2857939962057806,0.3727929849508236
frozenset({'math_cat_μέτριο'}),"frozenset({'lunch_free/reduced', 'writing_cat_μέτριο', 'test preparation course_none'})",0.485,0.087,0.055,0.1134020618556701,1.30347197535253,1.0,0.012805000000000004,1.029779069767442,0.45207413945278035,0.10638297872340427,0.028917920981018738,0.3727929849508236
"frozenset({'test preparation course_completed', 'math_cat_μέτριο', 'lunch_standard'})",frozenset({'reading_cat_μέτριο'}),0.119,0.49,0.076,0.638655462184874,1.303378494254845,1.0,0.017690000000000004,1.4113953488372095,0.2642033574287592,0.1425891181988743,0.2914812984017138,0.39687875150060026
frozenset({'reading_cat_μέτριο'}),"frozenset({'test preparation course_completed', 'math_cat_μέτριο', 'lunch_standard'})",0.49,0.119,0.076,0.15510204081632653,1.3033784942548448,1.0,0.017690000000000004,1.0427294685990338,0.45639834881320956,0.1425891181988743,0.04097847992772592,0.39687875150060026
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'gender_female'})","frozenset({'math_cat_μέτριο', 'lunch_standard'})",0.227,0.332,0.098,0.43171806167400884,1.3003556074518339,1.0,0.022636000000000003,1.175472868217054,0.29880930379913934,0.2125813449023861,0.14927853544013298,0.36344939228278755
"frozenset({'math_cat_μέτριο', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'gender_female'})",0.332,0.227,0.098,0.29518072289156627,1.3003556074518337,1.0,0.022636000000000003,1.0967350427350429,0.3457778320909203,0.2125813449023861,0.08820274630215558,0.36344939228278755
frozenset({'gender_female'}),"frozenset({'test preparation course_completed', 'writing_cat_υψηλό', 'reading_cat_υψηλό'})",0.518,0.104,0.07,0.13513513513513514,1.2993762993762996,1.0,0.01612800000000001,1.036,0.478008298755187,0.12681159420289856,0.03474903474903476,0.4041060291060291
"frozenset({'test preparation course_completed', 'writing_cat_υψηλό', 'reading_cat_υψηλό'})",frozenset({'gender_female'}),0.104,0.518,0.07,0.6730769230769231,1.2993762993762994,1.0,0.01612800000000001,1.4743529411764709,0.2571428571428573,0.12681159420289856,0.32173635493137576,0.4041060291060291
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'gender_male'})",frozenset({'race/ethnicity_group D'}),0.191,0.262,0.065,0.3403141361256545,1.2989089165101315,1.0,0.014957999999999999,1.1187142857142858,0.2844537415612817,0.16752577319587628,0.10611671561741798,0.2942028695895448
frozenset({'race/ethnicity_group D'}),"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'gender_male'})",0.262,0.191,0.065,0.2480916030534351,1.2989089165101315,1.0,0.014957999999999999,1.0759289340101523,0.3118198874296435,0.16752577319587628,0.07057058473848592,0.2942028695895448
frozenset({'math_cat_χαμηλό'}),"frozenset({'gender_female', 'test preparation course_none'})",0.339,0.334,0.147,0.43362831858407075,1.2982883789942237,1.0,0.03377399999999997,1.17590625,0.34758714378338296,0.27946768060836497,0.14959206994605218,0.43687403953155635
"frozenset({'gender_female', 'test preparation course_none'})",frozenset({'math_cat_χαμηλό'}),0.334,0.339,0.147,0.4401197604790419,1.2982883789942237,1.0,0.03377399999999997,1.1806096256684493,0.34497763069191617,0.27946768060836497,0.15297997046753686,0.43687403953155635
frozenset({'test preparation course_none'}),"frozenset({'writing_cat_χαμηλό', ""parental level of education_associate's degree""})",0.642,0.06,0.05,0.07788161993769471,1.2980269989615787,1.0,0.011480000000000004,1.0193918918918918,0.6413407821229052,0.0766871165644172,0.019022999933717782,0.45560747663551404
"frozenset({'writing_cat_χαμηλό', ""parental level of education_associate's degree""})",frozenset({'test preparation course_none'}),0.06,0.642,0.05,0.8333333333333334,1.2980269989615785,1.0,0.011480000000000004,2.1480000000000006,0.24425531914893625,0.0766871165644172,0.5344506517690876,0.45560747663551404
"frozenset({'test preparation course_none', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'gender_female'})",0.418,0.155,0.084,0.20095693779904308,1.2964963728970522,1.0,0.019210000000000005,1.0575149700598803,0.39293896252659144,0.1717791411042945,0.05438690863792081,0.37144621083500545
"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'gender_female'})","frozenset({'test preparation course_none', 'lunch_standard'})",0.155,0.418,0.084,0.5419354838709678,1.2964963728970522,1.0,0.019210000000000005,1.2705633802816905,0.270639616793463,0.1717791411042945,0.21294756678860446,0.37144621083500545
"frozenset({'reading_cat_μέτριο', 'test preparation course_none'})","frozenset({'math_cat_μέτριο', 'gender_female'})",0.318,0.245,0.101,0.3176100628930818,1.2963676036452318,1.0,0.023090000000000013,1.106405529953917,0.3352109404488838,0.21861471861471865,0.09617226873255869,0.36492748042613277
"frozenset({'math_cat_μέτριο', 'gender_female'})","frozenset({'reading_cat_μέτριο', 'test preparation course_none'})",0.245,0.318,0.101,0.41224489795918373,1.2963676036452318,1.0,0.023090000000000013,1.1603472222222222,0.30279981640548176,0.21861471861471865,0.1381889999401521,0.36492748042613277
"frozenset({'writing_cat_μέτριο', 'gender_female', 'test preparation course_none'})",frozenset({'math_cat_μέτριο'}),0.175,0.485,0.11,0.6285714285714287,1.2960235640648015,1.0,0.02512500000000001,1.386538461538462,0.2768595041322315,0.20000000000000004,0.27877947295423045,0.4276877761413844
frozenset({'math_cat_μέτριο'}),"frozenset({'writing_cat_μέτριο', 'gender_female', 'test preparation course_none'})",0.485,0.175,0.11,0.2268041237113402,1.2960235640648012,1.0,0.02512500000000001,1.067,0.4435127978817301,0.20000000000000004,0.06279287722586693,0.4276877761413844
"frozenset({'parental level of education_high school', 'writing_cat_μέτριο'})","frozenset({'test preparation course_none', 'lunch_standard'})",0.096,0.418,0.052,0.5416666666666666,1.29585326953748,1.0,0.011872,1.269818181818182,0.25255275697753576,0.11255411255411255,0.21248568155784645,0.33303429027113235
"frozenset({'test preparation course_none', 'lunch_standard'})","frozenset({'parental level of education_high school', 'writing_cat_μέτριο'})",0.418,0.096,0.052,0.12440191387559808,1.29585326953748,1.0,0.011872,1.0324371584699454,0.3922812582606397,0.11255411255411255,0.03141804632256425,0.33303429027113235
"frozenset({'race/ethnicity_group C', 'reading_cat_υψηλό'})",frozenset({'lunch_standard'}),0.067,0.645,0.056,0.835820895522388,1.295846349647113,1.0,0.012784999999999998,2.1622727272727267,0.2446983616597764,0.08536585365853659,0.5375236493588395,0.4613213004743723
frozenset({'lunch_standard'}),"frozenset({'race/ethnicity_group C', 'writing_cat_υψηλό'})",0.645,0.067,0.056,0.08682170542635659,1.295846349647113,1.0,0.012784999999999998,1.0217062818336164,0.6431086519114687,0.08536585365853659,0.021245129074337173,0.4613213004743723
"frozenset({'race/ethnicity_group C', 'writing_cat_υψηλό'})",frozenset({'lunch_standard'}),0.067,0.645,0.056,0.835820895522388,1.295846349647113,1.0,0.012784999999999998,2.1622727272727267,0.2446983616597764,0.08536585365853659,0.5375236493588395,0.4613213004743723
frozenset({'lunch_standard'}),"frozenset({'race/ethnicity_group C', 'reading_cat_υψηλό'})",0.645,0.067,0.056,0.08682170542635659,1.295846349647113,1.0,0.012784999999999998,1.0217062818336164,0.6431086519114687,0.08536585365853659,0.021245129074337173,0.4613213004743723
frozenset({'test preparation course_none'}),"frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό', 'lunch_standard'})",0.642,0.119,0.099,0.15420560747663553,1.2958454409801305,1.0,0.02260200000000001,1.0416243093922652,0.6377179617403084,0.14954682779456194,0.039960961948507984,0.4930691902929396
"frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό', 'lunch_standard'})",frozenset({'test preparation course_none'}),0.119,0.642,0.099,0.8319327731092437,1.2958454409801303,1.0,0.02260200000000001,2.1301000000000005,0.2591407835448699,0.14954682779456194,0.5305384723721892,0.4930691902929396
"frozenset({'test preparation course_completed', 'writing_cat_μέτριο'})",frozenset({'math_cat_μέτριο'}),0.183,0.485,0.115,0.6284153005464481,1.2957016506112333,1.0,0.026245000000000004,1.3859558823529414,0.27933585226970364,0.2079566003616637,0.278476311740676,0.4327643513041519
frozenset({'math_cat_μέτριο'}),"frozenset({'test preparation course_completed', 'writing_cat_μέτριο'})",0.485,0.183,0.115,0.2371134020618557,1.2957016506112333,1.0,0.026245000000000004,1.0709324324324325,0.4431405656395104,0.2079566003616637,0.06623427424951737,0.4327643513041519
"frozenset({'math_cat_χαμηλό', 'reading_cat_χαμηλό', 'lunch_standard'})",frozenset({'test preparation course_none'}),0.095,0.642,0.079,0.8315789473684211,1.295294310542712,1.0,0.018009999999999998,2.1256250000000003,0.2519057276732638,0.12006079027355622,0.5295501323140254,0.47731595343498934
frozenset({'test preparation course_none'}),"frozenset({'math_cat_χαμηλό', 'reading_cat_χαμηλό', 'lunch_standard'})",0.642,0.095,0.079,0.12305295950155763,1.295294310542712,1.0,0.018009999999999998,1.0319893428063942,0.636800792023195,0.12006079027355622,0.03099774530558854,0.47731595343498934
frozenset({'gender_male'}),"frozenset({'test preparation course_completed', 'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.482,0.101,0.063,0.13070539419087138,1.2941128137710036,1.0,0.014317999999999997,1.0341718377088305,0.43874486731629575,0.12115384615384615,0.03304270766504046,0.37723388521424756
"frozenset({'test preparation course_completed', 'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο'})",frozenset({'gender_male'}),0.101,0.482,0.063,0.6237623762376238,1.2941128137710036,1.0,0.014317999999999997,1.3767894736842106,0.25280293800872217,0.12115384615384615,0.2736725409992737,0.37723388521424756
frozenset({'writing_cat_μέτριο'}),"frozenset({'test preparation course_completed', 'math_cat_μέτριο'})",0.491,0.181,0.115,0.23421588594704687,1.2940104195969442,1.0,0.026129000000000013,1.0694920212765957,0.4463825061928763,0.20646319569120292,0.06497666171800595,0.4347875009845732
"frozenset({'test preparation course_completed', 'math_cat_μέτριο'})",frozenset({'writing_cat_μέτριο'}),0.181,0.491,0.115,0.6353591160220995,1.294010419596944,1.0,0.026129000000000013,1.3958939393939396,0.2774220948133993,0.20646319569120292,0.28361319454243517,0.4347875009845732
"frozenset({'lunch_free/reduced', 'math_cat_μέτριο'})",frozenset({'reading_cat_μέτριο'}),0.153,0.49,0.097,0.6339869281045752,1.2938508736828067,1.0,0.022030000000000008,1.3933928571428575,0.26813860928200206,0.17765567765567764,0.2823273100089711,0.41597305588902234
frozenset({'reading_cat_μέτριο'}),"frozenset({'lunch_free/reduced', 'math_cat_μέτριο'})",0.49,0.153,0.097,0.1979591836734694,1.2938508736828065,1.0,0.022030000000000008,1.0560559796437659,0.44532039619971714,0.17765567765567764,0.053080500204804495,0.41597305588902234
"frozenset({'reading_cat_μέτριο', 'gender_female'})","frozenset({'math_cat_μέτριο', 'lunch_standard'})",0.261,0.332,0.112,0.42911877394636017,1.2925264275492776,1.0,0.025347999999999996,1.1701208053691274,0.3062536245892132,0.23284823284823286,0.14538738614724575,0.3832340857683608
"frozenset({'math_cat_μέτριο', 'lunch_standard'})","frozenset({'reading_cat_μέτριο', 'gender_female'})",0.332,0.261,0.112,0.3373493975903614,1.2925264275492774,1.0,0.025347999999999996,1.1152181818181817,0.3388045337895637,0.23284823284823286,0.10331447576503575,0.3832340857683608
frozenset({'lunch_free/reduced'}),"frozenset({'math_cat_χαμηλό', 'writing_cat_μέτριο'})",0.355,0.109,0.05,0.14084507042253522,1.292156609381057,1.0,0.011305000000000003,1.037065573770492,0.3505426356589148,0.12077294685990339,0.03574081977837847,0.29978033337640525
"frozenset({'math_cat_χαμηλό', 'writing_cat_μέτριο'})",frozenset({'lunch_free/reduced'}),0.109,0.355,0.05,0.45871559633027525,1.292156609381057,1.0,0.011305000000000003,1.1916101694915255,0.2537598204264871,0.12077294685990339,0.16079937415546552,0.29978033337640525
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'race/ethnicity_group D', 'lunch_standard'})",0.325,0.167,0.07,0.2153846153846154,1.2897282358360203,1.0,0.015725000000000003,1.0616666666666665,0.3328042328042328,0.16587677725118485,0.05808477237048666,0.31727314601566103
"frozenset({'race/ethnicity_group D', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο'})",0.167,0.325,0.07,0.4191616766467066,1.2897282358360203,1.0,0.015725000000000003,1.1621134020618558,0.26967930029154524,0.16587677725118485,0.1394987802173431,0.31727314601566103
"frozenset({'writing_cat_χαμηλό', 'gender_male'})",frozenset({'lunch_free/reduced'}),0.201,0.355,0.092,0.4577114427860696,1.2893280078480835,1.0,0.020644999999999997,1.1894036697247705,0.28085378462208194,0.19827586206896547,0.15924254695514672,0.3584331861817672
frozenset({'lunch_free/reduced'}),"frozenset({'writing_cat_χαμηλό', 'gender_male'})",0.355,0.201,0.092,0.2591549295774648,1.2893280078480833,1.0,0.020644999999999997,1.0784980988593156,0.3479103471520053,0.19827586206896547,0.07278464277530008,0.3584331861817672
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'math_cat_μέτριο', 'gender_female'})",0.418,0.245,0.132,0.31578947368421056,1.288936627282492,1.0,0.02959000000000002,1.1034615384615387,0.3851660939289807,0.2485875706214689,0.09376089229696764,0.42728249194414614
"frozenset({'math_cat_μέτριο', 'gender_female'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.245,0.418,0.132,0.5387755102040817,1.288936627282492,1.0,0.02959000000000002,1.2618584070796461,0.29690949227373087,0.2485875706214689,0.20751805876989976,0.42728249194414614
"frozenset({'test preparation course_completed', 'gender_male'})","frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.174,0.281,0.063,0.3620689655172414,1.288501656644987,1.0,0.014106,1.127081081081081,0.271071140320535,0.1607142857142857,0.11275238597669174,0.29313412688673457
"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'test preparation course_completed', 'gender_male'})",0.281,0.174,0.063,0.22419928825622773,1.288501656644987,1.0,0.014106,1.0647064220183486,0.3114113517451487,0.1607142857142857,0.060773956726667966,0.29313412688673457
frozenset({'writing_cat_μέτριο'}),"frozenset({'math_cat_μέτριο', 'gender_female'})",0.491,0.245,0.155,0.31568228105906315,1.2884991063635232,1.0,0.034705,1.1032886904761905,0.4398884593446986,0.2667814113597246,0.09361891531001743,0.4741676711417765
"frozenset({'math_cat_μέτριο', 'gender_female'})",frozenset({'writing_cat_μέτριο'}),0.245,0.491,0.155,0.6326530612244898,1.2884991063635232,1.0,0.034705,1.3856111111111113,0.2965605639820551,0.2667814113597246,0.2782967804017482,0.4741676711417765
"frozenset({'math_cat_χαμηλό', 'reading_cat_χαμηλό', 'lunch_standard'})",frozenset({'gender_male'}),0.095,0.482,0.059,0.6210526315789473,1.2884909368857829,1.0,0.01321,1.3669444444444443,0.24740144208259202,0.1138996138996139,0.26844137370453147,0.37172963529154834
frozenset({'gender_male'}),"frozenset({'math_cat_χαμηλό', 'reading_cat_χαμηλό', 'lunch_standard'})",0.482,0.095,0.059,0.12240663900414937,1.2884909368857829,1.0,0.01321,1.0312293144208038,0.4322361102022119,0.1138996138996139,0.030283579010109803,0.37172963529154834
"frozenset({'race/ethnicity_group C', 'math_cat_μέτριο'})","frozenset({'writing_cat_μέτριο', 'gender_female'})",0.153,0.264,0.052,0.33986928104575165,1.287383640324817,1.0,0.011607999999999993,1.114930693069307,0.26355462719099065,0.1424657534246575,0.10308326229042342,0.2684194890077243
"frozenset({'writing_cat_μέτριο', 'gender_female'})","frozenset({'race/ethnicity_group C', 'math_cat_μέτριο'})",0.264,0.153,0.052,0.19696969696969696,1.2873836403248167,1.0,0.011607999999999993,1.0547547169811322,0.3033026755852841,0.1424657534246575,0.05191227505277091,0.2684194890077243
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'test preparation course_completed', 'math_cat_μέτριο', 'lunch_standard'})",0.418,0.119,0.064,0.15311004784688997,1.2866390575368907,1.0,0.014258000000000007,1.0402768361581922,0.38278565292096234,0.1353065539112051,0.03871742093858113,0.3454625869486551
"frozenset({'test preparation course_completed', 'math_cat_μέτριο', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.119,0.418,0.064,0.5378151260504203,1.2866390575368907,1.0,0.014258000000000007,1.259236363636364,0.2528731555051079,0.1353065539112051,0.20586791417597983,0.3454625869486551
"frozenset({'math_cat_χαμηλό', 'test preparation course_none'})",frozenset({'parental level of education_high school'}),0.246,0.196,0.062,0.25203252032520324,1.2858802057408327,1.0,0.013783999999999998,1.0749130434782608,0.2948575340121502,0.1631578947368421,0.06969218945920799,0.28417952546872405
frozenset({'parental level of education_high school'}),"frozenset({'math_cat_χαμηλό', 'test preparation course_none'})",0.196,0.246,0.062,0.31632653061224486,1.2858802057408327,1.0,0.013783999999999998,1.102865671641791,0.2765206226929866,0.1631578947368421,0.09327126075894544,0.28417952546872405
"frozenset({'writing_cat_μέτριο', 'test preparation course_none'})","frozenset({'math_cat_μέτριο', 'gender_male'})",0.308,0.24,0.095,0.30844155844155846,1.2851731601731604,1.0,0.02108,1.098967136150235,0.32065713416489205,0.20971302428256067,0.09005468215994536,0.3521374458874459
"frozenset({'math_cat_μέτριο', 'gender_male'})","frozenset({'writing_cat_μέτριο', 'test preparation course_none'})",0.24,0.308,0.095,0.39583333333333337,1.2851731601731604,1.0,0.02108,1.1453793103448275,0.2919667590027701,0.20971302428256067,0.12692678227360316,0.3521374458874459
"frozenset({'test preparation course_completed', 'math_cat_μέτριο'})","frozenset({'reading_cat_μέτριο', 'lunch_standard'})",0.181,0.327,0.076,0.4198895027624309,1.284065757683275,1.0,0.016812999999999995,1.1601238095238098,0.2701143885354411,0.17592592592592593,0.13802303530821827,0.3261527024515518
"frozenset({'reading_cat_μέτριο', 'lunch_standard'})","frozenset({'test preparation course_completed', 'math_cat_μέτριο'})",0.327,0.181,0.076,0.23241590214067276,1.284065757683275,1.0,0.016812999999999995,1.0669840637450199,0.3287127551419409,0.17592592592592593,0.06277887929264074,0.3261527024515518
"frozenset({'test preparation course_completed', 'math_cat_μέτριο', 'lunch_standard'})",frozenset({'writing_cat_μέτριο'}),0.119,0.491,0.075,0.6302521008403361,1.283609166680929,1.0,0.016571000000000002,1.3766136363636363,0.25079076806659106,0.14018691588785046,0.27357976589457,0.3915007958376833
frozenset({'writing_cat_μέτριο'}),"frozenset({'test preparation course_completed', 'math_cat_μέτριο', 'lunch_standard'})",0.491,0.119,0.075,0.15274949083503056,1.283609166680929,1.0,0.016571000000000002,1.0398341346153845,0.43407989521938445,0.14018691588785046,0.03830816212829803,0.3915007958376833
frozenset({'math_cat_μέτριο'}),"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none', 'gender_female'})",0.485,0.151,0.094,0.19381443298969073,1.2835392913224553,1.0,0.020765000000000006,1.0531074168797954,0.42894030159058055,0.17343173431734316,0.05042924969339309,0.40816549464054075
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none', 'gender_female'})",frozenset({'math_cat_μέτριο'}),0.151,0.485,0.094,0.6225165562913908,1.2835392913224553,1.0,0.020765000000000006,1.3642982456140353,0.2601934691627197,0.17343173431734316,0.26702243940075887,0.40816549464054075
"frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο'})","frozenset({'math_cat_μέτριο', 'lunch_standard'})",0.169,0.332,0.072,0.42603550295857984,1.2832394667427103,1.0,0.01589199999999999,1.1638350515463916,0.26561037571867885,0.1678321678321678,0.14077171101583807,0.32145148641904897
"frozenset({'math_cat_μέτριο', 'lunch_standard'})","frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο'})",0.332,0.169,0.072,0.21686746987951805,1.2832394667427103,1.0,0.01589199999999999,1.0611230769230768,0.3304224883566199,0.1678321678321678,0.05760225015585805,0.32145148641904897
"frozenset({'writing_cat_μέτριο', 'test preparation course_none'})","frozenset({'race/ethnicity_group D', 'lunch_standard'})",0.308,0.167,0.066,0.2142857142857143,1.2831479897348161,1.0,0.014564,1.0601818181818181,0.31888246628131023,0.16136919315403425,0.05676556336820444,0.30474764756201883
"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced'})","frozenset({'gender_female', 'test preparation course_none'})",0.182,0.334,0.078,0.4285714285714286,1.2831479897348161,1.0,0.017211999999999998,1.1655,0.26976365118174406,0.1780821917808219,0.14199914199914204,0.33105218135158254
"frozenset({'race/ethnicity_group D', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'test preparation course_none'})",0.167,0.308,0.066,0.39520958083832336,1.2831479897348161,1.0,0.014564,1.14419801980198,0.264905962384954,0.16136919315403425,0.1260254058357274,0.30474764756201883
"frozenset({'gender_female', 'test preparation course_none'})","frozenset({'math_cat_χαμηλό', 'lunch_free/reduced'})",0.334,0.182,0.078,0.2335329341317365,1.283147989734816,1.0,0.017211999999999998,1.067234375,0.33133133133133136,0.1780821917808219,0.06299869698256297,0.33105218135158254
frozenset({'math_cat_μέτριο'}),"frozenset({'test preparation course_completed', 'race/ethnicity_group D'})",0.485,0.082,0.051,0.10515463917525773,1.2823736484787527,1.0,0.011229999999999997,1.0258755760368663,0.42756520083761645,0.09883720930232559,0.025222918491566153,0.36355292934372635
"frozenset({'test preparation course_completed', 'race/ethnicity_group D'})",frozenset({'math_cat_μέτριο'}),0.082,0.485,0.051,0.621951219512195,1.2823736484787527,1.0,0.011229999999999997,1.3622580645161286,0.23986500918450163,0.09883720930232559,0.2659246980819321,0.36355292934372635
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'lunch_free/reduced', 'math_cat_μέτριο'})",0.418,0.153,0.082,0.19617224880382778,1.2821715608093318,1.0,0.018046000000000006,1.0537083333333332,0.37813259575894737,0.16768916155419225,0.05097077780853339,0.3660599806110642
"frozenset({'lunch_free/reduced', 'math_cat_μέτριο'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.153,0.418,0.082,0.5359477124183006,1.2821715608093318,1.0,0.018046000000000006,1.2541690140845072,0.2598266478532555,0.16768916155419225,0.20265929968780177,0.3660599806110642
frozenset({'parental level of education_high school'}),"frozenset({'gender_male', 'test preparation course_none', 'lunch_standard'})",0.196,0.203,0.051,0.260204081632653,1.2817935055795715,1.0,0.011211999999999993,1.0773241379310343,0.2734367378792311,0.14655172413793102,0.07177425549893728,0.2557178043631245
"frozenset({'gender_male', 'test preparation course_none', 'lunch_standard'})",frozenset({'parental level of education_high school'}),0.203,0.196,0.051,0.25123152709359603,1.2817935055795715,1.0,0.011211999999999993,1.0737631578947369,0.27583831525081787,0.14655172413793102,0.06869592922089057,0.2557178043631245
frozenset({'test preparation course_none'}),"frozenset({'math_cat_χαμηλό', 'gender_male', 'writing_cat_χαμηλό', 'lunch_standard'})",0.642,0.062,0.051,0.0794392523364486,1.2812782634911064,1.0,0.011195999999999998,1.018944162436548,0.6132106473874466,0.0781010719754977,0.018591953450371637,0.4510099487488694
frozenset({'test preparation course_none'}),"frozenset({'math_cat_χαμηλό', 'gender_male', 'lunch_standard'})",0.642,0.062,0.051,0.0794392523364486,1.2812782634911064,1.0,0.011195999999999998,1.018944162436548,0.6132106473874466,0.0781010719754977,0.018591953450371637,0.4510099487488694
"frozenset({'math_cat_χαμηλό', 'gender_male', 'writing_cat_χαμηλό', 'lunch_standard'})",frozenset({'test preparation course_none'}),0.062,0.642,0.051,0.8225806451612903,1.2812782634911062,1.0,0.011195999999999998,2.017818181818181,0.23403988461056063,0.0781010719754977,0.5044152099477381,0.4510099487488694
"frozenset({'math_cat_χαμηλό', 'gender_male', 'lunch_standard'})",frozenset({'test preparation course_none'}),0.062,0.642,0.051,0.8225806451612903,1.2812782634911062,1.0,0.011195999999999998,2.017818181818181,0.23403988461056063,0.0781010719754977,0.5044152099477381,0.4510099487488694
"frozenset({'math_cat_μέτριο', 'test preparation course_none'})","frozenset({'race/ethnicity_group D', 'lunch_standard'})",0.304,0.167,0.065,0.21381578947368424,1.280334068704696,1.0,0.014232000000000002,1.0595481171548118,0.3145888594164457,0.1600985221674877,0.05620142793959691,0.3015186731799559
"frozenset({'race/ethnicity_group D', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'test preparation course_none'})",0.167,0.304,0.065,0.38922155688622756,1.280334068704696,1.0,0.014232000000000002,1.139529411764706,0.2628497552867301,0.1600985221674877,0.12244476564113158,0.3015186731799559
frozenset({'math_cat_μέτριο'}),"frozenset({'lunch_free/reduced', 'writing_cat_μέτριο'})",0.485,0.153,0.095,0.1958762886597938,1.2802371807829662,1.0,0.020795000000000008,1.0533205128205128,0.4250383239652531,0.17495395948434622,0.05062135615087818,0.4083956606697662
"frozenset({'lunch_free/reduced', 'writing_cat_μέτριο'})",frozenset({'math_cat_μέτριο'}),0.153,0.485,0.095,0.6209150326797386,1.2802371807829662,1.0,0.020795000000000008,1.3585344827586208,0.2584353445597466,0.17495395948434622,0.2639126848150264,0.4083956606697662
"frozenset({'math_cat_χαμηλό', 'gender_female'})","frozenset({'race/ethnicity_group C', 'test preparation course_none'})",0.205,0.202,0.053,0.25853658536585367,1.2798840859695726,1.0,0.011589999999999996,1.0762500000000002,0.275068233060401,0.14971751412429377,0.0708478513356562,0.260456411494808
"frozenset({'race/ethnicity_group C', 'test preparation course_none'})","frozenset({'math_cat_χαμηλό', 'gender_female'})",0.202,0.205,0.053,0.26237623762376233,1.2798840859695724,1.0,0.011589999999999996,1.0777852348993289,0.2740341419586702,0.14971751412429377,0.0721713680802042,0.260456411494808
"frozenset({'lunch_free/reduced', 'gender_male'})",frozenset({'math_cat_χαμηλό'}),0.166,0.339,0.072,0.4337349397590361,1.2794540995841772,1.0,0.01572599999999999,1.1672978723404255,0.26189048760991196,0.16628175519630484,0.1433206350363632,0.3230621601450048
frozenset({'math_cat_χαμηλό'}),"frozenset({'lunch_free/reduced', 'gender_male'})",0.339,0.166,0.072,0.21238938053097342,1.2794540995841772,1.0,0.01572599999999999,1.0588988764044944,0.33043368633383746,0.16628175519630484,0.05562275843042376,0.3230621601450048
frozenset({'parental level of education_high school'}),frozenset({'math_cat_χαμηλό'}),0.196,0.339,0.085,0.43367346938775514,1.2792727710553249,1.0,0.018556000000000003,1.1671711711711712,0.27152472929470295,0.18888888888888888,0.14322763901324526,0.3422054662572994
frozenset({'math_cat_χαμηλό'}),frozenset({'parental level of education_high school'}),0.339,0.196,0.085,0.25073746312684364,1.2792727710553247,1.0,0.018556000000000003,1.0730551181102364,0.3302660852540714,0.18888888888888888,0.06808142179955676,0.3422054662572994
frozenset({'gender_female'}),"frozenset({'race/ethnicity_group C', 'math_cat_χαμηλό', 'test preparation course_none'})",0.518,0.08,0.053,0.1023166023166023,1.2789575289575288,1.0,0.011559999999999994,1.0248602150537636,0.45251702810616123,0.09724770642201835,0.02425717643108946,0.38240830115830116
"frozenset({'race/ethnicity_group C', 'math_cat_χαμηλό', 'test preparation course_none'})",frozenset({'gender_female'}),0.08,0.518,0.053,0.6625,1.2789575289575288,1.0,0.011559999999999994,1.428148148148148,0.23707957342083663,0.09724770642201835,0.2997925311203319,0.38240830115830116
frozenset({'test preparation course_completed'}),"frozenset({'parental level of education_some high school', 'lunch_standard'})",0.358,0.118,0.054,0.15083798882681565,1.2782880409052175,1.0,0.011756000000000003,1.038671052631579,0.3391023422176071,0.12796208530805686,0.037231279848997344,0.3042325537354417
"frozenset({'parental level of education_some high school', 'lunch_standard'})",frozenset({'test preparation course_completed'}),0.118,0.358,0.054,0.4576271186440678,1.2782880409052173,1.0,0.011756000000000003,1.1836875,0.24682959603594531,0.12796208530805686,0.15518242779449815,0.3042325537354417
"frozenset({'writing_cat_μέτριο', 'gender_male'})",frozenset({'race/ethnicity_group D'}),0.227,0.262,0.076,0.33480176211453744,1.2778693210478527,1.0,0.016525999999999992,1.1094437086092714,0.2813031933001973,0.18401937046004843,0.0986473741389396,0.3124390489962
frozenset({'race/ethnicity_group D'}),"frozenset({'writing_cat_μέτριο', 'gender_male'})",0.262,0.227,0.076,0.2900763358778626,1.2778693210478527,1.0,0.016525999999999992,1.0888494623655915,0.2946441306518327,0.18401937046004843,0.08159939958326336,0.3124390489962
"frozenset({'writing_cat_υψηλό', 'test preparation course_none'})",frozenset({'lunch_standard'}),0.091,0.645,0.075,0.8241758241758241,1.277791975466394,1.0,0.016305,2.0190624999999995,0.23916391639163917,0.113464447806354,0.5047206314811947,0.470227446971633
frozenset({'lunch_standard'}),"frozenset({'writing_cat_υψηλό', 'test preparation course_none'})",0.645,0.091,0.075,0.11627906976744186,1.277791975466394,1.0,0.016305,1.0286052631578948,0.6123943661971831,0.113464447806354,0.027809757719958042,0.470227446971633
"frozenset({'race/ethnicity_group C', 'math_cat_μέτριο', 'gender_female'})",frozenset({'writing_cat_μέτριο'}),0.083,0.491,0.052,0.6265060240963854,1.2759796824773635,1.0,0.011246999999999993,1.3628064516129028,0.2358652797584094,0.0996168582375479,0.26622008663337027,0.3662061688710033
frozenset({'writing_cat_μέτριο'}),"frozenset({'race/ethnicity_group C', 'math_cat_μέτριο', 'gender_female'})",0.491,0.083,0.052,0.10590631364562118,1.2759796824773635,1.0,0.011246999999999993,1.025619589977221,0.42492821520326407,0.0996168582375479,0.024979622296206302,0.3662061688710033
"frozenset({'test preparation course_none', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'gender_female'})",0.418,0.15,0.08,0.19138755980861244,1.2759170653907497,1.0,0.01730000000000001,1.0511834319526627,0.3715635738831617,0.1639344262295082,0.0486912468336617,0.3623604465709729
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'gender_female'})","frozenset({'test preparation course_none', 'lunch_standard'})",0.15,0.418,0.08,0.5333333333333333,1.2759170653907497,1.0,0.01730000000000001,1.2471428571428573,0.2544117647058825,0.1639344262295082,0.1981672394043528,0.3623604465709729
frozenset({'gender_male'}),"frozenset({'race/ethnicity_group C', 'reading_cat_χαμηλό'})",0.482,0.083,0.051,0.10580912863070539,1.2748087786831974,1.0,0.010993999999999997,1.025508120649652,0.41615565144976896,0.09922178988326849,0.02487364081865364,0.36013347997800327
"frozenset({'race/ethnicity_group C', 'reading_cat_χαμηλό'})",frozenset({'gender_male'}),0.083,0.482,0.051,0.6144578313253012,1.2748087786831974,1.0,0.010993999999999997,1.3435625,0.23508029165864813,0.09922178988326849,0.25571009908359305,0.36013347997800327
frozenset({'lunch_free/reduced'}),"frozenset({'writing_cat_χαμηλό', 'gender_male', 'reading_cat_χαμηλό'})",0.355,0.168,0.076,0.2140845070422535,1.2743125419181756,1.0,0.01636,1.058637992831541,0.3337413300693594,0.17002237136465323,0.05539003250270854,0.3332327297116029
"frozenset({'writing_cat_χαμηλό', 'gender_male', 'reading_cat_χαμηλό'})",frozenset({'lunch_free/reduced'}),0.168,0.355,0.076,0.45238095238095233,1.2743125419181756,1.0,0.01636,1.1778260869565216,0.2587297570850203,0.17002237136465323,0.15097822074566256,0.3332327297116029
frozenset({'gender_male'}),frozenset({'math_cat_υψηλό'}),0.482,0.176,0.108,0.22406639004149378,1.273104488872124,1.0,0.023168000000000008,1.0619465240641712,0.4141284141284143,0.19636363636363638,0.058332997623172074,0.4188513768389287
frozenset({'math_cat_υψηλό'}),frozenset({'gender_male'}),0.176,0.482,0.108,0.6136363636363636,1.2731044888721237,1.0,0.023168000000000008,1.3407058823529412,0.2603380079108235,0.19636363636363638,0.25412425412425416,0.4188513768389287
"frozenset({'math_cat_μέτριο', 'test preparation course_none'})","frozenset({'reading_cat_μέτριο', 'gender_female'})",0.304,0.261,0.101,0.33223684210526316,1.2729380923573301,1.0,0.02165600000000001,1.106679802955665,0.3080687379082737,0.21767241379310348,0.09639626807207463,0.3596050110909458
"frozenset({'reading_cat_μέτριο', 'gender_female'})","frozenset({'math_cat_μέτριο', 'test preparation course_none'})",0.261,0.304,0.101,0.38697318007662834,1.2729380923573301,1.0,0.02165600000000001,1.1353499999999999,0.29014322271198717,0.21767241379310348,0.11921433919055799,0.3596050110909458
frozenset({'gender_female'}),"frozenset({'reading_cat_υψηλό', 'lunch_standard'})",0.518,0.182,0.12,0.23166023166023164,1.2728584157155585,1.0,0.025723999999999997,1.0646331658291457,0.44474412171507605,0.20689655172413793,0.060709329657984895,0.4455004455004455
"frozenset({'reading_cat_υψηλό', 'lunch_standard'})",frozenset({'gender_female'}),0.182,0.518,0.12,0.6593406593406593,1.2728584157155585,1.0,0.025723999999999997,1.4149032258064516,0.2620619396903015,0.20689655172413793,0.29323788244950066,0.4455004455004455
"frozenset({'reading_cat_μέτριο', 'test preparation course_none'})","frozenset({'parental level of education_high school', 'lunch_standard'})",0.318,0.126,0.051,0.16037735849056603,1.2728361784965558,1.0,0.010931999999999997,1.040943820224719,0.3143005002587545,0.1297709923664122,0.03933336211735244,0.2825696316262354
"frozenset({'parental level of education_high school', 'lunch_standard'})","frozenset({'reading_cat_μέτριο', 'test preparation course_none'})",0.126,0.318,0.051,0.4047619047619047,1.2728361784965556,1.0,0.010931999999999997,1.1457599999999997,0.24525508143760932,0.1297709923664122,0.12721686915235295,0.2825696316262354
frozenset({'writing_cat_χαμηλό'}),"frozenset({'math_cat_μέτριο', 'test preparation course_none', 'gender_male'})",0.301,0.154,0.059,0.19601328903654486,1.2728135651723693,1.0,0.012645999999999998,1.0522561983471075,0.30663659950049704,0.148989898989899,0.04966109815194428,0.28956508607671394
"frozenset({'math_cat_μέτριο', 'test preparation course_none', 'gender_male'})",frozenset({'writing_cat_χαμηλό'}),0.154,0.301,0.059,0.3831168831168831,1.272813565172369,1.0,0.012645999999999998,1.1331157894736843,0.25335577192771563,0.148989898989899,0.11747765825019038,0.28956508607671394
frozenset({'math_cat_μέτριο'}),"frozenset({'test preparation course_completed', 'writing_cat_υψηλό', 'gender_female'})",0.485,0.081,0.05,0.10309278350515465,1.2727504136438845,1.0,0.010715000000000002,1.024632183908046,0.41611650485436896,0.09689922480620157,0.024040025576882094,0.36018836706121926
"frozenset({'test preparation course_completed', 'writing_cat_υψηλό', 'gender_female'})",frozenset({'math_cat_μέτριο'}),0.081,0.485,0.05,0.6172839506172839,1.2727504136438843,1.0,0.010715000000000002,1.3456451612903224,0.23318824809575628,0.09689922480620157,0.256862040033561,0.36018836706121926
"frozenset({'writing_cat_μέτριο', 'race/ethnicity_group B'})",frozenset({'gender_female'}),0.088,0.518,0.058,0.6590909090909092,1.2723762723762726,1.0,0.012416000000000003,1.413866666666667,0.23472474289171208,0.10583941605839418,0.2927197284043759,0.38553001053001057
frozenset({'gender_female'}),"frozenset({'writing_cat_μέτριο', 'race/ethnicity_group B'})",0.518,0.088,0.058,0.11196911196911197,1.2723762723762724,1.0,0.012416000000000003,1.026991304347826,0.4441264844756046,0.10583941605839418,0.026281921018763126,0.38553001053001057
frozenset({'test preparation course_completed'}),"frozenset({'math_cat_υψηλό', 'lunch_standard'})",0.358,0.156,0.071,0.1983240223463687,1.2713078355536456,1.0,0.015151999999999999,1.052794425087108,0.3324119169847747,0.1602708803611738,0.05014694590801979,0.3267261137372869
"frozenset({'math_cat_υψηλό', 'lunch_standard'})",frozenset({'test preparation course_completed'}),0.156,0.358,0.071,0.45512820512820507,1.2713078355536456,1.0,0.015151999999999999,1.1782588235294116,0.25285361457846606,0.1602708803611738,0.15129003914050634,0.3267261137372869
"frozenset({'race/ethnicity_group C', 'test preparation course_none'})","frozenset({'reading_cat_μέτριο', 'gender_female'})",0.202,0.261,0.067,0.3316831683168317,1.2708167368460985,1.0,0.014277999999999999,1.105762962962963,0.26704821755882235,0.1691919191919192,0.09564704779002935,0.2941940745798718
"frozenset({'reading_cat_μέτριο', 'gender_female'})","frozenset({'race/ethnicity_group C', 'test preparation course_none'})",0.261,0.202,0.067,0.2567049808429119,1.2708167368460983,1.0,0.014277999999999999,1.07359793814433,0.28836871124755115,0.1691919191919192,0.06855260757257126,0.2941940745798718
"frozenset({'math_cat_μέτριο', 'parental level of education_some high school'})",frozenset({'reading_cat_μέτριο'}),0.09,0.49,0.056,0.6222222222222222,1.26984126984127,1.0,0.0119,1.35,0.23351648351648352,0.10687022900763361,0.2592592592592593,0.3682539682539683
frozenset({'reading_cat_μέτριο'}),"frozenset({'math_cat_μέτριο', 'parental level of education_some high school'})",0.49,0.09,0.056,0.1142857142857143,1.26984126984127,1.0,0.0119,1.0274193548387098,0.4166666666666667,0.10687022900763361,0.02668759811616956,0.3682539682539683
"frozenset({'test preparation course_completed', 'writing_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'math_cat_μέτριο', 'lunch_standard'})",0.152,0.332,0.064,0.4210526315789474,1.2682308180088777,1.0,0.013536,1.1538181818181816,0.24941037735849056,0.1523809523809524,0.13331232272297514,0.3069118579581484
"frozenset({'math_cat_μέτριο', 'lunch_standard'})","frozenset({'test preparation course_completed', 'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.332,0.152,0.064,0.19277108433734938,1.2682308180088775,1.0,0.013536,1.0505074626865671,0.3166167664670659,0.1523809523809524,0.04807910888838371,0.3069118579581484
"frozenset({'math_cat_μέτριο', 'gender_male', 'test preparation course_none'})","frozenset({'writing_cat_μέτριο', 'lunch_standard'})",0.154,0.338,0.066,0.4285714285714286,1.267962806424345,1.0,0.013948000000000002,1.1584999999999999,0.24980299448384558,0.15492957746478875,0.13681484678463535,0.31191885038038886
"frozenset({'writing_cat_μέτριο', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'gender_male', 'test preparation course_none'})",0.338,0.154,0.066,0.1952662721893491,1.267962806424345,1.0,0.013948000000000002,1.0512794117647057,0.31923464249748246,0.15492957746478875,0.04877809951459706,0.31191885038038886
frozenset({'writing_cat_μέτριο'}),"frozenset({'math_cat_μέτριο', 'parental level of education_some high school'})",0.491,0.09,0.056,0.11405295315682282,1.2672550350758092,1.0,0.011810000000000001,1.0271494252873563,0.4143278136401909,0.10666666666666669,0.026431816655849254,0.36813758768952254
"frozenset({'math_cat_μέτριο', 'parental level of education_some high school'})",frozenset({'writing_cat_μέτριο'}),0.09,0.491,0.056,0.6222222222222222,1.267255035075809,1.0,0.011810000000000001,1.3473529411764706,0.23175039246467818,0.10666666666666669,0.25780397293167434,0.36813758768952254
"frozenset({'gender_female', 'test preparation course_none', 'lunch_standard'})",frozenset({'reading_cat_υψηλό'}),0.215,0.235,0.064,0.29767441860465116,1.2666996536368136,1.0,0.013475000000000008,1.0892384105960264,0.26821257961783457,0.1658031088082902,0.08192734458124336,0.285007422068283
frozenset({'reading_cat_υψηλό'}),"frozenset({'gender_female', 'test preparation course_none', 'lunch_standard'})",0.235,0.215,0.064,0.2723404255319149,1.2666996536368136,1.0,0.013475000000000008,1.0788011695906432,0.2752246732026145,0.1658031088082902,0.07304512806613363,0.285007422068283
frozenset({'writing_cat_χαμηλό'}),"frozenset({'race/ethnicity_group C', 'test preparation course_none'})",0.301,0.202,0.077,0.2558139534883721,1.2664057103384756,1.0,0.016197999999999997,1.0723125,0.3009494082455455,0.1807511737089202,0.0674360319403159,0.3185010361501266
"frozenset({'race/ethnicity_group C', 'test preparation course_none'})",frozenset({'writing_cat_χαμηλό'}),0.202,0.301,0.077,0.38118811881188114,1.2664057103384756,1.0,0.016197999999999997,1.129584,0.263613579403053,0.1807511737089202,0.1147183387866683,0.3185010361501266
frozenset({'test preparation course_none'}),"frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced', 'gender_female'})",0.642,0.064,0.052,0.08099688473520249,1.2655763239875388,1.0,0.010911999999999998,1.0184949152542373,0.5861624409110443,0.07951070336391437,0.018159064888036845,0.44674844236760125
"frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced', 'gender_female'})",frozenset({'test preparation course_none'}),0.064,0.642,0.052,0.8125,1.2655763239875388,1.0,0.010911999999999998,1.9093333333333333,0.22419460880999342,0.07951070336391437,0.4762569832402234,0.44674844236760125
frozenset({'gender_male'}),"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'race/ethnicity_group D'})",0.482,0.1,0.061,0.12655601659751037,1.2655601659751037,1.0,0.012799999999999999,1.0304038004750593,0.40508892967909355,0.11708253358925146,0.029506685108344845,0.3682780082987552
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'race/ethnicity_group D'})",frozenset({'gender_male'}),0.1,0.482,0.061,0.61,1.2655601659751037,1.0,0.012799999999999999,1.3282051282051281,0.2331511839708561,0.11708253358925146,0.2471042471042471,0.3682780082987552
frozenset({'race/ethnicity_group D'}),"frozenset({'reading_cat_μέτριο', 'math_cat_μέτριο', 'gender_male'})",0.262,0.175,0.058,0.22137404580152673,1.2649945474372957,1.0,0.012150000000000001,1.0595588235294118,0.283851976450799,0.15303430079155672,0.056210964607911196,0.2764013086150491
"frozenset({'reading_cat_μέτριο', 'math_cat_μέτριο', 'gender_male'})",frozenset({'race/ethnicity_group D'}),0.175,0.262,0.058,0.33142857142857146,1.2649945474372957,1.0,0.012150000000000001,1.1038461538461537,0.2539184952978057,0.15303430079155672,0.09407665505226484,0.2764013086150491
frozenset({'writing_cat_μέτριο'}),"frozenset({'math_cat_χαμηλό', 'gender_female', 'lunch_standard'})",0.491,0.095,0.059,0.12016293279022403,1.2648729767392002,1.0,0.012354999999999998,1.028599537037037,0.41140821151476803,0.11195445920303607,0.02780434562455693,0.3706077821845857
"frozenset({'math_cat_χαμηλό', 'gender_female', 'lunch_standard'})",frozenset({'writing_cat_μέτριο'}),0.095,0.491,0.059,0.6210526315789473,1.2648729767392002,1.0,0.012354999999999998,1.3431944444444441,0.2313887068077535,0.11195445920303607,0.2555061524144348,0.3706077821845857
frozenset({'gender_female'}),"frozenset({'test preparation course_completed', 'writing_cat_υψηλό', 'lunch_standard'})",0.518,0.087,0.057,0.11003861003861004,1.264811609639196,1.0,0.011934000000000007,1.025887201735358,0.43437431753658035,0.104014598540146,0.025233964993001144,0.3826055119158568
"frozenset({'test preparation course_completed', 'writing_cat_υψηλό', 'lunch_standard'})",frozenset({'gender_female'}),0.087,0.518,0.057,0.6551724137931035,1.264811609639196,1.0,0.011934000000000007,1.3978000000000004,0.22931919063815082,0.104014598540146,0.2845900701101733,0.3826055119158568
frozenset({'lunch_standard'}),"frozenset({'gender_male', 'reading_cat_υψηλό'})",0.645,0.076,0.062,0.09612403100775194,1.2647898816809466,1.0,0.012979999999999998,1.0222641509433963,0.5897319400272603,0.09408194233687404,0.02177925433739387,0.4559567523459812
"frozenset({'gender_male', 'reading_cat_υψηλό'})",frozenset({'lunch_standard'}),0.076,0.645,0.062,0.8157894736842105,1.2647898816809464,1.0,0.012979999999999998,1.9271428571428568,0.2265745007680491,0.09408194233687404,0.48109710896960706,0.4559567523459812
"frozenset({'lunch_free/reduced', 'math_cat_μέτριο'})",frozenset({'writing_cat_μέτριο'}),0.153,0.491,0.095,0.6209150326797386,1.2645927345819523,1.0,0.019877000000000006,1.3427068965517241,0.24702665755297343,0.17304189435336975,0.2552358205888774,0.4071988605353887
frozenset({'writing_cat_μέτριο'}),"frozenset({'lunch_free/reduced', 'math_cat_μέτριο'})",0.491,0.153,0.095,0.19348268839103872,1.2645927345819523,1.0,0.019877000000000006,1.0501944444444444,0.4110640057905078,0.17304189435336975,0.047795381807601796,0.4071988605353887
frozenset({'test preparation course_none'}),"frozenset({'writing_cat_χαμηλό', 'race/ethnicity_group B'})",0.642,0.069,0.056,0.08722741433021806,1.264165425075624,1.0,0.011701999999999997,1.0199692832764506,0.5836991221069432,0.08549618320610687,0.019578318292393187,0.44941080861438437
"frozenset({'writing_cat_χαμηλό', 'race/ethnicity_group B'})",frozenset({'test preparation course_none'}),0.069,0.642,0.056,0.8115942028985507,1.264165425075624,1.0,0.011701999999999997,1.9001538461538454,0.22445143470922196,0.08549618320610687,0.4737268237389683,0.44941080861438437
"frozenset({'writing_cat_υψηλό', 'test preparation course_none', 'reading_cat_υψηλό'})",frozenset({'lunch_standard'}),0.081,0.645,0.066,0.8148148148148149,1.2632787826586276,1.0,0.013755000000000003,1.9170000000000005,0.22677811850826,0.10000000000000002,0.4783515910276475,0.45857019810508187
frozenset({'lunch_standard'}),"frozenset({'writing_cat_υψηλό', 'test preparation course_none', 'reading_cat_υψηλό'})",0.645,0.081,0.066,0.10232558139534884,1.2632787826586276,1.0,0.013755000000000003,1.023756476683938,0.5870678617157492,0.10000000000000002,0.023205202824101014,0.45857019810508187
"frozenset({'test preparation course_completed', 'writing_cat_μέτριο', 'math_cat_μέτριο'})",frozenset({'gender_male'}),0.115,0.482,0.07,0.6086956521739131,1.2628540501533467,1.0,0.014570000000000007,1.323777777777778,0.23518966908797426,0.1328273244781784,0.24458620110794033,0.376961933970774
frozenset({'gender_male'}),"frozenset({'test preparation course_completed', 'writing_cat_μέτριο', 'math_cat_μέτριο'})",0.482,0.115,0.07,0.14522821576763487,1.2628540501533465,1.0,0.014570000000000007,1.035364077669903,0.40182018753447335,0.1328273244781784,0.034156176008626964,0.376961933970774
frozenset({'test preparation course_none'}),"frozenset({'math_cat_χαμηλό', 'gender_female', 'reading_cat_χαμηλό'})",0.642,0.095,0.077,0.11993769470404984,1.2625020495163142,1.0,0.016009999999999996,1.0283362831858407,0.5807879271566422,0.11666666666666665,0.02755546376138104,0.4652320052467617
"frozenset({'math_cat_χαμηλό', 'gender_female', 'reading_cat_χαμηλό'})",frozenset({'test preparation course_none'}),0.095,0.642,0.077,0.8105263157894737,1.2625020495163142,1.0,0.016009999999999996,1.889444444444444,0.22974815240008606,0.11666666666666665,0.47074389885327833,0.4652320052467617
"frozenset({'lunch_free/reduced', 'math_cat_μέτριο', 'reading_cat_μέτριο'})",frozenset({'gender_male'}),0.097,0.482,0.059,0.6082474226804123,1.261924113444839,1.0,0.012246,1.3222631578947368,0.22985528464440566,0.11346153846153845,0.24372089320542922,0.36532703084228085
frozenset({'gender_male'}),"frozenset({'lunch_free/reduced', 'math_cat_μέτριο', 'reading_cat_μέτριο'})",0.482,0.097,0.059,0.12240663900414937,1.261924113444839,1.0,0.012246,1.028950354609929,0.40069367188011257,0.11346153846153845,0.028135812850663754,0.36532703084228085
frozenset({'math_cat_χαμηλό'}),"frozenset({'test preparation course_completed', 'lunch_free/reduced'})",0.339,0.131,0.056,0.16519174041297935,1.261005652007476,1.0,0.011590999999999997,1.0409575971731448,0.3131348606008212,0.13526570048309178,0.039346076424602236,0.2963363282217568
"frozenset({'test preparation course_completed', 'lunch_free/reduced'})",frozenset({'math_cat_χαμηλό'}),0.131,0.339,0.056,0.42748091603053434,1.261005652007476,1.0,0.011590999999999997,1.1545466666666668,0.23818428407035996,0.13526570048309178,0.1338591770507327,0.2963363282217568
frozenset({'test preparation course_none'}),"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced', 'gender_female', 'writing_cat_χαμηλό'})",0.642,0.063,0.051,0.0794392523364486,1.2609405132769618,1.0,0.010553999999999994,1.0178578680203045,0.5780479789681233,0.07798165137614678,0.01754455959066018,0.444481530930129
"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced', 'gender_female', 'writing_cat_χαμηλό'})",frozenset({'test preparation course_none'}),0.063,0.642,0.051,0.8095238095238094,1.2609405132769618,1.0,0.010553999999999994,1.8794999999999988,0.22085504425889874,0.07798165137614678,0.467943602021814,0.444481530930129
"frozenset({'parental level of education_high school', 'writing_cat_μέτριο', 'test preparation course_none'})",frozenset({'lunch_standard'}),0.064,0.645,0.052,0.8125,1.2596899224806202,1.0,0.010719999999999993,1.8933333333333333,0.2202498356344509,0.0791476407914764,0.47183098591549294,0.44656007751937987
frozenset({'lunch_standard'}),"frozenset({'parental level of education_high school', 'writing_cat_μέτριο', 'test preparation course_none'})",0.645,0.064,0.052,0.08062015503875969,1.2596899224806202,1.0,0.010719999999999993,1.0180775716694772,0.5807150595882987,0.0791476407914764,0.01775657589611078,0.44656007751937987
frozenset({'gender_male'}),"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο', 'race/ethnicity_group D'})",0.482,0.089,0.054,0.11203319502074689,1.258799944053336,1.0,0.011102,1.0259392523364488,0.39689689689689694,0.1044487427466151,0.02528341934220296,0.3593873840272274
"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο', 'race/ethnicity_group D'})",frozenset({'gender_male'}),0.089,0.482,0.054,0.6067415730337079,1.258799944053336,1.0,0.011102,1.3172000000000001,0.22567792820262633,0.1044487427466151,0.24081384755542068,0.3593873840272274
frozenset({'test preparation course_none'}),"frozenset({'writing_cat_χαμηλό', 'gender_male', 'lunch_standard'})",0.642,0.109,0.088,0.13707165109034267,1.257538083397639,1.0,0.018021999999999996,1.0325306859205776,0.5720543423057389,0.13273001508295623,0.03150578124617583,0.4722055503158135
"frozenset({'writing_cat_χαμηλό', 'gender_male', 'lunch_standard'})",frozenset({'test preparation course_none'}),0.109,0.642,0.088,0.8073394495412843,1.257538083397639,1.0,0.018021999999999996,1.8581904761904755,0.22984899500051012,0.13273001508295623,0.4618420378248166,0.4722055503158135
frozenset({'test preparation course_none'}),frozenset({'writing_cat_χαμηλό'}),0.642,0.301,0.243,0.37850467289719625,1.2574906076318813,1.0,0.049758,1.1247067669172932,0.5719704807228085,0.3471428571428571,0.11087936036794886,0.592906821498432
frozenset({'writing_cat_χαμηλό'}),frozenset({'test preparation course_none'}),0.301,0.642,0.243,0.8073089700996677,1.2574906076318813,1.0,0.049758,1.8578965517241375,0.2929405323301365,0.3471428571428571,0.46175689971974226,0.592906821498432
"frozenset({'writing_cat_χαμηλό', 'parental level of education_high school'})",frozenset({'test preparation course_none'}),0.083,0.642,0.067,0.8072289156626506,1.2573659122471195,1.0,0.013713999999999997,1.8571250000000004,0.22321326844512435,0.10182370820668694,0.4615332839738845,0.45579514318958075
frozenset({'test preparation course_none'}),"frozenset({'writing_cat_χαμηλό', 'parental level of education_high school'})",0.642,0.083,0.067,0.1043613707165109,1.2573659122471192,1.0,0.013713999999999997,1.0238504347826087,0.5717501876094387,0.10182370820668694,0.02329484265704569,0.45579514318958075
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})","frozenset({'test preparation course_completed', 'gender_male'})",0.32,0.174,0.07,0.21875000000000003,1.2571839080459772,1.0,0.014320000000000006,1.05728,0.3008403361344539,0.16509433962264153,0.054176755447941934,0.31052442528735635
"frozenset({'test preparation course_completed', 'gender_male'})","frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})",0.174,0.32,0.07,0.4022988505747127,1.2571839080459772,1.0,0.014320000000000006,1.1376923076923078,0.24766516776202013,0.16509433962264153,0.12102772143340104,0.31052442528735635
"frozenset({'math_cat_μέτριο', 'test preparation course_none', 'gender_male'})",frozenset({'writing_cat_μέτριο'}),0.154,0.491,0.095,0.6168831168831169,1.2563810934483033,1.0,0.019386,1.3285762711864408,0.2412094064949608,0.1727272727272727,0.24731457147960104,0.4051829026370778
frozenset({'writing_cat_μέτριο'}),"frozenset({'math_cat_μέτριο', 'test preparation course_none', 'gender_male'})",0.491,0.154,0.095,0.19348268839103872,1.2563810934483033,1.0,0.019386,1.0489545454545455,0.4009099369248268,0.1727272727272727,0.046669844433851915,0.4051829026370778
frozenset({'reading_cat_μέτριο'}),"frozenset({'lunch_free/reduced', 'math_cat_μέτριο', 'test preparation course_none'})",0.49,0.091,0.056,0.1142857142857143,1.2558869701726847,1.0,0.011410000000000003,1.0262903225806452,0.3995098039215687,0.10666666666666669,0.02561684739902563,0.36483516483516487
"frozenset({'lunch_free/reduced', 'math_cat_μέτριο', 'test preparation course_none'})",frozenset({'reading_cat_μέτριο'}),0.091,0.49,0.056,0.6153846153846154,1.2558869701726845,1.0,0.011410000000000003,1.326,0.2241474147414742,0.10666666666666669,0.2458521870286577,0.36483516483516487
"frozenset({'gender_female', 'reading_cat_χαμηλό'})",frozenset({'test preparation course_none'}),0.098,0.642,0.079,0.8061224489795918,1.2556424438934453,1.0,0.016084,1.8465263157894736,0.22571500743776138,0.11951588502269289,0.458442594915061,0.4645877042405747
frozenset({'test preparation course_none'}),"frozenset({'gender_female', 'reading_cat_χαμηλό'})",0.642,0.098,0.079,0.12305295950155763,1.2556424438934453,1.0,0.016084,1.0285683836589699,0.568700940527544,0.11951588502269289,0.027774899669132627,0.4645877042405747
frozenset({'lunch_standard'}),"frozenset({'writing_cat_υψηλό', 'gender_female', 'test preparation course_none'})",0.645,0.073,0.059,0.09147286821705426,1.253052989274716,1.0,0.011915000000000002,1.0203327645051194,0.5688708522320364,0.08952959028831563,0.01992758167967019,0.449846023149623
"frozenset({'writing_cat_υψηλό', 'gender_female', 'test preparation course_none'})",frozenset({'lunch_standard'}),0.073,0.645,0.059,0.8082191780821918,1.253052989274716,1.0,0.011915000000000002,1.8510714285714285,0.2178523759896148,0.08952959028831563,0.45977233262589234,0.449846023149623
"frozenset({'writing_cat_χαμηλό', 'gender_male', 'reading_cat_χαμηλό', 'lunch_standard'})",frozenset({'test preparation course_none'}),0.092,0.642,0.074,0.8043478260869565,1.2528782337803062,1.0,0.014935999999999998,1.829777777777778,0.22228836766281695,0.11212121212121211,0.45348554772892885,0.45980631179737236
frozenset({'test preparation course_none'}),"frozenset({'writing_cat_χαμηλό', 'gender_male', 'reading_cat_χαμηλό', 'lunch_standard'})",0.642,0.092,0.074,0.11526479750778815,1.252878233780306,1.0,0.014935999999999998,1.0262957746478873,0.563792843122452,0.11212121212121211,0.025622023686991353,0.45980631179737236
frozenset({'math_cat_μέτριο'}),"frozenset({'lunch_free/reduced', 'reading_cat_μέτριο', 'writing_cat_μέτριο'})",0.485,0.135,0.082,0.16907216494845362,1.2523864070255823,1.0,0.016524999999999998,1.0410049627791564,0.39130949561922795,0.1524163568773234,0.03938978606757643,0.38823978617793053
"frozenset({'lunch_free/reduced', 'reading_cat_μέτριο', 'writing_cat_μέτριο'})",frozenset({'math_cat_μέτριο'}),0.135,0.485,0.082,0.6074074074074074,1.2523864070255823,1.0,0.016524999999999998,1.3117924528301887,0.23297617369237272,0.1524163568773234,0.23768428622797555,0.38823978617793053
frozenset({'test preparation course_none'}),"frozenset({'reading_cat_χαμηλό', 'race/ethnicity_group D'})",0.642,0.066,0.053,0.08255451713395638,1.2508260171811572,1.0,0.010627999999999999,1.018044142614601,0.5601349214714872,0.08091603053435115,0.017724322413229528,0.44279241008212966
"frozenset({'reading_cat_χαμηλό', 'race/ethnicity_group D'})",frozenset({'test preparation course_none'}),0.066,0.642,0.053,0.803030303030303,1.2508260171811572,1.0,0.010627999999999999,1.817538461538461,0.21469839602440305,0.08091603053435115,0.4498053157271033,0.44279241008212966
frozenset({'gender_male'}),"frozenset({'math_cat_υψηλό', 'lunch_standard'})",0.482,0.156,0.094,0.1950207468879668,1.250132992871582,1.0,0.018808000000000005,1.0484742268041236,0.3862646841370247,0.17279411764705882,0.04623311242650096,0.39879242472603466
"frozenset({'math_cat_υψηλό', 'lunch_standard'})",frozenset({'gender_male'}),0.156,0.482,0.094,0.6025641025641025,1.250132992871582,1.0,0.018808000000000005,1.3033548387096774,0.23706766159120707,0.17279411764705882,0.2327492327492327,0.39879242472603466
"frozenset({'gender_female', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})",0.329,0.18,0.074,0.2249240121580547,1.2495778453225261,1.0,0.014779999999999995,1.0579607843137255,0.297659805856527,0.17011494252873563,0.0547853806805545,0.3180175616345829
"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})","frozenset({'gender_female', 'lunch_standard'})",0.18,0.329,0.074,0.4111111111111111,1.2495778453225261,1.0,0.014779999999999995,1.139433962264151,0.24357284113381664,0.17011494252873563,0.12237125351879445,0.3180175616345829
frozenset({'reading_cat_μέτριο'}),"frozenset({'math_cat_μέτριο', 'gender_female'})",0.49,0.245,0.15,0.30612244897959184,1.2494793835901707,1.0,0.029950000000000004,1.0880882352941177,0.39150326797385626,0.25641025641025644,0.08095688606568456,0.45918367346938777
"frozenset({'math_cat_μέτριο', 'gender_female'})",frozenset({'reading_cat_μέτριο'}),0.245,0.49,0.15,0.6122448979591837,1.2494793835901707,1.0,0.029950000000000004,1.315263157894737,0.2644591611479029,0.25641025641025644,0.23969587835134057,0.45918367346938777
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})","frozenset({'test preparation course_completed', 'math_cat_μέτριο'})",0.283,0.181,0.064,0.22614840989399296,1.2494387286960937,1.0,0.01277700000000001,1.0583424657534246,0.27843880753138095,0.16,0.05512626360682902,0.2898697850574937
"frozenset({'test preparation course_completed', 'math_cat_μέτριο'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})",0.181,0.283,0.064,0.3535911602209945,1.2494387286960937,1.0,0.01277700000000001,1.1092051282051283,0.2437614468864471,0.16,0.09845350100557118,0.2898697850574937
"frozenset({'reading_cat_χαμηλό', 'lunch_standard'})",frozenset({'test preparation course_none'}),0.136,0.642,0.109,0.801470588235294,1.2483965548836355,1.0,0.021687999999999985,1.8032592592592585,0.23029221882432876,0.16292974588938713,0.44544857048964814,0.48562625984973423
frozenset({'test preparation course_none'}),"frozenset({'reading_cat_χαμηλό', 'lunch_standard'})",0.642,0.136,0.109,0.16978193146417445,1.2483965548836355,1.0,0.021687999999999985,1.0406904315196999,0.555789042078827,0.16292974588938713,0.03909945771316486,0.48562625984973423
"frozenset({'race/ethnicity_group C', 'test preparation course_none'})","frozenset({'writing_cat_χαμηλό', 'math_cat_χαμηλό'})",0.202,0.23,0.058,0.2871287128712871,1.2483857081360308,1.0,0.011539999999999995,1.080138888888889,0.24933022210699146,0.1550802139037433,0.07419313359907415,0.26965131295738265
"frozenset({'writing_cat_χαμηλό', 'math_cat_χαμηλό'})","frozenset({'race/ethnicity_group C', 'test preparation course_none'})",0.23,0.202,0.058,0.25217391304347825,1.2483857081360308,1.0,0.011539999999999995,1.067093023255814,0.2583967756381548,0.1550802139037433,0.06287457774871959,0.26965131295738265
"frozenset({'math_cat_χαμηλό', 'gender_female', 'lunch_standard'})",frozenset({'reading_cat_μέτριο'}),0.095,0.49,0.058,0.6105263157894737,1.2459720730397423,1.0,0.011450000000000002,1.3094594594594595,0.2181367879596114,0.11005692599620495,0.23632610939112492,0.3644468313641246
frozenset({'reading_cat_μέτριο'}),"frozenset({'math_cat_χαμηλό', 'gender_female', 'lunch_standard'})",0.49,0.095,0.058,0.11836734693877551,1.2459720730397423,1.0,0.011450000000000002,1.0265046296296296,0.3870858688302908,0.11005692599620495,0.025820272860525427,0.3644468313641246
"frozenset({'gender_female', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'test preparation course_none'})",0.329,0.205,0.084,0.2553191489361702,1.2454592631032693,1.0,0.016555,1.0675714285714286,0.2937158469945355,0.18666666666666668,0.06329452696373611,0.33253762324857294
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'test preparation course_none'})","frozenset({'gender_female', 'lunch_standard'})",0.205,0.329,0.084,0.40975609756097564,1.2454592631032693,1.0,0.016555,1.136818181818182,0.24790356394129978,0.18666666666666668,0.1203518592562975,0.33253762324857294
"frozenset({'writing_cat_υψηλό', 'gender_female', 'test preparation course_none', 'reading_cat_υψηλό'})",frozenset({'lunch_standard'}),0.066,0.645,0.053,0.803030303030303,1.2450082217524077,1.0,0.010429999999999995,1.8023076923076917,0.2106985576340349,0.08054711246200608,0.44515578318395205,0.44260042283298096
frozenset({'lunch_standard'}),"frozenset({'writing_cat_υψηλό', 'gender_female', 'test preparation course_none', 'reading_cat_υψηλό'})",0.645,0.066,0.053,0.08217054263565891,1.2450082217524077,1.0,0.010429999999999995,1.0176182432432432,0.554344937549827,0.08054711246200608,0.017313214813339306,0.44260042283298096
"frozenset({'test preparation course_none', 'lunch_standard'})","frozenset({'parental level of education_high school', 'reading_cat_μέτριο'})",0.418,0.098,0.051,0.12200956937799043,1.2449956058978615,1.0,0.010035999999999996,1.0273460490463215,0.3381173775352064,0.1096774193548387,0.026618147869168985,0.32120886632164825
"frozenset({'parental level of education_high school', 'reading_cat_μέτριο'})","frozenset({'test preparation course_none', 'lunch_standard'})",0.098,0.418,0.051,0.520408163265306,1.2449956058978613,1.0,0.010035999999999996,1.213531914893617,0.21816442763358107,0.1096774193548387,0.17595904341117877,0.32120886632164825
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'race/ethnicity_group C', 'test preparation course_none', 'lunch_standard'})",0.325,0.131,0.053,0.16307692307692306,1.2448620082207866,1.0,0.010424999999999997,1.038327205882353,0.29140461215932906,0.1315136476426799,0.0369124546339736,0.28382853787433937
"frozenset({'race/ethnicity_group C', 'test preparation course_none', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο'})",0.131,0.325,0.053,0.4045801526717557,1.2448620082207866,1.0,0.010424999999999997,1.1336538461538461,0.22634995766115892,0.1315136476426799,0.11789652247667505,0.28382853787433937
frozenset({'gender_male'}),"frozenset({'math_cat_χαμηλό', 'test preparation course_none', 'writing_cat_χαμηλό', 'lunch_standard'})",0.482,0.085,0.051,0.10580912863070539,1.2448132780082986,1.0,0.010029999999999997,1.0232714617169374,0.37966537966537955,0.09883720930232559,0.022742217082738127,0.3529045643153526
"frozenset({'math_cat_χαμηλό', 'test preparation course_none', 'writing_cat_χαμηλό', 'lunch_standard'})",frozenset({'gender_male'}),0.085,0.482,0.051,0.5999999999999999,1.2448132780082986,1.0,0.010029999999999997,1.2949999999999997,0.2149362477231329,0.09883720930232559,0.22779922779922757,0.3529045643153526
"frozenset({'writing_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'gender_male'})",0.221,0.24,0.066,0.2986425339366516,1.244343891402715,1.0,0.012960000000000006,1.0836129032258066,0.2520714202357336,0.16708860759493674,0.07716122886401525,0.28682126696832577
"frozenset({'math_cat_μέτριο', 'gender_male'})","frozenset({'writing_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",0.24,0.221,0.066,0.275,1.244343891402715,1.0,0.012960000000000006,1.0744827586206898,0.2583732057416269,0.16708860759493674,0.06931964056482673,0.28682126696832577
"frozenset({'race/ethnicity_group D', 'lunch_standard'})","frozenset({'gender_male', 'test preparation course_none'})",0.167,0.308,0.064,0.3832335329341317,1.2442647173186094,1.0,0.012563999999999999,1.1219805825242717,0.23566926770708282,0.15571776155717762,0.10871897822851403,0.29551287036316976
"frozenset({'gender_male', 'test preparation course_none'})","frozenset({'race/ethnicity_group D', 'lunch_standard'})",0.308,0.167,0.064,0.2077922077922078,1.2442647173186094,1.0,0.012563999999999999,1.0514918032786886,0.28368858381502887,0.15571776155717762,0.04897023744562761,0.29551287036316976
frozenset({'writing_cat_χαμηλό'}),frozenset({'parental level of education_some high school'}),0.301,0.179,0.067,0.22259136212624586,1.2435271627164575,1.0,0.013121000000000008,1.0560726495726496,0.28016569512950285,0.16222760290556903,0.0530954471696052,0.29844651905194974
frozenset({'parental level of education_some high school'}),frozenset({'writing_cat_χαμηλό'}),0.179,0.301,0.067,0.3743016759776537,1.2435271627164575,1.0,0.013121000000000008,1.1171517857142859,0.23853327758285323,0.16222760290556903,0.1048664892384173,0.29844651905194974
"frozenset({'race/ethnicity_group C', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})",0.205,0.208,0.053,0.25853658536585367,1.2429643527204504,1.0,0.010360000000000001,1.0681578947368422,0.24587634982793405,0.14722222222222223,0.0638088199063809,0.256672138836773
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})","frozenset({'race/ethnicity_group C', 'lunch_standard'})",0.208,0.205,0.053,0.2548076923076923,1.2429643527204504,1.0,0.010360000000000001,1.0668387096774195,0.24680769963788834,0.14722222222222223,0.06265118529269471,0.256672138836773
frozenset({'test preparation course_none'}),"frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό'})",0.642,0.252,0.201,0.31308411214953275,1.2423972704346538,1.0,0.039216,1.0889251700680271,0.5449845743350287,0.29004329004329005,0.08166325153680848,0.5553515798842902
"frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό'})",frozenset({'test preparation course_none'}),0.252,0.642,0.201,0.7976190476190477,1.2423972704346538,1.0,0.039216,1.7689411764705887,0.2608348631175672,0.29004329004329005,0.4346900771481778,0.5553515798842902
frozenset({'parental level of education_high school'}),"frozenset({'gender_male', 'test preparation course_none'})",0.196,0.308,0.075,0.3826530612244898,1.2423800689106812,1.0,0.014631999999999992,1.1209256198347106,0.24265339966832492,0.17482517482517482,0.10788014627816443,0.31307977736549164
"frozenset({'gender_male', 'test preparation course_none'})",frozenset({'parental level of education_high school'}),0.308,0.196,0.075,0.2435064935064935,1.2423800689106812,1.0,0.014631999999999992,1.0627982832618026,0.28192678227360296,0.17482517482517482,0.05908767849066355,0.31307977736549164
"frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό'})",frozenset({'parental level of education_some high school'}),0.252,0.179,0.056,0.22222222222222224,1.2414649286157668,1.0,0.010892000000000006,1.0555714285714284,0.2600267379679146,0.14933333333333335,0.052645824874813944,0.2675356921166977
"frozenset({'lunch_free/reduced', 'writing_cat_μέτριο', 'reading_cat_μέτριο'})",frozenset({'test preparation course_completed'}),0.135,0.358,0.06,0.4444444444444444,1.2414649286157666,1.0,0.01167,1.1556,0.2248554913294798,0.13856812933025403,0.1346486673589477,0.3060211049037864
frozenset({'parental level of education_some high school'}),"frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό'})",0.179,0.252,0.056,0.3128491620111732,1.2414649286157666,1.0,0.010892000000000006,1.0885528455284552,0.23690621193666275,0.14933333333333335,0.08134914707376094,0.2675356921166977
frozenset({'test preparation course_completed'}),"frozenset({'lunch_free/reduced', 'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.358,0.135,0.06,0.16759776536312848,1.2414649286157664,1.0,0.01167,1.0391610738255033,0.3029595015576324,0.13856812933025403,0.037685277876449104,0.3060211049037864
frozenset({'lunch_free/reduced'}),"frozenset({'gender_male', 'reading_cat_χαμηλό'})",0.355,0.177,0.078,0.21971830985915494,1.2413463833850562,1.0,0.015165000000000012,1.0547472924187726,0.3014311270125226,0.17180616740088106,0.05190560128694405,0.33019813798042497
"frozenset({'gender_male', 'reading_cat_χαμηλό'})",frozenset({'lunch_free/reduced'}),0.177,0.355,0.078,0.44067796610169496,1.2413463833850562,1.0,0.015165000000000012,1.1531818181818183,0.23623703149827105,0.17180616740088106,0.13283405597162012,0.33019813798042497
frozenset({'gender_male'}),"frozenset({'math_cat_υψηλό', 'test preparation course_none'})",0.482,0.092,0.055,0.1141078838174274,1.240303084972037,1.0,0.010656000000000006,1.0249555035128806,0.3740259740259742,0.10597302504816958,0.02434788966677026,0.3559669853869746
"frozenset({'math_cat_υψηλό', 'test preparation course_none'})",frozenset({'gender_male'}),0.092,0.482,0.055,0.5978260869565217,1.240303084972037,1.0,0.010656000000000006,1.288,0.2133760512615139,0.10597302504816958,0.2236024844720497,0.3559669853869746
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})","frozenset({'parental level of education_high school', 'lunch_standard'})",0.32,0.126,0.05,0.15625,1.2400793650793651,1.0,0.009680000000000001,1.0358518518518518,0.28470588235294125,0.12626262626262627,0.03461098398169336,0.27653769841269843
"frozenset({'parental level of education_high school', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})",0.126,0.32,0.05,0.39682539682539686,1.2400793650793651,1.0,0.009680000000000001,1.1273684210526316,0.22151029748283754,0.12626262626262627,0.11297852474323068,0.27653769841269843
"frozenset({'writing_cat_χαμηλό', 'math_cat_χαμηλό'})",frozenset({'test preparation course_none'}),0.23,0.642,0.183,0.7956521739130434,1.2393336042259244,1.0,0.03533999999999998,1.7519148936170208,0.2507983819459228,0.26560232220609575,0.4291960165168811,0.540349451442503
frozenset({'test preparation course_none'}),"frozenset({'writing_cat_χαμηλό', 'math_cat_χαμηλό'})",0.642,0.23,0.183,0.2850467289719626,1.2393336042259242,1.0,0.03533999999999998,1.0769934640522876,0.5394266874255883,0.26560232220609575,0.07148925840514621,0.540349451442503
"frozenset({'math_cat_χαμηλό', 'reading_cat_χαμηλό', 'writing_cat_χαμηλό'})",frozenset({'gender_male'}),0.206,0.482,0.123,0.5970873786407768,1.238770495105346,1.0,0.023708000000000007,1.2856385542168678,0.24275562654870889,0.2176991150442478,0.22217640664242622,0.426137050316239
frozenset({'gender_male'}),"frozenset({'math_cat_χαμηλό', 'reading_cat_χαμηλό', 'writing_cat_χαμηλό'})",0.482,0.206,0.123,0.2551867219917012,1.2387704951053458,1.0,0.023708000000000007,1.0660389972144848,0.37210032331983556,0.2176991150442478,0.0619480125840066,0.426137050316239
frozenset({'parental level of education_some high school'}),"frozenset({'writing_cat_χαμηλό', 'math_cat_χαμηλό'})",0.179,0.23,0.051,0.2849162011173184,1.2387660918144279,1.0,0.009829999999999998,1.076796875,0.23476869432303982,0.1424581005586592,0.07131974171080312,0.2533276657760505
"frozenset({'writing_cat_χαμηλό', 'math_cat_χαμηλό'})",frozenset({'parental level of education_some high school'}),0.23,0.179,0.051,0.22173913043478258,1.2387660918144279,1.0,0.009829999999999998,1.0549162011173183,0.2503183091418385,0.1424581005586592,0.05205740613250011,0.2533276657760505
frozenset({'lunch_free/reduced'}),"frozenset({'writing_cat_χαμηλό', 'gender_male', 'test preparation course_none'})",0.355,0.157,0.069,0.1943661971830986,1.2380012559433033,1.0,0.013265000000000006,1.0463811188811187,0.2980563981575105,0.15575620767494358,0.04432526356239456,0.31692832152148565
"frozenset({'writing_cat_χαμηλό', 'gender_male', 'test preparation course_none'})",frozenset({'lunch_free/reduced'}),0.157,0.355,0.069,0.4394904458598726,1.2380012559433031,1.0,0.013265000000000006,1.1507386363636365,0.2280502690528995,0.15575620767494358,0.130992939317632,0.31692832152148565
frozenset({'gender_male'}),frozenset({'race/ethnicity_group A'}),0.482,0.089,0.053,0.10995850622406639,1.2354888339782741,1.0,0.010102,1.0235477855477857,0.36796095286661323,0.10231660231660233,0.023006044153750155,0.3527320621007972
frozenset({'race/ethnicity_group A'}),frozenset({'gender_male'}),0.089,0.482,0.053,0.5955056179775281,1.2354888339782741,1.0,0.010102,1.2806111111111111,0.2092247789076901,0.10231660231660233,0.21912281462843264,0.3527320621007972
"frozenset({'math_cat_μέτριο', 'lunch_standard'})","frozenset({'test preparation course_completed', 'writing_cat_μέτριο'})",0.332,0.183,0.075,0.2259036144578313,1.234445980643887,1.0,0.014243999999999993,1.0554241245136184,0.28431137724550887,0.17045454545454544,0.05251360398755362,0.3178698400158009
"frozenset({'test preparation course_completed', 'writing_cat_μέτριο'})","frozenset({'math_cat_μέτριο', 'lunch_standard'})",0.183,0.332,0.075,0.4098360655737705,1.234445980643887,1.0,0.014243999999999993,1.1318888888888887,0.23246022031823735,0.17045454545454544,0.11652105624815942,0.3178698400158009
"frozenset({'writing_cat_μέτριο', 'gender_male', 'lunch_standard'})",frozenset({'race/ethnicity_group D'}),0.161,0.262,0.052,0.32298136645962733,1.2327533070978143,1.0,0.009817999999999993,1.0900733944954126,0.22503896580177857,0.14016172506738542,0.0826305778585736,0.2607273244511877
frozenset({'race/ethnicity_group D'}),"frozenset({'writing_cat_μέτριο', 'gender_male', 'lunch_standard'})",0.262,0.161,0.052,0.19847328244274806,1.232753307097814,1.0,0.009817999999999993,1.0467523809523809,0.2558369814467374,0.14016172506738542,0.044664222220200306,0.2607273244511877
frozenset({'math_cat_χαμηλό'}),"frozenset({'reading_cat_μέτριο', 'gender_female'})",0.339,0.261,0.109,0.3215339233038348,1.2319307406277193,1.0,0.020520999999999984,1.0892217391304346,0.2848200530194726,0.22199592668024434,0.0819132926980173,0.36957922218831585
"frozenset({'reading_cat_μέτριο', 'gender_female'})",frozenset({'math_cat_χαμηλό'}),0.261,0.339,0.109,0.4176245210727969,1.2319307406277193,1.0,0.020520999999999984,1.1350065789473684,0.25475785527181516,0.22199592668024434,0.1189478382341859,0.36957922218831585
frozenset({'reading_cat_μέτριο'}),"frozenset({'race/ethnicity_group C', 'gender_female', 'test preparation course_none'})",0.49,0.111,0.067,0.13673469387755102,1.2318440889869462,1.0,0.012610000000000003,1.0298108747044916,0.36903716710564827,0.1254681647940075,0.028947912123229495,0.3701691487405773
"frozenset({'race/ethnicity_group C', 'gender_female', 'test preparation course_none'})",frozenset({'reading_cat_μέτριο'}),0.111,0.49,0.067,0.6036036036036037,1.2318440889869462,1.0,0.012610000000000003,1.2865909090909093,0.21170861105048439,0.1254681647940075,0.22275216392863464,0.3701691487405773
"frozenset({'lunch_free/reduced', 'math_cat_μέτριο', 'test preparation course_none'})",frozenset({'writing_cat_μέτριο'}),0.091,0.491,0.055,0.6043956043956045,1.230948277791455,1.0,0.010319000000000002,1.286638888888889,0.2064006400640064,0.1043643263757116,0.22278114812495967,0.35820594883731344
frozenset({'writing_cat_μέτριο'}),"frozenset({'lunch_free/reduced', 'math_cat_μέτριο', 'test preparation course_none'})",0.491,0.091,0.055,0.1120162932790224,1.230948277791455,1.0,0.010319000000000002,1.0236674311926606,0.3686015359885695,0.1043643263757116,0.023120234630387686,0.35820594883731344
"frozenset({'race/ethnicity_group C', 'test preparation course_none', 'lunch_standard'})",frozenset({'reading_cat_μέτριο'}),0.131,0.49,0.079,0.6030534351145038,1.2307212961520486,1.0,0.014810000000000004,1.2848076923076923,0.2157288313353047,0.14575645756457564,0.22167340218530163,0.3821389624552111
frozenset({'reading_cat_μέτριο'}),"frozenset({'race/ethnicity_group C', 'test preparation course_none', 'lunch_standard'})",0.49,0.131,0.079,0.16122448979591839,1.2307212961520486,1.0,0.014810000000000004,1.0360340632603406,0.3675850086870192,0.14575645756457564,0.03478077076630424,0.3821389624552111
"frozenset({'race/ethnicity_group C', 'lunch_standard'})","frozenset({""parental level of education_associate's degree""})",0.205,0.222,0.056,0.2731707317073171,1.2304987914744012,1.0,0.010490000000000006,1.0704026845637584,0.23562443845462724,0.1509433962264151,0.06577214872405795,0.26271149197978466
"frozenset({""parental level of education_associate's degree""})","frozenset({'race/ethnicity_group C', 'lunch_standard'})",0.222,0.205,0.056,0.25225225225225223,1.2304987914744012,1.0,0.010490000000000006,1.0631927710843374,0.24077304443628364,0.1509433962264151,0.05943679528585187,0.26271149197978466
frozenset({'race/ethnicity_group D'}),"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})",0.262,0.18,0.058,0.22137404580152673,1.2298558100084818,1.0,0.010840000000000002,1.0531372549019609,0.2532473600598075,0.15104166666666666,0.05045615341649601,0.27179813401187447
"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})",frozenset({'race/ethnicity_group D'}),0.18,0.262,0.058,0.32222222222222224,1.2298558100084818,1.0,0.010840000000000002,1.0888524590163933,0.22792262405382677,0.15104166666666666,0.08160192713038243,0.27179813401187447
"frozenset({'math_cat_χαμηλό', 'race/ethnicity_group D'})",frozenset({'test preparation course_none'}),0.076,0.642,0.06,0.7894736842105263,1.2297097884899164,1.0,0.011207999999999996,1.7005000000000001,0.20216450216450207,0.09118541033434652,0.4119376653925316,0.44146581406788
frozenset({'test preparation course_none'}),"frozenset({'math_cat_χαμηλό', 'race/ethnicity_group D'})",0.642,0.076,0.06,0.09345794392523364,1.2297097884899164,1.0,0.011207999999999996,1.019257731958763,0.5217877094972065,0.09118541033434652,0.018893878706962818,0.44146581406788
frozenset({'math_cat_χαμηλό'}),"frozenset({'race/ethnicity_group C', 'gender_female'})",0.339,0.18,0.075,0.22123893805309733,1.2291052114060963,1.0,0.013979999999999992,1.0529545454545455,0.2819969742813917,0.16891891891891891,0.050291387869630895,0.318952802359882
"frozenset({'race/ethnicity_group C', 'gender_female'})",frozenset({'math_cat_χαμηλό'}),0.18,0.339,0.075,0.4166666666666667,1.2291052114060963,1.0,0.013979999999999992,1.1331428571428575,0.22731707317073158,0.16891891891891891,0.11749873928391324,0.318952802359882
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none', 'gender_female'})",frozenset({'lunch_standard'}),0.101,0.645,0.08,0.792079207920792,1.2280297797221582,1.0,0.014854999999999993,1.7073809523809518,0.20654894327030024,0.12012012012012012,0.41430762794589293,0.458055107836365
frozenset({'lunch_standard'}),"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none', 'gender_female'})",0.645,0.101,0.08,0.12403100775193798,1.2280297797221582,1.0,0.014854999999999993,1.02629203539823,0.52306338028169,0.12012012012012012,0.02561847358391321,0.458055107836365
"frozenset({'math_cat_χαμηλό', 'parental level of education_high school', 'writing_cat_χαμηλό'})",frozenset({'test preparation course_none'}),0.066,0.642,0.052,0.7878787878787878,1.2272255262909468,1.0,0.009627999999999998,1.6877142857142853,0.19823752264865754,0.07926829268292683,0.40748264770611126,0.4344378363069952
frozenset({'test preparation course_none'}),"frozenset({'gender_male', 'reading_cat_χαμηλό', 'lunch_standard'})",0.642,0.099,0.078,0.12149532710280374,1.2272255262909468,1.0,0.014441999999999997,1.0256063829787234,0.5171895143962182,0.11764705882352941,0.0249670667067744,0.4546870574907958
"frozenset({'gender_male', 'reading_cat_χαμηλό', 'lunch_standard'})",frozenset({'test preparation course_none'}),0.099,0.642,0.078,0.7878787878787878,1.2272255262909468,1.0,0.014441999999999997,1.6877142857142853,0.20549816443268157,0.11764705882352941,0.40748264770611126,0.4546870574907958
frozenset({'test preparation course_none'}),"frozenset({'math_cat_χαμηλό', 'parental level of education_high school', 'writing_cat_χαμηλό'})",0.642,0.066,0.052,0.08099688473520249,1.2272255262909466,1.0,0.009627999999999998,1.0163186440677965,0.5171895143962183,0.07926829268292683,0.0160566217721654,0.4344378363069952
frozenset({'gender_female'}),"frozenset({'race/ethnicity_group C', 'math_cat_χαμηλό'})",0.518,0.118,0.075,0.14478764478764478,1.2270139388783456,1.0,0.013876,1.0313227990970655,0.3838450899031812,0.1336898395721925,0.030371479351071182,0.39019043256331387
"frozenset({'race/ethnicity_group C', 'math_cat_χαμηλό'})",frozenset({'gender_female'}),0.118,0.518,0.075,0.635593220338983,1.2270139388783456,1.0,0.013876,1.3226976744186045,0.20976568405139834,0.1336898395721925,0.2439693368028693,0.39019043256331387
frozenset({'math_cat_μέτριο'}),"frozenset({'lunch_free/reduced', 'reading_cat_μέτριο'})",0.485,0.163,0.097,0.2,1.2269938650306749,1.0,0.017945000000000003,1.04625,0.35922330097087385,0.17604355716878403,0.044205495818399054,0.3975460122699387
"frozenset({'lunch_free/reduced', 'reading_cat_μέτριο'})",frozenset({'math_cat_μέτριο'}),0.163,0.485,0.097,0.5950920245398773,1.2269938650306749,1.0,0.017945000000000003,1.2718939393939395,0.22102747909199527,0.17604355716878403,0.213770921436655,0.3975460122699387
"frozenset({'writing_cat_μέτριο', 'lunch_standard'})","frozenset({'test preparation course_completed', 'math_cat_μέτριο'})",0.338,0.181,0.075,0.22189349112426032,1.2259308901892836,1.0,0.013821999999999994,1.0525551330798477,0.27838872104733126,0.16891891891891891,0.04993100259372446,0.3181290660041191
"frozenset({'test preparation course_completed', 'math_cat_μέτριο'})","frozenset({'writing_cat_μέτριο', 'lunch_standard'})",0.181,0.338,0.075,0.4143646408839779,1.2259308901892836,1.0,0.013821999999999994,1.1303962264150942,0.22502238502238495,0.16891891891891891,0.11535444242292732,0.3181290660041191
frozenset({'test preparation course_none'}),"frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό', 'math_cat_χαμηλό'})",0.642,0.206,0.162,0.25233644859813087,1.224934216495781,1.0,0.029747999999999997,1.0619750000000001,0.5129319263397475,0.2361516034985423,0.05835824760469884,0.5193721077942111
"frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό', 'math_cat_χαμηλό'})",frozenset({'test preparation course_none'}),0.206,0.642,0.162,0.7864077669902914,1.2249342164957808,1.0,0.029747999999999997,1.6760909090909097,0.2312715738408433,0.2361516034985423,0.4033736508108697,0.5193721077942111
"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'gender_female', 'test preparation course_none', 'lunch_standard'})",0.281,0.215,0.074,0.26334519572953735,1.2248613754862203,1.0,0.013584999999999993,1.0656280193236716,0.25532834642709457,0.17535545023696683,0.06158623659813676,0.3037656211205826
"frozenset({'gender_female', 'test preparation course_none', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.215,0.281,0.074,0.3441860465116279,1.22486137548622,1.0,0.013584999999999993,1.0963475177304964,0.23386124978481654,0.17535545023696683,0.08788045411909298,0.3037656211205826
frozenset({'test preparation course_none'}),"frozenset({'race/ethnicity_group C', 'writing_cat_χαμηλό'})",0.642,0.098,0.077,0.11993769470404984,1.2238540275923453,1.0,0.014084,1.0249274336283185,0.5109192483494159,0.11613876319758672,0.024321169294955477,0.45282599020916775
"frozenset({'race/ethnicity_group C', 'writing_cat_χαμηλό'})",frozenset({'test preparation course_none'}),0.098,0.642,0.077,0.7857142857142857,1.2238540275923453,1.0,0.014084,1.6706666666666665,0.20278169723845998,0.11613876319758672,0.40143655227454106,0.45282599020916775
frozenset({'gender_male'}),"frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced'})",0.482,0.156,0.092,0.19087136929460582,1.2235344185551655,1.0,0.016808000000000003,1.0430974358974359,0.3526943092160484,0.16849816849816848,0.04131678826375098,0.3903074795190978
"frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced'})",frozenset({'gender_male'}),0.156,0.482,0.092,0.5897435897435898,1.2235344185551655,1.0,0.016808000000000003,1.262625,0.2164640428600866,0.16849816849816848,0.20799920799920804,0.3903074795190978
frozenset({'reading_cat_υψηλό'}),"frozenset({'test preparation course_completed', 'math_cat_μέτριο'})",0.235,0.181,0.052,0.22127659574468087,1.2225226284236512,1.0,0.009465000000000001,1.0517213114754098,0.23793363499245856,0.14285714285714285,0.04917777258202793,0.25428470671211945
"frozenset({'test preparation course_completed', 'math_cat_μέτριο'})",frozenset({'reading_cat_υψηλό'}),0.181,0.235,0.052,0.287292817679558,1.2225226284236512,1.0,0.009465000000000001,1.0733720930232558,0.22224570301493385,0.14285714285714285,0.06835662441772289,0.25428470671211945
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})","frozenset({'gender_female', 'test preparation course_none', 'lunch_standard'})",0.32,0.215,0.084,0.2625,1.2209302325581397,1.0,0.015200000000000005,1.0644067796610168,0.2661064425770309,0.18625277161862527,0.0605095541401274,0.3265988372093024
"frozenset({'gender_female', 'test preparation course_none', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})",0.215,0.32,0.084,0.3906976744186047,1.2209302325581397,1.0,0.015200000000000005,1.116030534351145,0.2305125872004853,0.18625277161862527,0.10396716826265397,0.3265988372093024
frozenset({'lunch_standard'}),"frozenset({'reading_cat_μέτριο', 'writing_cat_μέτριο', 'test preparation course_none', 'gender_female', 'math_cat_μέτριο'})",0.645,0.094,0.074,0.11472868217054262,1.2205178954313045,1.0,0.013369999999999993,1.023415061295972,0.5089455652835932,0.1112781954887218,0.022879340144086775,0.45098136236186703
"frozenset({'reading_cat_μέτριο', 'writing_cat_μέτριο', 'test preparation course_none', 'gender_female', 'math_cat_μέτριο'})",frozenset({'lunch_standard'}),0.094,0.645,0.074,0.7872340425531914,1.2205178954313045,1.0,0.013369999999999993,1.6684999999999992,0.19942127558021588,0.1112781954887218,0.40065927479772223,0.45098136236186703
frozenset({'test preparation course_none'}),"frozenset({'math_cat_χαμηλό', 'parental level of education_some college'})",0.642,0.069,0.054,0.08411214953271028,1.2190166598943517,1.0,0.009701999999999995,1.0165,0.5018621973929235,0.0821917808219178,0.016232169208066884,0.43336042259244206
"frozenset({'math_cat_χαμηλό', 'parental level of education_some college'})",frozenset({'test preparation course_none'}),0.069,0.642,0.054,0.7826086956521738,1.2190166598943517,1.0,0.009701999999999995,1.6467999999999994,0.19298245614035078,0.0821917808219178,0.3927617196988096,0.43336042259244206
"frozenset({'writing_cat_χαμηλό', 'parental level of education_high school', 'reading_cat_χαμηλό'})",frozenset({'test preparation course_none'}),0.069,0.642,0.054,0.7826086956521738,1.2190166598943517,1.0,0.009701999999999995,1.6467999999999994,0.19298245614035078,0.0821917808219178,0.3927617196988096,0.43336042259244206
frozenset({'test preparation course_none'}),"frozenset({'writing_cat_χαμηλό', 'parental level of education_high school', 'reading_cat_χαμηλό'})",0.642,0.069,0.054,0.08411214953271028,1.2190166598943517,1.0,0.009701999999999995,1.0165,0.5018621973929235,0.0821917808219178,0.016232169208066884,0.43336042259244206
"frozenset({'writing_cat_χαμηλό', 'gender_male'})",frozenset({'test preparation course_none'}),0.201,0.642,0.157,0.7810945273631841,1.2166581423102556,1.0,0.027957999999999983,1.6354090909090908,0.22287413406885984,0.22886297376093295,0.38853219933850297,0.5128214069837728
frozenset({'test preparation course_none'}),"frozenset({'writing_cat_χαμηλό', 'gender_male'})",0.642,0.201,0.157,0.24454828660436137,1.2166581423102556,1.0,0.027957999999999983,1.057645360824742,0.4974202042486564,0.22886297376093295,0.05450348761497042,0.5128214069837728
"frozenset({'parental level of education_high school', 'reading_cat_χαμηλό'})",frozenset({'test preparation course_none'}),0.073,0.642,0.057,0.7808219178082193,1.2162335168352325,1.0,0.010134000000000004,1.633375000000001,0.19179015499914842,0.08662613981762919,0.38777072013469077,0.4348034822685956
frozenset({'test preparation course_none'}),"frozenset({'parental level of education_high school', 'reading_cat_χαμηλό'})",0.642,0.073,0.057,0.08878504672897196,1.2162335168352323,1.0,0.010134000000000004,1.017323076923077,0.49661864157600727,0.08662613981762919,0.017028097873756164,0.4348034822685956
"frozenset({'lunch_free/reduced', 'reading_cat_μέτριο', 'test preparation course_none'})",frozenset({'math_cat_μέτριο'}),0.095,0.485,0.056,0.5894736842105264,1.2154096581660339,1.0,0.009925000000000003,1.2544871794871797,0.19583662194159437,0.10687022900763361,0.2028615227388862,0.3524688008681498
frozenset({'math_cat_μέτριο'}),"frozenset({'lunch_free/reduced', 'reading_cat_μέτριο', 'test preparation course_none'})",0.485,0.095,0.056,0.1154639175257732,1.2154096581660336,1.0,0.009925000000000003,1.0231351981351982,0.34414008321775325,0.10687022900763361,0.022612063564390276,0.3524688008681498
"frozenset({'reading_cat_μέτριο', 'gender_female'})","frozenset({'race/ethnicity_group C', 'lunch_standard'})",0.261,0.205,0.065,0.24904214559386972,1.2148397346042425,1.0,0.011495000000000005,1.0586479591836735,0.23930467367544508,0.16209476309226933,0.05539892527530784,0.2830576581627885
"frozenset({'race/ethnicity_group C', 'lunch_standard'})","frozenset({'reading_cat_μέτριο', 'gender_female'})",0.205,0.261,0.065,0.3170731707317073,1.2148397346042425,1.0,0.011495000000000005,1.082107142857143,0.22244799225931308,0.16209476309226933,0.07587709165318987,0.2830576581627885
frozenset({'race/ethnicity_group C'}),"frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'gender_female'})",0.319,0.173,0.067,0.21003134796238246,1.2140540344646387,1.0,0.011813000000000004,1.046876984126984,0.2589037192890175,0.15764705882352942,0.04477792982150239,0.2986572924783011
"frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'gender_female'})",frozenset({'race/ethnicity_group C'}),0.173,0.319,0.067,0.3872832369942197,1.2140540344646384,1.0,0.011813000000000004,1.1114433962264152,0.21319641213521276,0.15764705882352942,0.1002690704760935,0.2986572924783011
frozenset({'lunch_free/reduced'}),"frozenset({'writing_cat_χαμηλό', 'gender_male', 'test preparation course_none', 'reading_cat_χαμηλό'})",0.355,0.13,0.056,0.15774647887323945,1.2134344528710725,1.0,0.009850000000000005,1.032943143812709,0.27270210409745305,0.13053613053613053,0.031892504451999364,0.2942578548212351
"frozenset({'writing_cat_χαμηλό', 'gender_male', 'test preparation course_none', 'reading_cat_χαμηλό'})",frozenset({'lunch_free/reduced'}),0.13,0.355,0.056,0.43076923076923074,1.2134344528710725,1.0,0.009850000000000005,1.1331081081081082,0.20217569786535314,0.13053613053613053,0.11747167561121047,0.2942578548212351
frozenset({'race/ethnicity_group B'}),"frozenset({'writing_cat_χαμηλό', 'test preparation course_none'})",0.19,0.243,0.056,0.2947368421052632,1.2129088152479968,1.0,0.009829999999999998,1.0733582089552238,0.21671075837742498,0.14854111405835543,0.06834457345477304,0.2625947585011913
"frozenset({'writing_cat_χαμηλό', 'test preparation course_none'})",frozenset({'race/ethnicity_group B'}),0.243,0.19,0.056,0.23045267489711935,1.2129088152479965,1.0,0.009829999999999998,1.0525668449197862,0.2318833742215512,0.14854111405835543,0.04994157394706092,0.2625947585011913
frozenset({'lunch_standard'}),"frozenset({'test preparation course_none', 'reading_cat_υψηλό'})",0.645,0.11,0.086,0.13333333333333333,1.2121212121212122,1.0,0.015049999999999994,1.0269230769230768,0.49295774647887314,0.12855007473841554,0.026217228464419474,0.4575757575757575
frozenset({'test preparation course_none'}),frozenset({'reading_cat_χαμηλό'}),0.642,0.275,0.214,0.3333333333333333,1.212121212121212,1.0,0.03744999999999998,1.0875,0.4888268156424579,0.30440967283072545,0.08045977011494247,0.5557575757575757
"frozenset({'test preparation course_none', 'reading_cat_υψηλό'})",frozenset({'lunch_standard'}),0.11,0.645,0.086,0.7818181818181817,1.212121212121212,1.0,0.015049999999999994,1.6270833333333328,0.196629213483146,0.12855007473841554,0.38540332906530067,0.4575757575757575
frozenset({'reading_cat_χαμηλό'}),frozenset({'test preparation course_none'}),0.275,0.642,0.214,0.7781818181818181,1.212121212121212,1.0,0.03744999999999998,1.6139344262295074,0.2413793103448275,0.30440967283072545,0.3803961401726762,0.5557575757575757
"frozenset({'reading_cat_μέτριο', 'test preparation course_none'})","frozenset({'race/ethnicity_group C', 'lunch_standard'})",0.318,0.205,0.079,0.24842767295597484,1.2118423071023163,1.0,0.013810000000000003,1.0577824267782427,0.2563198336983556,0.17792792792792791,0.05462600371820736,0.3168967633072557
"frozenset({'race/ethnicity_group C', 'lunch_standard'})","frozenset({'reading_cat_μέτριο', 'test preparation course_none'})",0.205,0.318,0.079,0.3853658536585366,1.2118423071023163,1.0,0.013810000000000003,1.1096031746031745,0.21988695167582203,0.17792792792792791,0.09877691152278094,0.3168967633072557
frozenset({'math_cat_μέτριο'}),"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', ""parental level of education_associate's degree""})",0.485,0.097,0.057,0.1175257731958763,1.2116059092358382,1.0,0.009954999999999999,1.0232593457943926,0.3391245103048884,0.1085714285714286,0.02273064584260941,0.3525773195876289
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', ""parental level of education_associate's degree""})",frozenset({'math_cat_μέτριο'}),0.097,0.485,0.057,0.5876288659793815,1.211605909235838,1.0,0.009954999999999999,1.2488750000000002,0.19340988129237818,0.1085714285714286,0.1992793514162747,0.3525773195876289
frozenset({'math_cat_χαμηλό'}),frozenset({'race/ethnicity_group B'}),0.339,0.19,0.078,0.23008849557522124,1.2109920819748485,1.0,0.013589999999999991,1.0520689655172415,0.26358664028860684,0.1729490022172949,0.04949196984595214,0.32030740568234745
frozenset({'race/ethnicity_group B'}),frozenset({'math_cat_χαμηλό'}),0.19,0.339,0.078,0.4105263157894737,1.2109920819748485,1.0,0.013589999999999991,1.1213392857142856,0.21509971509971496,0.1729490022172949,0.10820925232900705,0.32030740568234745
"frozenset({""parental level of education_associate's degree"", 'lunch_standard'})",frozenset({'race/ethnicity_group C'}),0.145,0.319,0.056,0.3862068965517242,1.2106799264944332,1.0,0.009745000000000004,1.109494382022472,0.20352965747702598,0.13725490196078433,0.09868854119195915,0.2808777429467085
frozenset({'race/ethnicity_group C'}),"frozenset({""parental level of education_associate's degree"", 'lunch_standard'})",0.319,0.145,0.056,0.1755485893416928,1.210679926494433,1.0,0.009745000000000004,1.0370532319391634,0.255532829872037,0.13725490196078433,0.03572934425928982,0.2808777429467085
frozenset({'math_cat_μέτριο'}),"frozenset({'writing_cat_μέτριο', 'gender_female'})",0.485,0.264,0.155,0.3195876288659794,1.210559200249922,1.0,0.026959999999999984,1.0816969696969696,0.33773880363294684,0.260942760942761,0.07552666965486332,0.45335442049359576
"frozenset({'writing_cat_μέτριο', 'gender_female'})",frozenset({'math_cat_μέτριο'}),0.264,0.485,0.155,0.587121212121212,1.2105592002499217,1.0,0.026959999999999984,1.2473394495412842,0.23632538569424952,0.260942760942761,0.1982936157693438,0.45335442049359576
"frozenset({'gender_female', 'test preparation course_none', 'reading_cat_υψηλό'})",frozenset({'lunch_standard'}),0.082,0.645,0.064,0.7804878048780488,1.210058612214029,1.0,0.011109999999999995,1.6172222222222223,0.18909994553376896,0.09653092006033183,0.3816557883888699,0.4398563055397996
frozenset({'lunch_standard'}),"frozenset({'gender_female', 'test preparation course_none', 'reading_cat_υψηλό'})",0.645,0.082,0.064,0.09922480620155039,1.210058612214029,1.0,0.011109999999999995,1.0191222030981069,0.4889964788732392,0.09653092006033183,0.018763405448311964,0.4398563055397996
frozenset({'parental level of education_some college'}),"frozenset({'math_cat_μέτριο', 'gender_female'})",0.226,0.245,0.067,0.29646017699115046,1.2100415387393897,1.0,0.011630000000000001,1.0731446540880503,0.224266265571368,0.16584158415841585,0.06815917482271584,0.2849647823731263
"frozenset({'math_cat_μέτριο', 'gender_female'})",frozenset({'parental level of education_some college'}),0.245,0.226,0.067,0.27346938775510204,1.2100415387393895,1.0,0.011630000000000001,1.0653370786516854,0.22991005238707127,0.16584158415841585,0.06132995833992511,0.2849647823731263
frozenset({'race/ethnicity_group D'}),"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.262,0.281,0.089,0.3396946564885496,1.2088777810980411,1.0,0.015377999999999989,1.0888901734104046,0.2341280716177947,0.19603524229074887,0.08163373642357379,0.3282103175681182
"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο'})",frozenset({'race/ethnicity_group D'}),0.281,0.262,0.089,0.3167259786476868,1.2088777810980411,1.0,0.015377999999999989,1.0800937499999999,0.240315044303105,0.19603524229074887,0.07415444261204174,0.3282103175681182
frozenset({'math_cat_χαμηλό'}),"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'gender_female'})",0.339,0.227,0.093,0.2743362831858407,1.2085298818759502,1.0,0.016046999999999992,1.0652317073170732,0.2610414328241666,0.19661733615221982,0.06123710632062184,0.3420139565708939
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'gender_female'})",frozenset({'math_cat_χαμηλό'}),0.227,0.339,0.093,0.4096916299559471,1.20852988187595,1.0,0.016046999999999992,1.1197537313432835,0.223219129491299,0.19661733615221982,0.10694649009976864,0.3420139565708939
"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})",frozenset({'race/ethnicity_group D'}),0.199,0.262,0.063,0.3165829145728643,1.2083317350109324,1.0,0.010861999999999997,1.0798676470588233,0.2152468144977508,0.15829145728643215,0.07396058885211965,0.2785204649200199
frozenset({'race/ethnicity_group D'}),"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})",0.262,0.199,0.063,0.24045801526717556,1.2083317350109324,1.0,0.010861999999999997,1.0545829145728642,0.2336215425646319,0.15829145728643215,0.05175782180671105,0.2785204649200199
"frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced'})",frozenset({'test preparation course_none'}),0.156,0.642,0.121,0.7756410256410257,1.2081635913411615,1.0,0.02084799999999999,1.5956571428571429,0.20414398182601537,0.17872968980797635,0.3732989543045409,0.4820572729451234
frozenset({'test preparation course_none'}),"frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced'})",0.642,0.156,0.121,0.18847352024922118,1.2081635913411615,1.0,0.02084799999999999,1.0400153550863722,0.4812779906736228,0.17872968980797635,0.03847573489244215,0.4820572729451234
"frozenset({'math_cat_χαμηλό', 'reading_cat_χαμηλό'})",frozenset({'test preparation course_none'}),0.218,0.642,0.169,0.7752293577981652,1.2075223641715969,1.0,0.029044000000000014,1.592734693877551,0.2197672482937091,0.2445730824891462,0.3721490441289529,0.5192346165937867
frozenset({'test preparation course_none'}),"frozenset({'math_cat_χαμηλό', 'reading_cat_χαμηλό'})",0.642,0.218,0.169,0.26323987538940813,1.2075223641715969,1.0,0.029044000000000014,1.0614038054968287,0.4800502462728507,0.2445730824891462,0.05785150305550912,0.5192346165937867
frozenset({'lunch_standard'}),frozenset({'writing_cat_υψηλό'}),0.645,0.208,0.162,0.25116279069767444,1.2075134168157426,1.0,0.027840000000000004,1.057639751552795,0.48408972352634333,0.2344428364688857,0.054498473103124306,0.5150044722719141
frozenset({'writing_cat_υψηλό'}),frozenset({'lunch_standard'}),0.208,0.645,0.162,0.7788461538461539,1.2075134168157424,1.0,0.027840000000000004,1.6052173913043477,0.2169846614291059,0.2344428364688857,0.3770314192849404,0.5150044722719141
frozenset({'lunch_standard'}),"frozenset({'writing_cat_υψηλό', 'reading_cat_υψηλό'})",0.645,0.185,0.144,0.22325581395348834,1.2067881835323695,1.0,0.02467499999999999,1.0492514970059879,0.4826877934272299,0.20991253644314867,0.04693964902268509,0.5008170961659333
"frozenset({'writing_cat_υψηλό', 'reading_cat_υψηλό'})",frozenset({'lunch_standard'}),0.185,0.645,0.144,0.7783783783783783,1.2067881835323695,1.0,0.02467499999999999,1.6018292682926825,0.2102505112474437,0.20991253644314867,0.37571374191092477,0.5008170961659333
frozenset({'race/ethnicity_group B'}),frozenset({'writing_cat_χαμηλό'}),0.19,0.301,0.069,0.3631578947368421,1.2065046336772165,1.0,0.011810000000000008,1.097603305785124,0.2113079262837718,0.16350710900473936,0.08892402680521048,0.29619688756775664
frozenset({'writing_cat_χαμηλό'}),frozenset({'race/ethnicity_group B'}),0.301,0.19,0.069,0.22923588039867113,1.2065046336772165,1.0,0.011810000000000008,1.0509051724137932,0.24486326221724627,0.16350710900473936,0.04843935851687793,0.29619688756775664
frozenset({'gender_male'}),"frozenset({'test preparation course_completed', 'reading_cat_μέτριο', 'math_cat_μέτριο'})",0.482,0.117,0.068,0.14107883817427389,1.2058020356775545,1.0,0.011606000000000005,1.0280338164251208,0.3294912559618443,0.12806026365348402,0.02726935240574618,0.3611377096854276
"frozenset({'test preparation course_completed', 'reading_cat_μέτριο', 'math_cat_μέτριο'})",frozenset({'gender_male'}),0.117,0.482,0.068,0.5811965811965812,1.2058020356775545,1.0,0.011606000000000005,1.236857142857143,0.19329158617014197,0.12806026365348402,0.19149919149919162,0.3611377096854276
frozenset({'test preparation course_none'}),"frozenset({'writing_cat_χαμηλό', 'gender_male', 'reading_cat_χαμηλό'})",0.642,0.168,0.13,0.20249221183800623,1.205310784750037,1.0,0.022143999999999997,1.04325,0.4758057584873227,0.19117647058823528,0.04145698538221901,0.48815086782376504
"frozenset({'writing_cat_χαμηλό', 'gender_male', 'reading_cat_χαμηλό'})",frozenset({'test preparation course_none'}),0.168,0.642,0.13,0.7738095238095238,1.205310784750037,1.0,0.022143999999999997,1.5827368421052632,0.20473372781065088,0.19117647058823528,0.36818302740090453,0.48815086782376504
"frozenset({'reading_cat_μέτριο', 'test preparation course_none'})","frozenset({'race/ethnicity_group D', 'lunch_standard'})",0.318,0.167,0.064,0.20125786163522014,1.205136896019282,1.0,0.010894000000000001,1.0428897637795274,0.2495876099706745,0.15201900237529692,0.04112588431599055,0.2922456972846759
"frozenset({'race/ethnicity_group D', 'lunch_standard'})","frozenset({'reading_cat_μέτριο', 'test preparation course_none'})",0.167,0.318,0.064,0.3832335329341317,1.205136896019282,1.0,0.010894000000000001,1.105766990291262,0.20434423769507806,0.15201900237529692,0.09565034154564767,0.2922456972846759
frozenset({'test preparation course_completed'}),"frozenset({'lunch_free/reduced', 'writing_cat_μέτριο'})",0.358,0.153,0.066,0.18435754189944137,1.204951254244715,1.0,0.011226000000000007,1.0384452054794522,0.2649391107335034,0.14831460674157304,0.03702189126262263,0.30786504545952464
"frozenset({'lunch_free/reduced', 'writing_cat_μέτριο'})",frozenset({'test preparation course_completed'}),0.153,0.358,0.066,0.43137254901960786,1.2049512542447147,1.0,0.011226000000000007,1.1290344827586207,0.20081571321240754,0.14831460674157304,0.1142874595320995,0.30786504545952464
frozenset({'writing_cat_χαμηλό'}),"frozenset({'parental level of education_some college', 'test preparation course_none'})",0.301,0.149,0.054,0.17940199335548174,1.2040402238622936,1.0,0.009151,1.0370485829959515,0.24243628463943195,0.13636363636363638,0.03572502156930875,0.270909050369016
"frozenset({'parental level of education_some college', 'test preparation course_none'})",frozenset({'writing_cat_χαμηλό'}),0.149,0.301,0.054,0.36241610738255037,1.2040402238622936,1.0,0.009151,1.0963263157894738,0.19913391652522086,0.13636363636363638,0.08786281456731097,0.270909050369016
"frozenset({'test preparation course_completed', 'math_cat_χαμηλό'})",frozenset({'gender_female'}),0.093,0.518,0.058,0.6236559139784946,1.2039689459044298,1.0,0.009826000000000001,1.2807428571428572,0.1867847774018173,0.10488245931283907,0.2192031410342212,0.3678125129738033
frozenset({'gender_female'}),"frozenset({'test preparation course_completed', 'math_cat_χαμηλό'})",0.518,0.093,0.058,0.11196911196911197,1.2039689459044298,1.0,0.009826000000000001,1.0213608695652174,0.35148089855487197,0.10488245931283907,0.020914125655029734,0.3678125129738033
"frozenset({'gender_female', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'test preparation course_none'})",0.329,0.308,0.122,0.3708206686930091,1.2039632100422375,1.0,0.020667999999999992,1.0998454106280193,0.2524736752094988,0.23689320388349513,0.0907813131401866,0.3834622823984526
"frozenset({'writing_cat_μέτριο', 'test preparation course_none'})","frozenset({'gender_female', 'lunch_standard'})",0.308,0.329,0.122,0.3961038961038961,1.2039632100422375,1.0,0.020667999999999992,1.1111182795698924,0.24481190182886378,0.23689320388349513,0.10000580641415217,0.3834622823984526
"frozenset({'gender_female', 'lunch_standard'})","frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο'})",0.329,0.154,0.061,0.18541033434650456,1.2039632100422375,1.0,0.010333999999999996,1.0385597014925372,0.2524736752094988,0.14454976303317535,0.03712805478310232,0.2907571152252003
"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο'})","frozenset({'gender_female', 'lunch_standard'})",0.154,0.329,0.061,0.3961038961038961,1.2039632100422375,1.0,0.010333999999999996,1.1111182795698924,0.200248033174437,0.14454976303317535,0.10000580641415217,0.2907571152252003
"frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'gender_female'})",frozenset({'math_cat_μέτριο'}),0.173,0.485,0.101,0.5838150289017342,1.2037423276324417,1.0,0.017095000000000013,1.2374305555555558,0.2046643600272967,0.18132854578096952,0.19187384252763925,0.3960312257910733
frozenset({'math_cat_μέτριο'}),"frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'gender_female'})",0.485,0.173,0.101,0.20824742268041238,1.2037423276324415,1.0,0.017095000000000013,1.0445182291666666,0.32865519561664924,0.18132854578096952,0.04262082549022055,0.3960312257910733
"frozenset({'reading_cat_μέτριο', 'race/ethnicity_group B'})",frozenset({'gender_female'}),0.085,0.518,0.053,0.6235294117647058,1.2037247331364975,1.0,0.008969999999999992,1.2803124999999995,0.1849675224249921,0.09636363636363637,0.21894068830851818,0.36292300704065406
frozenset({'gender_female'}),"frozenset({'reading_cat_μέτριο', 'race/ethnicity_group B'})",0.518,0.085,0.053,0.1023166023166023,1.2037247331364975,1.0,0.008969999999999992,1.0192903225806451,0.3511312925702651,0.09636363636363637,0.018925248433445136,0.36292300704065406
"frozenset({'gender_male', 'test preparation course_none', 'lunch_standard'})",frozenset({'race/ethnicity_group D'}),0.203,0.262,0.064,0.31527093596059114,1.2033241830556913,1.0,0.010813999999999997,1.0777985611510792,0.21200595984943535,0.1596009975062344,0.07218284005500153,0.27977287256044825
frozenset({'race/ethnicity_group D'}),"frozenset({'gender_male', 'test preparation course_none', 'lunch_standard'})",0.262,0.203,0.064,0.24427480916030533,1.203324183055691,1.0,0.010813999999999997,1.0546161616161616,0.2289549457994579,0.1596009975062344,0.05178771538306816,0.27977287256044825
frozenset({'gender_male'}),"frozenset({'reading_cat_μέτριο', 'math_cat_μέτριο', 'race/ethnicity_group D'})",0.482,0.1,0.058,0.1203319502074689,1.203319502074689,1.0,0.009800000000000003,1.0231132075471698,0.3261882572227401,0.11068702290076338,0.022591055786076546,0.35016597510373443
"frozenset({'reading_cat_μέτριο', 'math_cat_μέτριο', 'race/ethnicity_group D'})",frozenset({'gender_male'}),0.1,0.482,0.058,0.58,1.2033195020746887,1.0,0.009800000000000003,1.2333333333333332,0.18773946360153262,0.11068702290076338,0.18918918918918914,0.35016597510373443
"frozenset({'writing_cat_μέτριο', 'gender_male'})","frozenset({'test preparation course_completed', 'lunch_standard'})",0.227,0.227,0.062,0.27312775330396477,1.2032059616914748,1.0,0.010470999999999994,1.0634606060606062,0.21848266076868492,0.15816326530612243,0.05967367827162324,0.27312775330396477
"frozenset({'test preparation course_completed', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'gender_male'})",0.227,0.227,0.062,0.27312775330396477,1.2032059616914748,1.0,0.010470999999999994,1.0634606060606062,0.21848266076868492,0.15816326530612243,0.05967367827162324,0.27312775330396477
"frozenset({'test preparation course_completed', 'gender_male'})","frozenset({'reading_cat_μέτριο', 'math_cat_μέτριο'})",0.174,0.325,0.068,0.3908045977011495,1.2024756852343061,1.0,0.011450000000000009,1.1080188679245284,0.20385272753169076,0.1577726218097448,0.09748829289059184,0.30001768346595936
"frozenset({'reading_cat_μέτριο', 'math_cat_μέτριο'})","frozenset({'test preparation course_completed', 'gender_male'})",0.325,0.174,0.068,0.20923076923076925,1.2024756852343061,1.0,0.011450000000000009,1.0445525291828794,0.249455337690632,0.1577726218097448,0.042652262991246075,0.30001768346595936
frozenset({'test preparation course_completed'}),frozenset({'parental level of education_some high school'}),0.358,0.179,0.077,0.21508379888268156,1.2015854686183327,1.0,0.012917999999999999,1.0459715302491104,0.2613181211312052,0.16739130434782612,0.04395103396185331,0.3226256983240223
frozenset({'parental level of education_some high school'}),frozenset({'test preparation course_completed'}),0.179,0.358,0.077,0.4301675977653631,1.2015854686183327,1.0,0.012917999999999999,1.1266470588235296,0.20434376829017512,0.16739130434782612,0.11241058841956875,0.3226256983240223
frozenset({'gender_male'}),"frozenset({'lunch_free/reduced', 'math_cat_μέτριο', 'writing_cat_μέτριο'})",0.482,0.095,0.055,0.1141078838174274,1.2011356191308147,1.0,0.009210000000000003,1.0215690866510538,0.32327132327132335,0.10536398467432952,0.021113683776162868,0.34652762611924004
"frozenset({'lunch_free/reduced', 'math_cat_μέτριο', 'writing_cat_μέτριο'})",frozenset({'gender_male'}),0.095,0.482,0.055,0.5789473684210527,1.2011356191308147,1.0,0.009210000000000003,1.23025,0.1850326469111,0.10536398467432952,0.18715708189392408,0.34652762611924004
frozenset({'reading_cat_υψηλό'}),frozenset({'lunch_standard'}),0.235,0.645,0.182,0.774468085106383,1.2007257133432294,1.0,0.030425000000000008,1.5740566037735848,0.21852330675860093,0.26074498567335247,0.36469883128558583,0.5283193138710209
frozenset({'lunch_standard'}),frozenset({'reading_cat_υψηλό'}),0.645,0.235,0.182,0.2821705426356589,1.2007257133432294,1.0,0.030425000000000008,1.0657127429805615,0.4709023370995203,0.26074498567335247,0.06166084004661294,0.5283193138710209
frozenset({'parental level of education_high school'}),"frozenset({'writing_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",0.196,0.221,0.052,0.26530612244897955,1.2004801920768307,1.0,0.008683999999999997,1.0603055555555556,0.2077114427860696,0.1424657534246575,0.05687563857378633,0.25030012004801916
"frozenset({'writing_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",frozenset({'parental level of education_high school'}),0.221,0.196,0.052,0.23529411764705882,1.2004801920768307,1.0,0.008683999999999997,1.0513846153846156,0.21437740693196397,0.1424657534246575,0.04887328065554578,0.25030012004801916
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})","frozenset({'gender_female', 'lunch_standard'})",0.266,0.329,0.105,0.3947368421052631,1.1998080307150853,1.0,0.017485999999999988,1.1086086956521737,0.22688465031789268,0.21428571428571427,0.09796846811514619,0.3569428891377379
"frozenset({'gender_female', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})",0.329,0.266,0.105,0.3191489361702127,1.1998080307150853,1.0,0.017485999999999988,1.0780625,0.24818678589170373,0.21428571428571427,0.07240999478230616,0.3569428891377379
frozenset({'math_cat_μέτριο'}),"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'gender_female'})",0.485,0.227,0.132,0.2721649484536083,1.1989645306326355,1.0,0.021905000000000008,1.0620538243626063,0.322227125625184,0.22758620689655176,0.05842813512756568,0.4268313729052182
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'gender_female'})",frozenset({'math_cat_μέτριο'}),0.227,0.485,0.132,0.5814977973568282,1.1989645306326353,1.0,0.021905000000000008,1.230578947368421,0.21467913285507084,0.22758620689655176,0.18737436379966635,0.4268313729052182
"frozenset({'gender_male', 'lunch_standard'})",frozenset({'race/ethnicity_group E'}),0.316,0.14,0.053,0.16772151898734178,1.198010849909584,1.0,0.008759999999999997,1.0333079847908746,0.24164184045018203,0.1315136476426799,0.032234324403885774,0.27314647377938517
frozenset({'race/ethnicity_group E'}),"frozenset({'gender_male', 'lunch_standard'})",0.14,0.316,0.053,0.3785714285714285,1.1980108499095838,1.0,0.008759999999999997,1.1006896551724137,0.19218955682316802,0.1315136476426799,0.09147869674185453,0.27314647377938517
"frozenset({'race/ethnicity_group D', 'lunch_standard'})",frozenset({'math_cat_μέτριο'}),0.167,0.485,0.097,0.5808383233532934,1.1976047904191618,1.0,0.016005000000000005,1.2286428571428571,0.19807923169267713,0.17477477477477477,0.18609383175396788,0.3904191616766467
frozenset({'math_cat_μέτριο'}),"frozenset({'race/ethnicity_group D', 'lunch_standard'})",0.485,0.167,0.097,0.2,1.1976047904191616,1.0,0.016005000000000005,1.0412499999999998,0.32038834951456324,0.17477477477477477,0.03961584633853542,0.3904191616766467
frozenset({'race/ethnicity_group E'}),frozenset({'test preparation course_completed'}),0.14,0.358,0.06,0.4285714285714285,1.1971268954509175,1.0,0.009879999999999993,1.1235,0.19147286821705412,0.136986301369863,0.10992434356920328,0.2980845969672785
frozenset({'test preparation course_completed'}),frozenset({'race/ethnicity_group E'}),0.358,0.14,0.06,0.16759776536312848,1.1971268954509175,1.0,0.009879999999999993,1.0331543624161073,0.2564901349948077,0.136986301369863,0.03209042484084706,0.2980845969672785
"frozenset({'math_cat_χαμηλό', 'test preparation course_none'})","frozenset({'race/ethnicity_group C', 'gender_female'})",0.246,0.18,0.053,0.21544715447154472,1.1969286359530262,1.0,0.008719999999999999,1.0451813471502591,0.21820729693208546,0.1420911528150134,0.043228237160420395,0.25494579945799456
"frozenset({'race/ethnicity_group C', 'gender_female'})","frozenset({'math_cat_χαμηλό', 'test preparation course_none'})",0.18,0.246,0.053,0.29444444444444445,1.1969286359530262,1.0,0.008719999999999999,1.0686614173228348,0.2006442705936493,0.1420911528150134,0.06424992631889186,0.25494579945799456
"frozenset({'lunch_free/reduced', 'gender_female'})","frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό'})",0.189,0.252,0.057,0.30158730158730157,1.1967750062988158,1.0,0.009371999999999998,1.071,0.20273865922512815,0.1484375,0.06629318394024274,0.2638888888888889
"frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό'})","frozenset({'lunch_free/reduced', 'gender_female'})",0.252,0.189,0.057,0.2261904761904762,1.1967750062988158,1.0,0.009371999999999998,1.0480615384615384,0.21981424148606807,0.1484375,0.04585755387235042,0.2638888888888889
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'gender_female', 'parental level of education_some college'})",0.418,0.118,0.059,0.14114832535885166,1.1961722488038278,1.0,0.009676000000000004,1.0269526462395544,0.2817869415807561,0.12368972746331236,0.026245266846770598,0.32057416267942584
"frozenset({'test preparation course_none', 'lunch_standard'})","frozenset({'parental level of education_high school', 'gender_male'})",0.418,0.102,0.051,0.12200956937799043,1.1961722488038278,1.0,0.008364000000000003,1.0227901907356949,0.28178694158075607,0.10874200426439232,0.022282371244978214,0.31100478468899523
"frozenset({'parental level of education_high school', 'gender_male'})","frozenset({'test preparation course_none', 'lunch_standard'})",0.102,0.418,0.051,0.5,1.1961722488038278,1.0,0.008364000000000003,1.1640000000000001,0.18262806236080187,0.10874200426439232,0.140893470790378,0.31100478468899523
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'test preparation course_none', 'race/ethnicity_group D', 'lunch_standard'})",0.418,0.118,0.059,0.14114832535885166,1.1961722488038278,1.0,0.009676000000000004,1.0269526462395544,0.2817869415807561,0.12368972746331236,0.026245266846770598,0.32057416267942584
"frozenset({'test preparation course_none', 'race/ethnicity_group D', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.118,0.418,0.059,0.5,1.1961722488038278,1.0,0.009676000000000004,1.1640000000000001,0.1859410430839003,0.12368972746331236,0.140893470790378,0.32057416267942584
"frozenset({'gender_female', 'parental level of education_some college'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.118,0.418,0.059,0.5,1.1961722488038278,1.0,0.009676000000000004,1.1640000000000001,0.1859410430839003,0.12368972746331236,0.140893470790378,0.32057416267942584
"frozenset({'race/ethnicity_group C', 'test preparation course_none'})","frozenset({'reading_cat_μέτριο', 'lunch_standard'})",0.202,0.327,0.079,0.39108910891089105,1.1959911587489023,1.0,0.012946,1.105252032520325,0.20535516005202878,0.17555555555555555,0.0952289879805216,0.31633966148908466
"frozenset({'reading_cat_μέτριο', 'lunch_standard'})","frozenset({'race/ethnicity_group C', 'test preparation course_none'})",0.327,0.202,0.079,0.2415902140672783,1.1959911587489023,1.0,0.012946,1.052201612903226,0.24349690597551107,0.17555555555555555,0.049611797076789824,0.31633966148908466
"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced', 'test preparation course_none'})",frozenset({'gender_female'}),0.126,0.518,0.078,0.6190476190476191,1.1950726236440523,1.0,0.012731999999999993,1.26525,0.18676289385671527,0.13780918727915192,0.20964236316933413,0.38481338481338484
frozenset({'gender_female'}),"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced', 'test preparation course_none'})",0.518,0.126,0.078,0.15057915057915058,1.1950726236440523,1.0,0.012731999999999993,1.0289363636363638,0.3386530481966165,0.13780918727915192,0.028122597916648263,0.38481338481338484
"frozenset({'math_cat_μέτριο', 'gender_female'})","frozenset({'race/ethnicity_group C', 'lunch_standard'})",0.245,0.205,0.06,0.24489795918367346,1.1946241911398707,1.0,0.009774999999999999,1.052837837837838,0.2157836644591611,0.15384615384615385,0.05018611218072135,0.26879044300647087
"frozenset({'race/ethnicity_group C', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'gender_female'})",0.205,0.245,0.06,0.2926829268292683,1.1946241911398705,1.0,0.009774999999999999,1.0674137931034484,0.20492662473794548,0.15384615384615385,0.06315619447585202,0.26879044300647087
"frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced', 'reading_cat_χαμηλό'})",frozenset({'test preparation course_none'}),0.133,0.642,0.102,0.7669172932330827,1.1945752231044902,1.0,0.01661399999999999,1.5359354838709673,0.18786891919397505,0.1515601783060921,0.34893098668458844,0.46289789895298994
frozenset({'test preparation course_none'}),"frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced', 'reading_cat_χαμηλό'})",0.642,0.133,0.102,0.1588785046728972,1.1945752231044902,1.0,0.01661399999999999,1.0307666666666666,0.4549786395004927,0.1515601783060921,0.02984833295605211,0.46289789895298994
frozenset({'lunch_standard'}),"frozenset({'writing_cat_μέτριο', 'parental level of education_some high school'})",0.645,0.087,0.067,0.10387596899224806,1.193976655083311,1.0,0.010885000000000006,1.0188321799307958,0.4576413706117303,0.10075187969924812,0.01848408432885878,0.43699545576049187
"frozenset({'writing_cat_μέτριο', 'parental level of education_some high school'})",frozenset({'lunch_standard'}),0.087,0.645,0.067,0.7701149425287357,1.193976655083311,1.0,0.010885000000000006,1.5442500000000003,0.17794379689722262,0.10075187969924812,0.35243645782742444,0.43699545576049187
"frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'gender_female'})",frozenset({'math_cat_χαμηλό'}),0.173,0.339,0.07,0.4046242774566475,1.19358193940014,1.0,0.011353000000000009,1.1102233009708742,0.1961133183624116,0.15837104072398192,0.09928029872412625,0.3055569764864358
frozenset({'math_cat_χαμηλό'}),"frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'gender_female'})",0.339,0.173,0.07,0.20648967551622419,1.19358193940014,1.0,0.011353000000000009,1.0422044609665426,0.24536416684676912,0.15837104072398192,0.04049537547306433,0.3055569764864358
"frozenset({'gender_female', 'reading_cat_υψηλό'})",frozenset({'math_cat_μέτριο'}),0.159,0.485,0.092,0.5786163522012578,1.1930234066005316,1.0,0.014884999999999995,1.2221641791044775,0.19238225714728838,0.16666666666666666,0.1817793246626366,0.3841535369253712
frozenset({'math_cat_μέτριο'}),"frozenset({'gender_female', 'reading_cat_υψηλό'})",0.485,0.159,0.092,0.18969072164948453,1.1930234066005316,1.0,0.014884999999999995,1.0378753180661577,0.31416209371042625,0.16666666666666666,0.03649312919082583,0.3841535369253712
frozenset({'race/ethnicity_group D'}),"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})",0.262,0.32,0.1,0.3816793893129771,1.1927480916030535,1.0,0.016159999999999994,1.099753086419753,0.2189701897018969,0.20746887966804975,0.09070498428378988,0.34708969465648853
"frozenset({'math_cat_μέτριο', 'gender_male'})",frozenset({'race/ethnicity_group D'}),0.24,0.262,0.075,0.3125,1.1927480916030533,1.0,0.012119999999999992,1.0734545454545454,0.2126315789473683,0.1756440281030445,0.0684281842818428,0.2993797709923664
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})",frozenset({'race/ethnicity_group D'}),0.32,0.262,0.1,0.3125,1.1927480916030533,1.0,0.016159999999999994,1.0734545454545454,0.23764705882352935,0.20746887966804975,0.0684281842818428,0.34708969465648853
frozenset({'race/ethnicity_group D'}),"frozenset({'math_cat_μέτριο', 'gender_male'})",0.262,0.24,0.075,0.2862595419847328,1.1927480916030533,1.0,0.012119999999999992,1.0648128342245988,0.21897018970189688,0.1756440281030445,0.060867818400964224,0.2993797709923664
"frozenset({'gender_female', 'lunch_standard'})","frozenset({'race/ethnicity_group C', 'math_cat_μέτριο'})",0.329,0.153,0.06,0.182370820668693,1.1919661481613921,1.0,0.009662999999999998,1.0359219330855018,0.24001490312965715,0.14218009478672985,0.03467629358759505,0.2872638417068955
"frozenset({'race/ethnicity_group C', 'math_cat_μέτριο'})","frozenset({'gender_female', 'lunch_standard'})",0.153,0.329,0.06,0.39215686274509803,1.1919661481613921,1.0,0.009662999999999998,1.1039032258064516,0.19014167650531283,0.14218009478672985,0.09412349142339496,0.2872638417068955
"frozenset({'writing_cat_υψηλό', 'gender_female'})",frozenset({'math_cat_μέτριο'}),0.154,0.485,0.089,0.577922077922078,1.191591913241398,1.0,0.014310000000000003,1.2201538461538464,0.19005498446091326,0.1618181818181818,0.18043121926617078,0.3807136162806266
frozenset({'math_cat_μέτριο'}),"frozenset({'writing_cat_υψηλό', 'gender_female'})",0.485,0.154,0.089,0.18350515463917524,1.1915919132413977,1.0,0.014310000000000003,1.0361363636363636,0.3122068288425876,0.1618181818181818,0.03487606931344592,0.3807136162806266
frozenset({'lunch_standard'}),"frozenset({'test preparation course_completed', 'reading_cat_υψηλό'})",0.645,0.125,0.096,0.14883720930232558,1.1906976744186046,1.0,0.015375,1.0280054644808743,0.4511443661971831,0.142433234421365,0.027242524916943518,0.4584186046511628
"frozenset({'test preparation course_completed', 'reading_cat_υψηλό'})",frozenset({'lunch_standard'}),0.125,0.645,0.096,0.768,1.1906976744186046,1.0,0.015375,1.5301724137931034,0.18303571428571427,0.142433234421365,0.34647887323943666,0.4584186046511628
"frozenset({'math_cat_χαμηλό', 'lunch_standard'})",frozenset({'test preparation course_none'}),0.157,0.642,0.12,0.7643312101910827,1.190547056372403,1.0,0.019205999999999987,1.5190810810810806,0.1898576512455515,0.1767304860088365,0.34170729103654396,0.475623549020775
frozenset({'test preparation course_none'}),"frozenset({'math_cat_χαμηλό', 'lunch_standard'})",0.642,0.157,0.12,0.18691588785046728,1.190547056372403,1.0,0.019205999999999987,1.0367931034482758,0.447067039106145,0.1767304860088365,0.03548741144776665,0.475623549020775
"frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό'})",frozenset({'race/ethnicity_group B'}),0.252,0.19,0.057,0.2261904761904762,1.1904761904761905,1.0,0.009120000000000003,1.046769230769231,0.2139037433155081,0.14805194805194805,0.044679600235155784,0.2630952380952381
frozenset({'race/ethnicity_group B'}),"frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό'})",0.19,0.252,0.057,0.3,1.1904761904761905,1.0,0.009120000000000003,1.0685714285714287,0.1975308641975309,0.14805194805194805,0.06417112299465239,0.2630952380952381
frozenset({'writing_cat_μέτριο'}),"frozenset({'gender_male', 'race/ethnicity_group D', 'lunch_standard'})",0.491,0.089,0.052,0.10590631364562118,1.1899585802878785,1.0,0.008301000000000003,1.0189088838268794,0.313623998790993,0.0984848484848485,0.018557973266324024,0.34508798828348475
"frozenset({'gender_male', 'race/ethnicity_group D', 'lunch_standard'})",frozenset({'writing_cat_μέτριο'}),0.089,0.491,0.052,0.5842696629213483,1.1899585802878785,1.0,0.008301000000000003,1.2243513513513513,0.17523009372625187,0.0984848484848485,0.18324098805765876,0.34508798828348475
"frozenset({'gender_male', 'test preparation course_none', 'reading_cat_χαμηλό'})",frozenset({'lunch_free/reduced'}),0.135,0.355,0.057,0.4222222222222222,1.189358372456964,1.0,0.009075,1.1163461538461539,0.18405841192576816,0.13163972286374134,0.10422049956933681,0.2913928012519562
frozenset({'lunch_free/reduced'}),"frozenset({'gender_male', 'test preparation course_none', 'reading_cat_χαμηλό'})",0.355,0.135,0.057,0.16056338028169015,1.189358372456964,1.0,0.009075,1.0304530201342281,0.24683802529579763,0.13163972286374134,0.029553040788081086,0.2913928012519562
frozenset({'gender_female'}),"frozenset({'test preparation course_completed', 'reading_cat_υψηλό'})",0.518,0.125,0.077,0.14864864864864863,1.189189189189189,1.0,0.012249999999999997,1.027777777777778,0.3300641267446246,0.13604240282685512,0.027027027027027004,0.3823243243243243
"frozenset({'test preparation course_completed', 'reading_cat_υψηλό'})",frozenset({'gender_female'}),0.125,0.518,0.077,0.616,1.189189189189189,1.0,0.012249999999999997,1.2552083333333333,0.18181818181818177,0.13604240282685512,0.20331950207468874,0.3823243243243243
frozenset({'race/ethnicity_group C'}),"frozenset({'reading_cat_μέτριο', 'gender_female'})",0.319,0.261,0.099,0.3103448275862069,1.1890606420927468,1.0,0.015741000000000005,1.07155,0.23348017621145378,0.20582120582120578,0.0667724324576548,0.3448275862068966
"frozenset({'reading_cat_μέτριο', 'gender_female'})",frozenset({'race/ethnicity_group C'}),0.261,0.319,0.099,0.3793103448275862,1.1890606420927468,1.0,0.015741000000000005,1.0971666666666666,0.21515561569688774,0.20582120582120578,0.08856144614917207,0.3448275862068966
"frozenset({'gender_male', 'reading_cat_χαμηλό'})",frozenset({'test preparation course_none'}),0.177,0.642,0.135,0.7627118644067797,1.1880247109139872,1.0,0.02136600000000001,1.5087142857142861,0.19230457675172144,0.1973684210526316,0.3371839787898875,0.4864961191192777
frozenset({'test preparation course_none'}),"frozenset({'gender_male', 'reading_cat_χαμηλό'})",0.642,0.177,0.135,0.2102803738317757,1.188024710913987,1.0,0.02136600000000001,1.0421420118343194,0.4420856610800747,0.1973684210526316,0.040437878288913376,0.4864961191192777
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'lunch_standard'})",frozenset({'race/ethnicity_group D'}),0.225,0.262,0.07,0.3111111111111111,1.1874469889737065,1.0,0.011050000000000004,1.0712903225806452,0.20368663594470052,0.16786570743405277,0.06654622101776572,0.28914334181509754
frozenset({'race/ethnicity_group D'}),"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'lunch_standard'})",0.262,0.225,0.07,0.26717557251908397,1.1874469889737065,1.0,0.011050000000000004,1.0575520833333334,0.2138985675571042,0.16786570743405277,0.054420093573011565,0.28914334181509754
"frozenset({'gender_female', 'test preparation course_none'})","frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο'})",0.334,0.169,0.067,0.20059880239520958,1.1869751621018318,1.0,0.010553999999999994,1.0395280898876404,0.23652010219174388,0.15366972477064222,0.03802503296655785,0.2985242532686107
"frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο'})","frozenset({'gender_female', 'test preparation course_none'})",0.169,0.334,0.067,0.3964497041420118,1.1869751621018316,1.0,0.010553999999999994,1.103470588235294,0.18955762702731818,0.15366972477064222,0.09376832453755525,0.2985242532686107
frozenset({'writing_cat_χαμηλό'}),"frozenset({""parental level of education_associate's degree"", 'test preparation course_none'})",0.301,0.14,0.05,0.16611295681063123,1.1865211200759374,1.0,0.007859999999999999,1.0313147410358565,0.22489270386266091,0.1278772378516624,0.03036390326817584,0.2616279069767442
"frozenset({""parental level of education_associate's degree"", 'test preparation course_none'})",frozenset({'writing_cat_χαμηλό'}),0.14,0.301,0.05,0.35714285714285715,1.1865211200759374,1.0,0.007859999999999999,1.0873333333333335,0.18279069767441858,0.1278772378516624,0.08031882280809322,0.2616279069767442
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'parental level of education_some high school'})",frozenset({'lunch_standard'}),0.068,0.645,0.052,0.7647058823529411,1.1855905152758777,1.0,0.008139999999999994,1.5087499999999996,0.16795972268075263,0.07866868381240544,0.33719966859983413,0.42266301869585043
frozenset({'lunch_standard'}),"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'parental level of education_some high school'})",0.645,0.068,0.052,0.08062015503875969,1.1855905152758777,1.0,0.008139999999999994,1.013726812816189,0.4409534127843985,0.07866868381240544,0.013540938882789361,0.42266301869585043
frozenset({'gender_male'}),"frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced', 'reading_cat_χαμηλό'})",0.482,0.133,0.076,0.15767634854771784,1.1855364552459988,1.0,0.011894000000000002,1.029295566502463,0.3021235521235522,0.14100185528756956,0.028461763030816413,0.3645524599881446
"frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced', 'reading_cat_χαμηλό'})",frozenset({'gender_male'}),0.133,0.482,0.076,0.5714285714285714,1.1855364552459988,1.0,0.011894000000000002,1.2086666666666666,0.1805074971164937,0.14100185528756956,0.17264202978488688,0.3645524599881446
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'gender_male'})",frozenset({'test preparation course_completed'}),0.165,0.358,0.07,0.42424242424242425,1.1850347045877774,1.0,0.01093000000000001,1.1150526315789475,0.1869974337040207,0.1545253863134658,0.10318134617200042,0.30988657524970376
frozenset({'test preparation course_completed'}),"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'gender_male'})",0.358,0.165,0.07,0.19553072625698326,1.1850347045877772,1.0,0.01093000000000001,1.0379513888888887,0.24321317311971533,0.1545253863134658,0.03656374402033922,0.30988657524970376
frozenset({'math_cat_μέτριο'}),"frozenset({'reading_cat_μέτριο', 'gender_female'})",0.485,0.261,0.15,0.30927835051546393,1.1849745230477544,1.0,0.02341499999999999,1.0698955223880597,0.30310679611650476,0.2516778523489933,0.06532929704392952,0.4419954970968124
"frozenset({'reading_cat_μέτριο', 'gender_female'})",frozenset({'math_cat_μέτριο'}),0.261,0.485,0.15,0.5747126436781609,1.1849745230477544,1.0,0.02341499999999999,1.210945945945946,0.21123139377537206,0.2516778523489933,0.17419930811293377,0.4419954970968124
"frozenset({'reading_cat_μέτριο', 'gender_female', 'lunch_standard'})",frozenset({'race/ethnicity_group C'}),0.172,0.319,0.065,0.3779069767441861,1.1846613691040317,1.0,0.010132000000000009,1.0946915887850468,0.1882571534745449,0.15258215962441316,0.08650070006488414,0.29083436611503977
frozenset({'race/ethnicity_group C'}),"frozenset({'reading_cat_μέτριο', 'gender_female', 'lunch_standard'})",0.319,0.172,0.065,0.20376175548589343,1.1846613691040317,1.0,0.010132000000000009,1.0398897637795277,0.2288941601716934,0.15258215962441316,0.03835960807475053,0.29083436611503977
frozenset({'math_cat_χαμηλό'}),"frozenset({'writing_cat_μέτριο', 'gender_female'})",0.339,0.264,0.106,0.3126843657817109,1.1844104764458745,1.0,0.01650399999999999,1.070832618025751,0.23554933919447363,0.21327967806841044,0.06614723611645498,0.3570997586484312
"frozenset({'writing_cat_μέτριο', 'gender_female'})",frozenset({'math_cat_χαμηλό'}),0.264,0.339,0.106,0.4015151515151515,1.1844104764458745,1.0,0.01650399999999999,1.1044556962025316,0.21154634946677592,0.21327967806841044,0.09457662861596289,0.3570997586484312
frozenset({'lunch_standard'}),"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'gender_female', 'test preparation course_none'})",0.645,0.11,0.084,0.13023255813953488,1.1839323467230445,1.0,0.013050000000000006,1.0232620320855617,0.4376257545271632,0.12518628912071536,0.022733211392735823,0.4469344608879493
"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'gender_female', 'test preparation course_none'})",frozenset({'lunch_standard'}),0.11,0.645,0.084,0.7636363636363637,1.1839323467230445,1.0,0.013050000000000006,1.5019230769230771,0.17455858747993586,0.12518628912071536,0.3341869398207427,0.4469344608879493
"frozenset({'writing_cat_μέτριο', 'race/ethnicity_group D'})","frozenset({'gender_male', 'lunch_standard'})",0.139,0.316,0.052,0.3741007194244604,1.1838630361533555,1.0,0.008075999999999993,1.0928275862068964,0.1803805950147412,0.12903225806451613,0.08494257225798302,0.2693288407248884
"frozenset({'gender_male', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'race/ethnicity_group D'})",0.316,0.139,0.052,0.16455696202531644,1.1838630361533555,1.0,0.008075999999999993,1.0305909090909091,0.22705802968960848,0.12903225806451613,0.029682882723944752,0.2693288407248884
frozenset({'race/ethnicity_group E'}),"frozenset({'writing_cat_μέτριο', 'lunch_standard'})",0.14,0.338,0.056,0.39999999999999997,1.1834319526627217,1.0,0.008679999999999993,1.103333333333333,0.18023255813953473,0.13270142180094785,0.093655589123867,0.2828402366863905
"frozenset({'writing_cat_μέτριο', 'lunch_standard'})",frozenset({'race/ethnicity_group E'}),0.338,0.14,0.056,0.16568047337278105,1.1834319526627217,1.0,0.008679999999999993,1.0307801418439715,0.23413897280966753,0.13270142180094785,0.029861015549745393,0.2828402366863905
frozenset({'race/ethnicity_group D'}),"frozenset({'reading_cat_μέτριο', 'gender_male'})",0.262,0.229,0.071,0.2709923664122137,1.1833727790926363,1.0,0.011001999999999991,1.0576020942408377,0.20996984617733486,0.16904761904761903,0.054464807279135774,0.2905180172672422
"frozenset({'reading_cat_μέτριο', 'gender_male'})",frozenset({'race/ethnicity_group D'}),0.229,0.262,0.071,0.3100436681222707,1.1833727790926363,1.0,0.011001999999999991,1.069632911392405,0.20098280995962792,0.16904761904761903,0.0650998213038898,0.2905180172672422
"frozenset({'race/ethnicity_group C', 'gender_male'})","frozenset({'math_cat_μέτριο', 'test preparation course_none'})",0.139,0.304,0.05,0.3597122302158273,1.1832639151836426,1.0,0.007744000000000001,1.087011235955056,0.17988385598141696,0.1272264631043257,0.08004630778136111,0.2620929572131768
"frozenset({'math_cat_μέτριο', 'test preparation course_none'})","frozenset({'race/ethnicity_group C', 'gender_male'})",0.304,0.139,0.05,0.16447368421052633,1.1832639151836426,1.0,0.007744000000000001,1.030488188976378,0.22252873563218395,0.1272264631043257,0.029586160523259367,0.2620929572131768
"frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced', 'test preparation course_none'})",frozenset({'gender_male'}),0.121,0.482,0.069,0.5702479338842976,1.1830869997599536,1.0,0.010678000000000007,1.205346153846154,0.17605645413925586,0.12921348314606743,0.1703628067264433,0.356700730427626
frozenset({'gender_male'}),"frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced', 'test preparation course_none'})",0.482,0.121,0.069,0.14315352697095438,1.1830869997599536,1.0,0.010678000000000007,1.025854721549637,0.2987521683173859,0.12921348314606743,0.02520310235603456,0.356700730427626
"frozenset({'math_cat_μέτριο', 'test preparation course_none'})","frozenset({'lunch_free/reduced', 'writing_cat_μέτριο'})",0.304,0.153,0.055,0.18092105263157895,1.1824905400756796,1.0,0.008488000000000002,1.0340883534136547,0.22173458725182874,0.13681592039800997,0.032964643012489914,0.27019908840729273
"frozenset({'lunch_free/reduced', 'writing_cat_μέτριο'})","frozenset({'math_cat_μέτριο', 'test preparation course_none'})",0.153,0.304,0.055,0.35947712418300654,1.1824905400756793,1.0,0.008488000000000002,1.0866122448979592,0.18220457228721695,0.13681592039800997,0.07970851175719332,0.27019908840729273
frozenset({'reading_cat_μέτριο'}),"frozenset({'race/ethnicity_group C', 'test preparation course_none'})",0.49,0.202,0.117,0.23877551020408166,1.182056981208325,1.0,0.018020000000000008,1.0483109919571045,0.3019943019943021,0.20347826086956525,0.04608459925323515,0.40899171549808044
"frozenset({'race/ethnicity_group C', 'test preparation course_none'})",frozenset({'reading_cat_μέτριο'}),0.202,0.49,0.117,0.5792079207920792,1.1820569812083248,1.0,0.018020000000000008,1.212,0.1930038772144036,0.20347826086956525,0.17491749174917484,0.40899171549808044
"frozenset({'math_cat_χαμηλό', 'writing_cat_χαμηλό'})",frozenset({'gender_male'}),0.23,0.482,0.131,0.5695652173913044,1.1816705755006316,1.0,0.020140000000000005,1.2034343434343435,0.19966293248736003,0.22547332185886404,0.16904482121873438,0.4206747248782248
frozenset({'gender_male'}),"frozenset({'math_cat_χαμηλό', 'writing_cat_χαμηλό'})",0.482,0.23,0.131,0.27178423236514526,1.1816705755006316,1.0,0.020140000000000005,1.0573789173789174,0.29679625099472434,0.22547332185886404,0.05426523683785098,0.4206747248782248
"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο', 'gender_male'})",frozenset({'test preparation course_completed'}),0.149,0.358,0.063,0.4228187919463087,1.181058078062315,1.0,0.009658000000000007,1.1123023255813953,0.18014287579504984,0.14189189189189189,0.10096385038365847,0.2993982227887968
frozenset({'test preparation course_completed'}),"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο', 'gender_male'})",0.358,0.149,0.063,0.17597765363128492,1.181058078062315,1.0,0.009658000000000007,1.0327389830508475,0.2387875191613511,0.14189189189189189,0.03170112060080486,0.2993982227887968
frozenset({'math_cat_μέτριο'}),"frozenset({'writing_cat_μέτριο', ""parental level of education_associate's degree""})",0.485,0.11,0.063,0.12989690721649486,1.1808809746954079,1.0,0.009649999999999999,1.022867298578199,0.29742641393126823,0.11842105263157894,0.022356075524151524,0.3513120899718838
"frozenset({'writing_cat_μέτριο', ""parental level of education_associate's degree""})",frozenset({'math_cat_μέτριο'}),0.11,0.485,0.063,0.5727272727272728,1.1808809746954079,1.0,0.009649999999999999,1.2053191489361703,0.1721062957018013,0.11842105263157894,0.17034421888790827,0.3513120899718838
"frozenset({'math_cat_μέτριο', 'test preparation course_none'})",frozenset({'race/ethnicity_group D'}),0.304,0.262,0.094,0.3092105263157895,1.1801928485335476,1.0,0.014352000000000004,1.0683428571428573,0.219369038884813,0.19915254237288132,0.06397090286692342,0.33399457613499395
frozenset({'race/ethnicity_group D'}),"frozenset({'math_cat_μέτριο', 'test preparation course_none'})",0.262,0.304,0.094,0.35877862595419846,1.1801928485335476,1.0,0.014352000000000004,1.0854285714285714,0.20688462203770977,0.19915254237288132,0.0787049223479863,0.33399457613499395
frozenset({'writing_cat_μέτριο'}),"frozenset({'parental level of education_high school', 'lunch_standard'})",0.491,0.126,0.073,0.14867617107942974,1.179969611741506,1.0,0.011133999999999998,1.0266363636363636,0.2996474419355707,0.13419117647058823,0.0259452758345878,0.36402062522225453
"frozenset({'parental level of education_high school', 'lunch_standard'})",frozenset({'writing_cat_μέτριο'}),0.126,0.491,0.073,0.5793650793650793,1.1799696117415057,1.0,0.011133999999999998,1.210075471698113,0.17450863609291242,0.13419117647058823,0.1736052639785448,0.36402062522225453
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",frozenset({'race/ethnicity_group D'}),0.191,0.262,0.059,0.3089005235602094,1.1790096319091963,1.0,0.008957999999999994,1.0678636363636362,0.18767677190924126,0.149746192893401,0.06355084493253307,0.2670456816274329
frozenset({'race/ethnicity_group D'}),"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",0.262,0.191,0.059,0.22519083969465647,1.1790096319091963,1.0,0.008957999999999994,1.0441280788177338,0.2057323963070138,0.149746192893401,0.042263089857424566,0.2670456816274329
"frozenset({'parental level of education_high school', 'writing_cat_μέτριο'})",frozenset({'lunch_standard'}),0.096,0.645,0.073,0.7604166666666666,1.1789405684754521,1.0,0.011079999999999993,1.4817391304347822,0.16789913928961075,0.10928143712574849,0.32511737089201864,0.436797480620155
frozenset({'lunch_standard'}),"frozenset({'parental level of education_high school', 'writing_cat_μέτριο'})",0.645,0.096,0.073,0.1131782945736434,1.178940568475452,1.0,0.011079999999999993,1.0193706293706293,0.42755161103607925,0.10928143712574849,0.019002538245180743,0.436797480620155
frozenset({'reading_cat_χαμηλό'}),frozenset({'parental level of education_some high school'}),0.275,0.179,0.058,0.2109090909090909,1.1782630777044185,1.0,0.008774999999999998,1.0404377880184332,0.208680142687277,0.14646464646464646,0.03886612778208394,0.267465718638903
frozenset({'parental level of education_some high school'}),frozenset({'reading_cat_χαμηλό'}),0.179,0.275,0.058,0.32402234636871513,1.1782630777044185,1.0,0.008774999999999998,1.0725206611570248,0.18427905413919102,0.14646464646464646,0.0676170294740898,0.267465718638903
"frozenset({'math_cat_χαμηλό', 'test preparation course_none', 'reading_cat_χαμηλό', 'writing_cat_χαμηλό'})",frozenset({'gender_male'}),0.162,0.482,0.092,0.5679012345679012,1.1782183289790482,1.0,0.013915999999999998,1.1987999999999999,0.18050223098474627,0.16666666666666666,0.16583249916583245,0.3793863019312535
frozenset({'gender_male'}),"frozenset({'math_cat_χαμηλό', 'test preparation course_none', 'reading_cat_χαμηλό', 'writing_cat_χαμηλό'})",0.482,0.162,0.092,0.19087136929460582,1.1782183289790482,1.0,0.013915999999999998,1.0356820512820513,0.2920094007050528,0.16666666666666666,0.034452707988789764,0.3793863019312535
frozenset({'math_cat_μέτριο'}),"frozenset({""parental level of education_associate's degree"", 'reading_cat_μέτριο'})",0.485,0.112,0.064,0.13195876288659794,1.1782032400589102,1.0,0.009680000000000001,1.0229928741092635,0.2936893203883495,0.12007504690431522,0.022476084331754437,0.35169366715758466
"frozenset({""parental level of education_associate's degree"", 'reading_cat_μέτριο'})",frozenset({'math_cat_μέτριο'}),0.112,0.485,0.064,0.5714285714285714,1.1782032400589102,1.0,0.009680000000000001,1.2016666666666667,0.1703265765765766,0.12007504690431522,0.16782246879334253,0.35169366715758466
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'gender_female'})","frozenset({'race/ethnicity_group C', 'test preparation course_none'})",0.227,0.202,0.054,0.23788546255506607,1.1776508047280498,1.0,0.008145999999999994,1.047086705202312,0.19515116669062318,0.144,0.04496925132213791,0.2526060976141667
"frozenset({'race/ethnicity_group C', 'test preparation course_none'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'gender_female'})",0.202,0.227,0.054,0.2673267326732673,1.1776508047280496,1.0,0.008145999999999994,1.0550405405405405,0.1890374083356538,0.144,0.05216912376878044,0.2526060976141667
"frozenset({'gender_male', 'test preparation course_none'})",frozenset({'race/ethnicity_group D'}),0.308,0.262,0.095,0.30844155844155846,1.1772578566471696,1.0,0.014303999999999997,1.067154929577465,0.21758442348646181,0.19999999999999996,0.06292894097772148,0.3355184891444434
frozenset({'race/ethnicity_group D'}),"frozenset({'gender_male', 'test preparation course_none'})",0.262,0.308,0.095,0.36259541984732824,1.1772578566471696,1.0,0.014303999999999997,1.0856526946107783,0.20402225074882321,0.19999999999999996,0.0788951153863125,0.3355184891444434
frozenset({'race/ethnicity_group B'}),"frozenset({'math_cat_χαμηλό', 'test preparation course_none'})",0.19,0.246,0.055,0.2894736842105263,1.176722293538725,1.0,0.008260000000000003,1.0611851851851852,0.1854096520763188,0.14435695538057744,0.05765740611475641,0.25652545999144205
"frozenset({'math_cat_χαμηλό', 'test preparation course_none'})",frozenset({'race/ethnicity_group B'}),0.246,0.19,0.055,0.22357723577235772,1.176722293538725,1.0,0.008260000000000003,1.0432460732984294,0.19918013021461306,0.14435695538057744,0.04145337749673792,0.25652545999144205
"frozenset({'lunch_free/reduced', 'reading_cat_χαμηλό'})",frozenset({'test preparation course_none'}),0.139,0.642,0.105,0.7553956834532373,1.1766287904256032,1.0,0.015761999999999984,1.4635882352941167,0.17434876389580206,0.15532544378698224,0.31674771914312083,0.45947354266119805
frozenset({'test preparation course_none'}),"frozenset({'lunch_free/reduced', 'reading_cat_χαμηλό'})",0.642,0.139,0.105,0.16355140186915887,1.1766287904256032,1.0,0.015761999999999984,1.0293519553072625,0.41931364724660775,0.15532544378698224,0.028514984749313424,0.45947354266119805
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'race/ethnicity_group D'})","frozenset({'test preparation course_none', 'lunch_standard'})",0.12,0.418,0.059,0.49166666666666664,1.1762360446570972,1.0,0.00884,1.1449180327868855,0.17026194144838214,0.1231732776617954,0.12657502863688427,0.31640749601275914
"frozenset({'test preparation course_none', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'race/ethnicity_group D'})",0.418,0.12,0.059,0.14114832535885166,1.1762360446570972,1.0,0.00884,1.0246239554317549,0.25744073621061214,0.1231732776617954,0.024032187907785985,0.31640749601275914
frozenset({'test preparation course_none'}),"frozenset({'reading_cat_μέτριο', 'writing_cat_μέτριο', 'lunch_standard', 'gender_female', 'math_cat_μέτριο'})",0.642,0.098,0.074,0.11526479750778815,1.1761714031406953,1.0,0.011083999999999997,1.0195140845070423,0.41839045749660264,0.1111111111111111,0.01914057373368974,0.43518341916205733
"frozenset({'reading_cat_μέτριο', 'writing_cat_μέτριο', 'lunch_standard', 'gender_female', 'math_cat_μέτριο'})",frozenset({'test preparation course_none'}),0.098,0.642,0.074,0.7551020408163265,1.1761714031406953,1.0,0.011083999999999997,1.4618333333333329,0.16605740995984894,0.1111111111111111,0.3159274883137611,0.43518341916205733
"frozenset({'lunch_free/reduced', 'writing_cat_μέτριο', 'test preparation course_none'})",frozenset({'gender_female'}),0.087,0.518,0.053,0.6091954022988506,1.1760529001908313,1.0,0.007934000000000004,1.2333529411764708,0.16396288412655777,0.0960144927536232,0.18920207945819625,0.35575600230772647
frozenset({'gender_female'}),"frozenset({'lunch_free/reduced', 'writing_cat_μέτριο', 'test preparation course_none'})",0.518,0.087,0.053,0.1023166023166023,1.176052900190831,1.0,0.007934000000000004,1.017062365591398,0.31057699835590713,0.0960144927536232,0.016776125209860146,0.35575600230772647
"frozenset({'test preparation course_completed', 'gender_female'})",frozenset({'race/ethnicity_group C'}),0.184,0.319,0.069,0.37500000000000006,1.175548589341693,1.0,0.010304000000000008,1.0896000000000001,0.18300653594771252,0.15898617511520738,0.08223201174743032,0.2956504702194358
frozenset({'race/ethnicity_group C'}),"frozenset({'test preparation course_completed', 'gender_female'})",0.319,0.184,0.069,0.2163009404388715,1.1755485893416928,1.0,0.010304000000000008,1.0412160000000001,0.2192853646598141,0.15898617511520738,0.03958448583195036,0.2956504702194358
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'race/ethnicity_group D', 'lunch_standard'})",0.418,0.167,0.082,0.19617224880382778,1.1746841245738189,1.0,0.012193999999999997,1.0362916666666666,0.2555108540776128,0.16302186878727634,0.03502070684733225,0.343595106437842
"frozenset({'race/ethnicity_group D', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.167,0.418,0.082,0.49101796407185627,1.1746841245738189,1.0,0.012193999999999997,1.1434588235294119,0.1785201885632301,0.16302186878727634,0.12546041936745064,0.343595106437842
frozenset({'lunch_standard'}),"frozenset({'parental level of education_high school', 'math_cat_μέτριο', 'writing_cat_μέτριο'})",0.645,0.066,0.05,0.07751937984496124,1.1745360582569884,1.0,0.007429999999999999,1.0124873949579831,0.41859154929577463,0.07564296520423601,0.012333383131650147,0.4175475687103594
"frozenset({'parental level of education_high school', 'math_cat_μέτριο', 'writing_cat_μέτριο'})",frozenset({'lunch_standard'}),0.066,0.645,0.05,0.7575757575757576,1.1745360582569884,1.0,0.007429999999999999,1.464375,0.15910064239828692,0.07564296520423601,0.3171148100725565,0.4175475687103594
frozenset({'lunch_standard'}),"frozenset({'race/ethnicity_group E', 'reading_cat_μέτριο'})",0.645,0.066,0.05,0.07751937984496124,1.1745360582569884,1.0,0.007429999999999999,1.0124873949579831,0.41859154929577463,0.07564296520423601,0.012333383131650147,0.4175475687103594
"frozenset({'race/ethnicity_group E', 'reading_cat_μέτριο'})",frozenset({'lunch_standard'}),0.066,0.645,0.05,0.7575757575757576,1.1745360582569884,1.0,0.007429999999999999,1.464375,0.15910064239828692,0.07564296520423601,0.3171148100725565,0.4175475687103594
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο'})",frozenset({'race/ethnicity_group D'}),0.325,0.262,0.1,0.3076923076923077,1.1743981209630066,1.0,0.014850000000000002,1.066,0.22000000000000003,0.20533880903490762,0.061913696060037535,0.3446858485026424
frozenset({'race/ethnicity_group D'}),"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο'})",0.262,0.325,0.1,0.3816793893129771,1.1743981209630066,1.0,0.014850000000000002,1.0916666666666668,0.20121951219512196,0.20533880903490762,0.08396946564885498,0.3446858485026424
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})",frozenset({'race/ethnicity_group D'}),0.208,0.262,0.064,0.3076923076923077,1.1743981209630066,1.0,0.009503999999999999,1.066,0.18749999999999994,0.15763546798029557,0.061913696060037535,0.2759835584263065
frozenset({'race/ethnicity_group D'}),"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})",0.262,0.208,0.064,0.24427480916030533,1.1743981209630066,1.0,0.009503999999999999,1.048,0.2012195121951219,0.15763546798029557,0.04580152671755725,0.2759835584263065
"frozenset({'test preparation course_none', 'lunch_standard'})","frozenset({'race/ethnicity_group C', 'math_cat_μέτριο', 'reading_cat_μέτριο'})",0.418,0.108,0.053,0.12679425837320574,1.1740209108630162,1.0,0.007856000000000002,1.021523287671233,0.25468456201776574,0.1120507399577167,0.021069796382517644,0.30876749955697325
"frozenset({'race/ethnicity_group C', 'math_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'test preparation course_none', 'lunch_standard'})",0.108,0.418,0.053,0.49074074074074076,1.1740209108630162,1.0,0.007856000000000002,1.1428363636363637,0.16617311109230903,0.1120507399577167,0.12498409061982949,0.30876749955697325
frozenset({'reading_cat_χαμηλό'}),"frozenset({'lunch_free/reduced', 'gender_female'})",0.275,0.189,0.061,0.2218181818181818,1.1736411736411736,1.0,0.009024999999999991,1.0421728971962616,0.20407009609949103,0.15136476426799006,0.04046631543548928,0.27228475228475224
"frozenset({'lunch_free/reduced', 'gender_female'})",frozenset({'reading_cat_χαμηλό'}),0.189,0.275,0.061,0.32275132275132273,1.1736411736411734,1.0,0.009024999999999991,1.0705078125,0.18243011056982864,0.15136476426799006,0.0658638934501003,0.27228475228475224
frozenset({'test preparation course_none'}),"frozenset({'race/ethnicity_group C', 'writing_cat_χαμηλό', 'math_cat_χαμηλό'})",0.642,0.077,0.058,0.09034267912772585,1.1732815471133229,1.0,0.008566000000000004,1.0146678082191782,0.4125409362357929,0.08774583963691378,0.01445577370284491,0.42179471618723957
"frozenset({'race/ethnicity_group C', 'writing_cat_χαμηλό', 'math_cat_χαμηλό'})",frozenset({'test preparation course_none'}),0.077,0.642,0.058,0.7532467532467533,1.1732815471133229,1.0,0.008566000000000004,1.450842105263158,0.16001046064183516,0.08774583963691378,0.31074512080098676,0.42179471618723957
frozenset({'lunch_standard'}),"frozenset({'writing_cat_μέτριο', 'race/ethnicity_group E'})",0.645,0.074,0.056,0.08682170542635659,1.1732662895453594,1.0,0.00827,1.0140407470288626,0.4159959758551308,0.08446455505279035,0.01384633415373282,0.4217892310915567
"frozenset({'writing_cat_μέτριο', 'race/ethnicity_group E'})",frozenset({'lunch_standard'}),0.074,0.645,0.056,0.7567567567567568,1.1732662895453594,1.0,0.00827,1.4594444444444445,0.15948009873495833,0.08446455505279035,0.31480776551199097,0.4217892310915567
"frozenset({'reading_cat_μέτριο', 'math_cat_μέτριο'})","frozenset({'race/ethnicity_group C', 'gender_male'})",0.325,0.139,0.053,0.16307692307692306,1.1732152739346982,1.0,0.007824999999999992,1.0287683823529412,0.2187281621243883,0.1289537712895377,0.02796390601268647,0.27218594355285
"frozenset({'race/ethnicity_group C', 'gender_male'})","frozenset({'reading_cat_μέτριο', 'math_cat_μέτριο'})",0.139,0.325,0.053,0.38129496402877694,1.1732152739346982,1.0,0.007824999999999992,1.0909883720930234,0.1714767821532661,0.1289537712895377,0.08339994670929915,0.27218594355285
"frozenset({'lunch_free/reduced', 'reading_cat_μέτριο'})",frozenset({'race/ethnicity_group C'}),0.163,0.319,0.061,0.37423312883435583,1.1731446044964133,1.0,0.009002999999999997,1.0882647058823531,0.1763323344497326,0.14489311163895488,0.0811059160563228,0.2827278496836356
frozenset({'race/ethnicity_group C'}),"frozenset({'lunch_free/reduced', 'reading_cat_μέτριο'})",0.319,0.163,0.061,0.19122257053291536,1.1731446044964131,1.0,0.009002999999999997,1.0348953488372092,0.21672564454394447,0.14489311163895488,0.033718722261547616,0.2827278496836356
frozenset({'race/ethnicity_group D'}),"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'test preparation course_none'})",0.262,0.205,0.063,0.24045801526717556,1.1729659281325637,1.0,0.00929,1.0466834170854271,0.19981072826601282,0.15594059405940594,0.04460127706562964,0.2738875442189536
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'test preparation course_none'})",frozenset({'race/ethnicity_group D'}),0.205,0.262,0.063,0.3073170731707317,1.1729659281325637,1.0,0.00929,1.0654225352112676,0.18548467605071375,0.15594059405940594,0.06140524819882343,0.2738875442189536
frozenset({'reading_cat_μέτριο'}),"frozenset({'parental level of education_high school', 'gender_female'})",0.49,0.094,0.054,0.11020408163265306,1.1723838471558836,1.0,0.007940000000000003,1.018211009174312,0.28830791575889625,0.10188679245283021,0.017885299815290356,0.342336083369518
"frozenset({'parental level of education_high school', 'gender_female'})",frozenset({'reading_cat_μέτριο'}),0.094,0.49,0.054,0.5744680851063829,1.1723838471558836,1.0,0.007940000000000003,1.1985,0.16229253536096808,0.10188679245283021,0.16562369628702533,0.342336083369518
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'parental level of education_some college', 'lunch_standard'})",0.325,0.147,0.056,0.1723076923076923,1.1721611721611722,1.0,0.008225000000000003,1.0305762081784386,0.21759259259259267,0.13461538461538464,0.02966904139237082,0.27663003663003666
"frozenset({'parental level of education_some college', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο'})",0.147,0.325,0.056,0.380952380952381,1.1721611721611722,1.0,0.008225000000000003,1.0903846153846155,0.17218640093786644,0.13461538461538464,0.08289241622574958,0.27663003663003666
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})",frozenset({'race/ethnicity_group D'}),0.228,0.262,0.07,0.30701754385964913,1.1718226864872103,1.0,0.010264000000000002,1.0649620253164558,0.1899333826794967,0.16666666666666669,0.0609993819236438,0.2870965581893665
frozenset({'race/ethnicity_group D'}),"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})",0.262,0.228,0.07,0.26717557251908397,1.1718226864872103,1.0,0.010264000000000002,1.0534583333333334,0.19868370112272554,0.16666666666666669,0.05074556025788078,0.2870965581893665
frozenset({'math_cat_μέτριο'}),"frozenset({'parental level of education_some high school', 'lunch_standard'})",0.485,0.118,0.067,0.13814432989690723,1.1707146601432816,1.0,0.009770000000000008,1.0233732057416267,0.2831473699463848,0.125,0.022839376300348335,0.35297047003319937
"frozenset({'gender_female', 'parental level of education_some college'})",frozenset({'math_cat_μέτριο'}),0.118,0.485,0.067,0.5677966101694916,1.1707146601432816,1.0,0.009770000000000008,1.1915686274509805,0.1653298135174469,0.125,0.16077011683396422,0.35297047003319937
frozenset({'math_cat_μέτριο'}),"frozenset({'gender_female', 'parental level of education_some college'})",0.485,0.118,0.067,0.13814432989690723,1.1707146601432816,1.0,0.009770000000000008,1.0233732057416267,0.2831473699463848,0.125,0.022839376300348335,0.35297047003319937
"frozenset({'parental level of education_some high school', 'lunch_standard'})",frozenset({'math_cat_μέτριο'}),0.118,0.485,0.067,0.5677966101694916,1.1707146601432816,1.0,0.009770000000000008,1.1915686274509805,0.1653298135174469,0.125,0.16077011683396422,0.35297047003319937
"frozenset({'math_cat_χαμηλό', 'reading_cat_χαμηλό'})",frozenset({'gender_male'}),0.218,0.482,0.123,0.5642201834862385,1.1705812935399138,1.0,0.017923999999999995,1.188673684210526,0.18634728546773952,0.21317157712305027,0.15872622294640637,0.40970345273896985
frozenset({'gender_male'}),"frozenset({'math_cat_χαμηλό', 'reading_cat_χαμηλό'})",0.482,0.218,0.123,0.2551867219917012,1.1705812935399138,1.0,0.017923999999999995,1.0499275766016714,0.2813196471733056,0.21317157712305027,0.04755335293056423,0.40970345273896985
"frozenset({'race/ethnicity_group C', 'gender_female'})","frozenset({'reading_cat_μέτριο', 'test preparation course_none'})",0.18,0.318,0.067,0.37222222222222223,1.1705101327742837,1.0,0.009760000000000005,1.0863716814159292,0.17764834364761564,0.1554524361948956,0.079504724666015,0.2914570230607967
"frozenset({'reading_cat_μέτριο', 'test preparation course_none'})","frozenset({'race/ethnicity_group C', 'gender_female'})",0.318,0.18,0.067,0.21069182389937108,1.1705101327742837,1.0,0.009760000000000005,1.0388844621513946,0.2135947826848165,0.1554524361948956,0.03742905353581839,0.2914570230607967
"frozenset({'gender_female', 'reading_cat_υψηλό'})",frozenset({'lunch_standard'}),0.159,0.645,0.12,0.7547169811320754,1.170103846716396,1.0,0.01744499999999999,1.447307692307692,0.17285969084423294,0.17543859649122806,0.3090619186819025,0.4703817463799912
frozenset({'lunch_standard'}),"frozenset({'gender_female', 'reading_cat_υψηλό'})",0.645,0.159,0.12,0.18604651162790697,1.170103846716396,1.0,0.01744499999999999,1.0332285714285714,0.40950704225352086,0.17543859649122806,0.03215994248264801,0.4703817463799912
"frozenset({'parental level of education_some college', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})",0.147,0.32,0.055,0.3741496598639456,1.16921768707483,1.0,0.007960000000000002,1.0865217391304347,0.16966854950442295,0.13349514563106796,0.07963185274109648,0.2730123299319728
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})","frozenset({'parental level of education_some college', 'lunch_standard'})",0.32,0.147,0.055,0.171875,1.16921768707483,1.0,0.007960000000000002,1.0300377358490567,0.21283422459893056,0.13349514563106796,0.029161781946072694,0.2730123299319728
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'gender_male', 'race/ethnicity_group D'})",0.418,0.133,0.065,0.15550239234449761,1.1691909198834407,1.0,0.009406000000000005,1.0266458923512747,0.24863864657679097,0.13374485596707822,0.025954316429639687,0.3221120984278879
"frozenset({'gender_male', 'race/ethnicity_group D'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.133,0.418,0.065,0.48872180451127817,1.1691909198834407,1.0,0.009406000000000005,1.1383235294117648,0.1669062195013753,0.13374485596707822,0.12151512802625117,0.3221120984278879
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})","frozenset({'gender_female', 'lunch_standard'})",0.208,0.329,0.08,0.38461538461538464,1.1690437222352117,1.0,0.011567999999999995,1.090375,0.1825757575757575,0.175054704595186,0.08288432878596813,0.3138882394201543
"frozenset({'gender_female', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})",0.329,0.208,0.08,0.24316109422492402,1.1690437222352117,1.0,0.011567999999999995,1.0464578313253012,0.21549925484351704,0.175054704595186,0.04439532099106569,0.3138882394201543
"frozenset({'gender_female', 'lunch_standard'})","frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο'})",0.329,0.169,0.065,0.19756838905775076,1.1690437222352115,1.0,0.009398999999999998,1.0356022727272727,0.2154992548435171,0.15011547344110854,0.03437832618261221,0.29109188683656767
"frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο'})","frozenset({'gender_female', 'lunch_standard'})",0.169,0.329,0.065,0.3846153846153846,1.1690437222352115,1.0,0.009398999999999998,1.090375,0.17400722021660645,0.15011547344110854,0.08288432878596805,0.29109188683656767
"frozenset({'gender_female', 'test preparation course_none', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.215,0.418,0.105,0.4883720930232558,1.1683542895293202,1.0,0.015130000000000005,1.1375454545454546,0.18356081286017598,0.19886363636363635,0.12091424918085189,0.3697841326360298
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'gender_female', 'test preparation course_none', 'lunch_standard'})",0.418,0.215,0.105,0.2511961722488038,1.16835428952932,1.0,0.015130000000000005,1.048338658146965,0.24758631975126827,0.19886363636363635,0.04610977356535517,0.3697841326360298
frozenset({'math_cat_χαμηλό'}),"frozenset({'race/ethnicity_group C', 'test preparation course_none'})",0.339,0.202,0.08,0.2359882005899705,1.16825841876223,1.0,0.01152199999999999,1.0444864864864865,0.2178895612708016,0.1735357917570499,0.042591730062619654,0.3160139022751832
"frozenset({'race/ethnicity_group C', 'test preparation course_none'})",frozenset({'math_cat_χαμηλό'}),0.202,0.339,0.08,0.396039603960396,1.16825841876223,1.0,0.01152199999999999,1.0944426229508195,0.18048245614035072,0.1735357917570499,0.08629289555279271,0.3160139022751832
"frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced', 'gender_male'})",frozenset({'test preparation course_none'}),0.092,0.642,0.069,0.7500000000000001,1.1682242990654208,1.0,0.009936000000000007,1.4320000000000006,0.15859030837004415,0.1037593984962406,0.3016759776536316,0.42873831775700944
frozenset({'test preparation course_none'}),"frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced', 'gender_male'})",0.642,0.092,0.069,0.1074766355140187,1.1682242990654208,1.0,0.009936000000000007,1.0173403141361257,0.4022346368715086,0.1037593984962406,0.017044752768743066,0.42873831775700944
"frozenset({'math_cat_χαμηλό', 'lunch_standard'})",frozenset({'gender_female'}),0.157,0.518,0.095,0.6050955414012739,1.16813811081327,1.0,0.013673999999999992,1.2205483870967742,0.1707435849410001,0.16379310344827586,0.18069614398604533,0.3942466123994786
frozenset({'gender_female'}),"frozenset({'math_cat_χαμηλό', 'lunch_standard'})",0.518,0.157,0.095,0.1833976833976834,1.16813811081327,1.0,0.013673999999999992,1.0323262411347518,0.2986241537453591,0.16379310344827586,0.03131397793319501,0.3942466123994786
"frozenset({'writing_cat_υψηλό', 'gender_female'})",frozenset({'lunch_standard'}),0.154,0.645,0.116,0.7532467532467533,1.1678244236383772,1.0,0.016670000000000004,1.4386842105263158,0.16986630798076144,0.1698389458272328,0.3049204316809951,0.4665458572435317
frozenset({'lunch_standard'}),"frozenset({'writing_cat_υψηλό', 'gender_female'})",0.645,0.154,0.116,0.17984496124031008,1.167824423638377,1.0,0.016670000000000004,1.0315122873345937,0.40480815930063146,0.1698389458272328,0.030549599574834607,0.4665458572435317
frozenset({'gender_female'}),frozenset({'math_cat_χαμηλό'}),0.518,0.339,0.205,0.39575289575289574,1.1674126718374505,1.0,0.02939799999999998,1.0939233226837062,0.29752049387713775,0.3144171779141104,0.08585914637351848,0.5002363298823476
frozenset({'math_cat_χαμηλό'}),frozenset({'gender_female'}),0.339,0.518,0.205,0.6047197640117993,1.1674126718374502,1.0,0.02939799999999998,1.219388059701492,0.2169514040072321,0.3144171779141104,0.17991652284605655,0.5002363298823476
"frozenset({'lunch_free/reduced', 'math_cat_μέτριο'})","frozenset({'writing_cat_μέτριο', 'test preparation course_none'})",0.153,0.308,0.055,0.35947712418300654,1.1671335200746966,1.0,0.007876000000000001,1.0803673469387756,0.16906729634002363,0.1354679802955665,0.07438890777891119,0.26902427637721754
"frozenset({'writing_cat_μέτριο', 'test preparation course_none'})","frozenset({'lunch_free/reduced', 'math_cat_μέτριο'})",0.308,0.153,0.055,0.17857142857142858,1.1671335200746966,1.0,0.007876000000000001,1.0311304347826087,0.20693641618497113,0.1354679802955665,0.030190588632147082,0.26902427637721754
frozenset({'race/ethnicity_group B'}),"frozenset({'math_cat_χαμηλό', 'writing_cat_χαμηλό'})",0.19,0.23,0.051,0.26842105263157895,1.1670480549199085,1.0,0.007299999999999994,1.0525179856115108,0.17671266037279096,0.1382113821138211,0.04989747095010252,0.24508009153318078
"frozenset({'math_cat_χαμηλό', 'writing_cat_χαμηλό'})",frozenset({'race/ethnicity_group B'}),0.23,0.19,0.051,0.22173913043478258,1.1670480549199083,1.0,0.007299999999999994,1.040782122905028,0.18589253883371515,0.1382113821138211,0.03918411164787972,0.24508009153318078
frozenset({'parental level of education_high school'}),"frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",0.196,0.223,0.051,0.260204081632653,1.1668344467831975,1.0,0.007291999999999993,1.0502896551724137,0.1778363086528142,0.1385869565217391,0.04788170094292537,0.24445181660107987
"frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",frozenset({'parental level of education_high school'}),0.223,0.196,0.051,0.22869955156950672,1.1668344467831975,1.0,0.007291999999999993,1.0423953488372093,0.1840159487218309,0.1385869565217391,0.040671084041675014,0.24445181660107987
"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced'})",frozenset({'gender_female'}),0.182,0.518,0.11,0.6043956043956045,1.1667868810725954,1.0,0.015724000000000002,1.218388888888889,0.17474994443209604,0.1864406779661017,0.17924399252200093,0.4083754083754084
frozenset({'gender_female'}),"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced'})",0.518,0.182,0.11,0.21235521235521235,1.1667868810725954,1.0,0.015724000000000002,1.0385392156862747,0.29656733308185595,0.1864406779661017,0.0371090615589393,0.4083754083754084
frozenset({'gender_female'}),"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο'})",0.518,0.154,0.093,0.17953667953667954,1.165822594394023,1.0,0.013228000000000004,1.031124705882353,0.295096595725695,0.16062176165803108,0.03018520039796636,0.3917163917163917
"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο'})",frozenset({'gender_female'}),0.154,0.518,0.093,0.6038961038961039,1.165822594394023,1.0,0.013228000000000004,1.2168524590163936,0.16812832049619975,0.16062176165803108,0.17820768443175086,0.3917163917163917
frozenset({'test preparation course_completed'}),"frozenset({'lunch_free/reduced', 'reading_cat_μέτριο'})",0.358,0.163,0.068,0.1899441340782123,1.1653014360626521,1.0,0.009646000000000002,1.0332620689655172,0.22095473703500093,0.1501103752759382,0.03219131908985939,0.3035610240943209
"frozenset({'lunch_free/reduced', 'reading_cat_μέτριο'})",frozenset({'test preparation course_completed'}),0.163,0.358,0.068,0.4171779141104295,1.1653014360626521,1.0,0.009646000000000002,1.1015368421052631,0.16947782697308317,0.1501103752759382,0.09217743630908021,0.3035610240943209
"frozenset({'math_cat_χαμηλό', 'gender_male', 'writing_cat_χαμηλό'})",frozenset({'test preparation course_none'}),0.131,0.642,0.098,0.7480916030534351,1.165251718151768,1.0,0.013897999999999994,1.4211515151515153,0.16319485216411067,0.1451851851851852,0.29634525992579647,0.4503697890656584
frozenset({'test preparation course_none'}),"frozenset({'math_cat_χαμηλό', 'gender_male', 'writing_cat_χαμηλό'})",0.642,0.131,0.098,0.1526479750778816,1.1652517181517679,1.0,0.013897999999999994,1.0255477941176472,0.3961349903089726,0.1451851851851852,0.024911363725985736,0.4503697890656584
"frozenset({'writing_cat_χαμηλό', 'test preparation course_none', 'reading_cat_χαμηλό'})","frozenset({'gender_male', 'lunch_standard'})",0.201,0.316,0.074,0.36815920398009944,1.1650607720889223,1.0,0.010483999999999993,1.0825511811023618,0.17731623989446257,0.16704288939051917,0.07625614616973601,0.3011682095849864
"frozenset({'gender_male', 'lunch_standard'})","frozenset({'writing_cat_χαμηλό', 'test preparation course_none', 'reading_cat_χαμηλό'})",0.316,0.201,0.074,0.23417721518987342,1.1650607720889223,1.0,0.010483999999999993,1.0433223140495866,0.20712818081239123,0.16704288939051917,0.04152342326642479,0.3011682095849864
"frozenset({'math_cat_χαμηλό', 'gender_male', 'reading_cat_χαμηλό', 'writing_cat_χαμηλό'})",frozenset({'test preparation course_none'}),0.123,0.642,0.092,0.7479674796747967,1.165058379555758,1.0,0.013034000000000004,1.4204516129032256,0.16154380050567652,0.13670133729569092,0.2959985465776444,0.4456348301800775
frozenset({'test preparation course_none'}),"frozenset({'math_cat_χαμηλό', 'gender_male', 'reading_cat_χαμηλό'})",0.642,0.123,0.092,0.14330218068535824,1.165058379555758,1.0,0.013034000000000004,1.0236981818181818,0.39573718727228574,0.13670133729569092,0.023149578888663907,0.4456348301800775
frozenset({'test preparation course_none'}),"frozenset({'math_cat_χαμηλό', 'gender_male', 'reading_cat_χαμηλό', 'writing_cat_χαμηλό'})",0.642,0.123,0.092,0.14330218068535824,1.165058379555758,1.0,0.013034000000000004,1.0236981818181818,0.39573718727228574,0.13670133729569092,0.023149578888663907,0.4456348301800775
"frozenset({'math_cat_χαμηλό', 'gender_male', 'reading_cat_χαμηλό'})",frozenset({'test preparation course_none'}),0.123,0.642,0.092,0.7479674796747967,1.165058379555758,1.0,0.013034000000000004,1.4204516129032256,0.16154380050567652,0.13670133729569092,0.2959985465776444,0.4456348301800775
"frozenset({'math_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",frozenset({'race/ethnicity_group D'}),0.213,0.262,0.065,0.3051643192488263,1.1647493101100241,1.0,0.009194,1.0621216216216216,0.17972827680578632,0.15853658536585366,0.058488237464534285,0.2766279611511307
frozenset({'race/ethnicity_group D'}),"frozenset({'math_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",0.262,0.213,0.065,0.2480916030534351,1.164749310110024,1.0,0.009194,1.0466700507614215,0.19166145507608925,0.15853658536585366,0.04458907630677905,0.2766279611511307
"frozenset({'lunch_free/reduced', 'reading_cat_χαμηλό'})",frozenset({'gender_male'}),0.139,0.482,0.078,0.5611510791366906,1.1642138571300635,1.0,0.011001999999999998,1.1803606557377049,0.16382262723726138,0.143646408839779,0.1528013110746923,0.3614884026388847
frozenset({'gender_male'}),"frozenset({'lunch_free/reduced', 'reading_cat_χαμηλό'})",0.482,0.139,0.078,0.16182572614107885,1.1642138571300635,1.0,0.011001999999999998,1.0272326732673267,0.27229977229977226,0.143646408839779,0.026510715611009098,0.3614884026388847
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'race/ethnicity_group C', 'gender_female', 'test preparation course_none'})",0.418,0.111,0.054,0.1291866028708134,1.163843269106427,1.0,0.007601999999999998,1.0208846153846154,0.24188621611302014,0.11368421052631578,0.020457371058282794,0.30783654467864996
"frozenset({'race/ethnicity_group C', 'gender_female', 'test preparation course_none'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.111,0.418,0.054,0.48648648648648646,1.1638432691064269,1.0,0.007601999999999998,1.1333684210526316,0.15835520559930003,0.11368421052631578,0.11767437540633413,0.30783654467864996
"frozenset({'gender_male', 'race/ethnicity_group D'})",frozenset({'writing_cat_μέτριο'}),0.133,0.491,0.076,0.5714285714285714,1.1638056444573757,1.0,0.010696999999999998,1.1876666666666666,0.1623414071510957,0.1386861313868613,0.1580129104687061,0.3631073610707012
frozenset({'writing_cat_μέτριο'}),"frozenset({'gender_male', 'race/ethnicity_group D'})",0.491,0.133,0.076,0.15478615071283094,1.1638056444573754,1.0,0.010696999999999998,1.0257759036144578,0.2765225933202357,0.1386861313868613,0.025128201514222535,0.3631073610707012
"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'gender_female', 'test preparation course_none'})",0.139,0.334,0.054,0.3884892086330935,1.1631413432128548,1.0,0.0075739999999999905,1.0891058823529411,0.16290274013851228,0.12887828162291168,0.08181562857821845,0.27508292766984016
"frozenset({'gender_female', 'test preparation course_none'})","frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.334,0.139,0.054,0.16167664670658682,1.1631413432128548,1.0,0.0075739999999999905,1.02705,0.21059948837726591,0.12887828162291168,0.02633756876490919,0.27508292766984016
frozenset({'lunch_standard'}),"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})",0.645,0.18,0.135,0.20930232558139536,1.1627906976744187,1.0,0.018900000000000014,1.0370588235294118,0.39436619718309884,0.19565217391304351,0.03573454339194557,0.47965116279069775
"frozenset({'test preparation course_completed', 'writing_cat_υψηλό', 'reading_cat_υψηλό'})",frozenset({'lunch_standard'}),0.104,0.645,0.078,0.75,1.1627906976744187,1.0,0.01092,1.42,0.15624999999999997,0.11624441132637853,0.2957746478873239,0.43546511627906975
"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})",frozenset({'lunch_standard'}),0.18,0.645,0.135,0.7500000000000001,1.1627906976744187,1.0,0.018900000000000014,1.4200000000000006,0.17073170731707327,0.19565217391304351,0.2957746478873242,0.47965116279069775
frozenset({'lunch_standard'}),"frozenset({'test preparation course_completed', 'writing_cat_υψηλό', 'reading_cat_υψηλό'})",0.645,0.104,0.078,0.12093023255813953,1.1627906976744187,1.0,0.01092,1.0192592592592593,0.39436619718309857,0.11624441132637853,0.018895348837209305,0.43546511627906975
frozenset({'lunch_standard'}),"frozenset({'writing_cat_υψηλό', 'gender_female', 'reading_cat_υψηλό'})",0.645,0.136,0.102,0.15813953488372093,1.1627906976744184,1.0,0.014279999999999987,1.0262983425414365,0.39436619718309823,0.150220913107511,0.025624461670973285,0.4540697674418604
"frozenset({'writing_cat_υψηλό', 'gender_female', 'reading_cat_υψηλό'})",frozenset({'lunch_standard'}),0.136,0.645,0.102,0.7499999999999999,1.1627906976744184,1.0,0.014279999999999987,1.4199999999999993,0.1620370370370369,0.150220913107511,0.2957746478873236,0.4540697674418604
frozenset({'lunch_standard'}),"frozenset({'parental level of education_high school', 'reading_cat_μέτριο', 'test preparation course_none'})",0.645,0.068,0.051,0.07906976744186046,1.1627906976744184,1.0,0.007139999999999994,1.012020202020202,0.39436619718309823,0.0770392749244713,0.011877432877532682,0.41453488372093017
"frozenset({'parental level of education_high school', 'reading_cat_μέτριο', 'test preparation course_none'})",frozenset({'lunch_standard'}),0.068,0.645,0.051,0.7499999999999999,1.1627906976744184,1.0,0.007139999999999994,1.4199999999999993,0.150214592274678,0.0770392749244713,0.2957746478873236,0.41453488372093017
"frozenset({'lunch_free/reduced', 'math_cat_μέτριο', 'test preparation course_none'})",frozenset({'gender_male'}),0.091,0.482,0.051,0.5604395604395604,1.162737677260499,1.0,0.0071379999999999985,1.17845,0.15397225997109512,0.09770114942528736,0.15142772285629433,0.3331243445351329
frozenset({'gender_male'}),"frozenset({'lunch_free/reduced', 'math_cat_μέτριο', 'test preparation course_none'})",0.482,0.091,0.051,0.10580912863070539,1.162737677260499,1.0,0.0071379999999999985,1.0165614849187934,0.2701945643122113,0.09770114942528736,0.016291670660842018,0.3331243445351329
frozenset({'math_cat_μέτριο'}),"frozenset({'gender_male', 'race/ethnicity_group D'})",0.485,0.133,0.075,0.15463917525773196,1.1627005658476086,1.0,0.01049499999999999,1.0255975609756098,0.27171521035598684,0.13812154696132595,0.024958679651363273,0.35927447484691105
"frozenset({'gender_male', 'race/ethnicity_group D'})",frozenset({'math_cat_μέτριο'}),0.133,0.485,0.075,0.5639097744360901,1.1627005658476086,1.0,0.01049499999999999,1.1809482758620689,0.16139946174548236,0.13812154696132595,0.15322286298269933,0.35927447484691105
"frozenset({'race/ethnicity_group C', 'lunch_standard'})",frozenset({'reading_cat_υψηλό'}),0.205,0.235,0.056,0.2731707317073171,1.1624286455630515,1.0,0.007825000000000006,1.05251677852349,0.17576370170709804,0.14583333333333334,0.04989638131675439,0.2557343020238713
frozenset({'reading_cat_υψηλό'}),"frozenset({'race/ethnicity_group C', 'lunch_standard'})",0.235,0.205,0.056,0.23829787234042554,1.1624286455630515,1.0,0.007825000000000006,1.0437150837988827,0.18265639589169014,0.14583333333333334,0.04188411615147868,0.2557343020238713
frozenset({'race/ethnicity_group D'}),"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})",0.262,0.266,0.081,0.30916030534351147,1.1622567870056821,1.0,0.011307999999999999,1.062475138121547,0.18916658302385492,0.18120805369127518,0.058801505917590535,0.3068357917695001
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})",frozenset({'race/ethnicity_group D'}),0.266,0.262,0.081,0.30451127819548873,1.1622567870056821,1.0,0.011307999999999999,1.0611243243243242,0.1901974635852928,0.18120805369127518,0.0576033579884671,0.3068357917695001
frozenset({'gender_female'}),"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'parental level of education_some college'})",0.518,0.098,0.059,0.1138996138996139,1.1622409581593254,1.0,0.008235999999999993,1.0179433551198258,0.28961249032984016,0.10592459605026931,0.017627066407554202,0.3579702151130722
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'parental level of education_some college'})",frozenset({'gender_female'}),0.098,0.518,0.059,0.6020408163265305,1.1622409581593252,1.0,0.008235999999999993,1.2111794871794868,0.15475966778157754,0.10592459605026931,0.17435854009653629,0.3579702151130722
"frozenset({'parental level of education_high school', 'lunch_standard'})",frozenset({'test preparation course_none'}),0.126,0.642,0.094,0.746031746031746,1.1620432181179845,1.0,0.013107999999999995,1.4096250000000001,0.15955012415404832,0.13946587537091987,0.2905914693624191,0.44622459575730605
frozenset({'test preparation course_none'}),"frozenset({'parental level of education_high school', 'lunch_standard'})",0.642,0.126,0.094,0.14641744548286603,1.1620432181179843,1.0,0.013107999999999995,1.023919708029197,0.38951622489005094,0.13946587537091987,0.023360921605109875,0.44622459575730605
frozenset({'gender_female'}),"frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο', 'lunch_standard'})",0.518,0.108,0.065,0.12548262548262548,1.161876161876162,1.0,0.009056000000000002,1.019991169977925,0.289052026811363,0.11586452762923352,0.019599355922225878,0.36366723866723866
"frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο', 'lunch_standard'})",frozenset({'gender_female'}),0.108,0.518,0.065,0.6018518518518519,1.161876161876162,1.0,0.009056000000000002,1.2106046511627906,0.1561917902725078,0.11586452762923352,0.17396649761794988,0.36366723866723866
frozenset({'lunch_standard'}),"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none', 'gender_male'})",0.645,0.115,0.086,0.13333333333333333,1.1594202898550725,1.0,0.011824999999999988,1.021153846153846,0.38732394366197154,0.12759643916913946,0.0207156308851224,0.44057971014492747
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none', 'gender_male'})",frozenset({'lunch_standard'}),0.115,0.645,0.086,0.7478260869565216,1.1594202898550723,1.0,0.011824999999999988,1.4077586206896546,0.15536723163841792,0.12759643916913946,0.28965094917330036,0.44057971014492747
"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'gender_female', 'lunch_standard'})",0.139,0.329,0.053,0.38129496402877694,1.1589512584461306,1.0,0.007268999999999991,1.0845232558139535,0.15929261718493176,0.12771084337349395,0.07793586293409377,0.27119459447639455
"frozenset({'gender_female', 'lunch_standard'})","frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.329,0.139,0.053,0.16109422492401215,1.1589512584461306,1.0,0.007268999999999991,1.026336956521739,0.20439782920451002,0.12771084337349395,0.025661120701523973,0.27119459447639455
"frozenset({""parental level of education_associate's degree"", 'test preparation course_none'})",frozenset({'math_cat_χαμηλό'}),0.14,0.339,0.055,0.39285714285714285,1.1588706278971765,1.0,0.007539999999999991,1.0887058823529412,0.1594080338266383,0.12971698113207547,0.08147827966284844,0.2775495153813738
frozenset({'math_cat_χαμηλό'}),"frozenset({""parental level of education_associate's degree"", 'test preparation course_none'})",0.339,0.14,0.055,0.1622418879056047,1.1588706278971763,1.0,0.007539999999999991,1.0265492957746478,0.20739925732361408,0.12971698113207547,0.02586266035535429,0.2775495153813738
"frozenset({'test preparation course_none', 'race/ethnicity_group D'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})",0.18,0.283,0.059,0.3277777777777778,1.1582253631723598,1.0,0.008060000000000005,1.066611570247934,0.16659776767259207,0.14603960396039603,0.06245157291182399,0.26812917157440125
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})","frozenset({'test preparation course_none', 'race/ethnicity_group D'})",0.283,0.18,0.059,0.20848056537102475,1.1582253631723598,1.0,0.008060000000000005,1.035982142857143,0.19053022244285286,0.14603960396039603,0.03473239679393262,0.26812917157440125
frozenset({'test preparation course_none'}),"frozenset({'math_cat_χαμηλό', ""parental level of education_associate's degree""})",0.642,0.074,0.055,0.08566978193146417,1.157699755830597,1.0,0.007491999999999999,1.012763202725724,0.3804977145759268,0.08320726172465961,0.012602356297477508,0.4144565125873537
"frozenset({'math_cat_χαμηλό', ""parental level of education_associate's degree""})",frozenset({'test preparation course_none'}),0.074,0.642,0.055,0.7432432432432433,1.157699755830597,1.0,0.007491999999999999,1.3943157894736846,0.14710386805419198,0.08320726172465961,0.28280235542805393,0.4144565125873537
frozenset({'lunch_standard'}),"frozenset({'reading_cat_μέτριο', 'math_cat_μέτριο', 'gender_female'})",0.645,0.15,0.112,0.17364341085271318,1.157622739018088,1.0,0.01525,1.0286116322701688,0.3835513078470825,0.16398243045387995,0.027815777473780213,0.46015503875968994
"frozenset({'reading_cat_μέτριο', 'math_cat_μέτριο', 'gender_female'})",frozenset({'lunch_standard'}),0.15,0.645,0.112,0.7466666666666667,1.1576227390180878,1.0,0.01525,1.4013157894736843,0.16018907563025211,0.16398243045387995,0.2863849765258217,0.46015503875968994
frozenset({'lunch_standard'}),"frozenset({'race/ethnicity_group E', 'gender_male'})",0.645,0.071,0.053,0.08217054263565891,1.1573315864177314,1.0,0.007205000000000003,1.012170608108108,0.382939144299761,0.0799396681749623,0.012024265485101097,0.4143247079375478
"frozenset({'race/ethnicity_group E', 'gender_male'})",frozenset({'lunch_standard'}),0.071,0.645,0.053,0.7464788732394366,1.1573315864177312,1.0,0.007205000000000003,1.4002777777777777,0.14633304222434354,0.0799396681749623,0.2858559809561595,0.4143247079375478
"frozenset({'gender_female', 'test preparation course_none'})","frozenset({'math_cat_χαμηλό', 'lunch_free/reduced', 'writing_cat_χαμηλό'})",0.334,0.132,0.051,0.1526946107784431,1.1567773543821447,1.0,0.0069119999999999945,1.024424028268551,0.20349761526232102,0.12289156626506022,0.023841717486685586,0.2695291235710397
"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced', 'writing_cat_χαμηλό'})","frozenset({'gender_female', 'test preparation course_none'})",0.132,0.334,0.051,0.3863636363636363,1.1567773543821445,1.0,0.0069119999999999945,1.085333333333333,0.1561398753049606,0.12289156626506022,0.07862407862407851,0.2695291235710397
"frozenset({'gender_male', 'race/ethnicity_group D'})","frozenset({'writing_cat_μέτριο', 'lunch_standard'})",0.133,0.338,0.052,0.39097744360902253,1.1567379988432618,1.0,0.00704599999999999,1.0869876543209875,0.1562860438292962,0.1241050119331742,0.08002634986257177,0.2724117987275882
"frozenset({'writing_cat_μέτριο', 'lunch_standard'})","frozenset({'gender_male', 'race/ethnicity_group D'})",0.338,0.133,0.052,0.15384615384615383,1.1567379988432618,1.0,0.00704599999999999,1.0246363636363636,0.20468277945619306,0.1241050119331742,0.02404400674296865,0.2724117987275882
"frozenset({'writing_cat_μέτριο', 'gender_male'})",frozenset({'test preparation course_completed'}),0.227,0.358,0.094,0.41409691629955947,1.156695296926144,1.0,0.012733999999999995,1.0957443609022557,0.1752497866835484,0.1914460285132383,0.08737837429837926,0.33833337435089705
frozenset({'test preparation course_completed'}),"frozenset({'writing_cat_μέτριο', 'gender_male'})",0.358,0.227,0.094,0.26256983240223464,1.1566952969261437,1.0,0.012733999999999995,1.0482348484848485,0.21100947835885192,0.1914460285132383,0.046015307118026684,0.33833337435089705
frozenset({'test preparation course_none'}),"frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced', 'math_cat_χαμηλό'})",0.642,0.132,0.098,0.1526479750778816,1.1564240536203152,1.0,0.013256000000000004,1.0243676470588234,0.377836050621366,0.14497041420118342,0.023787989721061754,0.44753610875106203
"frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced', 'math_cat_χαμηλό'})",frozenset({'test preparation course_none'}),0.132,0.642,0.098,0.7424242424242424,1.1564240536203152,1.0,0.013256000000000004,1.3898823529411763,0.1558356061318537,0.14497041420118342,0.2805146436431353,0.44753610875106203
frozenset({'writing_cat_μέτριο'}),"frozenset({'gender_female', 'parental level of education_some college'})",0.491,0.118,0.067,0.1364562118126273,1.1564085746832822,1.0,0.009062000000000008,1.021372641509434,0.26572442307128424,0.12361623616236163,0.020925410218398298,0.35212641099105946
"frozenset({'gender_female', 'parental level of education_some college'})",frozenset({'writing_cat_μέτριο'}),0.118,0.491,0.067,0.5677966101694916,1.1564085746832822,1.0,0.009062000000000008,1.177686274509804,0.15334890174975477,0.12361623616236163,0.15087742665911902,0.35212641099105946
"frozenset({'parental level of education_some high school', 'lunch_standard'})",frozenset({'writing_cat_μέτριο'}),0.118,0.491,0.067,0.5677966101694916,1.1564085746832822,1.0,0.009062000000000008,1.177686274509804,0.15334890174975477,0.12361623616236163,0.15087742665911902,0.35212641099105946
frozenset({'writing_cat_μέτριο'}),"frozenset({'parental level of education_some high school', 'lunch_standard'})",0.491,0.118,0.067,0.1364562118126273,1.1564085746832822,1.0,0.009062000000000008,1.021372641509434,0.26572442307128424,0.12361623616236163,0.020925410218398298,0.35212641099105946
frozenset({'race/ethnicity_group B'}),"frozenset({'writing_cat_μέτριο', 'gender_female'})",0.19,0.264,0.058,0.30526315789473685,1.1562998405103668,1.0,0.00784,1.0593939393939396,0.16687952320136226,0.14646464646464646,0.056064073226544615,0.26248006379585326
"frozenset({'writing_cat_μέτριο', 'gender_female'})",frozenset({'race/ethnicity_group B'}),0.264,0.19,0.058,0.2196969696969697,1.1562998405103668,1.0,0.00784,1.0380582524271846,0.1836581709145427,0.14646464646464646,0.03666292555181443,0.26248006379585326
"frozenset({'gender_female', 'test preparation course_none', 'lunch_standard'})",frozenset({'writing_cat_μέτριο'}),0.215,0.491,0.122,0.5674418604651162,1.1556860701937195,1.0,0.016435000000000005,1.1767204301075267,0.17160906338101706,0.2089041095890411,0.15018047242655452,0.407957182778383
frozenset({'writing_cat_μέτριο'}),"frozenset({'gender_female', 'test preparation course_none', 'lunch_standard'})",0.491,0.215,0.122,0.2484725050916497,1.1556860701937195,1.0,0.016435000000000005,1.044539295392954,0.26466230796482987,0.2089041095890411,0.04264013387471299,0.407957182778383
frozenset({'math_cat_υψηλό'}),"frozenset({'test preparation course_none', 'lunch_standard'})",0.176,0.418,0.085,0.4829545454545455,1.1553936494127883,1.0,0.011432000000000012,1.1256263736263739,0.16322101656196475,0.1669941060903733,0.11160574820368649,0.3431519138755981
"frozenset({'test preparation course_none', 'lunch_standard'})",frozenset({'math_cat_υψηλό'}),0.418,0.176,0.085,0.20334928229665075,1.1553936494127883,1.0,0.011432000000000012,1.0343303303303304,0.23108954922175076,0.1669941060903733,0.03319087657360529,0.3431519138755981
"frozenset({'gender_male', 'lunch_standard'})","frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό'})",0.316,0.252,0.092,0.2911392405063291,1.1553144464536869,1.0,0.012368000000000004,1.0552142857142857,0.19654207983727442,0.1932773109243697,0.05232518784268597,0.3281093027928471
"frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό'})","frozenset({'gender_male', 'lunch_standard'})",0.252,0.316,0.092,0.36507936507936506,1.1553144464536869,1.0,0.012368000000000004,1.0773,0.17972564519879103,0.1932773109243697,0.07175345771836997,0.3281093027928471
frozenset({'test preparation course_none'}),"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced', 'reading_cat_χαμηλό', 'writing_cat_χαμηλό'})",0.642,0.12,0.089,0.13862928348909656,1.1552440290758048,1.0,0.011959999999999998,1.021627486437613,0.3753687778544975,0.13224368499257055,0.021169640328518823,0.4401479750778816
"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced', 'reading_cat_χαμηλό', 'writing_cat_χαμηλό'})",frozenset({'test preparation course_none'}),0.12,0.642,0.089,0.7416666666666667,1.1552440290758048,1.0,0.011959999999999998,1.3858064516129034,0.15270684371807966,0.13224368499257055,0.27839851024208573,0.4401479750778816
frozenset({'gender_female'}),"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'lunch_standard'})",0.518,0.102,0.061,0.11776061776061776,1.1545158603982135,1.0,0.008163999999999998,1.0178643326039387,0.27766818583769803,0.10912343470483006,0.01755079928799306,0.3578999167234461
"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'lunch_standard'})",frozenset({'gender_female'}),0.102,0.518,0.061,0.5980392156862745,1.1545158603982133,1.0,0.008163999999999998,1.1991219512195121,0.14903793493738357,0.10912343470483006,0.16605646407940763,0.3578999167234461
frozenset({'lunch_standard'}),"frozenset({'math_cat_μέτριο', 'parental level of education_some high school'})",0.645,0.09,0.067,0.10387596899224806,1.1541774332472008,1.0,0.008950000000000007,1.0154844290657439,0.37628757620348985,0.1002994011976048,0.01524831757389897,0.42416020671834626
"frozenset({'math_cat_μέτριο', 'parental level of education_some high school'})",frozenset({'lunch_standard'}),0.09,0.645,0.067,0.7444444444444445,1.1541774332472008,1.0,0.008950000000000007,1.3891304347826088,0.14679350500246033,0.1002994011976048,0.2801251956181534,0.42416020671834626
"frozenset({'writing_cat_μέτριο', 'gender_male', 'test preparation course_none'})",frozenset({'lunch_standard'}),0.133,0.645,0.099,0.7443609022556391,1.154047910473859,1.0,0.013215000000000005,1.3886764705882353,0.1539617629583028,0.14580265095729014,0.27988986550884254,0.4489246371743312
frozenset({'lunch_standard'}),"frozenset({'writing_cat_μέτριο', 'gender_male', 'test preparation course_none'})",0.645,0.133,0.099,0.15348837209302327,1.154047910473859,1.0,0.013215000000000005,1.0242032967032966,0.37601365770379863,0.14580265095729014,0.023631340361041822,0.4489246371743312
"frozenset({'math_cat_χαμηλό', 'test preparation course_none'})",frozenset({'gender_female'}),0.246,0.518,0.147,0.5975609756097561,1.153592617007251,1.0,0.01957199999999998,1.1976969696969695,0.176582038651004,0.23824959481361424,0.16506426475053124,0.4406723796967699
frozenset({'gender_female'}),"frozenset({'math_cat_χαμηλό', 'test preparation course_none'})",0.518,0.246,0.147,0.28378378378378377,1.153592617007251,1.0,0.01957199999999998,1.052754716981132,0.2762299940723174,0.23824959481361424,0.050111119076636304,0.4406723796967699
"frozenset({'gender_male', 'lunch_standard'})","frozenset({'test preparation course_none', 'reading_cat_χαμηλό'})",0.316,0.214,0.078,0.2468354430379747,1.1534366497101622,1.0,0.010375999999999996,1.0435966386554623,0.19448193132403654,0.17256637168141592,0.04177537282185076,0.30566071217319296
"frozenset({'test preparation course_none', 'reading_cat_χαμηλό'})","frozenset({'gender_male', 'lunch_standard'})",0.214,0.316,0.078,0.3644859813084112,1.153436649710162,1.0,0.010375999999999996,1.0762941176470586,0.16924381809877986,0.17256637168141592,0.07088593758539649,0.30566071217319296
"frozenset({'reading_cat_μέτριο', 'test preparation course_none'})",frozenset({'race/ethnicity_group C'}),0.318,0.319,0.117,0.3679245283018868,1.153368427278642,1.0,0.015558000000000002,1.077402985074627,0.1949770659448079,0.225,0.07184218546532571,0.3673478440882475
frozenset({'race/ethnicity_group C'}),"frozenset({'reading_cat_μέτριο', 'test preparation course_none'})",0.319,0.318,0.117,0.3667711598746082,1.153368427278642,1.0,0.015558000000000002,1.077019801980198,0.19526337588011597,0.225,0.07151196462552518,0.3673478440882475
"frozenset({""parental level of education_bachelor's degree""})",frozenset({'math_cat_μέτριο'}),0.118,0.485,0.066,0.5593220338983051,1.1532413070068148,1.0,0.008770000000000007,1.1686538461538463,0.15065622208479362,0.12290502793296092,0.14431462892874788,0.3477022540625546
frozenset({'math_cat_μέτριο'}),"frozenset({""parental level of education_bachelor's degree""})",0.485,0.118,0.066,0.13608247422680414,1.1532413070068148,1.0,0.008770000000000007,1.0209307875894988,0.25801706384230677,0.12290502793296092,0.020501671458961618,0.3477022540625546
"frozenset({'race/ethnicity_group C', 'writing_cat_χαμηλό', 'reading_cat_χαμηλό'})",frozenset({'test preparation course_none'}),0.077,0.642,0.057,0.7402597402597403,1.1530525549217139,1.0,0.007566000000000003,1.3783,0.14381022979985178,0.08610271903323265,0.274468548211565,0.4145223934943561
frozenset({'test preparation course_none'}),"frozenset({'race/ethnicity_group C', 'writing_cat_χαμηλό', 'reading_cat_χαμηλό'})",0.642,0.077,0.057,0.08878504672897196,1.1530525549217137,1.0,0.007566000000000003,1.0129333333333332,0.37077330197000896,0.08610271903323265,0.012768197972884029,0.4145223934943561
frozenset({'math_cat_μέτριο'}),"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'gender_female'})",0.485,0.093,0.052,0.10721649484536082,1.1528655359716218,1.0,0.006894999999999998,1.0159237875288682,0.2574682598954443,0.09885931558935362,0.0156741949783471,0.33317813989579864
"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'gender_female'})",frozenset({'math_cat_μέτριο'}),0.093,0.485,0.052,0.5591397849462365,1.1528655359716218,1.0,0.006894999999999998,1.168170731707317,0.1461920108557374,0.09885931558935362,0.1439607474684204,0.33317813989579864
frozenset({'lunch_standard'}),"frozenset({'test preparation course_completed', 'writing_cat_υψηλό'})",0.645,0.117,0.087,0.13488372093023254,1.152852315643013,1.0,0.01153499999999999,1.0206720430107525,0.37348227294803277,0.12888888888888886,0.020253364586899813,0.43923673225998805
"frozenset({'test preparation course_completed', 'writing_cat_υψηλό'})",frozenset({'lunch_standard'}),0.117,0.645,0.087,0.7435897435897435,1.152852315643013,1.0,0.01153499999999999,1.3844999999999994,0.15015425469598143,0.12888888888888886,0.2777175875767422,0.43923673225998805
"frozenset({'parental level of education_high school', 'test preparation course_none'})","frozenset({'gender_male', 'lunch_standard'})",0.14,0.316,0.051,0.3642857142857142,1.1528028933092223,1.0,0.006759999999999995,1.075955056179775,0.15412676698586403,0.1259259259259259,0.07059314954051786,0.26283905967450266
"frozenset({'gender_male', 'lunch_standard'})","frozenset({'parental level of education_high school', 'test preparation course_none'})",0.316,0.14,0.051,0.16139240506329114,1.1528028933092223,1.0,0.006759999999999995,1.0255094339622641,0.19378511638573548,0.1259259259259259,0.024874889608478053,0.26283905967450266
frozenset({'gender_female'}),"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.518,0.139,0.083,0.16023166023166024,1.1527457570623039,1.0,0.010997999999999994,1.0252827586206896,0.27490876368544703,0.14459930313588848,0.024659303404948006,0.3786769811949668
"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'reading_cat_μέτριο'})",frozenset({'gender_female'}),0.139,0.518,0.083,0.5971223021582733,1.1527457570623036,1.0,0.010997999999999994,1.196392857142857,0.15389782125015733,0.14459930313588848,0.16415415385533882,0.3786769811949668
frozenset({'writing_cat_μέτριο'}),"frozenset({'race/ethnicity_group E', 'lunch_standard'})",0.491,0.099,0.056,0.11405295315682282,1.1520500318870992,1.0,0.007391000000000002,1.0169908045977012,0.2592969407802414,0.10486891385767792,0.01670694024064685,0.33985475940669424
"frozenset({'race/ethnicity_group E', 'lunch_standard'})",frozenset({'writing_cat_μέτριο'}),0.099,0.491,0.056,0.5656565656565656,1.152050031887099,1.0,0.007391000000000002,1.1718837209302324,0.14648406532424293,0.10486891385767792,0.1466730170070052,0.33985475940669424
frozenset({'test preparation course_none'}),"frozenset({'parental level of education_high school', 'reading_cat_μέτριο', 'lunch_standard'})",0.642,0.069,0.051,0.0794392523364486,1.1512935121224432,1.0,0.006701999999999993,1.0113401015228427,0.36707196845218504,0.07727272727272727,0.011212945581577429,0.40928484355952854
"frozenset({'parental level of education_high school', 'reading_cat_μέτριο', 'lunch_standard'})",frozenset({'test preparation course_none'}),0.069,0.642,0.051,0.7391304347826085,1.1512935121224432,1.0,0.006701999999999993,1.3723333333333325,0.1411511973210335,0.07727272727272727,0.2713140636385713,0.40928484355952854
"frozenset({'parental level of education_high school', 'gender_male', 'lunch_standard'})",frozenset({'test preparation course_none'}),0.069,0.642,0.051,0.7391304347826085,1.1512935121224432,1.0,0.006701999999999993,1.3723333333333325,0.1411511973210335,0.07727272727272727,0.2713140636385713,0.40928484355952854
frozenset({'test preparation course_none'}),"frozenset({'parental level of education_high school', 'gender_male', 'lunch_standard'})",0.642,0.069,0.051,0.0794392523364486,1.1512935121224432,1.0,0.006701999999999993,1.0113401015228427,0.36707196845218504,0.07727272727272727,0.011212945581577429,0.40928484355952854
"frozenset({'test preparation course_none', 'lunch_standard'})","frozenset({'gender_male', 'race/ethnicity_group D'})",0.418,0.133,0.064,0.15311004784688997,1.1512033672698494,1.0,0.008406000000000004,1.0237457627118645,0.22567654639175266,0.1314168377823409,0.023194980215559362,0.31715652768284347
"frozenset({'gender_male', 'race/ethnicity_group D'})","frozenset({'test preparation course_none', 'lunch_standard'})",0.133,0.418,0.064,0.48120300751879697,1.1512033672698492,1.0,0.008406000000000004,1.1218260869565218,0.15149221453287204,0.1314168377823409,0.10859623285016662,0.31715652768284347
"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο', 'gender_female'})",frozenset({'lunch_standard'}),0.132,0.645,0.098,0.7424242424242424,1.1510453370918488,1.0,0.012859999999999996,1.378235294117647,0.15118028778331605,0.14432989690721648,0.27443448570209134,0.44718111346018324
frozenset({'lunch_standard'}),"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο', 'gender_female'})",0.645,0.132,0.098,0.15193798449612403,1.1510453370918488,1.0,0.012859999999999996,1.023510054844607,0.3696464501293474,0.14432989690721648,0.02297002822134104,0.44718111346018324
"frozenset({'lunch_free/reduced', 'math_cat_μέτριο'})","frozenset({'reading_cat_μέτριο', 'test preparation course_none'})",0.153,0.318,0.056,0.36601307189542487,1.1509845028158014,1.0,0.007345999999999998,1.0757319587628866,0.15487434643278794,0.13493975903614458,0.0704003986736435,0.2710568504131212
"frozenset({'reading_cat_μέτριο', 'test preparation course_none'})","frozenset({'lunch_free/reduced', 'math_cat_μέτριο'})",0.318,0.153,0.056,0.1761006289308176,1.1509845028158012,1.0,0.007345999999999998,1.0280381679389312,0.19234394637620442,0.13493975903614458,0.027273469812063286,0.2710568504131212
frozenset({'test preparation course_none'}),"frozenset({'math_cat_χαμηλό', 'gender_male'})",0.642,0.134,0.099,0.15420560747663553,1.15078811549728,1.0,0.012971999999999997,1.023889502762431,0.3660064330455391,0.14623338257016247,0.023332110250156492,0.44650578881294467
"frozenset({'math_cat_χαμηλό', 'gender_male'})",frozenset({'test preparation course_none'}),0.134,0.642,0.099,0.7388059701492538,1.1507881154972799,1.0,0.012971999999999997,1.3706285714285715,0.15130519980404503,0.14623338257016247,0.2704077378470775,0.44650578881294467
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})","frozenset({'race/ethnicity_group C', 'test preparation course_none'})",0.228,0.202,0.053,0.23245614035087717,1.1507729720340454,1.0,0.006943999999999992,1.03968,0.16971355948773076,0.1405835543766578,0.038165589412126756,0.24741618898731976
"frozenset({'race/ethnicity_group C', 'test preparation course_none'})","frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})",0.202,0.228,0.053,0.26237623762376233,1.1507729720340454,1.0,0.006943999999999992,1.0466040268456376,0.16418404501820572,0.1405835543766578,0.04452880521212736,0.24741618898731976
frozenset({'gender_female'}),"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'test preparation course_none'})",0.518,0.099,0.059,0.1138996138996139,1.1505011505011504,1.0,0.007717999999999996,1.0168148148148148,0.2713974259793233,0.10573476702508959,0.016536752385808977,0.3549296049296049
"frozenset({'math_cat_υψηλό', 'writing_cat_υψηλό', 'reading_cat_υψηλό', 'lunch_standard'})",frozenset({'gender_female'}),0.099,0.518,0.059,0.5959595959595959,1.1505011505011504,1.0,0.007717999999999996,1.1929499999999997,0.145187080268628,0.10573476702508959,0.16174190033111183,0.3549296049296049
"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'test preparation course_none'})",frozenset({'gender_female'}),0.099,0.518,0.059,0.5959595959595959,1.1505011505011504,1.0,0.007717999999999996,1.1929499999999997,0.145187080268628,0.10573476702508959,0.16174190033111183,0.3549296049296049
frozenset({'gender_female'}),"frozenset({'math_cat_υψηλό', 'writing_cat_υψηλό', 'reading_cat_υψηλό', 'lunch_standard'})",0.518,0.099,0.059,0.1138996138996139,1.1505011505011504,1.0,0.007717999999999996,1.0168148148148148,0.2713974259793233,0.10573476702508959,0.016536752385808977,0.3549296049296049
frozenset({'lunch_standard'}),"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'gender_female'})",0.645,0.155,0.115,0.17829457364341086,1.1502875718929733,1.0,0.015024999999999997,1.0283490566037736,0.36803429271279847,0.1678832116788321,0.027567542773267298,0.4601150287571893
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'gender_female'})",frozenset({'lunch_standard'}),0.155,0.645,0.115,0.7419354838709677,1.1502875718929733,1.0,0.015024999999999997,1.3756249999999999,0.15461795729354255,0.1678832116788321,0.27305770104497956,0.4601150287571893
frozenset({'parental level of education_some college'}),"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'gender_female'})",0.226,0.227,0.059,0.26106194690265483,1.1500526295271138,1.0,0.0076979999999999965,1.0460958083832335,0.16857180396794108,0.149746192893401,0.04406461436307221,0.26048692058789125
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'gender_female'})",frozenset({'parental level of education_some college'}),0.227,0.226,0.059,0.2599118942731277,1.1500526295271138,1.0,0.0076979999999999965,1.0458214285714287,0.16878987874668355,0.149746192893401,0.04381381689034588,0.26048692058789125
frozenset({'race/ethnicity_group C'}),"frozenset({'reading_cat_μέτριο', 'math_cat_μέτριο', 'gender_female'})",0.319,0.15,0.055,0.1724137931034483,1.149425287356322,1.0,0.007150000000000004,1.0270833333333333,0.1908957415565346,0.13285024154589373,0.026369168356997992,0.2695402298850575
"frozenset({'reading_cat_μέτριο', 'math_cat_μέτριο', 'gender_female'})",frozenset({'race/ethnicity_group C'}),0.15,0.319,0.055,0.3666666666666667,1.149425287356322,1.0,0.007150000000000004,1.075263157894737,0.1529411764705883,0.13285024154589373,0.06999510523739602,0.2695402298850575
"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'test preparation course_none', 'lunch_standard'})",0.281,0.418,0.135,0.4804270462633452,1.1493470006300126,1.0,0.017542000000000002,1.120150684931507,0.180724256941225,0.23936170212765956,0.10726296608822201,0.40169677672018933
"frozenset({'test preparation course_none', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.418,0.281,0.135,0.3229665071770335,1.1493470006300124,1.0,0.017542000000000002,1.0619858657243817,0.2232658775614102,0.23936170212765956,0.05836788202647217,0.40169677672018933
frozenset({'lunch_standard'}),"frozenset({'reading_cat_μέτριο', 'parental level of education_some high school'})",0.645,0.081,0.06,0.09302325581395349,1.1484352569623888,1.0,0.007754999999999998,1.0132564102564103,0.3640845070422534,0.0900900900900901,0.01308297694663056,0.41688199827734707
"frozenset({'reading_cat_μέτριο', 'parental level of education_some high school'})",frozenset({'lunch_standard'}),0.081,0.645,0.06,0.7407407407407407,1.1484352569623886,1.0,0.007754999999999998,1.369285714285714,0.14064200217627854,0.0900900900900901,0.26969222743870613,0.41688199827734707
frozenset({'writing_cat_μέτριο'}),"frozenset({'parental level of education_high school', 'gender_female'})",0.491,0.094,0.053,0.10794297352342158,1.1483295055683147,1.0,0.006845999999999998,1.0156301369863014,0.25377173147496007,0.09962406015037595,0.015389595500465321,0.3358863803787321
"frozenset({'parental level of education_high school', 'gender_female'})",frozenset({'writing_cat_μέτριο'}),0.094,0.491,0.053,0.5638297872340425,1.1483295055683147,1.0,0.006845999999999998,1.1669756097560975,0.14257153567412217,0.09962406015037595,0.14308406136354135,0.3358863803787321
"frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced', 'gender_male', 'reading_cat_χαμηλό'})",frozenset({'test preparation course_none'}),0.076,0.642,0.056,0.736842105263158,1.1477291359239221,1.0,0.007207999999999999,1.3604000000000003,0.1393011750154607,0.08459214501510576,0.26492208174066467,0.412034759796688
frozenset({'test preparation course_none'}),"frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced', 'gender_male', 'reading_cat_χαμηλό'})",0.642,0.076,0.056,0.08722741433021806,1.147729135923922,1.0,0.007207999999999999,1.0123003412969285,0.35953711093375895,0.08459214501510576,0.012150881309759812,0.412034759796688
frozenset({'gender_male'}),"frozenset({'writing_cat_μέτριο', 'race/ethnicity_group D', 'lunch_standard'})",0.482,0.094,0.052,0.1078838174273859,1.1477001853977222,1.0,0.0066919999999999966,1.0155627906976745,0.24844074844074834,0.09923664122137406,0.015324301796231675,0.330537653394544
"frozenset({'writing_cat_μέτριο', 'race/ethnicity_group D', 'lunch_standard'})",frozenset({'gender_male'}),0.094,0.482,0.052,0.5531914893617021,1.1477001853977222,1.0,0.0066919999999999966,1.1593333333333333,0.14204448972660885,0.09923664122137406,0.13743530764807368,0.330537653394544
"frozenset({'test preparation course_none', 'lunch_standard'})",frozenset({'parental level of education_high school'}),0.418,0.196,0.094,0.22488038277511962,1.147348891709794,1.0,0.012072,1.0372592592592593,0.22066242597060756,0.18076923076923077,0.03592087409840748,0.3522361097549067
frozenset({'parental level of education_high school'}),"frozenset({'test preparation course_none', 'lunch_standard'})",0.196,0.418,0.094,0.47959183673469385,1.147348891709794,1.0,0.012072,1.1183529411764708,0.15973324865036517,0.18076923076923077,0.10582789816957709,0.3522361097549067
"frozenset({'gender_female', 'test preparation course_none'})","frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο'})",0.334,0.154,0.059,0.17664670658682632,1.1470565362780931,1.0,0.007563999999999994,1.0275054545454545,0.1924975823280907,0.13752913752913754,0.026769156722016935,0.2798817948518547
"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο'})","frozenset({'gender_female', 'test preparation course_none'})",0.154,0.334,0.059,0.3831168831168831,1.147056536278093,1.0,0.007563999999999994,1.0796210526315788,0.15154064991785862,0.13752913752913754,0.07374907374907368,0.2798817948518547
"frozenset({'reading_cat_μέτριο', 'test preparation course_none'})","frozenset({'gender_female', 'lunch_standard'})",0.318,0.329,0.12,0.3773584905660377,1.1469862935137924,1.0,0.015377999999999989,1.0776666666666666,0.1879032258064515,0.22770398481973433,0.07206928549334976,0.37105006595171186
"frozenset({'gender_female', 'lunch_standard'})","frozenset({'reading_cat_μέτριο', 'test preparation course_none'})",0.329,0.318,0.12,0.364741641337386,1.1469862935137924,1.0,0.015377999999999989,1.073578947368421,0.1909836065573769,0.22770398481973433,0.06853613099323462,0.37105006595171186
frozenset({'race/ethnicity_group C'}),"frozenset({'math_cat_χαμηλό', 'gender_female'})",0.319,0.205,0.075,0.23510971786833854,1.1468766725284807,1.0,0.009605000000000002,1.0393647540983608,0.1880567792462066,0.16703786191536749,0.03787385895388497,0.30048168820246196
"frozenset({'math_cat_χαμηλό', 'gender_female'})",frozenset({'race/ethnicity_group C'}),0.205,0.319,0.075,0.36585365853658536,1.1468766725284807,1.0,0.009605000000000002,1.0738846153846155,0.16109014675052416,0.16703786191536749,0.06880126069983164,0.30048168820246196
"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'test preparation course_none', 'race/ethnicity_group D'})",0.281,0.18,0.058,0.20640569395017794,1.146698299723211,1.0,0.007420000000000003,1.033273542600897,0.17792911610953918,0.14392059553349876,0.03220206579289993,0.26431395808620006
"frozenset({'test preparation course_none', 'race/ethnicity_group D'})","frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.18,0.281,0.058,0.32222222222222224,1.1466982997232107,1.0,0.007420000000000003,1.0608196721311474,0.15601345668629105,0.14392059553349876,0.05733271519085149,0.26431395808620006
"frozenset({'race/ethnicity_group C', 'math_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})",frozenset({'test preparation course_none'}),0.072,0.642,0.053,0.7361111111111112,1.1465905157493943,1.0,0.006776000000000004,1.3566315789473686,0.137768379960963,0.08018154311649017,0.26288019863438866,0.40933281412253375
frozenset({'test preparation course_none'}),"frozenset({'race/ethnicity_group C', 'math_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})",0.642,0.072,0.053,0.08255451713395638,1.1465905157493943,1.0,0.006776000000000004,1.0115042444821731,0.35712026984294326,0.08018154311649017,0.011373402084004726,0.40933281412253375
frozenset({'writing_cat_μέτριο'}),"frozenset({'race/ethnicity_group D', 'lunch_standard'})",0.491,0.167,0.094,0.1914460285132383,1.146383404270888,1.0,0.012003,1.0302342569269523,0.2508673661330101,0.16666666666666666,0.029346973005087987,0.37716014000512216
"frozenset({'race/ethnicity_group D', 'lunch_standard'})",frozenset({'writing_cat_μέτριο'}),0.167,0.491,0.094,0.562874251497006,1.146383404270888,1.0,0.012003,1.1644246575342465,0.15329110367551277,0.16666666666666666,0.1412067809371434,0.37716014000512216
frozenset({'race/ethnicity_group C'}),"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'gender_female'})",0.319,0.227,0.083,0.2601880877742947,1.146203029842708,1.0,0.010587,1.0448601694915254,0.1873042832121437,0.17926565874730022,0.042934136836086274,0.3129134271470593
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'gender_female'})",frozenset({'race/ethnicity_group C'}),0.227,0.319,0.083,0.3656387665198238,1.1462030298427077,1.0,0.010587,1.0735208333333333,0.1650119235025483,0.17926565874730022,0.06848570707756797,0.3129134271470593
"frozenset({'gender_male', 'lunch_standard'})","frozenset({'writing_cat_χαμηλό', 'test preparation course_none'})",0.316,0.243,0.088,0.27848101265822783,1.146012397770485,1.0,0.011212,1.049175438596491,0.18627060074428498,0.18683651804670912,0.04687055833319397,0.3203104651768505
"frozenset({'writing_cat_χαμηλό', 'test preparation course_none'})","frozenset({'gender_male', 'lunch_standard'})",0.243,0.316,0.088,0.3621399176954732,1.1460123977704848,1.0,0.011212,1.0723354838709676,0.16830791401465114,0.18683651804670912,0.06745602002262166,0.3203104651768505
"frozenset({'gender_male', 'lunch_standard'})",frozenset({'writing_cat_χαμηλό'}),0.316,0.301,0.109,0.3449367088607595,1.1459691324277725,1.0,0.013884000000000007,1.0670724637681162,0.18622243682601008,0.21456692913385828,0.06285652197533549,0.3535314773539678
frozenset({'writing_cat_χαμηλό'}),"frozenset({'gender_male', 'lunch_standard'})",0.301,0.316,0.109,0.36212624584717606,1.1459691324277723,1.0,0.013884000000000007,1.0723125,0.1822262471945506,0.21456692913385828,0.06743603194031589,0.3535314773539678
frozenset({'gender_female'}),"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})",0.518,0.091,0.054,0.10424710424710425,1.1455725741440028,1.0,0.006862,1.0147887931034483,0.2636391578300292,0.0972972972972973,0.014573271999014575,0.34882684882684883
"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})",frozenset({'gender_female'}),0.091,0.518,0.054,0.5934065934065934,1.1455725741440026,1.0,0.006862,1.1854594594594594,0.13979546102758425,0.0972972972972973,0.15644521453650082,0.34882684882684883
"frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο', 'gender_female'})",frozenset({'math_cat_μέτριο'}),0.099,0.485,0.055,0.5555555555555556,1.1454753722794961,1.0,0.006984999999999998,1.1587500000000002,0.14095449500554935,0.10396975425330815,0.13700107874865164,0.33447880870561286
frozenset({'math_cat_μέτριο'}),"frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο', 'gender_female'})",0.485,0.099,0.055,0.1134020618556701,1.145475372279496,1.0,0.006984999999999998,1.0162441860465117,0.24660194174757274,0.10396975425330815,0.015984530361454048,0.33447880870561286
frozenset({'test preparation course_none'}),"frozenset({'parental level of education_high school', 'gender_male'})",0.642,0.102,0.075,0.11682242990654206,1.1453179402602163,1.0,0.009515999999999997,1.0167830687830688,0.35441340782122893,0.11210762331838564,0.01650604666652791,0.42605827377680044
"frozenset({'parental level of education_high school', 'gender_male'})",frozenset({'test preparation course_none'}),0.102,0.642,0.075,0.7352941176470589,1.1453179402602163,1.0,0.009515999999999997,1.3524444444444446,0.1412917594654788,0.11210762331838564,0.26059809398619793,0.42605827377680044
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'gender_female', 'test preparation course_none', 'lunch_standard'})",0.325,0.215,0.08,0.24615384615384614,1.144901610017889,1.0,0.010124999999999995,1.041326530612245,0.1874999999999999,0.17391304347826086,0.039686428221460054,0.30912343470483006
"frozenset({'gender_female', 'test preparation course_none', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο'})",0.215,0.325,0.08,0.37209302325581395,1.144901610017889,1.0,0.010124999999999995,1.075,0.16122611464968142,0.17391304347826086,0.06976744186046509,0.30912343470483006
frozenset({'test preparation course_none'}),"frozenset({'race/ethnicity_group C', 'reading_cat_χαμηλό'})",0.642,0.083,0.061,0.09501557632398754,1.1447659798070786,1.0,0.007713999999999992,1.0132771084337349,0.3532374759593366,0.09186746987951808,0.013103136667380078,0.414977667680066
"frozenset({'race/ethnicity_group C', 'reading_cat_χαμηλό'})",frozenset({'test preparation course_none'}),0.083,0.642,0.061,0.7349397590361445,1.1447659798070786,1.0,0.007713999999999992,1.3506363636363632,0.13790514328619682,0.09186746987951808,0.25960826546409077,0.414977667680066
frozenset({'lunch_standard'}),"frozenset({'race/ethnicity_group E', 'test preparation course_none'})",0.645,0.08,0.059,0.09147286821705426,1.1434108527131783,1.0,0.007399999999999997,1.012627986348123,0.3533062783480543,0.08858858858858859,0.012470508931580717,0.4144864341085271
"frozenset({'race/ethnicity_group E', 'test preparation course_none'})",frozenset({'lunch_standard'}),0.08,0.645,0.059,0.7374999999999999,1.143410852713178,1.0,0.007399999999999997,1.352380952380952,0.13633014001473834,0.08858858858858859,0.2605633802816899,0.4144864341085271
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'race/ethnicity_group C', 'test preparation course_none'})",0.325,0.202,0.075,0.23076923076923075,1.1424219345011424,1.0,0.009349999999999997,1.0374,0.1846913580246913,0.165929203539823,0.036051667630614966,0.301028179741051
"frozenset({'race/ethnicity_group C', 'test preparation course_none'})","frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο'})",0.202,0.325,0.075,0.37128712871287123,1.1424219345011422,1.0,0.009349999999999997,1.0736220472440943,0.1562238930659983,0.165929203539823,0.06857352401906847,0.301028179741051
"frozenset({'math_cat_μέτριο', 'test preparation course_none'})","frozenset({'parental level of education_some college', 'lunch_standard'})",0.304,0.147,0.051,0.16776315789473684,1.1412459720730397,1.0,0.006311999999999998,1.0249486166007904,0.17782285327924272,0.1275,0.024341333991485165,0.2573509667024705
"frozenset({'parental level of education_some college', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'test preparation course_none'})",0.147,0.304,0.051,0.3469387755102041,1.1412459720730397,1.0,0.006311999999999998,1.0657499999999998,0.1450934418315978,0.1275,0.06169364297443116,0.2573509667024705
frozenset({'math_cat_μέτριο'}),frozenset({'race/ethnicity_group D'}),0.485,0.262,0.145,0.29896907216494845,1.141103328873849,1.0,0.01793,1.052735294117647,0.24010713090056918,0.24086378737541528,0.050093593719442324,0.42620209333438264
frozenset({'race/ethnicity_group D'}),frozenset({'math_cat_μέτριο'}),0.262,0.485,0.145,0.5534351145038168,1.141103328873849,1.0,0.01793,1.1532478632478633,0.16755443416503132,0.24086378737541528,0.13288371748313937,0.42620209333438264
frozenset({'gender_female'}),"frozenset({'math_cat_υψηλό', 'writing_cat_υψηλό', 'reading_cat_υψηλό'})",0.518,0.11,0.065,0.12548262548262548,1.1407511407511408,1.0,0.00802,1.0177041942604856,0.2559846792211937,0.11545293072824157,0.01739620840744436,0.3581958581958582
"frozenset({'math_cat_υψηλό', 'writing_cat_υψηλό', 'reading_cat_υψηλό'})",frozenset({'gender_female'}),0.11,0.518,0.065,0.5909090909090909,1.1407511407511408,1.0,0.00802,1.1782222222222223,0.1386343993085566,0.11545293072824157,0.15126367408525088,0.3581958581958582
"frozenset({'writing_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",frozenset({'race/ethnicity_group D'}),0.221,0.262,0.066,0.2986425339366516,1.139856999758212,1.0,0.008098000000000001,1.0522451612903228,0.1575057377368032,0.15827338129496404,0.049651129995462825,0.2752754654416082
frozenset({'race/ethnicity_group D'}),"frozenset({'writing_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",0.262,0.221,0.066,0.25190839694656486,1.139856999758212,1.0,0.008098000000000001,1.0413163265306122,0.16625605649995895,0.15827338129496404,0.03967701790316413,0.2752754654416082
"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced', 'reading_cat_χαμηλό'})",frozenset({'test preparation course_none'}),0.123,0.642,0.09,0.7317073170731707,1.139731023478459,1.0,0.011034000000000002,1.3343636363636362,0.13979475484606615,0.13333333333333333,0.2505790979697506,0.4359471164805106
frozenset({'test preparation course_none'}),"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced', 'reading_cat_χαμηλό'})",0.642,0.123,0.09,0.14018691588785046,1.139731023478459,1.0,0.011034000000000002,1.0199891304347826,0.3424581005586593,0.13333333333333333,0.019597395539168137,0.4359471164805106
frozenset({'test preparation course_none'}),"frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο', 'lunch_standard'})",0.642,0.108,0.079,0.12305295950155763,1.1393792546440522,1.0,0.009664000000000006,1.0171651865008882,0.34170143554204113,0.11773472429210134,0.016875515136275374,0.42726722049151955
"frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο', 'lunch_standard'})",frozenset({'test preparation course_none'}),0.108,0.642,0.079,0.7314814814814815,1.1393792546440522,1.0,0.009664000000000006,1.3332413793103448,0.13714026224669362,0.11773472429210134,0.24994827229464106,0.42726722049151955
frozenset({'reading_cat_χαμηλό'}),"frozenset({'gender_male', 'lunch_standard'})",0.275,0.316,0.099,0.36,1.1392405063291138,1.0,0.0121,1.0687499999999999,0.16858237547892718,0.20121951219512196,0.06432748538011694,0.33664556962025316
"frozenset({'gender_male', 'lunch_standard'})",frozenset({'reading_cat_χαμηλό'}),0.316,0.275,0.099,0.31329113924050633,1.1392405063291138,1.0,0.0121,1.0557603686635944,0.17868745938921377,0.20121951219512196,0.05281536446966388,0.33664556962025316
"frozenset({'test preparation course_none', 'race/ethnicity_group D', 'lunch_standard'})",frozenset({'writing_cat_μέτριο'}),0.118,0.491,0.066,0.5593220338983051,1.1391487452103974,1.0,0.008062000000000007,1.1550384615384617,0.13849378135092433,0.12154696132596687,0.13422796443674884,0.34687079291656603
frozenset({'writing_cat_μέτριο'}),"frozenset({'test preparation course_none', 'race/ethnicity_group D', 'lunch_standard'})",0.491,0.118,0.066,0.13441955193482688,1.1391487452103974,1.0,0.008062000000000007,1.018969411764706,0.2399833303566115,0.12154696132596687,0.018616272034951118,0.34687079291656603
frozenset({'reading_cat_μέτριο'}),"frozenset({'gender_female', 'test preparation course_none', 'lunch_standard'})",0.49,0.215,0.12,0.24489795918367346,1.1390602752728998,1.0,0.014649999999999996,1.0395945945945948,0.23937908496732022,0.20512820512820512,0.03808657220850123,0.40151874703369717
"frozenset({'gender_female', 'test preparation course_none', 'lunch_standard'})",frozenset({'reading_cat_μέτριο'}),0.215,0.49,0.12,0.5581395348837209,1.1390602752728998,1.0,0.014649999999999996,1.1542105263157896,0.15552016985137998,0.20512820512820512,0.13360693114455086,0.40151874703369717
frozenset({'gender_male'}),"frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced', 'test preparation course_none', 'reading_cat_χαμηλό'})",0.482,0.102,0.056,0.11618257261410789,1.1390448295500775,1.0,0.006836000000000009,1.0160469483568075,0.23565912851627166,0.10606060606060608,0.015793510706133513,0.3326010902286226
"frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced', 'test preparation course_none', 'reading_cat_χαμηλό'})",frozenset({'gender_male'}),0.102,0.482,0.056,0.5490196078431373,1.1390448295500775,1.0,0.006836000000000009,1.1486086956521742,0.13593700286350638,0.10606060606060608,0.12938148232265892,0.3326010902286226
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'gender_female'})","frozenset({'race/ethnicity_group C', 'lunch_standard'})",0.227,0.205,0.053,0.23348017621145373,1.1389276888363598,1.0,0.0064649999999999985,1.0371551724137933,0.15780224071859209,0.13984168865435356,0.03582412102069653,0.2460083807886537
"frozenset({'race/ethnicity_group C', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'gender_female'})",0.205,0.227,0.053,0.25853658536585367,1.1389276888363598,1.0,0.0064649999999999985,1.0425328947368422,0.15343538625845493,0.13984168865435356,0.04079765247846528,0.2460083807886537
"frozenset({'race/ethnicity_group C', 'test preparation course_completed'})",frozenset({'gender_female'}),0.117,0.518,0.069,0.5897435897435898,1.1385011385011385,1.0,0.008393999999999999,1.174875,0.13777143138510017,0.12190812720848056,0.14884562187466752,0.36147411147411146
frozenset({'gender_female'}),"frozenset({'race/ethnicity_group C', 'test preparation course_completed'})",0.518,0.117,0.069,0.13320463320463322,1.1385011385011385,1.0,0.008393999999999999,1.018694877505568,0.2523904023092188,0.12190812720848056,0.01835179298372957,0.36147411147411146
frozenset({'test preparation course_none'}),"frozenset({'lunch_free/reduced', 'gender_male', 'reading_cat_χαμηλό'})",0.642,0.078,0.057,0.08878504672897196,1.138269829858615,1.0,0.006924,1.0118358974358974,0.33931196706850925,0.08597285067873305,0.011697447645305812,0.4097771387491014
"frozenset({'lunch_free/reduced', 'gender_male', 'reading_cat_χαμηλό'})",frozenset({'test preparation course_none'}),0.078,0.642,0.057,0.7307692307692308,1.138269829858615,1.0,0.006924,1.329714285714286,0.13175019979449706,0.08597285067873305,0.24795874516544925,0.4097771387491014
frozenset({'test preparation course_none'}),"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'gender_female', 'lunch_standard'})",0.642,0.115,0.084,0.13084112149532712,1.137748882568062,1.0,0.010169999999999998,1.018225806451613,0.33818834796488423,0.12481426448736999,0.017899572311104082,0.4306379520520114
"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'gender_female', 'lunch_standard'})",frozenset({'test preparation course_none'}),0.115,0.642,0.084,0.7304347826086957,1.1377488825680617,1.0,0.010169999999999998,1.3280645161290323,0.13680387409200967,0.12481426448736999,0.24702453242652414,0.4306379520520114
"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'gender_male', 'lunch_standard'})",0.281,0.316,0.101,0.3594306049822064,1.1374386233614127,1.0,0.012203999999999993,1.0678,0.16805519216733902,0.20362903225806453,0.0634950365236936,0.3395254290733817
"frozenset({'gender_male', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.316,0.281,0.101,0.319620253164557,1.1374386233614127,1.0,0.012203999999999993,1.0567627906976744,0.17665450755601866,0.20362903225806453,0.05371384306614322,0.3395254290733817
"frozenset({'race/ethnicity_group C', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})",0.205,0.266,0.062,0.3024390243902439,1.1369888134971575,1.0,0.0074699999999999975,1.0522377622377623,0.15155203895313446,0.15158924205378974,0.04964444739815242,0.2677608655785806
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})","frozenset({'race/ethnicity_group C', 'lunch_standard'})",0.266,0.205,0.062,0.23308270676691728,1.1369888134971575,1.0,0.0074699999999999975,1.0366176470588235,0.16414696317131047,0.15158924205378974,0.03532415945524187,0.2677608655785806
"frozenset({'math_cat_μέτριο', 'test preparation course_none', 'gender_female'})",frozenset({'lunch_standard'}),0.15,0.645,0.11,0.7333333333333334,1.136950904392765,1.0,0.013249999999999998,1.3312500000000003,0.14171122994652405,0.16058394160583941,0.24882629107981233,0.45193798449612405
frozenset({'lunch_standard'}),"frozenset({'math_cat_μέτριο', 'test preparation course_none', 'gender_female'})",0.645,0.15,0.11,0.17054263565891473,1.1369509043927648,1.0,0.013249999999999998,1.0247663551401869,0.3393085787451984,0.16058394160583941,0.024167806657546743,0.45193798449612405
"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})",frozenset({'gender_female'}),0.09,0.518,0.053,0.5888888888888889,1.136851136851137,1.0,0.006379999999999997,1.1724324324324324,0.13228281152809448,0.0954954954954955,0.1470723835869064,0.3456027456027456
frozenset({'gender_female'}),"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})",0.518,0.09,0.053,0.1023166023166023,1.1368511368511367,1.0,0.006379999999999997,1.0137204301075269,0.24974555703436926,0.0954954954954955,0.013534727820442098,0.3456027456027456
"frozenset({'math_cat_χαμηλό', 'parental level of education_high school'})",frozenset({'test preparation course_none'}),0.085,0.642,0.062,0.7294117647058823,1.1361553967381344,1.0,0.007429999999999992,1.3230434782608693,0.13097126740701553,0.09323308270676692,0.2441669405192243,0.4129924867143119
frozenset({'test preparation course_none'}),"frozenset({'math_cat_χαμηλό', 'parental level of education_high school'})",0.642,0.085,0.062,0.09657320872274143,1.1361553967381344,1.0,0.007429999999999992,1.0128103448275863,0.3347449990989364,0.09323308270676692,0.012648315543979698,0.4129924867143119
frozenset({'gender_male'}),"frozenset({'parental level of education_high school', 'lunch_standard'})",0.482,0.126,0.069,0.14315352697095438,1.1361391029440824,1.0,0.008268000000000005,1.0200193704600484,0.2313244921940575,0.12801484230055662,0.019626461065165187,0.34538628729500104
"frozenset({'parental level of education_high school', 'lunch_standard'})",frozenset({'gender_male'}),0.126,0.482,0.069,0.5476190476190477,1.1361391029440824,1.0,0.008268000000000005,1.1450526315789475,0.1371007859914437,0.12801484230055662,0.12667769810626966,0.34538628729500104
"frozenset({'writing_cat_μέτριο', 'race/ethnicity_group D'})","frozenset({'test preparation course_none', 'lunch_standard'})",0.139,0.418,0.066,0.4748201438848921,1.135933358576297,1.0,0.007898000000000002,1.1081917808219177,0.13898567557104147,0.1344195519348269,0.09762911320428194,0.31635744036349867
"frozenset({'test preparation course_none', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'race/ethnicity_group D'})",0.418,0.139,0.066,0.15789473684210528,1.135933358576297,1.0,0.007898000000000002,1.0224375,0.20561282932416955,0.1344195519348269,0.021945106669111814,0.31635744036349867
"frozenset({'gender_female', 'race/ethnicity_group B'})",frozenset({'writing_cat_μέτριο'}),0.104,0.491,0.058,0.5576923076923077,1.1358295472348425,1.0,0.006936000000000005,1.1507826086956523,0.133466748768473,0.10800744878957172,0.1310261447786006,0.33790929030236566
frozenset({'writing_cat_μέτριο'}),"frozenset({'gender_female', 'race/ethnicity_group B'})",0.491,0.104,0.058,0.11812627291242363,1.1358295472348425,1.0,0.006936000000000005,1.0160184757505775,0.23494343201680115,0.10800744878957172,0.015765929589758518,0.33790929030236566
"frozenset({'test preparation course_none', 'race/ethnicity_group D', 'lunch_standard'})",frozenset({'math_cat_μέτριο'}),0.118,0.485,0.065,0.5508474576271187,1.135767953870348,1.0,0.007770000000000006,1.146603773584906,0.13553113553113563,0.120817843866171,0.12785914102353155,0.34243403809190986
frozenset({'math_cat_μέτριο'}),"frozenset({'test preparation course_none', 'race/ethnicity_group D', 'lunch_standard'})",0.485,0.118,0.065,0.13402061855670103,1.1357679538703478,1.0,0.007770000000000006,1.0185,0.2321135175504109,0.120817843866171,0.018163966617574866,0.34243403809190986
frozenset({'parental level of education_some college'}),"frozenset({'math_cat_μέτριο', 'test preparation course_none'})",0.226,0.304,0.078,0.34513274336283184,1.1353050768514206,1.0,0.009295999999999999,1.0628108108108107,0.15397866560657256,0.17256637168141592,0.059098769199471046,0.3008558453656265
"frozenset({'math_cat_μέτριο', 'test preparation course_none'})",frozenset({'parental level of education_some college'}),0.304,0.226,0.078,0.2565789473684211,1.1353050768514206,1.0,0.009295999999999999,1.041132743362832,0.1712348953728264,0.17256637168141592,0.03950768393852851,0.3008558453656265
"frozenset({'writing_cat_μέτριο', 'parental level of education_some college'})",frozenset({'gender_female'}),0.114,0.518,0.067,0.5877192982456141,1.1345932398563978,1.0,0.007947999999999997,1.1691063829787236,0.1338903675752164,0.11858407079646019,0.1446458469826018,0.3585314637946217
frozenset({'gender_female'}),"frozenset({'writing_cat_μέτριο', 'parental level of education_some college'})",0.518,0.114,0.067,0.12934362934362933,1.1345932398563976,1.0,0.007947999999999997,1.0176230598669622,0.24611382919427746,0.11858407079646019,0.017317866076331075,0.3585314637946217
frozenset({'lunch_standard'}),"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'test preparation course_none'})",0.645,0.205,0.15,0.23255813953488372,1.1344299489506524,1.0,0.017774999999999985,1.035909090909091,0.3338028169014082,0.2142857142857143,0.034664326458973244,0.4821327283040272
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'test preparation course_none'})",frozenset({'lunch_standard'}),0.205,0.645,0.15,0.7317073170731707,1.1344299489506522,1.0,0.017774999999999985,1.323181818181818,0.1490566037735848,0.2142857142857143,0.24424596358639633,0.4821327283040272
"frozenset({'writing_cat_μέτριο', 'race/ethnicity_group D'})",frozenset({'gender_male'}),0.139,0.482,0.076,0.5467625899280575,1.134362219767754,1.0,0.009001999999999996,1.1428888888888888,0.1375695335900727,0.13944954128440365,0.12502430488041988,0.35221946923788766
frozenset({'gender_male'}),"frozenset({'writing_cat_μέτριο', 'race/ethnicity_group D'})",0.482,0.139,0.076,0.15767634854771784,1.134362219767754,1.0,0.009001999999999996,1.0221724137931034,0.22866287339971542,0.13944954128440365,0.021691461727895267,0.35221946923788766
frozenset({'parental level of education_high school'}),"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'lunch_standard'})",0.196,0.225,0.05,0.25510204081632654,1.1337868480725624,1.0,0.0059000000000000025,1.0404109589041095,0.14676616915422888,0.1347708894878706,0.038841342988808425,0.2386621315192744
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'lunch_standard'})",frozenset({'parental level of education_high school'}),0.225,0.196,0.05,0.22222222222222224,1.1337868480725624,1.0,0.0059000000000000025,1.0337142857142858,0.15225806451612908,0.1347708894878706,0.03261470425649531,0.2386621315192744
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})",frozenset({'lunch_standard'}),0.208,0.645,0.152,0.7307692307692308,1.132975551580203,1.0,0.017839999999999995,1.318571428571429,0.1481924508240297,0.21683309557774608,0.2416034669555798,0.4832140727489565
frozenset({'lunch_standard'}),"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})",0.645,0.208,0.152,0.23565891472868217,1.1329755515802027,1.0,0.017839999999999995,1.036186612576065,0.3306152705707931,0.21683309557774608,0.034922872132174466,0.4832140727489565
"frozenset({'race/ethnicity_group C', 'gender_male', 'test preparation course_none'})",frozenset({'math_cat_μέτριο'}),0.091,0.485,0.05,0.5494505494505495,1.1328877308258753,1.0,0.005865000000000002,1.1430487804878051,0.12904290429042908,0.09505703422053234,0.12514669796223205,0.32627166647785205
frozenset({'math_cat_μέτριο'}),"frozenset({'race/ethnicity_group C', 'gender_male', 'test preparation course_none'})",0.485,0.091,0.05,0.10309278350515465,1.1328877308258753,1.0,0.005865000000000002,1.0134827586206896,0.2277669902912622,0.09505703422053234,0.013303392194889603,0.32627166647785205
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'race/ethnicity_group C', 'test preparation course_none', 'lunch_standard'})",0.418,0.131,0.062,0.14832535885167464,1.1322546477227071,1.0,0.0072419999999999984,1.0203426966292135,0.20069837046890582,0.12731006160164274,0.019937121808601417,0.31080390079988307
"frozenset({'race/ethnicity_group C', 'test preparation course_none', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.131,0.418,0.062,0.47328244274809156,1.1322546477227071,1.0,0.0072419999999999984,1.1049565217391304,0.13441478896766765,0.12731006160164274,0.09498701503108517,0.31080390079988307
"frozenset({'lunch_free/reduced', 'gender_female'})","frozenset({'writing_cat_χαμηλό', 'test preparation course_none'})",0.189,0.243,0.052,0.2751322751322751,1.132231584906482,1.0,0.006072999999999995,1.0443284671532846,0.14400550128047035,0.13684210526315788,0.04244686279032381,0.24456202233980012
"frozenset({'writing_cat_χαμηλό', 'test preparation course_none'})","frozenset({'lunch_free/reduced', 'gender_female'})",0.243,0.189,0.052,0.2139917695473251,1.132231584906482,1.0,0.006072999999999995,1.0317958115183246,0.15427802052636916,0.13684210526315788,0.03081599204355746,0.24456202233980012
frozenset({'test preparation course_completed'}),"frozenset({'lunch_free/reduced', 'math_cat_μέτριο'})",0.358,0.153,0.062,0.17318435754189945,1.131923905502611,1.0,0.007226000000000003,1.024412162162162,0.18153954376444587,0.13808463251670378,0.023830410321014708,0.28920655785591703
"frozenset({'lunch_free/reduced', 'math_cat_μέτριο'})",frozenset({'test preparation course_completed'}),0.153,0.358,0.062,0.40522875816993464,1.1319239055026107,1.0,0.007226000000000003,1.0794065934065935,0.13760140153102038,0.13808463251670378,0.07356504387840289,0.28920655785591703
frozenset({'parental level of education_some college'}),frozenset({'race/ethnicity_group D'}),0.226,0.262,0.067,0.29646017699115046,1.1315273930959941,1.0,0.007788000000000003,1.0489811320754716,0.15017933587874585,0.15914489311163896,0.04669400676307649,0.2760926839154225
frozenset({'race/ethnicity_group D'}),frozenset({'parental level of education_some college'}),0.262,0.226,0.067,0.25572519083969464,1.131527393095994,1.0,0.007788000000000003,1.0399384615384617,0.1575051571411237,0.15914489311163896,0.03840463932777084,0.2760926839154225
"frozenset({'math_cat_χαμηλό', 'gender_female', 'lunch_standard'})",frozenset({'test preparation course_none'}),0.095,0.642,0.069,0.7263157894736842,1.1313330054107231,1.0,0.008010000000000003,1.3080769230769231,0.1282728801345184,0.10329341317365272,0.23551896501029115,0.41689621249385145
frozenset({'test preparation course_none'}),"frozenset({'math_cat_χαμηλό', 'gender_female', 'lunch_standard'})",0.642,0.095,0.069,0.1074766355140187,1.1313330054107231,1.0,0.008010000000000003,1.013979057591623,0.32426524168083565,0.10329341317365272,0.013786337584551053,0.41689621249385145
"frozenset({'race/ethnicity_group C', 'gender_male'})","frozenset({'reading_cat_μέτριο', 'test preparation course_none'})",0.139,0.318,0.05,0.3597122302158273,1.1311705352698973,1.0,0.005797999999999998,1.0651460674157303,0.13468060394889655,0.12285012285012285,0.06116162788244475,0.2584724673091715
"frozenset({'reading_cat_μέτριο', 'test preparation course_none'})","frozenset({'race/ethnicity_group C', 'gender_male'})",0.318,0.139,0.05,0.15723270440251572,1.131170535269897,1.0,0.005797999999999998,1.021634328358209,0.1700293255131964,0.12285012285012285,0.021176195589449134,0.2584724673091715
"frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο'})",frozenset({'gender_female'}),0.169,0.518,0.099,0.5857988165680473,1.1308857462703616,1.0,0.011457999999999996,1.1636857142857142,0.13927481797517893,0.1683673469387755,0.1406614451619239,0.38845925384386926
frozenset({'gender_female'}),"frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο'})",0.518,0.169,0.099,0.19111969111969113,1.1308857462703616,1.0,0.011457999999999996,1.027346062052506,0.24011903265015289,0.1683673469387755,0.02661816019216741,0.38845925384386926
frozenset({'race/ethnicity_group C'}),"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})",0.319,0.208,0.075,0.23510971786833854,1.13033518205932,1.0,0.008648000000000003,1.0354426229508198,0.16931962799804215,0.165929203539823,0.034229441752952706,0.2978433204726308
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})",frozenset({'race/ethnicity_group C'}),0.208,0.319,0.075,0.3605769230769231,1.13033518205932,1.0,0.008648000000000003,1.0650225563909776,0.14558922558922563,0.165929203539823,0.061052750480063235,0.2978433204726308
frozenset({'math_cat_χαμηλό'}),frozenset({'test preparation course_none'}),0.339,0.642,0.246,0.7256637168141592,1.1303173159099054,1.0,0.02836199999999997,1.3049677419354835,0.17442160805874304,0.33469387755102037,0.23369753300044466,0.5544206434538086
frozenset({'test preparation course_none'}),frozenset({'math_cat_χαμηλό'}),0.642,0.339,0.246,0.38317757009345793,1.1303173159099054,1.0,0.02836199999999997,1.071621212121212,0.3220466003542714,0.33469387755102037,0.06683444794774267,0.5544206434538086
"frozenset({'math_cat_χαμηλό', 'gender_female', 'test preparation course_none'})",frozenset({'race/ethnicity_group C'}),0.147,0.319,0.053,0.36054421768707484,1.1302326573262533,1.0,0.006107000000000001,1.064968085106383,0.13508372226769008,0.12832929782082325,0.06100472494430959,0.26334420915701706
frozenset({'race/ethnicity_group C'}),"frozenset({'math_cat_χαμηλό', 'gender_female', 'test preparation course_none'})",0.319,0.147,0.053,0.16614420062695923,1.1302326573262533,1.0,0.006107000000000001,1.0229586466165412,0.16920178427950022,0.12832929782082325,0.022443377053879526,0.26334420915701706
"frozenset({'math_cat_μέτριο', 'test preparation course_none'})","frozenset({'lunch_free/reduced', 'reading_cat_μέτριο'})",0.304,0.163,0.056,0.1842105263157895,1.130125928317727,1.0,0.006448000000000002,1.026,0.16543513957307068,0.1362530413625304,0.025341130604288515,0.26388440426218923
"frozenset({'lunch_free/reduced', 'reading_cat_μέτριο'})","frozenset({'math_cat_μέτριο', 'test preparation course_none'})",0.163,0.304,0.056,0.34355828220858897,1.130125928317727,1.0,0.006448000000000002,1.0602616822429907,0.13756613756613761,0.1362530413625304,0.05683661236866233,0.26388440426218923
"frozenset({'reading_cat_μέτριο', 'math_cat_μέτριο'})","frozenset({'gender_male', 'lunch_standard'})",0.325,0.316,0.116,0.35692307692307695,1.1295034079844208,1.0,0.013300000000000006,1.0636363636363635,0.16985951468710095,0.22095238095238096,0.05982905982905987,0.36200584225900684
"frozenset({'gender_male', 'lunch_standard'})","frozenset({'reading_cat_μέτριο', 'math_cat_μέτριο'})",0.316,0.325,0.116,0.3670886075949367,1.1295034079844206,1.0,0.013300000000000006,1.0665,0.16762452107279702,0.22095238095238096,0.06235349273323957,0.36200584225900684
"frozenset({'math_cat_χαμηλό', 'test preparation course_none', 'reading_cat_χαμηλό'})",frozenset({'gender_male'}),0.169,0.482,0.092,0.5443786982248521,1.1294163863586144,1.0,0.010541999999999996,1.1369090909090909,0.13789044106105788,0.16457960644007155,0.1204221973452743,0.36762503375972894
frozenset({'gender_male'}),"frozenset({'math_cat_χαμηλό', 'test preparation course_none', 'reading_cat_χαμηλό'})",0.482,0.169,0.092,0.19087136929460582,1.1294163863586142,1.0,0.010541999999999996,1.0270307692307692,0.22121034077555807,0.16457960644007155,0.02631933729796127,0.36762503375972894
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none', 'race/ethnicity_group D'})",frozenset({'lunch_standard'}),0.081,0.645,0.059,0.728395061728395,1.1292946693463488,1.0,0.006754999999999997,1.307045454545454,0.12458272624997689,0.08845577211394302,0.23491566684054915,0.4099339649727246
frozenset({'lunch_standard'}),"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none', 'race/ethnicity_group D'})",0.645,0.081,0.059,0.09147286821705426,1.1292946693463488,1.0,0.006754999999999997,1.0115273037542662,0.3225113392217712,0.08845577211394302,0.011395939300385483,0.4099339649727246
frozenset({'lunch_standard'}),"frozenset({'parental level of education_high school', 'reading_cat_μέτριο', 'writing_cat_μέτριο'})",0.645,0.081,0.059,0.09147286821705426,1.1292946693463488,1.0,0.006754999999999997,1.0115273037542662,0.3225113392217712,0.08845577211394302,0.011395939300385483,0.4099339649727246
"frozenset({'parental level of education_high school', 'reading_cat_μέτριο', 'writing_cat_μέτριο'})",frozenset({'lunch_standard'}),0.081,0.645,0.059,0.728395061728395,1.1292946693463488,1.0,0.006754999999999997,1.307045454545454,0.12458272624997689,0.08845577211394302,0.23491566684054915,0.4099339649727246
frozenset({'reading_cat_χαμηλό'}),frozenset({'race/ethnicity_group B'}),0.275,0.19,0.059,0.2145454545454545,1.1291866028708133,1.0,0.006749999999999992,1.03125,0.1578024547048508,0.14532019704433496,0.030303030303030255,0.26253588516746407
frozenset({'race/ethnicity_group B'}),frozenset({'reading_cat_χαμηλό'}),0.19,0.275,0.059,0.31052631578947365,1.129186602870813,1.0,0.006749999999999992,1.0515267175572518,0.14124293785310718,0.14532019704433496,0.04900181488203259,0.26253588516746407
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})","frozenset({'race/ethnicity_group C', 'gender_female'})",0.266,0.18,0.054,0.20300751879699247,1.1278195488721805,1.0,0.0061200000000000004,1.028867924528302,0.1544050862851953,0.1377551020408163,0.028057949752429846,0.25150375939849623
"frozenset({'race/ethnicity_group C', 'gender_female'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})",0.18,0.266,0.054,0.3,1.1278195488721803,1.0,0.0061200000000000004,1.0485714285714287,0.13821138211382114,0.1377551020408163,0.04632152588555855,0.25150375939849623
"frozenset({'writing_cat_μέτριο', 'test preparation course_none'})",frozenset({'race/ethnicity_group D'}),0.308,0.262,0.091,0.29545454545454547,1.1276891047883415,1.0,0.010303999999999994,1.0474838709677419,0.16362827923521556,0.18997912317327761,0.04533136240453314,0.32139139486467727
frozenset({'race/ethnicity_group D'}),"frozenset({'writing_cat_μέτριο', 'test preparation course_none'})",0.262,0.308,0.091,0.34732824427480913,1.1276891047883413,1.0,0.010303999999999994,1.0602573099415202,0.15342922659995822,0.18997912317327761,0.05683272294047563,0.32139139486467727
"frozenset({'test preparation course_completed', 'gender_female', 'reading_cat_υψηλό'})",frozenset({'lunch_standard'}),0.077,0.645,0.056,0.7272727272727273,1.127554615926709,1.0,0.006335,1.3016666666666667,0.1225622968580715,0.08408408408408409,0.23175416133162613,0.40704721634954194
frozenset({'lunch_standard'}),"frozenset({'test preparation course_completed', 'gender_female', 'reading_cat_υψηλό'})",0.645,0.077,0.056,0.08682170542635659,1.127554615926709,1.0,0.006335,1.0107555178268253,0.31866197183098594,0.08408408408408409,0.010641067634189153,0.40704721634954194
frozenset({'parental level of education_some high school'}),"frozenset({'math_cat_μέτριο', 'lunch_standard'})",0.179,0.332,0.067,0.3743016759776537,1.1274146866796797,1.0,0.007572000000000002,1.0676071428571428,0.13765520751904306,0.15090090090090091,0.0633258622419965,0.28805445244665817
"frozenset({'math_cat_μέτριο', 'lunch_standard'})",frozenset({'parental level of education_some high school'}),0.332,0.179,0.067,0.20180722891566266,1.1274146866796797,1.0,0.007572000000000002,1.0285735849056603,0.16918402001966223,0.15090090090090091,0.027779815975228588,0.28805445244665817
"frozenset({'writing_cat_μέτριο', 'gender_female'})","frozenset({'race/ethnicity_group C', 'lunch_standard'})",0.264,0.205,0.061,0.23106060606060605,1.1271249076127126,1.0,0.006879999999999997,1.0338916256157635,0.15324305060584456,0.14950980392156862,0.03278063655422146,0.26431079083518105
"frozenset({'race/ethnicity_group C', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'gender_female'})",0.205,0.264,0.061,0.2975609756097561,1.1271249076127123,1.0,0.006879999999999997,1.047777777777778,0.14187029590679445,0.14950980392156862,0.045599151643690314,0.26431079083518105
frozenset({'writing_cat_μέτριο'}),"frozenset({'parental level of education_high school', 'test preparation course_none', 'lunch_standard'})",0.491,0.094,0.052,0.10590631364562118,1.1266629111236297,1.0,0.005845999999999997,1.0133166287015947,0.22087048511409996,0.09756097560975611,0.013141626540420732,0.3295489015036617
"frozenset({'parental level of education_high school', 'test preparation course_none', 'lunch_standard'})",frozenset({'writing_cat_μέτριο'}),0.094,0.491,0.052,0.5531914893617021,1.1266629111236297,1.0,0.005845999999999997,1.1391904761904763,0.12408728137204952,0.09756097560975611,0.12218367261631072,0.3295489015036617
"frozenset({'lunch_free/reduced', 'test preparation course_none', 'reading_cat_χαμηλό'})",frozenset({'gender_male'}),0.105,0.482,0.057,0.5428571428571429,1.126259632483699,1.0,0.006390000000000007,1.1331250000000002,0.12525727727139088,0.10754716981132077,0.11748483177054622,0.33055720213396567
frozenset({'gender_male'}),"frozenset({'lunch_free/reduced', 'test preparation course_none', 'reading_cat_χαμηλό'})",0.482,0.105,0.057,0.11825726141078839,1.126259632483699,1.0,0.006390000000000007,1.015035294117647,0.21641942694574293,0.10754716981132077,0.014812582581886476,0.33055720213396567
"frozenset({'test preparation course_completed', 'reading_cat_υψηλό', 'lunch_standard'})",frozenset({'gender_female'}),0.096,0.518,0.056,0.5833333333333334,1.1261261261261262,1.0,0.006272,1.1568,0.1238938053097345,0.10035842293906812,0.13554633471645924,0.34572072072072074
frozenset({'writing_cat_υψηλό'}),"frozenset({""parental level of education_associate's degree""})",0.208,0.222,0.052,0.25,1.1261261261261262,1.0,0.005823999999999996,1.0373333333333334,0.14141414141414133,0.13756613756613756,0.03598971722365038,0.24211711711711711
"frozenset({""parental level of education_associate's degree""})",frozenset({'writing_cat_υψηλό'}),0.222,0.208,0.052,0.23423423423423423,1.1261261261261262,1.0,0.005823999999999996,1.034258823529412,0.14395886889460144,0.13756613756613756,0.03312403312403313,0.24211711711711711
frozenset({'gender_female'}),"frozenset({'test preparation course_completed', 'reading_cat_υψηλό', 'lunch_standard'})",0.518,0.096,0.056,0.1081081081081081,1.126126126126126,1.0,0.006272,1.0135757575757576,0.23236514522821577,0.10035842293906812,0.013393924898349665,0.34572072072072074
"frozenset({'test preparation course_none', 'lunch_standard'})","frozenset({'reading_cat_μέτριο', 'race/ethnicity_group D'})",0.418,0.136,0.064,0.15311004784688997,1.1258091753447792,1.0,0.007151999999999999,1.0202033898305085,0.19201030927835047,0.13061224489795917,0.019803296119085607,0.3118491415705038
"frozenset({'reading_cat_μέτριο', 'race/ethnicity_group D'})","frozenset({'test preparation course_none', 'lunch_standard'})",0.136,0.418,0.064,0.47058823529411764,1.1258091753447792,1.0,0.007151999999999999,1.0993333333333335,0.12934027777777776,0.13061224489795917,0.09035779260157673,0.3118491415705038
frozenset({'gender_male'}),"frozenset({'parental level of education_high school', 'test preparation course_none', 'lunch_standard'})",0.482,0.094,0.051,0.10580912863070539,1.1256290279862275,1.0,0.005691999999999996,1.0132064965197216,0.21545915663562706,0.09714285714285716,0.013034358312036858,0.3241811600600335
"frozenset({'parental level of education_high school', 'test preparation course_none', 'lunch_standard'})",frozenset({'gender_male'}),0.094,0.482,0.051,0.5425531914893617,1.1256290279862275,1.0,0.005691999999999996,1.1323720930232557,0.12318746483140709,0.09714285714285716,0.11689805306826577,0.3241811600600335
frozenset({'gender_male'}),"frozenset({'test preparation course_none', 'race/ethnicity_group D', 'lunch_standard'})",0.482,0.118,0.064,0.13278008298755187,1.1252549405724734,1.0,0.007124000000000005,1.017043062200957,0.21488899613899629,0.11940298507462686,0.016757463704707338,0.337576482171742
"frozenset({'test preparation course_none', 'race/ethnicity_group D', 'lunch_standard'})",frozenset({'gender_male'}),0.118,0.482,0.064,0.5423728813559322,1.1252549405724734,1.0,0.007124000000000005,1.131925925925926,0.12620464852607718,0.11940298507462686,0.11654996400759116,0.337576482171742
"frozenset({'test preparation course_none', 'race/ethnicity_group D'})","frozenset({'gender_male', 'lunch_standard'})",0.18,0.316,0.064,0.35555555555555557,1.1251758087201125,1.0,0.007120000000000001,1.0613793103448277,0.13567073170731708,0.14814814814814814,0.05782975958414557,0.2790436005625879
"frozenset({'gender_male', 'lunch_standard'})","frozenset({'test preparation course_none', 'race/ethnicity_group D'})",0.316,0.18,0.064,0.20253164556962025,1.1251758087201125,1.0,0.007120000000000001,1.0282539682539682,0.1626461988304094,0.14814814814814814,0.027477616548317384,0.2790436005625879
"frozenset({'lunch_free/reduced', 'gender_female'})",frozenset({'writing_cat_χαμηλό'}),0.189,0.301,0.064,0.3386243386243386,1.1249978027386665,1.0,0.007111000000000006,1.056888,0.13700292848335402,0.15023474178403756,0.05382594939104236,0.2756244616709733
frozenset({'writing_cat_χαμηλό'}),"frozenset({'lunch_free/reduced', 'gender_female'})",0.301,0.189,0.064,0.21262458471760798,1.1249978027386665,1.0,0.007111000000000006,1.0300042194092827,0.15895475679542215,0.15023474178403756,0.02913019077386927,0.2756244616709733
frozenset({'gender_female'}),"frozenset({'race/ethnicity_group C', 'math_cat_μέτριο', 'lunch_standard'})",0.518,0.103,0.06,0.11583011583011582,1.1245642313603479,1.0,0.006645999999999999,1.0145109170305677,0.22980636237897648,0.10695187165775402,0.014303362129449082,0.349177193837388
"frozenset({'race/ethnicity_group C', 'math_cat_μέτριο', 'lunch_standard'})",frozenset({'gender_female'}),0.103,0.518,0.06,0.5825242718446602,1.1245642313603477,1.0,0.006645999999999999,1.1545581395348836,0.12348569305091042,0.10695187165775402,0.13386778391008328,0.349177193837388
"frozenset({'writing_cat_μέτριο', 'test preparation course_none', 'race/ethnicity_group D'})",frozenset({'lunch_standard'}),0.091,0.645,0.066,0.7252747252747254,1.1244569384104268,1.0,0.007305000000000006,1.2922000000000005,0.12176217621762185,0.09850746268656718,0.22612598668936718,0.4138001533350371
frozenset({'lunch_standard'}),"frozenset({'writing_cat_μέτριο', 'test preparation course_none', 'race/ethnicity_group D'})",0.645,0.091,0.066,0.10232558139534884,1.1244569384104268,1.0,0.007305000000000006,1.0126165803108809,0.31177976952624864,0.09850746268656718,0.012459385473431065,0.4138001533350371
frozenset({'race/ethnicity_group C'}),"frozenset({'gender_female', 'lunch_standard'})",0.319,0.329,0.118,0.36990595611285265,1.1243342131089746,1.0,0.013048999999999991,1.0649203980099502,0.1623858234401054,0.22264150943396224,0.06096267677027218,0.36428428504730775
"frozenset({'gender_female', 'lunch_standard'})",frozenset({'race/ethnicity_group C'}),0.329,0.319,0.118,0.35866261398176286,1.1243342131089744,1.0,0.013048999999999991,1.0618436018957345,0.16480588042132904,0.22264150943396224,0.058241723908609176,0.36428428504730775
"frozenset({'race/ethnicity_group D', 'lunch_standard'})",frozenset({'reading_cat_μέτριο'}),0.167,0.49,0.092,0.5508982035928144,1.1242820481486009,1.0,0.010169999999999998,1.1356,0.13270525601544964,0.16283185840707962,0.11940824233885175,0.36932665281681537
frozenset({'reading_cat_μέτριο'}),"frozenset({'race/ethnicity_group D', 'lunch_standard'})",0.49,0.167,0.092,0.18775510204081633,1.1242820481486007,1.0,0.010169999999999998,1.0255527638190955,0.21675191815856773,0.16283185840707962,0.024916088884533397,0.36932665281681537
"frozenset({'race/ethnicity_group C', 'gender_female', 'lunch_standard'})",frozenset({'reading_cat_μέτριο'}),0.118,0.49,0.065,0.5508474576271187,1.1241784849533036,1.0,0.007180000000000006,1.1354716981132078,0.12523983952555393,0.11970534069981585,0.11930874044533087,0.3417502594258043
"frozenset({'gender_female', 'parental level of education_some college'})",frozenset({'reading_cat_μέτριο'}),0.118,0.49,0.065,0.5508474576271187,1.1241784849533036,1.0,0.007180000000000006,1.1354716981132078,0.12523983952555393,0.11970534069981585,0.11930874044533087,0.3417502594258043
frozenset({'reading_cat_μέτριο'}),"frozenset({'race/ethnicity_group C', 'gender_female', 'lunch_standard'})",0.49,0.118,0.065,0.1326530612244898,1.1241784849533034,1.0,0.007180000000000006,1.016894117647059,0.2165912518853697,0.11970534069981585,0.01661344810032858,0.3417502594258043
frozenset({'reading_cat_μέτριο'}),"frozenset({'gender_female', 'parental level of education_some college'})",0.49,0.118,0.065,0.1326530612244898,1.1241784849533034,1.0,0.007180000000000006,1.016894117647059,0.2165912518853697,0.11970534069981585,0.01661344810032858,0.3417502594258043
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'race/ethnicity_group D'})",frozenset({'gender_male'}),0.12,0.482,0.065,0.5416666666666667,1.1237897648686033,1.0,0.007160000000000007,1.1301818181818184,0.12517482517482528,0.12104283054003727,0.11518661518661535,0.33826071922544954
frozenset({'gender_male'}),"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'race/ethnicity_group D'})",0.482,0.12,0.065,0.13485477178423239,1.1237897648686033,1.0,0.007160000000000007,1.0171702637889688,0.21265221265221282,0.12104283054003727,0.016880422482082262,0.33826071922544954
frozenset({'parental level of education_some college'}),"frozenset({'writing_cat_μέτριο', 'gender_female'})",0.226,0.264,0.067,0.29646017699115046,1.1229552158755698,1.0,0.007336000000000002,1.0461383647798743,0.14146322650314325,0.15839243498817968,0.044103501346671806,0.2751240278895146
"frozenset({'writing_cat_μέτριο', 'gender_female'})",frozenset({'parental level of education_some college'}),0.264,0.226,0.067,0.2537878787878788,1.1229552158755698,1.0,0.007336000000000002,1.037238578680203,0.14876703439325117,0.15839243498817968,0.03590165218072194,0.2751240278895146
"frozenset({'race/ethnicity_group C', 'gender_female'})",frozenset({'reading_cat_μέτριο'}),0.18,0.49,0.099,0.55,1.1224489795918369,1.0,0.010800000000000004,1.1333333333333335,0.1330376940133038,0.17338003502626972,0.11764705882352951,0.3760204081632653
frozenset({'reading_cat_μέτριο'}),"frozenset({'race/ethnicity_group C', 'gender_female'})",0.49,0.18,0.099,0.20204081632653062,1.1224489795918369,1.0,0.010800000000000004,1.0276214833759592,0.2139037433155081,0.17338003502626972,0.026879044300647102,0.3760204081632653
"frozenset({'parental level of education_some college', 'lunch_standard'})",frozenset({'math_cat_μέτριο'}),0.147,0.485,0.08,0.54421768707483,1.1220983238656288,1.0,0.008705000000000004,1.1299253731343286,0.12756447831184065,0.14492753623188404,0.11498580014530095,0.3545830703415387
frozenset({'math_cat_μέτριο'}),"frozenset({'parental level of education_some college', 'lunch_standard'})",0.485,0.147,0.08,0.16494845360824742,1.1220983238656288,1.0,0.008705000000000004,1.0214938271604939,0.2112864077669904,0.14492753623188404,0.021041563432880925,0.3545830703415387
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})","frozenset({'test preparation course_none', 'lunch_standard'})",0.32,0.418,0.15,0.46875,1.1214114832535886,1.0,0.016240000000000004,1.095529411764706,0.15921568627450985,0.25510204081632654,0.08719931271477666,0.41380083732057416
"frozenset({'test preparation course_none', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})",0.418,0.32,0.15,0.3588516746411483,1.1214114832535884,1.0,0.016240000000000004,1.060597014925373,0.18602520045819018,0.25510204081632654,0.05713481564874751,0.41380083732057416
"frozenset({'test preparation course_completed', 'gender_female', 'lunch_standard'})",frozenset({'math_cat_μέτριο'}),0.114,0.485,0.062,0.543859649122807,1.1213601012841383,1.0,0.006710000000000001,1.1290384615384617,0.12215102308308455,0.11545623836126631,0.11429058082098457,0.3358473503345994
frozenset({'math_cat_μέτριο'}),"frozenset({'test preparation course_completed', 'gender_female', 'lunch_standard'})",0.485,0.114,0.062,0.12783505154639174,1.121360101284138,1.0,0.006710000000000001,1.0158628841607564,0.21014719699342313,0.11545623836126631,0.015615182332270584,0.3358473503345994
"frozenset({'race/ethnicity_group C', 'gender_male', 'test preparation course_none'})",frozenset({'reading_cat_μέτριο'}),0.091,0.49,0.05,0.5494505494505495,1.121327651939897,1.0,0.005410000000000005,1.1319512195121952,0.11903190319031913,0.09416195856873825,0.11656970480499904,0.32574568288854006
frozenset({'reading_cat_μέτριο'}),"frozenset({'race/ethnicity_group C', 'gender_male', 'test preparation course_none'})",0.49,0.091,0.05,0.10204081632653061,1.121327651939897,1.0,0.005410000000000005,1.0122954545454546,0.21215686274509823,0.09416195856873825,0.012146112570440723,0.32574568288854006
"frozenset({'gender_female', 'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})",frozenset({'race/ethnicity_group C'}),0.151,0.319,0.054,0.3576158940397351,1.1210529593722103,1.0,0.005831000000000003,1.060113402061856,0.12718666841163903,0.12980769230769232,0.05670469021987531,0.26344744545246945
frozenset({'race/ethnicity_group C'}),"frozenset({'gender_female', 'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})",0.319,0.151,0.054,0.16927899686520376,1.1210529593722103,1.0,0.005831000000000003,1.0220037735849057,0.15856311524446629,0.12980769230769232,0.021530031643349542,0.26344744545246945
frozenset({'lunch_standard'}),"frozenset({'race/ethnicity_group C', 'math_cat_μέτριο', 'gender_female'})",0.645,0.083,0.06,0.09302325581395349,1.1207621182404035,1.0,0.006464999999999992,1.011051282051282,0.303521126760563,0.08982035928143713,0.010930486165707179,0.4079574110395068
"frozenset({'race/ethnicity_group C', 'math_cat_μέτριο', 'gender_female'})",frozenset({'lunch_standard'}),0.083,0.645,0.06,0.7228915662650601,1.1207621182404033,1.0,0.006464999999999992,1.2810869565217384,0.11750272628135208,0.08982035928143713,0.21941286271847918,0.4079574110395068
frozenset({'test preparation course_none'}),"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'race/ethnicity_group D', 'lunch_standard'})",0.642,0.082,0.059,0.09190031152647975,1.1207355064204847,1.0,0.006355999999999994,1.010902229845626,0.3009184736293909,0.08872180451127819,0.010784653078953968,0.40570625332421545
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'race/ethnicity_group D', 'lunch_standard'})",frozenset({'test preparation course_none'}),0.082,0.642,0.059,0.7195121951219512,1.1207355064204847,1.0,0.006355999999999994,1.2763478260869563,0.11735164875743129,0.08872180451127819,0.2165145115138301,0.40570625332421545
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'parental level of education_high school', 'lunch_standard'})",0.418,0.126,0.059,0.14114832535885166,1.1202248044353307,1.0,0.006331999999999997,1.0176378830083566,0.18440212010018048,0.12164948453608246,0.0173321800444527,0.3047011468064099
"frozenset({'parental level of education_high school', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.126,0.418,0.059,0.4682539682539682,1.1202248044353307,1.0,0.006331999999999997,1.0945074626865672,0.12279408912849549,0.12164948453608246,0.08634702449135431,0.3047011468064099
"frozenset({'gender_male', 'race/ethnicity_group D', 'lunch_standard'})",frozenset({'test preparation course_none'}),0.089,0.642,0.064,0.7191011235955057,1.120095208092688,1.0,0.006862,1.2744800000000003,0.11769346871569704,0.095952023988006,0.21536626702655223,0.40939479855787747
frozenset({'test preparation course_none'}),"frozenset({'gender_male', 'race/ethnicity_group D', 'lunch_standard'})",0.642,0.089,0.064,0.09968847352024922,1.1200952080926878,1.0,0.006862,1.0118719723183391,0.2994937150837989,0.095952023988006,0.011732682239571045,0.40939479855787747
"frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό'})","frozenset({'race/ethnicity_group C', 'test preparation course_none'})",0.252,0.202,0.057,0.2261904761904762,1.1197548326261197,1.0,0.006095999999999997,1.0312615384615385,0.1429777652687869,0.14357682619647355,0.03031387993794007,0.2541843470061292
"frozenset({'race/ethnicity_group C', 'test preparation course_none'})","frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό'})",0.202,0.252,0.057,0.28217821782178215,1.1197548326261197,1.0,0.006095999999999997,1.042041379310345,0.13401925867299821,0.14357682619647355,0.040345210991687364,0.2541843470061292
"frozenset({'test preparation course_none', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο'})",0.418,0.325,0.152,0.36363636363636365,1.118881118881119,1.0,0.016149999999999998,1.0607142857142857,0.1825601374570446,0.25719120135363793,0.057239057239057235,0.4156643356643357
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'test preparation course_none', 'lunch_standard'})",0.325,0.418,0.152,0.4676923076923077,1.118881118881119,1.0,0.016149999999999998,1.0933526011560695,0.15740740740740736,0.25719120135363793,0.08538197197991013,0.4156643356643357
"frozenset({'gender_female', 'race/ethnicity_group D'})",frozenset({'math_cat_μέτριο'}),0.129,0.485,0.07,0.5426356589147288,1.1188364101334614,1.0,0.007435000000000011,1.1260169491525427,0.12194521896014451,0.1286764705882353,0.11191390080529859,0.34348277791097265
frozenset({'math_cat_μέτριο'}),"frozenset({'gender_female', 'race/ethnicity_group D'})",0.485,0.129,0.07,0.1443298969072165,1.1188364101334614,1.0,0.007435000000000011,1.0179156626506025,0.20624133148405022,0.1286764705882353,0.017600340880845586,0.34348277791097265
"frozenset({'test preparation course_none', 'lunch_standard'})","frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο'})",0.418,0.169,0.079,0.1889952153110048,1.1183148834970698,1.0,0.008358000000000004,1.0246548672566371,0.18178259167427907,0.15551181102362205,0.024061630939837292,0.32822541830638996
"frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο'})","frozenset({'test preparation course_none', 'lunch_standard'})",0.169,0.418,0.079,0.46745562130177515,1.1183148834970698,1.0,0.008358000000000004,1.0928666666666667,0.1273134396563543,0.15551181102362205,0.08497529433294701,0.32822541830638996
"frozenset({'math_cat_χαμηλό', 'lunch_standard'})",frozenset({'race/ethnicity_group C'}),0.157,0.319,0.056,0.356687898089172,1.118143881157279,1.0,0.005916999999999999,1.0585841584158417,0.1253389256058295,0.13333333333333333,0.055341994257227606,0.2661182437154324
frozenset({'race/ethnicity_group C'}),"frozenset({'math_cat_χαμηλό', 'lunch_standard'})",0.319,0.157,0.056,0.1755485893416928,1.1181438811572788,1.0,0.005916999999999999,1.0224980988593155,0.15515523389972724,0.13333333333333333,0.02200307157970675,0.2661182437154324
"frozenset({'gender_female', 'test preparation course_none'})","frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'lunch_standard'})",0.334,0.225,0.084,0.25149700598802394,1.1177644710578842,1.0,0.008849999999999997,1.0354,0.15819390819390813,0.17684210526315788,0.034189685145837326,0.3124151696606786
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'lunch_standard'})","frozenset({'gender_female', 'test preparation course_none'})",0.225,0.334,0.084,0.37333333333333335,1.1177644710578842,1.0,0.008849999999999997,1.0627659574468082,0.13594470046082943,0.17684210526315788,0.05905905905905906,0.3124151696606786
"frozenset({'gender_female', 'reading_cat_υψηλό'})","frozenset({'math_cat_μέτριο', 'lunch_standard'})",0.159,0.332,0.059,0.3710691823899371,1.1176782602106539,1.0,0.006211999999999995,1.0621199999999997,0.12519397811322266,0.13657407407407407,0.05848679998493573,0.2743900128817155
"frozenset({'math_cat_μέτριο', 'lunch_standard'})","frozenset({'gender_female', 'reading_cat_υψηλό'})",0.332,0.159,0.059,0.17771084337349397,1.1176782602106539,1.0,0.006211999999999995,1.0227545787545786,0.15761696945092854,0.13657407407407407,0.022248327435783546,0.2743900128817155
"frozenset({'parental level of education_high school', 'lunch_standard'})",frozenset({'reading_cat_μέτριο'}),0.126,0.49,0.069,0.5476190476190477,1.1175898931000974,1.0,0.0072600000000000095,1.1273684210526318,0.12038603124067272,0.12614259597806218,0.11297852474323075,0.34421768707482997
frozenset({'reading_cat_μέτριο'}),"frozenset({'parental level of education_high school', 'lunch_standard'})",0.49,0.126,0.069,0.14081632653061227,1.1175898931000974,1.0,0.0072600000000000095,1.0172446555819479,0.20630861040068224,0.12614259597806218,0.01695231868491106,0.34421768707482997
frozenset({'writing_cat_μέτριο'}),"frozenset({'test preparation course_completed', 'gender_male', 'lunch_standard'})",0.491,0.113,0.062,0.12627291242362526,1.1174594019789845,1.0,0.006517000000000002,1.0151911421911421,0.20650865073832314,0.1143911439114391,0.014963824603861621,0.3374727393976533
"frozenset({'test preparation course_completed', 'gender_male', 'lunch_standard'})",frozenset({'writing_cat_μέτριο'}),0.113,0.491,0.062,0.5486725663716814,1.1174594019789845,1.0,0.006517000000000002,1.1277843137254902,0.11850383678219445,0.1143911439114391,0.11330563137854889,0.3374727393976533
"frozenset({'reading_cat_μέτριο', 'math_cat_μέτριο'})",frozenset({'gender_male'}),0.325,0.482,0.175,0.5384615384615384,1.117140121289499,1.0,0.018349999999999977,1.1223333333333334,0.15534391534391517,0.2768987341772152,0.10899910899910897,0.45076603894031275
frozenset({'gender_male'}),"frozenset({'reading_cat_μέτριο', 'math_cat_μέτριο'})",0.482,0.325,0.175,0.3630705394190871,1.117140121289499,1.0,0.018349999999999977,1.059771986970684,0.20242691671263077,0.2768987341772152,0.056400799139388315,0.45076603894031275
frozenset({'test preparation course_none'}),"frozenset({'math_cat_χαμηλό', 'gender_female'})",0.642,0.205,0.147,0.2289719626168224,1.1169364030088897,1.0,0.015389999999999987,1.031090909090909,0.2924409987458669,0.21,0.030153412096631964,0.47302256667426484
"frozenset({'math_cat_χαμηλό', 'gender_female'})",frozenset({'test preparation course_none'}),0.205,0.642,0.147,0.7170731707317073,1.1169364030088897,1.0,0.015389999999999987,1.2653448275862067,0.13169041201386203,0.21,0.20970159422264603,0.47302256667426484
"frozenset({'math_cat_υψηλό', 'writing_cat_υψηλό', 'lunch_standard'})",frozenset({'gender_female'}),0.102,0.518,0.059,0.5784313725490197,1.1166628813687638,1.0,0.006163999999999996,1.1433488372093024,0.11634139896568638,0.10516934046345812,0.12537629159547645,0.3461654932243168
frozenset({'gender_female'}),"frozenset({'math_cat_υψηλό', 'writing_cat_υψηλό', 'lunch_standard'})",0.518,0.102,0.059,0.1138996138996139,1.1166628813687638,1.0,0.006163999999999996,1.0134291938997821,0.21675223292777257,0.10516934046345812,0.013251240422732628,0.3461654932243168
frozenset({'gender_female'}),"frozenset({'lunch_free/reduced', 'reading_cat_μέτριο', 'writing_cat_μέτριο'})",0.518,0.135,0.078,0.15057915057915058,1.1154011154011154,1.0,0.008069999999999994,1.018340909090909,0.21465049473348213,0.13565217391304346,0.018010578704220317,0.36417846417846417
"frozenset({'lunch_free/reduced', 'reading_cat_μέτριο', 'writing_cat_μέτριο'})",frozenset({'gender_female'}),0.135,0.518,0.078,0.5777777777777777,1.1154011154011152,1.0,0.008069999999999994,1.1415789473684208,0.11960871498443744,0.13565217391304346,0.12402028584601185,0.36417846417846417
frozenset({'race/ethnicity_group D'}),"frozenset({'math_cat_μέτριο', 'lunch_standard'})",0.262,0.332,0.097,0.3702290076335878,1.11514761335418,1.0,0.010015999999999997,1.0607030303030303,0.1399156259603833,0.19517102615694162,0.05722905334369428,0.33119884116619147
"frozenset({'math_cat_μέτριο', 'lunch_standard'})",frozenset({'race/ethnicity_group D'}),0.332,0.262,0.097,0.29216867469879515,1.11514761335418,1.0,0.010015999999999997,1.0426212765957446,0.15457744305204021,0.19517102615694162,0.04087896300649748,0.33119884116619147
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'gender_female', 'lunch_standard'})",frozenset({'race/ethnicity_group C'}),0.149,0.319,0.053,0.35570469798657717,1.1150617491742232,1.0,0.005469000000000002,1.0569687500000002,0.12125579229762991,0.12771084337349398,0.053898234928894506,0.26092444930676817
frozenset({'race/ethnicity_group C'}),"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'gender_female', 'lunch_standard'})",0.319,0.149,0.053,0.16614420062695923,1.115061749174223,1.0,0.005469000000000002,1.0205601503759398,0.15152522649821298,0.12771084337349398,0.0201459466826783,0.26092444930676817
"frozenset({'writing_cat_υψηλό', 'gender_female'})","frozenset({'math_cat_μέτριο', 'lunch_standard'})",0.154,0.332,0.057,0.37012987012987014,1.1148490064152714,1.0,0.005872000000000002,1.0605360824742267,0.12177014640620468,0.13286713286713286,0.05708064390699121,0.27090830855891096
"frozenset({'math_cat_μέτριο', 'lunch_standard'})","frozenset({'writing_cat_υψηλό', 'gender_female'})",0.332,0.154,0.057,0.1716867469879518,1.1148490064152714,1.0,0.005872000000000002,1.0213527272727272,0.15421788002941492,0.13286713286713286,0.020906320316727903,0.27090830855891096
"frozenset({'test preparation course_completed', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'gender_female'})",0.227,0.245,0.062,0.27312775330396477,1.1148071563427133,1.0,0.006385000000000002,1.0386969696969697,0.13322622376163254,0.15121951219512195,0.03725530238935732,0.26309448889688036
"frozenset({'math_cat_μέτριο', 'gender_female'})","frozenset({'test preparation course_completed', 'lunch_standard'})",0.245,0.227,0.062,0.2530612244897959,1.114807156342713,1.0,0.006385000000000002,1.0348907103825136,0.1364024781029695,0.15121951219512195,0.03371439131927023,0.26309448889688036
"frozenset({'gender_male', 'lunch_standard'})",frozenset({'parental level of education_high school'}),0.316,0.196,0.069,0.21835443037974686,1.1140532162231982,1.0,0.007064000000000001,1.0285991902834009,0.14967370116111536,0.15575620767494358,0.027804017885257278,0.28519762335313875
frozenset({'parental level of education_high school'}),"frozenset({'gender_male', 'lunch_standard'})",0.196,0.316,0.069,0.3520408163265306,1.1140532162231982,1.0,0.007064000000000001,1.0556220472440945,0.12733434277885933,0.15575620767494358,0.05269125193937224,0.28519762335313875
"frozenset({'reading_cat_μέτριο', 'lunch_standard'})","frozenset({'parental level of education_high school', 'test preparation course_none'})",0.327,0.14,0.051,0.15596330275229356,1.1140235910878111,1.0,0.0052199999999999885,1.0189130434782607,0.15208460798881185,0.12259615384615383,0.01856197994452738,0.26012450851900387
"frozenset({'parental level of education_high school', 'test preparation course_none'})","frozenset({'reading_cat_μέτριο', 'lunch_standard'})",0.14,0.327,0.051,0.3642857142857142,1.114023591087811,1.0,0.0052199999999999885,1.0586516853932584,0.11901504787961671,0.12259615384615383,0.055402250053067155,0.26012450851900387
frozenset({'parental level of education_some high school'}),"frozenset({'gender_male', 'lunch_standard'})",0.179,0.316,0.063,0.35195530726256985,1.113782617919525,1.0,0.006436000000000004,1.0554827586206896,0.12443207083889188,0.14583333333333334,0.052566238687967615,0.27566119793508237
"frozenset({'gender_male', 'lunch_standard'})",frozenset({'parental level of education_some high school'}),0.316,0.179,0.063,0.19936708860759494,1.113782617919525,1.0,0.006436000000000004,1.0254387351778655,0.14935486865311437,0.14583333333333334,0.02480765969256389,0.27566119793508237
"frozenset({'test preparation course_completed', 'reading_cat_μέτριο'})",frozenset({'lunch_free/reduced'}),0.172,0.355,0.068,0.39534883720930236,1.113658696364232,1.0,0.006940000000000016,1.0667307692307693,0.1232594487070193,0.1481481481481482,0.06255633675860833,0.2934490664919751
frozenset({'lunch_free/reduced'}),"frozenset({'test preparation course_completed', 'reading_cat_μέτριο'})",0.355,0.172,0.068,0.1915492957746479,1.113658696364232,1.0,0.006940000000000016,1.0241811846689897,0.1582307341541271,0.1481481481481482,0.023610260597400873,0.2934490664919751
"frozenset({'gender_female', 'race/ethnicity_group D'})",frozenset({'lunch_free/reduced'}),0.129,0.355,0.051,0.3953488372093023,1.1136586963642319,1.0,0.005204999999999994,1.0667307692307693,0.11717430944823383,0.11778290993071593,0.06255633675860825,0.2695054045201441
frozenset({'lunch_free/reduced'}),"frozenset({'gender_female', 'race/ethnicity_group D'})",0.355,0.129,0.051,0.14366197183098592,1.1136586963642319,1.0,0.005204999999999994,1.0171217105263157,0.15823073415412658,0.11778290993071593,0.01683349234326741,0.2695054045201441
"frozenset({'gender_female', 'test preparation course_none'})","frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})",0.334,0.199,0.074,0.22155688622754488,1.1133511870730899,1.0,0.007533999999999985,1.028976923076923,0.15286908530151744,0.16122004357298472,0.028160906651117194,0.29670809135497844
"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})","frozenset({'gender_female', 'test preparation course_none'})",0.199,0.334,0.074,0.371859296482412,1.1133511870730897,1.0,0.007533999999999985,1.0602719999999999,0.12710463272261002,0.16122004357298472,0.05684579051413212,0.29670809135497844
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})",frozenset({'lunch_standard'}),0.266,0.645,0.191,0.7180451127819548,1.1132482368712477,1.0,0.019430000000000003,1.2590666666666663,0.13859366306689302,0.2652777777777778,0.20576088107592902,0.5070845718948533
frozenset({'lunch_standard'}),"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})",0.645,0.266,0.191,0.2961240310077519,1.1132482368712477,1.0,0.019430000000000003,1.0427973568281939,0.2865570385664774,0.2652777777777778,0.04104091417949851,0.5070845718948533
frozenset({'lunch_standard'}),"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'parental level of education_some college'})",0.645,0.078,0.056,0.08682170542635659,1.113098787517392,1.0,0.005690000000000001,1.009660441426146,0.2862173038229377,0.08395802098950526,0.009568010223814087,0.40238521168753727
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'parental level of education_some college'})",frozenset({'lunch_standard'}),0.078,0.645,0.056,0.717948717948718,1.113098787517392,1.0,0.005690000000000001,1.2586363636363636,0.11020297489928726,0.08395802098950526,0.20548934633441673,0.40238521168753727
frozenset({'lunch_standard'}),"frozenset({'race/ethnicity_group C', ""parental level of education_associate's degree""})",0.645,0.078,0.056,0.08682170542635659,1.113098787517392,1.0,0.005690000000000001,1.009660441426146,0.2862173038229377,0.08395802098950526,0.009568010223814087,0.40238521168753727
"frozenset({'race/ethnicity_group C', ""parental level of education_associate's degree""})",frozenset({'lunch_standard'}),0.078,0.645,0.056,0.717948717948718,1.113098787517392,1.0,0.005690000000000001,1.2586363636363636,0.11020297489928726,0.08395802098950526,0.20548934633441673,0.40238521168753727
frozenset({'writing_cat_μέτριο'}),"frozenset({'math_cat_υψηλό', 'gender_male'})",0.491,0.108,0.059,0.12016293279022403,1.1126197480576299,1.0,0.005971999999999998,1.013824074074074,0.19886117678398982,0.10925925925925925,0.013635574876932765,0.33322961454326017
"frozenset({'math_cat_υψηλό', 'gender_male'})",frozenset({'writing_cat_μέτριο'}),0.108,0.491,0.059,0.5462962962962963,1.1126197480576299,1.0,0.005971999999999998,1.1218775510204082,0.11347571634871167,0.10925925925925925,0.10863712435421667,0.33322961454326017
"frozenset({'race/ethnicity_group C', 'math_cat_μέτριο', 'gender_male'})",frozenset({'test preparation course_none'}),0.07,0.642,0.05,0.7142857142857143,1.1125945705384959,1.0,0.005059999999999995,1.2530000000000001,0.10881720430107517,0.07552870090634442,0.20191540303272149,0.3960836671117045
"frozenset({'gender_male', 'race/ethnicity_group D'})",frozenset({'test preparation course_none'}),0.133,0.642,0.095,0.7142857142857143,1.1125945705384959,1.0,0.009613999999999998,1.2530000000000001,0.11672433679354093,0.13970588235294118,0.20191540303272149,0.4311303960836671
frozenset({'parental level of education_high school'}),frozenset({'test preparation course_none'}),0.196,0.642,0.14,0.7142857142857143,1.1125945705384959,1.0,0.014168000000000014,1.2530000000000001,0.12587064676616924,0.20057306590257878,0.20191540303272149,0.46617712505562975
frozenset({'test preparation course_none'}),frozenset({'parental level of education_high school'}),0.642,0.196,0.14,0.2180685358255452,1.1125945705384959,1.0,0.014168000000000014,1.0282231075697212,0.2826815642458103,0.20057306590257878,0.027448427643712917,0.46617712505562975
frozenset({'test preparation course_none'}),"frozenset({'race/ethnicity_group C', 'math_cat_μέτριο', 'gender_male'})",0.642,0.07,0.05,0.07788161993769471,1.1125945705384959,1.0,0.005059999999999995,1.0085472972972973,0.2826815642458098,0.07552870090634442,0.008474860148058825,0.3960836671117045
frozenset({'test preparation course_none'}),"frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο', 'gender_male'})",0.642,0.07,0.05,0.07788161993769471,1.1125945705384959,1.0,0.005059999999999995,1.0085472972972973,0.2826815642458098,0.07552870090634442,0.008474860148058825,0.3960836671117045
"frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο', 'gender_male'})",frozenset({'test preparation course_none'}),0.07,0.642,0.05,0.7142857142857143,1.1125945705384959,1.0,0.005059999999999995,1.2530000000000001,0.10881720430107517,0.07552870090634442,0.20191540303272149,0.3960836671117045
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'gender_female', 'lunch_standard'})",frozenset({'test preparation course_none'}),0.112,0.642,0.08,0.7142857142857143,1.1125945705384959,1.0,0.008095999999999992,1.2530000000000001,0.11396396396396384,0.11869436201780414,0.20191540303272149,0.4194481530930129
frozenset({'test preparation course_none'}),"frozenset({'gender_male', 'race/ethnicity_group D'})",0.642,0.133,0.095,0.14797507788161993,1.1125945705384956,1.0,0.009613999999999998,1.0175758683729432,0.28268156424581,0.13970588235294118,0.017272292827704636,0.4311303960836671
frozenset({'test preparation course_none'}),"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'gender_female', 'lunch_standard'})",0.642,0.112,0.08,0.12461059190031153,1.1125945705384956,1.0,0.008095999999999992,1.014405693950178,0.2826815642458098,0.11869436201780414,0.01420111700485532,0.4194481530930129
frozenset({'lunch_standard'}),"frozenset({'writing_cat_μέτριο', 'test preparation course_none'})",0.645,0.308,0.221,0.3426356589147287,1.1124534380348334,1.0,0.02234,1.052688679245283,0.28474921929768654,0.3019125683060109,0.05005153022359638,0.5300840632235981
"frozenset({'writing_cat_μέτριο', 'test preparation course_none'})",frozenset({'lunch_standard'}),0.308,0.645,0.221,0.7175324675324676,1.1124534380348334,1.0,0.02234,1.2567816091954025,0.14607799544895772,0.3019125683060109,0.20431680995061285,0.5300840632235981
"frozenset({'reading_cat_μέτριο', 'test preparation course_none'})","frozenset({'parental level of education_some college', 'lunch_standard'})",0.318,0.147,0.052,0.16352201257861634,1.112394643391948,1.0,0.005254000000000002,1.019751879699248,0.14815023685991435,0.12590799031476999,0.019369299623231362,0.2586317545886279
"frozenset({'parental level of education_some college', 'lunch_standard'})","frozenset({'reading_cat_μέτριο', 'test preparation course_none'})",0.147,0.318,0.052,0.35374149659863946,1.112394643391948,1.0,0.005254000000000002,1.0553052631578945,0.11845071692668414,0.12590799031476999,0.052406886508269,0.2586317545886279
"frozenset({'test preparation course_completed', 'writing_cat_μέτριο', 'reading_cat_μέτριο'})",frozenset({'lunch_free/reduced'}),0.152,0.355,0.06,0.39473684210526316,1.1119347664936992,1.0,0.006040000000000004,1.0656521739130436,0.11871069182389946,0.1342281879194631,0.06160750713994292,0.2818754633061527
frozenset({'lunch_free/reduced'}),"frozenset({'test preparation course_completed', 'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.355,0.152,0.06,0.16901408450704225,1.111934766493699,1.0,0.006040000000000004,1.0204745762711864,0.15607235142118872,0.1342281879194631,0.02006377889981398,0.2818754633061527
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'gender_male'})",frozenset({'test preparation course_completed'}),0.191,0.358,0.076,0.39790575916230364,1.1114686010120214,1.0,0.007622000000000004,1.0662782608695651,0.1239672109817189,0.16067653276955604,0.06215850336807423,0.30509813097779986
frozenset({'test preparation course_completed'}),"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'gender_male'})",0.358,0.191,0.076,0.2122905027932961,1.1114686010120214,1.0,0.007622000000000004,1.0270283687943262,0.15621413346450244,0.16067653276955604,0.026317061549191702,0.30509813097779986
"frozenset({'parental level of education_high school', 'test preparation course_none'})",frozenset({'gender_male'}),0.14,0.482,0.075,0.5357142857142857,1.111440426793124,1.0,0.007519999999999999,1.1156923076923078,0.11658914728682168,0.13711151736745886,0.10369553226696084,0.3456579727326615
frozenset({'gender_male'}),"frozenset({'parental level of education_high school', 'test preparation course_none'})",0.482,0.14,0.075,0.15560165975103735,1.1114404267931237,1.0,0.007519999999999999,1.0184766584766585,0.19356499356499351,0.13711151736745886,0.018141464826787596,0.3456579727326615
frozenset({'gender_female'}),"frozenset({'race/ethnicity_group C', 'lunch_standard'})",0.518,0.205,0.118,0.2277992277992278,1.1112157453620868,1.0,0.011810000000000001,1.029525,0.20764470075251426,0.19504132231404958,0.02867827396129283,0.40170449194839436
"frozenset({'race/ethnicity_group C', 'lunch_standard'})",frozenset({'gender_female'}),0.205,0.518,0.118,0.5756097560975609,1.1112157453620868,1.0,0.011810000000000001,1.1357471264367816,0.1258927619656753,0.19504132231404958,0.11952231555510563,0.40170449194839436
frozenset({'gender_male'}),"frozenset({'math_cat_χαμηλό', 'test preparation course_none', 'writing_cat_χαμηλό'})",0.482,0.183,0.098,0.20331950207468882,1.1110355304627806,1.0,0.009794000000000011,1.0255052083333334,0.19293199905444824,0.1728395061728395,0.024870871572446548,0.3694193138788745
"frozenset({'math_cat_χαμηλό', 'test preparation course_none', 'writing_cat_χαμηλό'})",frozenset({'gender_male'}),0.183,0.482,0.098,0.5355191256830601,1.1110355304627804,1.0,0.009794000000000011,1.1152235294117647,0.12232408263182888,0.1728395061728395,0.1033187754499231,0.3694193138788745
"frozenset({'gender_female', 'test preparation course_none'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})",0.334,0.283,0.105,0.3143712574850299,1.1108524999471023,1.0,0.010478000000000001,1.045755458515284,0.14983554983554986,0.205078125,0.04375349718972098,0.342697996233681
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})","frozenset({'gender_female', 'test preparation course_none'})",0.283,0.334,0.105,0.3710247349823322,1.1108524999471023,1.0,0.010478000000000001,1.0588651685393258,0.13917779106063627,0.205078125,0.05559269516866692,0.342697996233681
"frozenset({'race/ethnicity_group E', 'lunch_standard'})",frozenset({'gender_male'}),0.099,0.482,0.053,0.5353535353535354,1.1106919820612766,1.0,0.005281999999999995,1.1148260869565219,0.11061085167424027,0.10037878787878789,0.10299910299910303,0.32265602078880085
frozenset({'gender_male'}),"frozenset({'race/ethnicity_group E', 'lunch_standard'})",0.482,0.099,0.053,0.10995850622406639,1.1106919820612766,1.0,0.005281999999999995,1.0123123543123544,0.19239455088511676,0.10037878787878789,0.012162604022271236,0.32265602078880085
"frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",frozenset({'race/ethnicity_group C'}),0.223,0.319,0.079,0.3542600896860987,1.1105331965081464,1.0,0.007862999999999995,1.0546041666666668,0.12809735594545713,0.17062634989200864,0.05177693052290554,0.3009544962537076
frozenset({'race/ethnicity_group C'}),"frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",0.319,0.223,0.079,0.2476489028213166,1.1105331965081462,1.0,0.007862999999999995,1.0327625,0.14615513299503696,0.17062634989200864,0.03172316965420412,0.3009544962537076
"frozenset({'math_cat_χαμηλό', 'test preparation course_none', 'lunch_standard'})",frozenset({'gender_female'}),0.12,0.518,0.069,0.5750000000000001,1.1100386100386102,1.0,0.006840000000000006,1.1341176470588237,0.1126482213438736,0.12126537785588755,0.11825726141078849,0.3541023166023166
frozenset({'gender_female'}),"frozenset({'math_cat_χαμηλό', 'test preparation course_none', 'lunch_standard'})",0.518,0.12,0.069,0.13320463320463322,1.1100386100386102,1.0,0.006840000000000006,1.0152338530066816,0.20566480245354518,0.12126537785588755,0.015005265005265023,0.3541023166023166
frozenset({'lunch_standard'}),"frozenset({'gender_male', 'parental level of education_some high school'})",0.645,0.088,0.063,0.09767441860465116,1.1099365750528543,1.0,0.0062400000000000025,1.0107216494845361,0.279007377598927,0.09402985074626867,0.01060791513667891,0.40679175475687107
"frozenset({'gender_male', 'parental level of education_some high school'})",frozenset({'lunch_standard'}),0.088,0.645,0.063,0.7159090909090909,1.109936575052854,1.0,0.0062400000000000025,1.2496,0.10860484544695076,0.09402985074626867,0.19974391805377725,0.40679175475687107
frozenset({'test preparation course_none'}),"frozenset({'parental level of education_high school', 'writing_cat_μέτριο', 'lunch_standard'})",0.642,0.073,0.052,0.08099688473520249,1.1095463662356506,1.0,0.005134,1.0087016949152543,0.2757842715943275,0.0784313725490196,0.008626628624813906,0.39666282592924507
"frozenset({'parental level of education_high school', 'writing_cat_μέτριο', 'lunch_standard'})",frozenset({'test preparation course_none'}),0.073,0.642,0.052,0.7123287671232876,1.1095463662356504,1.0,0.005134,1.2444761904761903,0.10650568417558709,0.0784313725490196,0.19644907017678107,0.39666282592924507
"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο', 'gender_female'})",frozenset({'test preparation course_none'}),0.132,0.642,0.094,0.712121212121212,1.109223071839894,1.0,0.009256,1.2435789473684207,0.11344249436219238,0.13823529411764704,0.19586930760115093,0.42926932880203905
frozenset({'test preparation course_none'}),"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο', 'gender_female'})",0.642,0.132,0.094,0.14641744548286603,1.109223071839894,1.0,0.009256,1.016890510948905,0.2750505170569357,0.13823529411764704,0.01660996023371662,0.42926932880203905
"frozenset({""parental level of education_associate's degree"", 'test preparation course_none', 'lunch_standard'})",frozenset({'reading_cat_μέτριο'}),0.092,0.49,0.05,0.5434782608695653,1.1091393078970722,1.0,0.004920000000000008,1.1171428571428574,0.1083700440528636,0.09398496240601506,0.10485933503836335,0.32275953859804796
frozenset({'reading_cat_μέτριο'}),"frozenset({""parental level of education_associate's degree"", 'test preparation course_none', 'lunch_standard'})",0.49,0.092,0.05,0.10204081632653061,1.109139307897072,1.0,0.004920000000000008,1.0111818181818182,0.19294117647058853,0.09398496240601506,0.011058167760496273,0.32275953859804796
frozenset({'gender_male'}),"frozenset({'parental level of education_some high school', 'lunch_standard'})",0.482,0.118,0.063,0.13070539419087138,1.1076728321260287,1.0,0.0061240000000000044,1.014615751789976,0.18765704479990206,0.11731843575418996,0.01440520883318751,0.33230184963780857
"frozenset({'parental level of education_some high school', 'lunch_standard'})",frozenset({'gender_male'}),0.118,0.482,0.063,0.5338983050847458,1.1076728321260287,1.0,0.0061240000000000044,1.1113454545454546,0.11021128027930757,0.11731843575418996,0.10018977815587991,0.33230184963780857
frozenset({'parental level of education_some high school'}),"frozenset({'writing_cat_μέτριο', 'lunch_standard'})",0.179,0.338,0.067,0.3743016759776537,1.1074014082179102,1.0,0.006498000000000004,1.058017857142857,0.11813041976475729,0.1488888888888889,0.05483636854630462,0.2862632640243298
"frozenset({'writing_cat_μέτριο', 'lunch_standard'})",frozenset({'parental level of education_some high school'}),0.338,0.179,0.067,0.1982248520710059,1.1074014082179102,1.0,0.006498000000000004,1.0239778597785978,0.14650313387744068,0.1488888888888889,0.023416384982954824,0.2862632640243298
frozenset({'reading_cat_μέτριο'}),"frozenset({'parental level of education_high school', 'test preparation course_none', 'lunch_standard'})",0.49,0.094,0.051,0.10408163265306122,1.107251411202779,1.0,0.00494,1.01125284738041,0.18992695117262592,0.09568480300187618,0.011127629859890974,0.32331741207121145
"frozenset({'parental level of education_high school', 'test preparation course_none', 'lunch_standard'})",frozenset({'reading_cat_μέτριο'}),0.094,0.49,0.051,0.5425531914893617,1.107251411202779,1.0,0.00494,1.1148837209302325,0.10691252218326625,0.09568480300187618,0.10304547350855228,0.32331741207121145
frozenset({'reading_cat_μέτριο'}),"frozenset({'test preparation course_none', 'race/ethnicity_group D', 'lunch_standard'})",0.49,0.118,0.064,0.1306122448979592,1.1068834313386373,1.0,0.006180000000000005,1.0145070422535212,0.18933823529411778,0.11764705882352941,0.014299597389976418,0.3364925631269457
"frozenset({'test preparation course_none', 'race/ethnicity_group D', 'lunch_standard'})",frozenset({'reading_cat_μέτριο'}),0.118,0.49,0.064,0.5423728813559322,1.106883431338637,1.0,0.006180000000000005,1.1144444444444443,0.10948129251700689,0.11764705882352941,0.10269192422731806,0.3364925631269457
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'gender_female'})","frozenset({'test preparation course_none', 'lunch_standard'})",0.227,0.418,0.105,0.46255506607929514,1.1065910671753472,1.0,0.010113999999999998,1.0829016393442625,0.12461036160906792,0.19444444444444442,0.07655509635617724,0.3568756191640495
"frozenset({'test preparation course_none', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'gender_female'})",0.418,0.227,0.105,0.2511961722488038,1.1065910671753472,1.0,0.010113999999999998,1.0323130990415337,0.16550482736049743,0.19444444444444442,0.031301645858737126,0.3568756191640495
"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced', 'reading_cat_χαμηλό', 'writing_cat_χαμηλό'})",frozenset({'gender_male'}),0.12,0.482,0.064,0.5333333333333333,1.1065006915629323,1.0,0.006160000000000006,1.11,0.1093750000000001,0.11895910780669144,0.09909909909909911,0.3330567081604426
frozenset({'gender_male'}),"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced', 'reading_cat_χαμηλό', 'writing_cat_χαμηλό'})",0.482,0.12,0.064,0.13278008298755187,1.1065006915629323,1.0,0.006160000000000006,1.0147368421052632,0.18581081081081097,0.11895910780669144,0.014522821576763491,0.3330567081604426
"frozenset({'math_cat_χαμηλό', 'writing_cat_χαμηλό'})","frozenset({'gender_female', 'test preparation course_none'})",0.23,0.334,0.085,0.3695652173913044,1.1064826868003124,1.0,0.008179999999999993,1.0564137931034483,0.12498090145148956,0.17745302713987474,0.05340122731427082,0.3120281176776881
"frozenset({'gender_female', 'test preparation course_none'})","frozenset({'math_cat_χαμηλό', 'writing_cat_χαμηλό'})",0.334,0.23,0.085,0.25449101796407186,1.1064826868003124,1.0,0.008179999999999993,1.03285140562249,0.14449743861508554,0.17745302713987474,0.03180651683645695,0.3120281176776881
"frozenset({'writing_cat_μέτριο', 'gender_female'})","frozenset({'race/ethnicity_group C', 'test preparation course_none'})",0.264,0.202,0.059,0.22348484848484845,1.1063606360636062,1.0,0.0056719999999999896,1.0276682926829268,0.13061901252763425,0.14496314496314494,0.02692336902863213,0.25778202820282026
"frozenset({'race/ethnicity_group C', 'test preparation course_none'})","frozenset({'writing_cat_μέτριο', 'gender_female'})",0.202,0.264,0.059,0.29207920792079206,1.1063606360636062,1.0,0.0056719999999999896,1.0396643356643358,0.12047066819591329,0.14496314496314494,0.03815109771846746,0.25778202820282026
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})",frozenset({'parental level of education_some college'}),0.208,0.226,0.052,0.25,1.1061946902654867,1.0,0.0049919999999999964,1.032,0.12121212121212113,0.13612565445026178,0.031007751937984485,0.2400442477876106
frozenset({'parental level of education_some college'}),"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})",0.226,0.208,0.052,0.2300884955752212,1.1061946902654867,1.0,0.0049919999999999964,1.0286896551724136,0.1240310077519379,0.13612565445026178,0.027889514615178305,0.2400442477876106
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})",frozenset({'race/ethnicity_group D'}),0.283,0.262,0.082,0.2897526501766785,1.1059261457125134,1.0,0.007854000000000014,1.0390746268656716,0.13358505969996962,0.17710583153347736,0.03760521704156978,0.30136487470665985
frozenset({'race/ethnicity_group D'}),"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})",0.262,0.283,0.082,0.31297709923664124,1.1059261457125134,1.0,0.007854000000000014,1.0436333333333334,0.12978385881419813,0.17710583153347736,0.04180906448625002,0.30136487470665985
frozenset({'gender_male'}),"frozenset({'race/ethnicity_group D', 'lunch_standard'})",0.482,0.167,0.089,0.18464730290456433,1.105672472482421,1.0,0.008506,1.021643765903308,0.18450392607695978,0.15892857142857142,0.021185237580509383,0.3587907173205456
"frozenset({'race/ethnicity_group D', 'lunch_standard'})",frozenset({'gender_male'}),0.167,0.482,0.089,0.5329341317365269,1.105672472482421,1.0,0.008506,1.1090512820512821,0.1147335338629834,0.15892857142857142,0.09832843964580491,0.3587907173205456
"frozenset({'writing_cat_μέτριο', 'gender_female'})","frozenset({'test preparation course_none', 'lunch_standard'})",0.264,0.418,0.122,0.4621212121212121,1.105553139045962,1.0,0.011647999999999992,1.0820281690140847,0.12972202423378465,0.2178571428571429,0.07580964281995207,0.37699362041467305
"frozenset({'test preparation course_none', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'gender_female'})",0.418,0.264,0.122,0.291866028708134,1.105553139045962,1.0,0.011647999999999992,1.0393513513513515,0.16404709593825686,0.2178571428571429,0.037861452049095086,0.37699362041467305
"frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο', 'test preparation course_none'})",frozenset({'gender_female'}),0.117,0.518,0.067,0.5726495726495726,1.1055011055011055,1.0,0.006393999999999997,1.1278799999999998,0.10807795676205602,0.11795774647887323,0.11338085611944523,0.35099660099660096
frozenset({'gender_female'}),"frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο', 'test preparation course_none'})",0.518,0.117,0.067,0.12934362934362933,1.1055011055011053,1.0,0.006393999999999997,1.0141773835920178,0.19799343531306113,0.11795774647887323,0.013979195179648162,0.35099660099660096
frozenset({'reading_cat_μέτριο'}),"frozenset({'parental level of education_some college', 'test preparation course_none', 'lunch_standard'})",0.49,0.096,0.052,0.10612244897959183,1.1054421768707483,1.0,0.004959999999999999,1.011324200913242,0.18702865761689288,0.09737827715355807,0.011197399313707777,0.32389455782312926
"frozenset({'parental level of education_some college', 'test preparation course_none', 'lunch_standard'})",frozenset({'reading_cat_μέτριο'}),0.096,0.49,0.052,0.5416666666666666,1.1054421768707483,1.0,0.004959999999999999,1.1127272727272726,0.10551395507147718,0.09737827715355807,0.1013071895424836,0.32389455782312926
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'gender_female'})",frozenset({'test preparation course_none'}),0.155,0.642,0.11,0.7096774193548387,1.1054165410511507,1.0,0.01049,1.2331111111111113,0.11285637439483594,0.1601164483260553,0.18904307082357188,0.44050849160888356
frozenset({'test preparation course_none'}),"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'gender_female'})",0.642,0.155,0.11,0.17133956386292834,1.1054165410511505,1.0,0.01049,1.0197180451127819,0.2663788725241239,0.1601164483260553,0.019336761967962534,0.44050849160888356
frozenset({'writing_cat_μέτριο'}),"frozenset({'math_cat_υψηλό', 'gender_male', 'lunch_standard'})",0.491,0.094,0.051,0.10386965376782077,1.1049963166789443,1.0,0.004845999999999996,1.0110136363636364,0.18667899379791195,0.0955056179775281,0.010893657580376127,0.3232114226285912
"frozenset({'math_cat_υψηλό', 'gender_male', 'lunch_standard'})",frozenset({'writing_cat_μέτριο'}),0.094,0.491,0.051,0.5425531914893617,1.1049963166789443,1.0,0.004845999999999996,1.1126976744186046,0.10487815435224855,0.0955056179775281,0.10128328386907988,0.3232114226285912
"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced', 'gender_female'})",frozenset({'test preparation course_none'}),0.11,0.642,0.078,0.7090909090909091,1.1045029736618521,1.0,0.007379999999999998,1.230625,0.10630942091616247,0.11572700296735905,0.18740477399695277,0.41529311809685643
frozenset({'test preparation course_none'}),"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced', 'gender_female'})",0.642,0.11,0.078,0.12149532710280374,1.1045029736618521,1.0,0.007379999999999998,1.0130851063829787,0.2642887838418564,0.11572700296735905,0.012916097868318805,0.41529311809685643
"frozenset({'writing_cat_μέτριο', 'test preparation course_none'})","frozenset({'parental level of education_some college', 'lunch_standard'})",0.308,0.147,0.05,0.16233766233766234,1.1043378390317167,1.0,0.004724000000000006,1.0183100775193799,0.13653179190751463,0.12345679012345681,0.017980846820237217,0.25123685837971554
"frozenset({'parental level of education_some college', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'test preparation course_none'})",0.147,0.308,0.05,0.34013605442176875,1.1043378390317167,1.0,0.004724000000000006,1.048701030927835,0.11076201641266134,0.12345679012345681,0.04643938500255601,0.25123685837971554
"frozenset({'race/ethnicity_group C', 'gender_female'})","frozenset({'reading_cat_μέτριο', 'lunch_standard'})",0.18,0.327,0.065,0.36111111111111116,1.1043153244988109,1.0,0.00614,1.0533913043478262,0.11519699812382737,0.14705882352941177,0.05068515766881299,0.27994393476044854
"frozenset({'reading_cat_μέτριο', 'lunch_standard'})","frozenset({'race/ethnicity_group C', 'gender_female'})",0.327,0.18,0.065,0.19877675840978593,1.1043153244988106,1.0,0.00614,1.023435114503817,0.14035889815978966,0.14705882352941177,0.022898485865592597,0.27994393476044854
"frozenset({'writing_cat_μέτριο', 'gender_female'})",frozenset({'race/ethnicity_group C'}),0.264,0.319,0.093,0.35227272727272724,1.104303220290681,1.0,0.008784,1.0513684210526317,0.12833099579242638,0.18979591836734694,0.04885863035642765,0.3219043887147335
frozenset({'race/ethnicity_group C'}),"frozenset({'writing_cat_μέτριο', 'gender_female'})",0.319,0.264,0.093,0.2915360501567398,1.1043032202906808,1.0,0.008784,1.0388672566371682,0.1386954668182464,0.18979591836734694,0.037413111626005115,0.3219043887147335
frozenset({'parental level of education_some high school'}),frozenset({'math_cat_χαμηλό'}),0.179,0.339,0.067,0.3743016759776537,1.1041347373972084,1.0,0.006319000000000005,1.0564196428571428,0.11487628847237634,0.14855875831485588,0.05340646895257739,0.28597089698587697
frozenset({'math_cat_χαμηλό'}),frozenset({'parental level of education_some high school'}),0.339,0.179,0.067,0.1976401179941003,1.1041347373972084,1.0,0.006319000000000005,1.0232316176470588,0.14268295436584108,0.14855875831485588,0.02270416320840477,0.28597089698587697
frozenset({'gender_male'}),"frozenset({""parental level of education_associate's degree"", 'math_cat_μέτριο'})",0.482,0.094,0.05,0.1037344398340249,1.103557870574733,1.0,0.004692000000000002,1.0108611111111112,0.1811583011583012,0.09505703422053234,0.010744414827842054,0.31782466672552306
"frozenset({""parental level of education_associate's degree"", 'math_cat_μέτριο'})",frozenset({'gender_male'}),0.094,0.482,0.05,0.5319148936170213,1.1035578705747329,1.0,0.004692000000000002,1.1066363636363636,0.10357615894039737,0.09505703422053234,0.09636079848845808,0.31782466672552306
"frozenset({'race/ethnicity_group C', 'gender_female'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.18,0.418,0.083,0.46111111111111114,1.103136629452419,1.0,0.007760000000000017,1.0800000000000003,0.11401704378489591,0.16116504854368932,0.07407407407407414,0.32983785220627326
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'race/ethnicity_group C', 'gender_female'})",0.418,0.18,0.083,0.19856459330143542,1.103136629452419,1.0,0.007760000000000017,1.0231641791044777,0.16064257028112483,0.16116504854368932,0.02263974792857978,0.32983785220627326
"frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό'})","frozenset({'test preparation course_none', 'race/ethnicity_group D'})",0.252,0.18,0.05,0.19841269841269843,1.1022927689594357,1.0,0.004640000000000005,1.022970297029703,0.12406417112299478,0.13089005235602094,0.022454510259388334,0.2380952380952381
"frozenset({'test preparation course_none', 'race/ethnicity_group D'})","frozenset({'writing_cat_χαμηλό', 'reading_cat_χαμηλό'})",0.18,0.252,0.05,0.2777777777777778,1.1022927689594357,1.0,0.004640000000000005,1.0356923076923077,0.11317073170731717,0.13089005235602094,0.034462269756387415,0.2380952380952381
"frozenset({'writing_cat_υψηλό', 'reading_cat_υψηλό', 'lunch_standard'})","frozenset({'gender_female', 'test preparation course_none'})",0.144,0.334,0.053,0.3680555555555556,1.101962741184298,1.0,0.004903999999999999,1.05389010989011,0.10809381061541172,0.12470588235294118,0.05113446780113448,0.26336909514304724
"frozenset({'gender_female', 'test preparation course_none'})","frozenset({'writing_cat_υψηλό', 'reading_cat_υψηλό', 'lunch_standard'})",0.334,0.144,0.053,0.1586826347305389,1.101962741184298,1.0,0.004903999999999999,1.0174519572953735,0.13893138421440307,0.12470588235294118,0.017152610666517416,0.26336909514304724
frozenset({'parental level of education_some college'}),"frozenset({'reading_cat_μέτριο', 'gender_female'})",0.226,0.261,0.065,0.28761061946902655,1.1019563964330519,1.0,0.0060139999999999985,1.0373540372670806,0.11953885907374276,0.15402843601895735,0.03600895733291819,0.26832638253144814
"frozenset({'reading_cat_μέτριο', 'gender_female'})",frozenset({'parental level of education_some college'}),0.261,0.226,0.065,0.24904214559386972,1.1019563964330519,1.0,0.0060139999999999985,1.0306836734693878,0.1252003747267617,0.15402843601895735,0.02977021394556811,0.26832638253144814
"frozenset({'writing_cat_μέτριο', 'lunch_standard'})",frozenset({'parental level of education_high school'}),0.338,0.196,0.073,0.2159763313609467,1.1019200579640138,1.0,0.006751999999999994,1.0254792452830188,0.13971775027935263,0.15835140997830802,0.024846183284759578,0.2942126554763917
frozenset({'parental level of education_high school'}),"frozenset({'writing_cat_μέτριο', 'lunch_standard'})",0.196,0.338,0.073,0.3724489795918367,1.1019200579640138,1.0,0.006751999999999994,1.054894308943089,0.11504123219518834,0.15835140997830802,0.052037733522411915,0.2942126554763917
frozenset({'race/ethnicity_group C'}),"frozenset({""parental level of education_associate's degree""})",0.319,0.222,0.078,0.2445141065830721,1.1014148945183428,1.0,0.007181999999999994,1.0298008298755186,0.13520840393087077,0.16846652267818574,0.02893844033813894,0.29793272896721174
"frozenset({""parental level of education_associate's degree""})",frozenset({'race/ethnicity_group C'}),0.222,0.319,0.078,0.35135135135135137,1.1014148945183428,1.0,0.007181999999999994,1.0498750000000001,0.11835080087008097,0.16846652267818574,0.04750565543517087,0.29793272896721174
"frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'gender_male'})",frozenset({'lunch_standard'}),0.145,0.645,0.103,0.7103448275862069,1.1013098102111734,1.0,0.009474999999999997,1.225595238095238,0.10759098393232269,0.1499272197962154,0.18406993686255452,0.43501737503341353
frozenset({'lunch_standard'}),"frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'gender_male'})",0.645,0.145,0.103,0.15968992248062014,1.1013098102111734,1.0,0.009474999999999997,1.017481549815498,0.25912758102010114,0.1499272197962154,0.017181195883766258,0.43501737503341353
frozenset({'test preparation course_none'}),"frozenset({'race/ethnicity_group C', 'math_cat_χαμηλό', 'gender_female'})",0.642,0.075,0.053,0.08255451713395638,1.1007268951194185,1.0,0.00485,1.0082342954159593,0.25561294402867085,0.07981927710843374,0.008167045550223118,0.3946105919003115
"frozenset({'race/ethnicity_group C', 'math_cat_χαμηλό', 'gender_female'})",frozenset({'test preparation course_none'}),0.075,0.642,0.053,0.7066666666666667,1.1007268951194185,1.0,0.00485,1.2204545454545455,0.09892911779704233,0.07981927710843374,0.180633147113594,0.3946105919003115
"frozenset({'math_cat_υψηλό', 'writing_cat_υψηλό'})",frozenset({'gender_female'}),0.114,0.518,0.065,0.5701754385964912,1.1007247849353112,1.0,0.005947999999999995,1.1213877551020408,0.10328181976037497,0.11463844797178131,0.10824779791803156,0.34782903203955834
frozenset({'gender_female'}),"frozenset({'math_cat_υψηλό', 'writing_cat_υψηλό'})",0.518,0.114,0.065,0.12548262548262548,1.1007247849353112,1.0,0.005947999999999995,1.013130242825607,0.18984998404085526,0.11463844797178131,0.012960073908155166,0.34782903203955834
"frozenset({'reading_cat_μέτριο', 'parental level of education_some college'})",frozenset({'gender_female'}),0.114,0.518,0.065,0.5701754385964912,1.1007247849353112,1.0,0.005947999999999995,1.1213877551020408,0.10328181976037497,0.11463844797178131,0.10824779791803156,0.34782903203955834
frozenset({'gender_female'}),"frozenset({'reading_cat_μέτριο', 'parental level of education_some college'})",0.518,0.114,0.065,0.12548262548262548,1.1007247849353112,1.0,0.005947999999999995,1.013130242825607,0.18984998404085526,0.11463844797178131,0.012960073908155166,0.34782903203955834
frozenset({'test preparation course_none'}),"frozenset({'race/ethnicity_group D', 'lunch_standard'})",0.642,0.167,0.118,0.1838006230529595,1.1006025332512543,1.0,0.01078599999999999,1.0205839694656489,0.25532620017043817,0.170767004341534,0.020168815189627234,0.4451937247001324
"frozenset({'race/ethnicity_group D', 'lunch_standard'})",frozenset({'test preparation course_none'}),0.167,0.642,0.118,0.7065868263473053,1.1006025332512543,1.0,0.01078599999999999,1.2201224489795914,0.10973202840458209,0.170767004341534,0.18041012946174662,0.4451937247001324
frozenset({'test preparation course_none'}),"frozenset({'parental level of education_high school', 'math_cat_μέτριο'})",0.642,0.092,0.065,0.10124610591900311,1.1005011512935121,1.0,0.005936000000000004,1.0102876949740034,0.25509239363987984,0.09715994020926756,0.0101829360341444,0.40388392252471894
"frozenset({'parental level of education_high school', 'math_cat_μέτριο'})",frozenset({'test preparation course_none'}),0.092,0.642,0.065,0.7065217391304348,1.1005011512935121,1.0,0.005936000000000004,1.219851851851852,0.10057607590647244,0.09715994020926756,0.1802283215933933,0.40388392252471894
frozenset({'writing_cat_μέτριο'}),"frozenset({'test preparation course_completed', 'gender_male'})",0.491,0.174,0.094,0.1914460285132383,1.1002645316852777,1.0,0.008566000000000004,1.0215768261964737,0.17903273000877826,0.1646234676007005,0.02112109989496164,0.3658379567853548
"frozenset({'test preparation course_completed', 'gender_male'})",frozenset({'writing_cat_μέτριο'}),0.174,0.491,0.094,0.5402298850574713,1.1002645316852775,1.0,0.008566000000000004,1.107075,0.11032404306836334,0.1646234676007005,0.09671883115416754,0.3658379567853548
"frozenset({'reading_cat_μέτριο', 'math_cat_μέτριο', 'writing_cat_μέτριο'})",frozenset({'gender_male'}),0.281,0.482,0.149,0.5302491103202847,1.100101888631296,1.0,0.013557999999999987,1.1027121212121211,0.12655533879082606,0.24267100977198697,0.09314500061831017,0.41968887051283943
frozenset({'gender_male'}),"frozenset({'reading_cat_μέτριο', 'math_cat_μέτριο', 'writing_cat_μέτριο'})",0.482,0.281,0.149,0.3091286307053942,1.100101888631296,1.0,0.013557999999999987,1.0407147147147147,0.17566271928688018,0.24267100977198697,0.03912187858886534,0.41968887051283943
"frozenset({'test preparation course_none', 'lunch_standard'})","frozenset({'reading_cat_μέτριο', 'gender_female'})",0.418,0.261,0.12,0.28708133971291866,1.0999285046471978,1.0,0.010901999999999995,1.0365838926174495,0.15609965635738823,0.21466905187835417,0.03529274656687233,0.37342572732772367
"frozenset({'reading_cat_μέτριο', 'gender_female'})","frozenset({'test preparation course_none', 'lunch_standard'})",0.261,0.418,0.12,0.4597701149425287,1.0999285046471978,1.0,0.010901999999999995,1.0773191489361702,0.12293640054127195,0.21466905187835417,0.07176995694592558,0.37342572732772367
"frozenset({'gender_female', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'test preparation course_none'})",0.329,0.304,0.11,0.3343465045592705,1.0998240281554952,1.0,0.009983999999999993,1.0455890410958903,0.13526622408887676,0.21032504780114722,0.043601299654124306,0.3480943049112142
"frozenset({'math_cat_μέτριο', 'test preparation course_none'})","frozenset({'gender_female', 'lunch_standard'})",0.304,0.329,0.11,0.3618421052631579,1.0998240281554952,1.0,0.009983999999999993,1.0514639175257734,0.1304075235109717,0.21032504780114722,0.04894501529531728,0.3480943049112142
"frozenset({'reading_cat_μέτριο', 'gender_male', 'writing_cat_μέτριο', 'test preparation course_none', 'math_cat_μέτριο'})",frozenset({'lunch_standard'}),0.086,0.645,0.061,0.7093023255813954,1.0996935280331712,1.0,0.00553,1.2212,0.09918570864870682,0.091044776119403,0.18113331149688838,0.40193798449612406
frozenset({'lunch_standard'}),"frozenset({'reading_cat_μέτριο', 'gender_male', 'writing_cat_μέτριο', 'test preparation course_none', 'math_cat_μέτριο'})",0.645,0.086,0.061,0.09457364341085271,1.0996935280331712,1.0,0.00553,1.0094691780821918,0.2553682752251213,0.091044776119403,0.009380353841195534,0.40193798449612406
"frozenset({'writing_cat_μέτριο', 'gender_male'})",frozenset({'lunch_standard'}),0.227,0.645,0.161,0.7092511013215859,1.0996141105761021,1.0,0.014584999999999987,1.2209848484848482,0.11719283584967807,0.22644163150492266,0.18098901780728407,0.4794317522111805
frozenset({'lunch_standard'}),"frozenset({'writing_cat_μέτριο', 'gender_male'})",0.645,0.227,0.161,0.2496124031007752,1.0996141105761021,1.0,0.014584999999999987,1.030134297520661,0.2551832735543695,0.22644163150492266,0.029252785382632856,0.4794317522111805
"frozenset({'test preparation course_completed', 'writing_cat_μέτριο', 'lunch_standard'})",frozenset({'gender_male'}),0.117,0.482,0.062,0.5299145299145299,1.0994077384118879,1.0,0.005606,1.1019272727272729,0.1024001753552771,0.11545623836126631,0.09249909249909254,0.3292726176543604
frozenset({'gender_male'}),"frozenset({'test preparation course_completed', 'writing_cat_μέτριο', 'lunch_standard'})",0.482,0.117,0.062,0.1286307053941909,1.0994077384118879,1.0,0.005606,1.013347619047619,0.1745547390708681,0.11545623836126631,0.013171806788438142,0.3292726176543604
"frozenset({'parental level of education_high school', 'test preparation course_none'})","frozenset({'writing_cat_μέτριο', 'lunch_standard'})",0.14,0.338,0.052,0.3714285714285714,1.0989010989010988,1.0,0.00467999999999999,1.053181818181818,0.10465116279069746,0.1220657276995305,0.05049633146309874,0.2626373626373626
"frozenset({'writing_cat_μέτριο', 'lunch_standard'})","frozenset({'parental level of education_high school', 'test preparation course_none'})",0.338,0.14,0.052,0.15384615384615383,1.0989010989010985,1.0,0.00467999999999999,1.0163636363636364,0.13595166163141967,0.1220657276995305,0.01610017889087653,0.2626373626373626
"frozenset({'reading_cat_μέτριο', 'lunch_standard'})","frozenset({'gender_female', 'test preparation course_none'})",0.327,0.334,0.12,0.36697247706422015,1.0987199912102399,1.0,0.010781999999999986,1.052086956521739,0.1335066864784545,0.22181146025878,0.04950822382015035,0.3631269570949843
"frozenset({'gender_female', 'test preparation course_none'})","frozenset({'reading_cat_μέτριο', 'lunch_standard'})",0.334,0.327,0.12,0.35928143712574845,1.0987199912102399,1.0,0.010781999999999986,1.0503831775700934,0.13490990990990975,0.22181146025878,0.04796647418387583,0.3631269570949843
"frozenset({'math_cat_χαμηλό', 'race/ethnicity_group B'})",frozenset({'test preparation course_none'}),0.078,0.642,0.055,0.7051282051282052,1.098330537582874,1.0,0.004923999999999998,1.2140869565217394,0.09710116347860377,0.08270676691729324,0.17633576851453955,0.39539899352983465
frozenset({'test preparation course_none'}),"frozenset({'math_cat_χαμηλό', 'race/ethnicity_group B'})",0.642,0.078,0.055,0.08566978193146417,1.098330537582874,1.0,0.004923999999999998,1.008388415672913,0.2500761808024377,0.08270676691729324,0.008318635500503439,0.39539899352983465
"frozenset({'race/ethnicity_group C', 'test preparation course_none'})",frozenset({'reading_cat_χαμηλό'}),0.202,0.275,0.061,0.30198019801980197,1.098109810981098,1.0,0.00544999999999999,1.0386524822695036,0.11196022843995213,0.14663461538461536,0.037214066234209586,0.26189918991899186
frozenset({'reading_cat_χαμηλό'}),"frozenset({'race/ethnicity_group C', 'test preparation course_none'})",0.275,0.202,0.061,0.2218181818181818,1.098109810981098,1.0,0.00544999999999999,1.0254672897196262,0.12323346523459558,0.14663461538461536,0.024834814308498484,0.26189918991899186
"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο'})",frozenset({'lunch_standard'}),0.281,0.645,0.199,0.7081850533807829,1.097961323070981,1.0,0.017754999999999993,1.216524390243902,0.12409055010798073,0.27372764786795045,0.17798606586136012,0.5083560925818643
frozenset({'lunch_standard'}),"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.645,0.281,0.199,0.3085271317829457,1.097961323070981,1.0,0.017754999999999993,1.0398094170403587,0.25132705782433284,0.27372764786795045,0.03828530150618316,0.5083560925818643
frozenset({'gender_female'}),"frozenset({'lunch_free/reduced', 'writing_cat_μέτριο'})",0.518,0.153,0.087,0.16795366795366792,1.0977363918540388,1.0,0.007745999999999989,1.0179721577726217,0.18471884389755305,0.14897260273972598,0.017654861810705934,0.36829055946703004
"frozenset({'lunch_free/reduced', 'writing_cat_μέτριο'})",frozenset({'gender_female'}),0.153,0.518,0.087,0.5686274509803921,1.0977363918540388,1.0,0.007745999999999989,1.1173636363636363,0.10511745307983539,0.14897260273972598,0.10503620535351062,0.36829055946703004
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'gender_female', 'lunch_standard'})",frozenset({'test preparation course_none'}),0.149,0.642,0.105,0.7046979865771812,1.0976604152292542,1.0,0.009342000000000003,1.2123181818181816,0.10454926976666112,0.1530612244897959,0.17513404071838318,0.43412469422317
frozenset({'test preparation course_none'}),"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'gender_female', 'lunch_standard'})",0.642,0.149,0.105,0.16355140186915887,1.0976604152292542,1.0,0.009342000000000003,1.0173966480446928,0.24852354349561062,0.1530612244897959,0.017099179634734285,0.43412469422317
frozenset({'lunch_standard'}),"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο', 'race/ethnicity_group D'})",0.645,0.089,0.063,0.09767441860465116,1.0974653775803502,1.0,0.005595000000000003,1.0096134020618557,0.2501676727028841,0.09388971684053651,0.009521864549562201,0.40276979357198855
"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο', 'race/ethnicity_group D'})",frozenset({'lunch_standard'}),0.089,0.645,0.063,0.7078651685393259,1.0974653775803502,1.0,0.005595000000000003,1.215192307692308,0.09748575610266065,0.09388971684053651,0.177084981800918,0.40276979357198855
frozenset({'gender_female'}),"frozenset({'writing_cat_μέτριο', 'test preparation course_none'})",0.518,0.308,0.175,0.33783783783783783,1.0968760968760969,1.0,0.015455999999999998,1.0450612244897959,0.18323651452282155,0.26881720430107525,0.043118262771441956,0.453009828009828
"frozenset({'writing_cat_μέτριο', 'test preparation course_none'})",frozenset({'gender_female'}),0.308,0.518,0.175,0.5681818181818181,1.0968760968760967,1.0,0.015455999999999998,1.1162105263157893,0.1276300578034682,0.26881720430107525,0.10411165597887574,0.453009828009828
frozenset({'lunch_standard'}),frozenset({'race/ethnicity_group E'}),0.645,0.14,0.099,0.15348837209302327,1.0963455149501662,1.0,0.0087,1.0159340659340659,0.2475458813486982,0.1443148688046647,0.01568415359653867,0.43031561461794016
frozenset({'race/ethnicity_group E'}),frozenset({'lunch_standard'}),0.14,0.645,0.099,0.7071428571428571,1.096345514950166,1.0,0.0087,1.2121951219512193,0.10218463706835798,0.1443148688046647,0.17505030181086495,0.43031561461794016
"frozenset({'math_cat_μέτριο', 'parental level of education_some college'})",frozenset({'gender_female'}),0.118,0.518,0.067,0.5677966101694916,1.0961324520646556,1.0,0.005876000000000006,1.11521568627451,0.09943479879514006,0.11775043936731108,0.10331246923131028,0.34857011975656044
frozenset({'gender_female'}),"frozenset({'math_cat_μέτριο', 'parental level of education_some college'})",0.518,0.118,0.067,0.12934362934362933,1.0961324520646554,1.0,0.005876000000000006,1.013028824833703,0.18195330401932264,0.11775043936731108,0.01286125775921694,0.34857011975656044
frozenset({'gender_female'}),"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})",0.518,0.266,0.151,0.2915057915057915,1.0958864342322987,1.0,0.013211999999999974,1.036,0.1815283998790906,0.23854660347551343,0.034749034749034693,0.4295874822190611
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})",frozenset({'gender_female'}),0.266,0.518,0.151,0.5676691729323308,1.0958864342322987,1.0,0.013211999999999974,1.114886956521739,0.11920529801324481,0.23854660347551343,0.10304807662309284,0.4295874822190611
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})",frozenset({'race/ethnicity_group D'}),0.418,0.262,0.12,0.28708133971291866,1.095730304247781,1.0,0.010483999999999993,1.035181208053691,0.15011454753722786,0.2142857142857143,0.033985555166556436,0.3725483034442456
frozenset({'race/ethnicity_group D'}),"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.262,0.418,0.12,0.4580152671755725,1.095730304247781,1.0,0.010483999999999993,1.073830985915493,0.1183830171635049,0.2142857142857143,0.06875475459720364,0.3725483034442456
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'test preparation course_completed', 'lunch_free/reduced'})",0.418,0.131,0.06,0.14354066985645933,1.095730304247781,1.0,0.005241999999999997,1.0146424581005586,0.15011454753722786,0.1226993865030675,0.014431150582807048,0.3007779685160159
"frozenset({'test preparation course_completed', 'lunch_free/reduced'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.131,0.418,0.06,0.4580152671755725,1.095730304247781,1.0,0.005241999999999997,1.073830985915493,0.10053701572688908,0.1226993865030675,0.06875475459720364,0.3007779685160159
frozenset({'writing_cat_μέτριο'}),"frozenset({'gender_female', 'lunch_standard'})",0.491,0.329,0.177,0.3604887983706721,1.0957106333455078,1.0,0.015460999999999975,1.049238853503185,0.17161155694671035,0.2752721617418351,0.04692816448684364,0.44924135967165824
"frozenset({'gender_female', 'lunch_standard'})",frozenset({'writing_cat_μέτριο'}),0.329,0.491,0.177,0.5379939209726443,1.0957106333455078,1.0,0.015460999999999975,1.1017171052631578,0.13017925854824972,0.2752721617418351,0.09232597440598098,0.44924135967165824
frozenset({'lunch_standard'}),"frozenset({'race/ethnicity_group C', 'math_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})",0.645,0.075,0.053,0.08217054263565891,1.0956072351421189,1.0,0.004624999999999997,1.0078125,0.24581450969970756,0.07946026986506748,0.007751937984496126,0.3944186046511628
"frozenset({'race/ethnicity_group C', 'math_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})",frozenset({'lunch_standard'}),0.075,0.645,0.053,0.7066666666666667,1.0956072351421189,1.0,0.004624999999999997,1.2102272727272727,0.09433962264150937,0.07946026986506748,0.17370892018779338,0.3944186046511628
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})","frozenset({'gender_male', 'lunch_standard'})",0.208,0.316,0.072,0.34615384615384615,1.095423563777994,1.0,0.006272,1.0461176470588234,0.10998877665544332,0.15929203539823006,0.04408457040035986,0.28700097370983446
"frozenset({'gender_male', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})",0.316,0.208,0.072,0.22784810126582278,1.095423563777994,1.0,0.006272,1.0257049180327868,0.12735542560103966,0.15929203539823006,0.025060733921493418,0.28700097370983446
"frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",frozenset({'race/ethnicity_group D'}),0.223,0.262,0.064,0.28699551569506726,1.0954027316605621,1.0,0.005573999999999996,1.0350566037735849,0.11208976833976825,0.15201900237529692,0.03386926245944072,0.2656351624276863
frozenset({'race/ethnicity_group D'}),"frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",0.262,0.223,0.064,0.24427480916030533,1.0954027316605621,1.0,0.005573999999999996,1.028151515151515,0.11801321138211372,0.15201900237529692,0.02738070677001973,0.2656351624276863
frozenset({'lunch_standard'}),"frozenset({'parental level of education_high school', 'math_cat_μέτριο'})",0.645,0.092,0.065,0.10077519379844961,1.0953825412874958,1.0,0.005659999999999998,1.0097586206896552,0.24528710725893818,0.0967261904761905,0.009664310350715431,0.4036484664644422
"frozenset({'parental level of education_high school', 'math_cat_μέτριο'})",frozenset({'lunch_standard'}),0.092,0.645,0.065,0.7065217391304348,1.0953825412874958,1.0,0.005659999999999998,1.2096296296296296,0.09589969501863772,0.0967261904761905,0.17330067360685858,0.4036484664644422
"frozenset({'parental level of education_some college', 'test preparation course_none', 'lunch_standard'})",frozenset({'math_cat_μέτριο'}),0.096,0.485,0.051,0.53125,1.0953608247422681,1.0,0.0044399999999999995,1.0986666666666667,0.0963040083289953,0.09622641509433963,0.08980582524271848,0.31820231958762885
frozenset({'math_cat_μέτριο'}),"frozenset({'parental level of education_some college', 'test preparation course_none', 'lunch_standard'})",0.485,0.096,0.051,0.10515463917525773,1.095360824742268,1.0,0.0044399999999999995,1.0102304147465437,0.16904625928041117,0.09622641509433963,0.010126813246966517,0.31820231958762885
"frozenset({'gender_female', 'lunch_standard'})","frozenset({""parental level of education_associate's degree""})",0.329,0.222,0.08,0.24316109422492402,1.0953202442564145,1.0,0.006961999999999996,1.0279598393574298,0.12969448584202672,0.16985138004246284,0.027199349903501303,0.3017607272926422
"frozenset({""parental level of education_associate's degree""})","frozenset({'gender_female', 'lunch_standard'})",0.222,0.329,0.08,0.36036036036036034,1.0953202442564143,1.0,0.006961999999999996,1.0490281690140846,0.11185732647814903,0.16985138004246284,0.04673675165478439,0.3017607272926422
frozenset({'gender_male'}),"frozenset({'test preparation course_none', 'race/ethnicity_group D'})",0.482,0.18,0.095,0.19709543568464732,1.0949746426924851,1.0,0.008240000000000011,1.0212919896640826,0.16744564112985186,0.16754850088183423,0.020848092298350402,0.3624366067312126
"frozenset({'test preparation course_none', 'race/ethnicity_group D'})",frozenset({'gender_male'}),0.18,0.482,0.095,0.5277777777777778,1.0949746426924851,1.0,0.008240000000000011,1.0969411764705883,0.10577663671373569,0.16754850088183423,0.08837408837408843,0.3624366067312126
"frozenset({'reading_cat_μέτριο', 'parental level of education_some college', 'lunch_standard'})",frozenset({'test preparation course_none'}),0.074,0.642,0.052,0.7027027027027027,1.0945524964216553,1.0,0.004491999999999996,1.2041818181818182,0.093287921581658,0.0783132530120482,0.16956062207458863,0.3918497937189526
frozenset({'test preparation course_none'}),"frozenset({'reading_cat_μέτριο', 'parental level of education_some college', 'lunch_standard'})",0.642,0.074,0.052,0.08099688473520249,1.0945524964216553,1.0,0.004491999999999996,1.007613559322034,0.2412978083369143,0.0783132530120482,0.007556031031536167,0.3918497937189526
"frozenset({'test preparation course_none', 'race/ethnicity_group D'})","frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο'})",0.18,0.325,0.064,0.35555555555555557,1.0940170940170941,1.0,0.005500000000000005,1.0474137931034484,0.10480182926829276,0.14512471655328799,0.04526748971193416,0.27623931623931625
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'test preparation course_none', 'race/ethnicity_group D'})",0.325,0.18,0.064,0.19692307692307692,1.0940170940170941,1.0,0.005500000000000005,1.021072796934866,0.1273148148148149,0.14512471655328799,0.02063789868667918,0.27623931623931625
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})","frozenset({'test preparation course_none', 'race/ethnicity_group D'})",0.32,0.18,0.063,0.196875,1.09375,1.0,0.005400000000000002,1.0210116731517511,0.12605042016806728,0.14416475972540047,0.020579268292682928,0.2734375
"frozenset({'test preparation course_none', 'race/ethnicity_group D'})","frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})",0.18,0.32,0.063,0.35000000000000003,1.09375,1.0,0.005400000000000002,1.0461538461538462,0.1045296167247387,0.14416475972540047,0.044117647058823574,0.2734375
frozenset({'test preparation course_none'}),"frozenset({'writing_cat_μέτριο', 'race/ethnicity_group D', 'lunch_standard'})",0.642,0.094,0.066,0.102803738317757,1.0936567906144363,1.0,0.005652000000000004,1.0098125,0.2392077196546472,0.09850746268656718,0.009717150461100449,0.40246569894611256
"frozenset({'writing_cat_μέτριο', 'race/ethnicity_group D', 'lunch_standard'})",frozenset({'test preparation course_none'}),0.094,0.642,0.066,0.7021276595744681,1.0936567906144363,1.0,0.005652000000000004,1.201857142857143,0.0945213726670681,0.09850746268656718,0.1679543563532628,0.40246569894611256
"frozenset({'reading_cat_μέτριο', 'math_cat_μέτριο'})","frozenset({'lunch_free/reduced', 'gender_male'})",0.325,0.166,0.059,0.1815384615384615,1.093605189990732,1.0,0.005049999999999992,1.018984962406015,0.12680477087256728,0.13657407407407407,0.01863124884707614,0.26848007414272473
"frozenset({'lunch_free/reduced', 'gender_male'})","frozenset({'reading_cat_μέτριο', 'math_cat_μέτριο'})",0.166,0.325,0.059,0.35542168674698793,1.093605189990732,1.0,0.005049999999999992,1.047196261682243,0.10262976059830087,0.13657407407407407,0.045069165551093214,0.26848007414272473
"frozenset({'math_cat_μέτριο', 'gender_female', 'lunch_standard'})",frozenset({'race/ethnicity_group C'}),0.172,0.319,0.06,0.3488372093023256,1.0935335714806445,1.0,0.005132000000000005,1.0458214285714287,0.10330112721417078,0.13921113689095127,0.043813816890345934,0.26846249179849824
frozenset({'race/ethnicity_group C'}),"frozenset({'math_cat_μέτριο', 'gender_female', 'lunch_standard'})",0.319,0.172,0.06,0.18808777429467083,1.0935335714806445,1.0,0.005132000000000005,1.019814671814672,0.1255996084189918,0.13921113689095127,0.01942967909984401,0.26846249179849824
"frozenset({""parental level of education_associate's degree"", 'test preparation course_none'})",frozenset({'reading_cat_μέτριο'}),0.14,0.49,0.075,0.5357142857142857,1.0932944606413995,1.0,0.006399999999999989,1.0984615384615384,0.09922480620155022,0.13513513513513511,0.08963585434173668,0.3443877551020408
frozenset({'reading_cat_μέτριο'}),"frozenset({""parental level of education_associate's degree"", 'test preparation course_none'})",0.49,0.14,0.075,0.15306122448979592,1.0932944606413992,1.0,0.006399999999999989,1.015421686746988,0.1673202614379082,0.13513513513513511,0.015187470336971987,0.3443877551020408
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'parental level of education_some college'})",frozenset({'lunch_standard'}),0.078,0.645,0.055,0.7051282051282052,1.0932220234545817,1.0,0.00469,1.203913043478261,0.09248668901597316,0.08233532934131738,0.16937522571325397,0.3951997614788313
frozenset({'lunch_standard'}),"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'parental level of education_some college'})",0.645,0.078,0.055,0.08527131782945736,1.0932220234545815,1.0,0.00469,1.007949152542373,0.2402048655569782,0.08233532934131738,0.007886461854075232,0.3951997614788313
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'test preparation course_none', 'lunch_standard'})",0.418,0.418,0.191,0.45693779904306225,1.0931526292896228,1.0,0.016276000000000013,1.0717004405286348,0.1464169410410033,0.2961240310077519,0.06690343478189392,0.45693779904306225
"frozenset({'test preparation course_none', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.418,0.418,0.191,0.45693779904306225,1.0931526292896228,1.0,0.016276000000000013,1.0717004405286348,0.1464169410410033,0.2961240310077519,0.06690343478189392,0.45693779904306225
"frozenset({""parental level of education_associate's degree"", 'gender_female'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.116,0.418,0.053,0.4568965517241379,1.093053951493153,1.0,0.004511999999999995,1.0716190476190477,0.09630325279603849,0.11018711018711018,0.06683256309989333,0.29184540504867185
frozenset({'race/ethnicity_group C'}),"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",0.319,0.152,0.053,0.16614420062695923,1.0930539514931528,1.0,0.004512000000000002,1.0169624060150375,0.12501038982628215,0.12679425837320574,0.016679481871414187,0.2574142055766375
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",frozenset({'race/ethnicity_group C'}),0.152,0.319,0.053,0.34868421052631576,1.0930539514931528,1.0,0.004512000000000002,1.0455757575757576,0.10039159843360632,0.12679425837320574,0.04358914908416411,0.2574142055766375
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({""parental level of education_associate's degree"", 'gender_female'})",0.418,0.116,0.053,0.12679425837320574,1.0930539514931528,1.0,0.004511999999999995,1.0123616438356164,0.14627504376580416,0.11018711018711018,0.012210699517201053,0.29184540504867185
"frozenset({""parental level of education_associate's degree""})",frozenset({'reading_cat_υψηλό'}),0.222,0.235,0.057,0.25675675675675674,1.0925819436457735,1.0,0.004830000000000001,1.0292727272727273,0.10891624949262618,0.14250000000000002,0.028440204910793146,0.24965497412305923
frozenset({'reading_cat_υψηλό'}),"frozenset({""parental level of education_associate's degree""})",0.235,0.222,0.057,0.24255319148936172,1.0925819436457735,1.0,0.004830000000000001,1.0271348314606743,0.1107671138630891,0.14250000000000002,0.026417983919488065,0.24965497412305923
"frozenset({'gender_female', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})",0.329,0.32,0.115,0.3495440729483283,1.0923252279635258,1.0,0.009719999999999993,1.0454205607476637,0.12596384371152713,0.2153558052434457,0.04344716610048277,0.35445953647416417
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})","frozenset({'gender_female', 'lunch_standard'})",0.32,0.329,0.115,0.359375,1.0923252279635258,1.0,0.009719999999999993,1.0474146341463415,0.12429667519181578,0.2153558052434457,0.04526825633383008,0.35445953647416417
"frozenset({'math_cat_χαμηλό', 'writing_cat_χαμηλό'})","frozenset({'gender_male', 'test preparation course_none', 'lunch_standard'})",0.23,0.203,0.051,0.22173913043478258,1.0923109873634609,1.0,0.004309999999999994,1.0240782122905026,0.10975299210593314,0.13350785340314134,0.023512083356063446,0.23648532876418932
"frozenset({'gender_male', 'test preparation course_none', 'lunch_standard'})","frozenset({'math_cat_χαμηλό', 'writing_cat_χαμηλό'})",0.203,0.23,0.051,0.25123152709359603,1.0923109873634609,1.0,0.004309999999999994,1.0283552631578947,0.1060348857234235,0.13350785340314134,0.02757341180986496,0.23648532876418932
"frozenset({'race/ethnicity_group C', 'test preparation course_none'})","frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.202,0.281,0.062,0.3069306930693069,1.0922800465099889,1.0,0.005237999999999993,1.0374142857142858,0.10586951249090454,0.14726840855106887,0.036064941681928926,0.26378563123216237
"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'race/ethnicity_group C', 'test preparation course_none'})",0.281,0.202,0.062,0.22064056939501778,1.0922800465099889,1.0,0.005237999999999993,1.023917808219178,0.11750190677015551,0.14726840855106887,0.023359109517566118,0.26378563123216237
"frozenset({'reading_cat_μέτριο', 'test preparation course_none'})",frozenset({'race/ethnicity_group D'}),0.318,0.262,0.091,0.2861635220125786,1.0922271832541168,1.0,0.007683999999999996,1.0338502202643172,0.12381167219876893,0.1860940695296523,0.03274189974604146,0.3167458831436939
frozenset({'race/ethnicity_group D'}),"frozenset({'reading_cat_μέτριο', 'test preparation course_none'})",0.262,0.318,0.091,0.34732824427480913,1.0922271832541168,1.0,0.007683999999999996,1.0449356725146197,0.11441674856308998,0.1860940695296523,0.043003290725526576,0.3167458831436939
frozenset({'race/ethnicity_group E'}),"frozenset({'reading_cat_μέτριο', 'lunch_standard'})",0.14,0.327,0.05,0.35714285714285715,1.09217999126256,1.0,0.004219999999999995,1.046888888888889,0.09813953488372079,0.1199040767386091,0.044788792188495,0.2550240279598078
"frozenset({""parental level of education_associate's degree"", 'test preparation course_none'})","frozenset({'reading_cat_μέτριο', 'lunch_standard'})",0.14,0.327,0.05,0.35714285714285715,1.09217999126256,1.0,0.004219999999999995,1.046888888888889,0.09813953488372079,0.1199040767386091,0.044788792188495,0.2550240279598078
"frozenset({'reading_cat_μέτριο', 'lunch_standard'})","frozenset({""parental level of education_associate's degree"", 'test preparation course_none'})",0.327,0.14,0.05,0.1529051987767584,1.0921799912625598,1.0,0.004219999999999995,1.0152346570397113,0.12540861812778584,0.1199040767386091,0.015006045089253938,0.2550240279598078
"frozenset({'reading_cat_μέτριο', 'lunch_standard'})",frozenset({'race/ethnicity_group E'}),0.327,0.14,0.05,0.1529051987767584,1.0921799912625598,1.0,0.004219999999999995,1.0152346570397113,0.12540861812778584,0.1199040767386091,0.015006045089253938,0.2550240279598078
"frozenset({'race/ethnicity_group C', 'lunch_free/reduced'})",frozenset({'reading_cat_μέτριο'}),0.114,0.49,0.061,0.5350877192982456,1.0920157536698891,1.0,0.005139999999999999,1.0969811320754717,0.09510417052140767,0.1123388581952118,0.08840729274165808,0.3297887576083065
frozenset({'reading_cat_μέτριο'}),"frozenset({'race/ethnicity_group C', 'lunch_free/reduced'})",0.49,0.114,0.061,0.12448979591836734,1.092015753669889,1.0,0.005139999999999999,1.0119813519813519,0.1652201864352298,0.1123388581952118,0.011839498779195639,0.3297887576083065
"frozenset({'test preparation course_none', 'race/ethnicity_group B'})",frozenset({'gender_female'}),0.122,0.518,0.069,0.5655737704918034,1.091841255775682,1.0,0.005804000000000004,1.1095094339622642,0.09580403420157808,0.12084063047285466,0.09870076865519364,0.3493892018482183
frozenset({'gender_female'}),"frozenset({'test preparation course_none', 'race/ethnicity_group B'})",0.518,0.122,0.069,0.13320463320463322,1.091841255775682,1.0,0.005804000000000004,1.0129265033407573,0.17451440254976255,0.12084063047285466,0.012761541235345352,0.3493892018482183
"frozenset({'parental level of education_high school', 'reading_cat_μέτριο'})",frozenset({'lunch_standard'}),0.098,0.645,0.069,0.7040816326530612,1.0915994304698624,1.0,0.0057900000000000035,1.1996551724137932,0.09302998168321608,0.1023738872403561,0.16642713423397526,0.4055291884195539
frozenset({'lunch_standard'}),"frozenset({'parental level of education_high school', 'reading_cat_μέτριο'})",0.645,0.098,0.069,0.10697674418604652,1.0915994304698624,1.0,0.0057900000000000035,1.0100520833333333,0.23637477036129836,0.1023738872403561,0.009952044552158005,0.4055291884195539
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'gender_male', 'test preparation course_none', 'lunch_standard'})",0.325,0.203,0.072,0.22153846153846152,1.0913224706328153,1.0,0.006024999999999989,1.0238142292490118,0.12397119341563763,0.15789473684210525,0.023260303059550194,0.28810913224706325
"frozenset({'gender_male', 'test preparation course_none', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο'})",0.203,0.325,0.072,0.354679802955665,1.0913224706328153,1.0,0.006024999999999989,1.045992366412214,0.10499442353269187,0.15789473684210525,0.043970078452836986,0.28810913224706325
"frozenset({'reading_cat_μέτριο', 'parental level of education_some college'})","frozenset({'test preparation course_none', 'lunch_standard'})",0.114,0.418,0.052,0.45614035087719296,1.091244858557878,1.0,0.004347999999999998,1.0701290322580645,0.0943740232679284,0.10833333333333332,0.06553324892988482,0.2902711323763955
"frozenset({'test preparation course_none', 'lunch_standard'})","frozenset({'reading_cat_μέτριο', 'parental level of education_some college'})",0.418,0.114,0.052,0.12440191387559808,1.0912448585578778,1.0,0.004347999999999998,1.011879781420765,0.1436690457309013,0.10833333333333332,0.011740309114670517,0.2902711323763955
frozenset({'math_cat_χαμηλό'}),frozenset({'race/ethnicity_group C'}),0.339,0.319,0.118,0.34808259587020646,1.0911680121323086,1.0,0.009858999999999979,1.0446108597285069,0.1264006769404341,0.2185185185185185,0.04270572080793898,0.35899427599152955
frozenset({'race/ethnicity_group C'}),frozenset({'math_cat_χαμηλό'}),0.319,0.339,0.118,0.36990595611285265,1.0911680121323086,1.0,0.009858999999999979,1.049049751243781,0.12268846910077377,0.2185185185185185,0.04675636325696313,0.35899427599152955
frozenset({'lunch_standard'}),"frozenset({'test preparation course_completed', 'writing_cat_υψηλό', 'gender_female'})",0.645,0.081,0.057,0.08837209302325581,1.0910134941142693,1.0,0.004755000000000002,1.0080867346938776,0.23498888065233517,0.0852017937219731,0.008021864007895332,0.39603789836347975
"frozenset({'test preparation course_completed', 'writing_cat_υψηλό', 'gender_female'})",frozenset({'lunch_standard'}),0.081,0.645,0.057,0.7037037037037037,1.0910134941142693,1.0,0.004755000000000002,1.198125,0.09077372429986831,0.0852017937219731,0.1653625456442358,0.39603789836347975
frozenset({'parental level of education_high school'}),"frozenset({'reading_cat_μέτριο', 'test preparation course_none'})",0.196,0.318,0.068,0.3469387755102041,1.0910024387113335,1.0,0.005672000000000003,1.0443124999999998,0.10374597600234128,0.15246636771300448,0.04243222215572445,0.28038762674881273
"frozenset({'reading_cat_μέτριο', 'test preparation course_none'})",frozenset({'parental level of education_high school'}),0.318,0.196,0.068,0.2138364779874214,1.0910024387113335,1.0,0.005672000000000003,1.022688,0.12230464033120587,0.15246636771300448,0.022184674113708187,0.28038762674881273
"frozenset({'math_cat_μέτριο', 'test preparation course_none'})",frozenset({'parental level of education_high school'}),0.304,0.196,0.065,0.21381578947368424,1.0908968850698175,1.0,0.005416000000000004,1.0226610878661089,0.11971706454465085,0.14942528735632185,0.022158942131448044,0.2727242212674543
frozenset({'parental level of education_high school'}),"frozenset({'math_cat_μέτριο', 'test preparation course_none'})",0.196,0.304,0.065,0.33163265306122447,1.0908968850698173,1.0,0.005416000000000004,1.0413435114503815,0.10363566781477236,0.14942528735632185,0.03970208773164437,0.2727242212674543
"frozenset({'math_cat_μέτριο', 'gender_female'})",frozenset({'race/ethnicity_group D'}),0.245,0.262,0.07,0.28571428571428575,1.0905125408942205,1.0,0.00581000000000001,1.0332000000000001,0.10993377483443725,0.16018306636155608,0.03213317847464193,0.27644492911668483
frozenset({'race/ethnicity_group D'}),"frozenset({'math_cat_μέτριο', 'gender_female'})",0.262,0.245,0.07,0.26717557251908397,1.0905125408942202,1.0,0.00581000000000001,1.0302604166666667,0.1124661246612468,0.16018306636155608,0.0293716192305748,0.27644492911668483
"frozenset({'gender_female', 'test preparation course_none'})","frozenset({'writing_cat_υψηλό', 'lunch_standard'})",0.334,0.162,0.059,0.17664670658682632,1.0904117690544834,1.0,0.004891999999999994,1.017789090909091,0.12449737873466672,0.13501144164759724,0.017478170151344057,0.2704221187255119
"frozenset({'writing_cat_υψηλό', 'lunch_standard'})","frozenset({'gender_female', 'test preparation course_none'})",0.162,0.334,0.059,0.3641975308641975,1.0904117690544834,1.0,0.004891999999999994,1.0474951456310677,0.09894421746693083,0.13501144164759724,0.04534163793423043,0.2704221187255119
"frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'race/ethnicity_group D'})",frozenset({'lunch_standard'}),0.091,0.645,0.064,0.7032967032967034,1.090382485731323,1.0,0.005305000000000004,1.1964814814814817,0.09118880638063813,0.09523809523809525,0.16421606562451643,0.40126075474912687
frozenset({'lunch_standard'}),"frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'race/ethnicity_group D'})",0.645,0.091,0.064,0.09922480620155039,1.090382485731323,1.0,0.005305000000000004,1.0091308089500861,0.23349471830985932,0.09523809523809525,0.009048191640869514,0.40126075474912687
"frozenset({'reading_cat_μέτριο', 'gender_female'})","frozenset({'math_cat_χαμηλό', 'test preparation course_none'})",0.261,0.246,0.07,0.26819923371647514,1.0902407874653461,1.0,0.0057940000000000075,1.0303350785340315,0.11200463947419306,0.16018306636155608,0.02944195453113414,0.27637603962246526
"frozenset({'math_cat_χαμηλό', 'test preparation course_none'})","frozenset({'reading_cat_μέτριο', 'gender_female'})",0.246,0.261,0.07,0.28455284552845533,1.0902407874653461,1.0,0.0057940000000000075,1.0329204545454547,0.10977643046608576,0.16018306636155608,0.0318712388747704,0.27637603962246526
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})",frozenset({'lunch_standard'}),0.32,0.645,0.225,0.703125,1.0901162790697674,1.0,0.018600000000000005,1.1957894736842105,0.12156862745098043,0.304054054054054,0.16373239436619713,0.5259811046511628
frozenset({'lunch_standard'}),"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})",0.645,0.32,0.225,0.3488372093023256,1.0901162790697674,1.0,0.018600000000000005,1.044285714285714,0.23286384976525829,0.304054054054054,0.0424076607387141,0.5259811046511628
frozenset({'reading_cat_μέτριο'}),"frozenset({'gender_male', 'race/ethnicity_group D'})",0.49,0.133,0.071,0.14489795918367346,1.089458339726868,1.0,0.005829999999999988,1.0139140811455847,0.16100524716928993,0.1286231884057971,0.013723136313348847,0.3393662728249194
"frozenset({'gender_male', 'race/ethnicity_group D'})",frozenset({'reading_cat_μέτριο'}),0.133,0.49,0.071,0.5338345864661653,1.089458339726868,1.0,0.005829999999999988,1.0940322580645159,0.09470896892311173,0.1286231884057971,0.08595016954150066,0.3393662728249194
frozenset({'race/ethnicity_group C'}),frozenset({'gender_female'}),0.319,0.518,0.18,0.5642633228840125,1.0893114341390202,1.0,0.014757999999999993,1.1061726618705034,0.12039484418339037,0.273972602739726,0.09598199768467318,0.45587683518718
frozenset({'gender_female'}),frozenset({'race/ethnicity_group C'}),0.518,0.319,0.18,0.34749034749034746,1.0893114341390202,1.0,0.014757999999999993,1.043662721893491,0.1701014292300599,0.273972602739726,0.04183604624133253,0.45587683518718
"frozenset({'test preparation course_none', 'lunch_standard'})",frozenset({'reading_cat_μέτριο'}),0.418,0.49,0.223,0.5334928229665072,1.0887608631969536,1.0,0.01818000000000003,1.0932307692307695,0.14007674171328208,0.32554744525547447,0.08528004503236715,0.4942974318914169
frozenset({'reading_cat_μέτριο'}),"frozenset({'test preparation course_none', 'lunch_standard'})",0.49,0.418,0.223,0.45510204081632655,1.0887608631969534,1.0,0.01818000000000003,1.0680898876404497,0.1598522817198631,0.32554744525547447,0.06374921102461609,0.4942974318914169
frozenset({'lunch_standard'}),"frozenset({'math_cat_μέτριο', 'gender_female'})",0.645,0.245,0.172,0.26666666666666666,1.08843537414966,1.0,0.013974999999999987,1.0295454545454545,0.22887323943661955,0.2395543175487465,0.02869757174392936,0.4843537414965986
"frozenset({'math_cat_μέτριο', 'gender_female'})",frozenset({'lunch_standard'}),0.245,0.645,0.172,0.7020408163265306,1.0884353741496597,1.0,0.013974999999999987,1.1914383561643833,0.107615894039735,0.2395543175487465,0.16067835584938192,0.4843537414965986
"frozenset({'gender_male', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})",0.316,0.32,0.11,0.34810126582278483,1.0878164556962027,1.0,0.008879999999999999,1.0431067961165048,0.11802232854864435,0.2091254752851711,0.041325390915860044,0.34592563291139244
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})","frozenset({'gender_male', 'lunch_standard'})",0.32,0.316,0.11,0.34375,1.0878164556962024,1.0,0.008879999999999999,1.0422857142857143,0.11871657754010695,0.2091254752851711,0.04057017543859649,0.34592563291139244
frozenset({'lunch_standard'}),"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'gender_male'})",0.645,0.191,0.134,0.20775193798449612,1.0877064815942206,1.0,0.010805000000000009,1.0211448140900194,0.22713895312171556,0.1908831908831909,0.020706969078487167,0.4546613093063842
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'gender_male'})",frozenset({'lunch_standard'}),0.191,0.645,0.134,0.7015706806282723,1.0877064815942206,1.0,0.010805000000000009,1.1895614035087723,0.0996716048927182,0.1908831908831909,0.15935402993879524,0.4546613093063842
"frozenset({'test preparation course_none', 'race/ethnicity_group D'})","frozenset({'math_cat_μέτριο', 'lunch_standard'})",0.18,0.332,0.065,0.36111111111111116,1.0876840696117807,1.0,0.005240000000000002,1.0455652173913044,0.09831144465290809,0.14541387024608501,0.043579507651363994,0.2784471218206158
"frozenset({'math_cat_μέτριο', 'lunch_standard'})","frozenset({'test preparation course_none', 'race/ethnicity_group D'})",0.332,0.18,0.065,0.19578313253012047,1.0876840696117804,1.0,0.005240000000000002,1.019625468164794,0.12068171349608481,0.14541387024608501,0.019247722597707902,0.2784471218206158
frozenset({'lunch_standard'}),"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο'})",0.645,0.325,0.228,0.35348837209302325,1.0876565295169947,1.0,0.018375000000000002,1.044064748201439,0.22702001482579692,0.30727762803234504,0.04220499569336776,0.5275134168157424
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο'})",frozenset({'lunch_standard'}),0.325,0.645,0.228,0.7015384615384616,1.0876565295169947,1.0,0.018375000000000002,1.1894329896907216,0.11939571150097467,0.30727762803234504,0.1592632719393283,0.5275134168157424
"frozenset({'test preparation course_none', 'race/ethnicity_group D'})","frozenset({'reading_cat_μέτριο', 'lunch_standard'})",0.18,0.327,0.064,0.35555555555555557,1.0873258579680598,1.0,0.005139999999999999,1.0443103448275863,0.09794207317073167,0.14446952595936793,0.042430245996367834,0.2756371049949032
"frozenset({'reading_cat_μέτριο', 'lunch_standard'})","frozenset({'test preparation course_none', 'race/ethnicity_group D'})",0.327,0.18,0.064,0.19571865443425077,1.0873258579680598,1.0,0.005139999999999999,1.0195437262357414,0.1193350668647845,0.14446952595936793,0.01916909077347655,0.2756371049949032
"frozenset({'gender_female', 'test preparation course_none'})",frozenset({'race/ethnicity_group B'}),0.334,0.19,0.069,0.2065868263473054,1.0872990860384495,1.0,0.005540000000000003,1.0209056603773585,0.12055533794664236,0.15164835164835166,0.02047756339173507,0.28487236054207377
frozenset({'race/ethnicity_group B'}),"frozenset({'gender_female', 'test preparation course_none'})",0.19,0.334,0.069,0.3631578947368421,1.0872990860384495,1.0,0.005540000000000003,1.045785123966942,0.09912327786723926,0.15164835164835166,0.04378062272799115,0.28487236054207377
frozenset({'lunch_standard'}),"frozenset({'test preparation course_completed', 'parental level of education_some high school'})",0.645,0.077,0.054,0.08372093023255814,1.0872848082150408,1.0,0.004334999999999999,1.0073350253807107,0.2261345852895148,0.08083832335329343,0.007281614553150746,0.3925098157656297
"frozenset({'test preparation course_completed', 'parental level of education_some high school'})",frozenset({'lunch_standard'}),0.077,0.645,0.054,0.7012987012987013,1.0872848082150408,1.0,0.004334999999999999,1.1884782608695652,0.08697484049596722,0.08083832335329343,0.15858789098225717,0.3925098157656297
"frozenset({'reading_cat_μέτριο', 'test preparation course_none'})",frozenset({'lunch_standard'}),0.318,0.645,0.223,0.7012578616352201,1.087221490907318,1.0,0.01788999999999999,1.188315789473684,0.11763081414462863,0.3013513513513513,0.15847284967667624,0.5234971478718736
frozenset({'lunch_standard'}),"frozenset({'reading_cat_μέτριο', 'test preparation course_none'})",0.645,0.318,0.223,0.3457364341085271,1.087221490907318,1.0,0.01788999999999999,1.0423933649289099,0.22598370492010345,0.3013513513513513,0.04066925822364679,0.5234971478718736
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})",frozenset({'parental level of education_some college'}),0.228,0.226,0.056,0.24561403508771928,1.0867877658748641,1.0,0.004471999999999997,1.026,0.10344189489267203,0.1407035175879397,0.025341130604288473,0.24670082285359415
frozenset({'parental level of education_some college'}),"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})",0.226,0.228,0.056,0.24778761061946902,1.0867877658748641,1.0,0.004471999999999997,1.0263058823529412,0.1031746031746031,0.1407035175879397,0.025631619973405458,0.24670082285359415
frozenset({'test preparation course_none'}),"frozenset({'reading_cat_μέτριο', 'gender_female', 'lunch_standard'})",0.642,0.172,0.12,0.18691588785046728,1.0867202782003913,1.0,0.009576000000000001,1.018344827586207,0.22290502793296094,0.17291066282420747,0.01801435730732765,0.44229515322755925
"frozenset({'reading_cat_μέτριο', 'gender_female', 'lunch_standard'})",frozenset({'test preparation course_none'}),0.172,0.642,0.12,0.6976744186046512,1.0867202782003913,1.0,0.009576000000000001,1.1841538461538461,0.0963768115942029,0.17291066282420747,0.15551513576718204,0.44229515322755925
frozenset({'lunch_standard'}),"frozenset({'math_cat_μέτριο', 'test preparation course_none'})",0.645,0.304,0.213,0.3302325581395349,1.0862913096695228,1.0,0.01691999999999999,1.0391666666666666,0.22376512596706993,0.28940217391304346,0.037690457097032906,0.5154452264381886
"frozenset({'math_cat_μέτριο', 'test preparation course_none'})",frozenset({'lunch_standard'}),0.304,0.645,0.213,0.7006578947368421,1.0862913096695226,1.0,0.01691999999999999,1.185934065934066,0.1141330743079164,0.28940217391304346,0.15678280207561163,0.5154452264381886
"frozenset({""parental level of education_associate's degree"", 'test preparation course_none'})","frozenset({'gender_female', 'lunch_standard'})",0.14,0.329,0.05,0.35714285714285715,1.0855405992184108,1.0,0.003939999999999999,1.043777777777778,0.09162790697674415,0.11933174224343675,0.041941664892484555,0.25455927051671734
"frozenset({'gender_female', 'lunch_standard'})","frozenset({""parental level of education_associate's degree"", 'test preparation course_none'})",0.329,0.14,0.05,0.1519756838905775,1.0855405992184106,1.0,0.003939999999999999,1.014121863799283,0.11743666169895674,0.11933174224343675,0.013925213826252893,0.25455927051671734
frozenset({'test preparation course_completed'}),"frozenset({'reading_cat_μέτριο', 'math_cat_μέτριο', 'gender_male'})",0.358,0.175,0.068,0.1899441340782123,1.085395051875499,1.0,0.0053500000000000075,1.018448275862069,0.12254901960784328,0.14623655913978498,0.018114101912984636,0.2892577813248205
"frozenset({'reading_cat_μέτριο', 'math_cat_μέτριο', 'gender_male'})",frozenset({'test preparation course_completed'}),0.175,0.358,0.068,0.3885714285714286,1.085395051875499,1.0,0.0053500000000000075,1.05,0.09536541889483079,0.14623655913978498,0.04761904761904772,0.2892577813248205
"frozenset({'math_cat_χαμηλό', 'gender_female'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.205,0.418,0.093,0.45365853658536587,1.0853075037927413,1.0,0.007310000000000011,1.065267857142857,0.09887062960708745,0.17547169811320754,0.06126896320509601,0.33807328743143894
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'math_cat_χαμηλό', 'gender_female'})",0.418,0.205,0.093,0.22248803827751196,1.0853075037927413,1.0,0.007310000000000011,1.0224923076923078,0.13505524147359882,0.17547169811320754,0.021997532424543356,0.33807328743143894
frozenset({'lunch_standard'}),"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'race/ethnicity_group D'})",0.645,0.1,0.07,0.10852713178294575,1.0852713178294575,1.0,0.005500000000000005,1.0095652173913043,0.22132796780684125,0.1037037037037037,0.009474590869939714,0.4042635658914729
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'race/ethnicity_group D'})",frozenset({'lunch_standard'}),0.1,0.645,0.07,0.7000000000000001,1.0852713178294575,1.0,0.005500000000000005,1.1833333333333336,0.08730158730158737,0.1037037037037037,0.1549295774647889,0.4042635658914729
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'race/ethnicity_group D'})",frozenset({'lunch_standard'}),0.1,0.645,0.07,0.7000000000000001,1.0852713178294575,1.0,0.005500000000000005,1.1833333333333336,0.08730158730158737,0.1037037037037037,0.1549295774647889,0.4042635658914729
frozenset({'lunch_standard'}),"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'race/ethnicity_group D'})",0.645,0.1,0.07,0.10852713178294575,1.0852713178294575,1.0,0.005500000000000005,1.0095652173913043,0.22132796780684125,0.1037037037037037,0.009474590869939714,0.4042635658914729
"frozenset({'math_cat_χαμηλό', 'gender_female'})",frozenset({'reading_cat_μέτριο'}),0.205,0.49,0.109,0.5317073170731708,1.0851169736187158,1.0,0.008550000000000002,1.0890625,0.09866712826726676,0.18600682593856657,0.08177905308464857,0.37707814833250375
frozenset({'reading_cat_μέτριο'}),"frozenset({'math_cat_χαμηλό', 'gender_female'})",0.49,0.205,0.109,0.22244897959183674,1.0851169736187158,1.0,0.008550000000000002,1.0224409448818899,0.15380464112250408,0.18600682593856657,0.021948402002310376,0.37707814833250375
"frozenset({'writing_cat_μέτριο', 'lunch_standard'})","frozenset({'test preparation course_none', 'race/ethnicity_group D'})",0.338,0.18,0.066,0.1952662721893491,1.0848126232741617,1.0,0.005159999999999998,1.0189705882352942,0.11809942323537487,0.14601769911504425,0.018617405108962336,0.2809664694280079
"frozenset({'test preparation course_none', 'race/ethnicity_group D'})","frozenset({'writing_cat_μέτριο', 'lunch_standard'})",0.18,0.338,0.066,0.3666666666666667,1.0848126232741617,1.0,0.005159999999999998,1.0452631578947367,0.09534368070953432,0.14601769911504425,0.04330312185297081,0.2809664694280079
"frozenset({'lunch_free/reduced', 'math_cat_μέτριο'})",frozenset({'gender_male'}),0.153,0.482,0.08,0.5228758169934641,1.0848045995715023,1.0,0.0062540000000000096,1.0856712328767124,0.0922963400236129,0.14414414414414414,0.07891084361672605,0.34442546036395194
frozenset({'gender_male'}),"frozenset({'lunch_free/reduced', 'math_cat_μέτριο'})",0.482,0.153,0.08,0.16597510373443985,1.0848045995715023,1.0,0.0062540000000000096,1.0155572139303484,0.15091698841698864,0.14414414414414414,0.015318894609728287,0.34442546036395194
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})","frozenset({'race/ethnicity_group C', 'test preparation course_none'})",0.283,0.202,0.062,0.21908127208480568,1.0845607528950776,1.0,0.004833999999999998,1.021873303167421,0.10874162055158135,0.14657210401891255,0.021405102863165,0.2630059825770563
"frozenset({'race/ethnicity_group C', 'test preparation course_none'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})",0.202,0.283,0.062,0.3069306930693069,1.0845607528950776,1.0,0.004833999999999998,1.0345285714285717,0.09770393726251107,0.14657210401891255,0.0333761409613765,0.2630059825770563
"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced', 'writing_cat_χαμηλό'})",frozenset({'gender_male'}),0.132,0.482,0.069,0.5227272727272727,1.084496416446624,1.0,0.005376000000000006,1.0853333333333333,0.08976157082748956,0.126605504587156,0.07862407862407861,0.33294039984911356
frozenset({'gender_male'}),"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced', 'writing_cat_χαμηλό'})",0.482,0.132,0.069,0.14315352697095438,1.084496416446624,1.0,0.005376000000000006,1.0130169491525425,0.15041128084606362,0.126605504587156,0.01284968545040826,0.33294039984911356
"frozenset({""parental level of education_associate's degree"", 'lunch_standard'})","frozenset({'reading_cat_μέτριο', 'test preparation course_none'})",0.145,0.318,0.05,0.3448275862068966,1.0843634786380396,1.0,0.0038900000000000046,1.0409473684210526,0.09099415204678372,0.12106537530266345,0.03933663666700377,0.25103014530470613
"frozenset({'reading_cat_μέτριο', 'test preparation course_none'})","frozenset({""parental level of education_associate's degree"", 'lunch_standard'})",0.318,0.145,0.05,0.15723270440251572,1.0843634786380394,1.0,0.0038900000000000046,1.0145149253731343,0.11407624633431099,0.12106537530266345,0.014307256611129504,0.25103014530470613
"frozenset({'writing_cat_υψηλό', 'lunch_standard'})",frozenset({'race/ethnicity_group C'}),0.162,0.319,0.056,0.345679012345679,1.0836332675413134,1.0,0.0043219999999999995,1.0407735849056605,0.09209853392430957,0.13176470588235295,0.03917622958249484,0.2606138008436859
frozenset({'race/ethnicity_group C'}),"frozenset({'writing_cat_υψηλό', 'lunch_standard'})",0.319,0.162,0.056,0.1755485893416928,1.0836332675413134,1.0,0.0043219999999999995,1.0164334600760456,0.1133312355779316,0.13176470588235295,0.01616776771085058,0.2606138008436859
frozenset({'test preparation course_none'}),"frozenset({'reading_cat_μέτριο', 'race/ethnicity_group D', 'lunch_standard'})",0.642,0.092,0.064,0.09968847352024922,1.083570364350535,1.0,0.004936000000000003,1.0085397923875432,0.21543296089385489,0.09552238805970151,0.008467481850494736,0.39767032371664635
"frozenset({'reading_cat_μέτριο', 'race/ethnicity_group D', 'lunch_standard'})",frozenset({'test preparation course_none'}),0.092,0.642,0.064,0.6956521739130435,1.083570364350535,1.0,0.004936000000000003,1.1762857142857142,0.08493942731277537,0.09552238805970151,0.14986640757833364,0.39767032371664635
"frozenset({'gender_female', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.329,0.418,0.149,0.45288753799392095,1.0834630095548348,1.0,0.011477999999999988,1.063766666666667,0.11480410886286109,0.2491638795986622,0.05994422335725252,0.4046734340687308
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'gender_female', 'lunch_standard'})",0.418,0.329,0.149,0.35645933014354064,1.0834630095548348,1.0,0.011477999999999988,1.0426691449814125,0.13236006365460443,0.2491638795986622,0.04092299574298156,0.4046734340687308
"frozenset({'gender_female', 'lunch_standard'})","frozenset({'race/ethnicity_group C', 'test preparation course_none'})",0.329,0.202,0.072,0.21884498480243159,1.0833910138734235,1.0,0.005541999999999991,1.0215642023346305,0.11471270077827436,0.15686274509803919,0.021109003511819012,0.287640314183394
"frozenset({'race/ethnicity_group C', 'test preparation course_none'})","frozenset({'gender_female', 'lunch_standard'})",0.202,0.329,0.072,0.35643564356435636,1.0833910138734235,1.0,0.005541999999999991,1.042630769230769,0.09645641882483973,0.15686274509803919,0.040887695326909605,0.287640314183394
frozenset({'test preparation course_completed'}),"frozenset({'math_cat_μέτριο', 'gender_female'})",0.358,0.245,0.095,0.26536312849162014,1.0831148101698782,1.0,0.007290000000000005,1.0277186311787072,0.11952779144121994,0.18700787401574803,0.026971031114728664,0.3265591152662182
"frozenset({'math_cat_μέτριο', 'gender_female'})",frozenset({'test preparation course_completed'}),0.245,0.358,0.095,0.3877551020408163,1.083114810169878,1.0,0.007290000000000005,1.0486,0.10163820146392478,0.18700787401574803,0.046347510967003624,0.3265591152662182
"frozenset({'reading_cat_μέτριο', 'race/ethnicity_group D'})",frozenset({'gender_male'}),0.136,0.482,0.071,0.5220588235294117,1.0831095923846716,1.0,0.0054479999999999945,1.0838153846153844,0.08881064162754296,0.12979890310786105,0.07733363615716544,0.33468086404686354
frozenset({'gender_male'}),"frozenset({'reading_cat_μέτριο', 'race/ethnicity_group D'})",0.482,0.136,0.071,0.14730290456431536,1.0831095923846716,1.0,0.0054479999999999945,1.0132554744525548,0.14813203545597897,0.12979890310786105,0.013082065467957578,0.33468086404686354
frozenset({'race/ethnicity_group D'}),frozenset({'writing_cat_υψηλό'}),0.262,0.208,0.059,0.22519083969465647,1.0826482677627716,1.0,0.004503999999999994,1.0221871921182266,0.10344035643746255,0.1435523114355231,0.021705605675071318,0.2544223429242513
frozenset({'writing_cat_υψηλό'}),frozenset({'race/ethnicity_group D'}),0.208,0.262,0.059,0.28365384615384615,1.0826482677627716,1.0,0.004503999999999994,1.030228187919463,0.09638760486218101,0.1435523114355231,0.029341254951011024,0.2544223429242513
"frozenset({'race/ethnicity_group C', 'gender_female', 'test preparation course_none'})",frozenset({'writing_cat_μέτριο'}),0.111,0.491,0.059,0.5315315315315315,1.0825489440560725,1.0,0.004498999999999996,1.0865192307692308,0.08577529503727281,0.10865561694290977,0.07962972795978694,0.3258472321608778
frozenset({'writing_cat_μέτριο'}),"frozenset({'race/ethnicity_group C', 'gender_female', 'test preparation course_none'})",0.491,0.111,0.059,0.12016293279022403,1.0825489440560723,1.0,0.004498999999999996,1.0104143518518518,0.1498118610768871,0.10865561694290977,0.010307011012625449,0.3258472321608778
"frozenset({'gender_male', 'test preparation course_none'})","frozenset({'lunch_free/reduced', 'math_cat_μέτριο'})",0.308,0.153,0.051,0.16558441558441558,1.0822510822510822,1.0,0.0038759999999999975,1.0150817120622568,0.10982658959537567,0.12439024390243902,0.014857633511706715,0.24945887445887444
"frozenset({'lunch_free/reduced', 'math_cat_μέτριο'})","frozenset({'gender_male', 'test preparation course_none'})",0.153,0.308,0.051,0.3333333333333333,1.0822510822510822,1.0,0.0038759999999999975,1.0379999999999998,0.08972845336481694,0.12439024390243902,0.036608863198458554,0.24945887445887444
"frozenset({'race/ethnicity_group C', 'math_cat_μέτριο', 'reading_cat_μέτριο'})",frozenset({'test preparation course_none'}),0.108,0.642,0.075,0.6944444444444444,1.0816891658013152,1.0,0.005664000000000002,1.1716363636363636,0.08466367713004487,0.1111111111111111,0.14649286157666036,0.4056334371754932
frozenset({'test preparation course_none'}),"frozenset({'race/ethnicity_group C', 'math_cat_μέτριο', 'reading_cat_μέτριο'})",0.642,0.108,0.075,0.11682242990654206,1.0816891658013152,1.0,0.005664000000000002,1.009989417989418,0.21094972067039117,0.1111111111111111,0.009890616487154771,0.4056334371754932
frozenset({'parental level of education_some college'}),"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'lunch_standard'})",0.226,0.225,0.055,0.24336283185840707,1.0816125860373647,1.0,0.004150000000000001,1.0242690058479533,0.09748649283533006,0.1388888888888889,0.023693976591492985,0.24390363815142574
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'lunch_standard'})",frozenset({'parental level of education_some college'}),0.225,0.226,0.055,0.24444444444444444,1.0816125860373647,1.0,0.004150000000000001,1.0244117647058824,0.09736070381231673,0.1388888888888889,0.023830031581969547,0.24390363815142574
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'gender_female', 'test preparation course_none'})",0.418,0.334,0.151,0.361244019138756,1.0815689195771137,1.0,0.011387999999999981,1.0426516853932584,0.12958284973032,0.2512479201331115,0.040906935643777724,0.4066699137609947
"frozenset({'gender_female', 'test preparation course_none'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.334,0.418,0.151,0.4520958083832335,1.0815689195771137,1.0,0.011387999999999981,1.0622295081967215,0.11323906688145081,0.2512479201331115,0.058583863201432135,0.4066699137609947
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})",frozenset({'parental level of education_some college'}),0.266,0.226,0.065,0.24436090225563908,1.0812429303346862,1.0,0.0048839999999999995,1.0242985074626867,0.10236847621043804,0.1522248243559719,0.023722095937518187,0.26598576086233283
frozenset({'parental level of education_some college'}),"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})",0.226,0.266,0.065,0.28761061946902655,1.0812429303346862,1.0,0.0048839999999999995,1.030335403726708,0.09707811568276684,0.1522248243559719,0.029442260856984384,0.26598576086233283
frozenset({'race/ethnicity_group C'}),frozenset({'reading_cat_μέτριο'}),0.319,0.49,0.169,0.5297805642633229,1.0811848250271896,1.0,0.012690000000000007,1.0846,0.1102624925057999,0.26406250000000003,0.07800110639867232,0.4373392617234982
frozenset({'reading_cat_μέτριο'}),frozenset({'race/ethnicity_group C'}),0.49,0.319,0.169,0.3448979591836735,1.0811848250271896,1.0,0.012690000000000007,1.039532710280374,0.14723285764009753,0.26406250000000003,0.03802930863975549,0.4373392617234982
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",frozenset({'gender_female'}),0.15,0.518,0.084,0.56,1.0810810810810811,1.0,0.0063,1.0954545454545455,0.08823529411764705,0.14383561643835616,0.08713692946058099,0.3610810810810811
frozenset({'gender_female'}),"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",0.518,0.15,0.084,0.16216216216216217,1.0810810810810811,1.0,0.0063,1.014516129032258,0.15560165975103735,0.14383561643835616,0.014308426073131974,0.3610810810810811
frozenset({'race/ethnicity_group C'}),"frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'gender_male'})",0.319,0.145,0.05,0.15673981191222572,1.0809642200843155,1.0,0.0037450000000000053,1.0139219330855018,0.1099853157121881,0.12077294685990339,0.013730774166345881,0.25078369905956116
"frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'gender_male'})",frozenset({'race/ethnicity_group C'}),0.145,0.319,0.05,0.3448275862068966,1.0809642200843153,1.0,0.0037450000000000053,1.039421052631579,0.08760233918128667,0.12077294685990339,0.03792597093523725,0.25078369905956116
"frozenset({'reading_cat_μέτριο', 'gender_male'})","frozenset({'race/ethnicity_group C', 'test preparation course_none'})",0.229,0.202,0.05,0.2183406113537118,1.0808941156124345,1.0,0.0037419999999999953,1.020905027932961,0.09706874189364449,0.13123359580052493,0.02047695658359873,0.23293268191447966
"frozenset({'race/ethnicity_group C', 'test preparation course_none'})","frozenset({'reading_cat_μέτριο', 'gender_male'})",0.202,0.229,0.05,0.24752475247524752,1.0808941156124345,1.0,0.0037419999999999953,1.0246184210526317,0.09378446115288208,0.13123359580052493,0.02402691631030806,0.23293268191447966
frozenset({'math_cat_μέτριο'}),"frozenset({'test preparation course_completed', 'lunch_standard'})",0.485,0.227,0.119,0.24536082474226803,1.0808846904945728,1.0,0.008904999999999996,1.0243306010928963,0.14530472383128,0.20067453625632378,0.02375268401328334,0.38479494981606793
"frozenset({'test preparation course_completed', 'lunch_standard'})",frozenset({'math_cat_μέτριο'}),0.227,0.485,0.119,0.5242290748898678,1.0808846904945728,1.0,0.008904999999999996,1.0824537037037036,0.09680715753312964,0.20067453625632378,0.07617296095119964,0.38479494981606793
"frozenset({'writing_cat_μέτριο', 'gender_female', 'test preparation course_none'})",frozenset({'lunch_standard'}),0.175,0.645,0.122,0.6971428571428572,1.080841638981174,1.0,0.009125000000000008,1.1721698113207548,0.09066070541480387,0.17478510028653294,0.1468812877263582,0.4431450719822813
frozenset({'lunch_standard'}),"frozenset({'writing_cat_μέτριο', 'gender_female', 'test preparation course_none'})",0.645,0.175,0.122,0.18914728682170542,1.080841638981174,1.0,0.009125000000000008,1.0174474187380498,0.2106903717386287,0.17478510028653294,0.017148226450552043,0.4431450719822813
frozenset({'test preparation course_none'}),"frozenset({'parental level of education_high school', 'reading_cat_μέτριο'})",0.642,0.098,0.068,0.10591900311526481,1.080806154237396,1.0,0.005084000000000005,1.008857142857143,0.20883996056523188,0.1011904761904762,0.00877938261115832,0.3998982770678365
"frozenset({'parental level of education_high school', 'reading_cat_μέτριο'})",frozenset({'test preparation course_none'}),0.098,0.642,0.068,0.6938775510204082,1.080806154237396,1.0,0.005084000000000005,1.1694666666666667,0.08288770053475943,0.1011904761904762,0.1449093603922015,0.3998982770678365
"frozenset({'gender_female', 'test preparation course_none'})","frozenset({'writing_cat_μέτριο', 'lunch_standard'})",0.334,0.338,0.122,0.3652694610778443,1.0806788789285333,1.0,0.009107999999999977,1.0429622641509433,0.11209570225963641,0.2218181818181818,0.04119253939251403,0.3631081033199872
"frozenset({'writing_cat_μέτριο', 'lunch_standard'})","frozenset({'gender_female', 'test preparation course_none'})",0.338,0.334,0.122,0.36094674556213013,1.0806788789285333,1.0,0.009107999999999977,1.0421666666666665,0.1127730176811448,0.2218181818181818,0.040460578922117296,0.3631081033199872
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'race/ethnicity_group C', 'lunch_standard'})",0.325,0.205,0.072,0.22153846153846152,1.0806754221388368,1.0,0.005374999999999991,1.0212450592885376,0.11059670781892986,0.15720524017467247,0.020803096274794375,0.28637898686679175
"frozenset({'race/ethnicity_group C', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο'})",0.205,0.325,0.072,0.35121951219512193,1.0806754221388366,1.0,0.005374999999999991,1.0404135338345866,0.09390286512928006,0.15720524017467247,0.03884372177055099,0.28637898686679175
"frozenset({'race/ethnicity_group C', 'math_cat_μέτριο', 'test preparation course_none'})",frozenset({'gender_male'}),0.096,0.482,0.05,0.5208333333333334,1.0805670816044262,1.0,0.003728000000000002,1.0810434782608698,0.08247787610619473,0.09469696969696972,0.07496782496782507,0.3122838865836791
frozenset({'gender_male'}),"frozenset({'race/ethnicity_group C', 'math_cat_μέτριο', 'test preparation course_none'})",0.482,0.096,0.05,0.1037344398340249,1.080567081604426,1.0,0.003728000000000002,1.0086296296296295,0.143938223938224,0.09469696969696972,0.008555796276576218,0.3122838865836791
frozenset({'writing_cat_μέτριο'}),frozenset({'race/ethnicity_group D'}),0.491,0.262,0.139,0.2830957230142567,1.080518026771972,1.0,0.010358000000000006,1.0294261363636363,0.14640075758646529,0.22638436482084692,0.028584990534223118,0.40681503707964745
frozenset({'race/ethnicity_group D'}),frozenset({'writing_cat_μέτριο'}),0.262,0.491,0.139,0.5305343511450382,1.0805180267719718,1.0,0.010358000000000006,1.0842113821138213,0.1009728802323995,0.22638436482084692,0.07767063093327746,0.40681503707964745
frozenset({'race/ethnicity_group C'}),"frozenset({'writing_cat_μέτριο', 'gender_female', 'lunch_standard'})",0.319,0.177,0.061,0.19122257053291536,1.0803535058356801,1.0,0.004536999999999999,1.0175852713178295,0.10921739967742711,0.14022988505747125,0.017281373673044192,0.26792766944724866
"frozenset({'writing_cat_μέτριο', 'gender_female', 'lunch_standard'})",frozenset({'race/ethnicity_group C'}),0.177,0.319,0.061,0.3446327683615819,1.08035350583568,1.0,0.004536999999999999,1.0391120689655173,0.09037308527378841,0.14022988505747125,0.03763989480408503,0.26792766944724866
frozenset({'race/ethnicity_group C'}),"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})",0.319,0.18,0.062,0.19435736677115986,1.0797631487286659,1.0,0.004580000000000001,1.0178210116731519,0.10847425512765858,0.14187643020594964,0.01750898386726813,0.26940090560780217
"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})",frozenset({'race/ethnicity_group C'}),0.18,0.319,0.062,0.34444444444444444,1.0797631487286659,1.0,0.004580000000000001,1.038813559322034,0.0900865460267506,0.14187643020594964,0.03736335454397127,0.26940090560780217
frozenset({'gender_male'}),frozenset({'parental level of education_high school'}),0.482,0.196,0.102,0.21161825726141079,1.079684986027606,1.0,0.007527999999999993,1.0198105263157895,0.14247861306684823,0.17708333333333334,0.019425693111207433,0.3660132102633584
frozenset({'parental level of education_high school'}),frozenset({'gender_male'}),0.196,0.482,0.102,0.520408163265306,1.0796849860276059,1.0,0.007527999999999993,1.0800851063829786,0.09179592234903904,0.17708333333333334,0.07414703333070666,0.3660132102633584
"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced', 'reading_cat_χαμηλό'})",frozenset({'gender_male'}),0.123,0.482,0.064,0.5203252032520326,1.079512869817495,1.0,0.004714000000000003,1.079898305084746,0.08398660205245158,0.11829944547134937,0.07398687886492777,0.3265526431197922
frozenset({'gender_male'}),"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced', 'reading_cat_χαμηλό'})",0.482,0.123,0.064,0.13278008298755187,1.079512869817495,1.0,0.004714000000000003,1.0112775119617226,0.1421935328185329,0.11829944547134937,0.01115174799036701,0.3265526431197922
"frozenset({'parental level of education_some college', 'test preparation course_none'})",frozenset({'math_cat_μέτριο'}),0.149,0.485,0.078,0.5234899328859061,1.079360686362693,1.0,0.005735000000000004,1.080774647887324,0.08639910813823864,0.14028776978417265,0.07473773375904091,0.34215733757697364
frozenset({'math_cat_μέτριο'}),"frozenset({'parental level of education_some college', 'test preparation course_none'})",0.485,0.149,0.078,0.16082474226804125,1.079360686362693,1.0,0.005735000000000004,1.0140909090909092,0.1427682350012448,0.14028776978417265,0.013895114298520864,0.34215733757697364
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'test preparation course_none'})",frozenset({'parental level of education_some college'}),0.205,0.226,0.05,0.24390243902439027,1.0792143319663285,1.0,0.0036700000000000066,1.0236774193548388,0.09232704402515739,0.13123359580052493,0.023129766181382762,0.2325706885387438
frozenset({'parental level of education_some college'}),"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'test preparation course_none'})",0.226,0.205,0.05,0.22123893805309736,1.0792143319663285,1.0,0.0036700000000000066,1.0208522727272729,0.0948320413436694,0.13123359580052493,0.02042633717370738,0.2325706885387438
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})",frozenset({'parental level of education_some college'}),0.32,0.226,0.078,0.24375,1.0785398230088494,1.0,0.005679999999999991,1.0234710743801654,0.10708898944193045,0.16666666666666666,0.022932816537467683,0.29444137168141593
frozenset({'parental level of education_some college'}),"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})",0.226,0.32,0.078,0.34513274336283184,1.0785398230088494,1.0,0.005679999999999991,1.0383783783783782,0.09408334989730323,0.16666666666666666,0.036959916710046815,0.29444137168141593
frozenset({'test preparation course_none'}),"frozenset({'gender_female', 'race/ethnicity_group D', 'lunch_standard'})",0.642,0.078,0.054,0.08411214953271028,1.0783608914450036,1.0,0.003923999999999997,1.0066734693877553,0.2029795158286777,0.08108108108108109,0.006629229428102251,0.3882099209202013
"frozenset({'gender_female', 'race/ethnicity_group D', 'lunch_standard'})",frozenset({'test preparation course_none'}),0.078,0.642,0.054,0.6923076923076923,1.0783608914450036,1.0,0.003923999999999997,1.1635,0.0788141720896601,0.08108108108108109,0.14052428018908458,0.3882099209202013
"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced'})",frozenset({'test preparation course_none'}),0.182,0.642,0.126,0.6923076923076923,1.0783608914450036,1.0,0.009155999999999997,1.1635,0.08883455582722083,0.1805157593123209,0.14052428018908458,0.4442846872753415
"frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο'})",frozenset({'test preparation course_none'}),0.169,0.642,0.117,0.6923076923076923,1.0783608914450036,1.0,0.008501999999999996,1.1635,0.0874448455675892,0.1685878962536023,0.14052428018908458,0.43727534148094893
frozenset({'test preparation course_none'}),"frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο'})",0.642,0.169,0.117,0.1822429906542056,1.0783608914450036,1.0,0.008501999999999996,1.0161942857142856,0.20297951582867774,0.1685878962536023,0.015936210173532597,0.43727534148094893
frozenset({'test preparation course_none'}),"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced'})",0.642,0.182,0.126,0.19626168224299065,1.0783608914450036,1.0,0.009155999999999997,1.0177441860465117,0.2029795158286778,0.1805157593123209,0.017434819367959233,0.4442846872753415
frozenset({'lunch_standard'}),"frozenset({'gender_female', 'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})",0.645,0.151,0.105,0.1627906976744186,1.0780840905590636,1.0,0.007605000000000001,1.0140833333333332,0.20402414486921533,0.15195369030390737,0.01388774755526336,0.4290774680425073
"frozenset({'gender_female', 'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})",frozenset({'lunch_standard'}),0.151,0.645,0.105,0.695364238410596,1.0780840905590636,1.0,0.007605000000000001,1.1653260869565216,0.08531044926804646,0.15195369030390737,0.14187109411435495,0.4290774680425073
"frozenset({'gender_female', 'lunch_standard'})",frozenset({'math_cat_μέτριο'}),0.329,0.485,0.172,0.5227963525835866,1.0779306238836837,1.0,0.012434999999999974,1.0792038216560509,0.10774442865559884,0.2679127725856697,0.07339097589045941,0.43871776392065925
frozenset({'math_cat_μέτριο'}),"frozenset({'gender_female', 'lunch_standard'})",0.485,0.329,0.172,0.3546391752577319,1.0779306238836837,1.0,0.012434999999999974,1.0397284345047924,0.14038157597651812,0.2679127725856697,0.03821039531703711,0.43871776392065925
"frozenset({'race/ethnicity_group C', 'test preparation course_none'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.202,0.418,0.091,0.45049504950495045,1.0777393528826567,1.0,0.006564,1.0591351351351352,0.09039081219532348,0.17202268431001888,0.05583341839338568,0.33409919939362354
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'race/ethnicity_group C', 'test preparation course_none'})",0.418,0.202,0.091,0.21770334928229665,1.0777393528826567,1.0,0.006564,1.0200733944954128,0.12393791775235075,0.17202268431001888,0.019678382559268966,0.33409919939362354
frozenset({'race/ethnicity_group D'}),"frozenset({'test preparation course_none', 'lunch_standard'})",0.262,0.418,0.118,0.45038167938931295,1.0774681325103181,1.0,0.008483999999999992,1.0589166666666667,0.09742317762160663,0.20996441281138792,0.055638624380262826,0.36633916505350816
"frozenset({'test preparation course_none', 'lunch_standard'})",frozenset({'race/ethnicity_group D'}),0.418,0.262,0.118,0.2822966507177033,1.077468132510318,1.0,0.008483999999999992,1.02828,0.12353660667482078,0.20996441281138792,0.027502236744855437,0.36633916505350816
"frozenset({'test preparation course_none', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'gender_male'})",0.418,0.191,0.086,0.20574162679425836,1.077181292116536,1.0,0.006162000000000001,1.0185602409638552,0.1231119635578998,0.1644359464627151,0.01822203559240836,0.3280017034494852
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'gender_male'})","frozenset({'test preparation course_none', 'lunch_standard'})",0.191,0.418,0.086,0.450261780104712,1.077181292116536,1.0,0.006162000000000001,1.0586857142857142,0.08856756834449654,0.1644359464627151,0.05543261186376638,0.3280017034494852
"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'gender_male', 'test preparation course_none'})",frozenset({'lunch_standard'}),0.095,0.645,0.066,0.6947368421052632,1.0771113831089352,1.0,0.004725,1.1629310344827588,0.07910597689603213,0.09792284866468844,0.1401037805782062,0.39853121175030604
frozenset({'lunch_standard'}),"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'gender_male', 'test preparation course_none'})",0.645,0.095,0.066,0.10232558139534884,1.0771113831089352,1.0,0.004725,1.008160621761658,0.20166453265044815,0.09792284866468844,0.008094565077733519,0.39853121175030604
"frozenset({'parental level of education_high school', 'gender_female'})",frozenset({'test preparation course_none'}),0.094,0.642,0.065,0.6914893617021277,1.0770862331808844,1.0,0.004652000000000003,1.1604137931034484,0.07899473594837839,0.09687034277198212,0.1382384405087366,0.3963677338105654
frozenset({'test preparation course_none'}),"frozenset({'parental level of education_high school', 'gender_female'})",0.642,0.094,0.065,0.10124610591900311,1.0770862331808841,1.0,0.004652000000000003,1.0080623916811091,0.19991405242801907,0.09687034277198212,0.007997909402873192,0.3963677338105654
frozenset({'gender_female'}),"frozenset({'lunch_free/reduced', 'reading_cat_μέτριο', 'test preparation course_none'})",0.518,0.095,0.053,0.1023166023166023,1.0770168664905506,1.0,0.0037899999999999948,1.0081505376344086,0.14835982149847315,0.09464285714285715,0.008084643443759454,0.3301056695793538
"frozenset({'lunch_free/reduced', 'reading_cat_μέτριο', 'test preparation course_none'})",frozenset({'gender_female'}),0.095,0.518,0.053,0.5578947368421052,1.0770168664905506,1.0,0.0037899999999999948,1.090238095238095,0.07901594912957353,0.09464285714285715,0.08276916357283237,0.3301056695793538
"frozenset({'test preparation course_none', 'lunch_standard'})",frozenset({'writing_cat_μέτριο'}),0.418,0.491,0.221,0.5287081339712919,1.0767986435260528,1.0,0.015762000000000026,1.080010152284264,0.12254513224798265,0.3212209302325581,0.07408277793966977,0.489404983482591
frozenset({'writing_cat_μέτριο'}),"frozenset({'test preparation course_none', 'lunch_standard'})",0.491,0.418,0.221,0.45010183299389,1.0767986435260526,1.0,0.015762000000000026,1.0583777777777779,0.14012036732480532,0.3212209302325581,0.05515778864929557,0.489404983482591
"frozenset({'test preparation course_none', 'race/ethnicity_group D'})",frozenset({'math_cat_μέτριο'}),0.18,0.485,0.094,0.5222222222222223,1.0767468499427264,1.0,0.0067000000000000115,1.0779069767441862,0.08692267773741581,0.1646234676007005,0.07227615965480051,0.3580183276059565
frozenset({'math_cat_μέτριο'}),"frozenset({'test preparation course_none', 'race/ethnicity_group D'})",0.485,0.18,0.094,0.19381443298969073,1.0767468499427264,1.0,0.0067000000000000115,1.0171355498721228,0.1384011567857883,0.1646234676007005,0.016846869499622844,0.3580183276059565
"frozenset({'reading_cat_μέτριο', 'lunch_standard'})",frozenset({'parental level of education_high school'}),0.327,0.196,0.069,0.21100917431192662,1.0765774199588092,1.0,0.004907999999999996,1.0190232558139536,0.10569158214354922,0.15198237885462557,0.01866812725364006,0.2815249953192286
frozenset({'parental level of education_high school'}),"frozenset({'reading_cat_μέτριο', 'lunch_standard'})",0.196,0.327,0.069,0.3520408163265306,1.0765774199588092,1.0,0.004907999999999996,1.0386456692913386,0.08847069002812018,0.15198237885462557,0.03720775085665765,0.2815249953192286
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'test preparation course_none', 'race/ethnicity_group D'})",0.418,0.18,0.081,0.1937799043062201,1.0765550239234452,1.0,0.005760000000000015,1.0170919881305638,0.1221840397098132,0.15667311411992263,0.01680476134904892,0.32188995215311006
"frozenset({'test preparation course_none', 'race/ethnicity_group D'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.18,0.418,0.081,0.45,1.0765550239234452,1.0,0.005760000000000015,1.0581818181818183,0.08672086720867231,0.15667311411992263,0.05498281786941585,0.32188995215311006
"frozenset({""parental level of education_associate's degree"", 'test preparation course_none'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.14,0.418,0.063,0.44999999999999996,1.076555023923445,1.0,0.004479999999999998,1.0581818181818183,0.08268733850129195,0.12727272727272726,0.05498281786941575,0.30035885167464116
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({""parental level of education_associate's degree"", 'test preparation course_none'})",0.418,0.14,0.063,0.1507177033492823,1.076555023923445,1.0,0.004479999999999998,1.0126197183098593,0.12218403970981283,0.12727272727272726,0.01246244575497941,0.30035885167464116
frozenset({'math_cat_μέτριο'}),frozenset({'parental level of education_some college'}),0.485,0.226,0.118,0.24329896907216494,1.076544110938783,1.0,0.008389999999999995,1.0228610354223433,0.13806154352476543,0.19898819561551434,0.022350089240523174,0.3827114314387373
frozenset({'parental level of education_some college'}),frozenset({'math_cat_μέτριο'}),0.226,0.485,0.118,0.5221238938053097,1.0765441109387828,1.0,0.008389999999999995,1.077685185185185,0.09186265492926898,0.19898819561551434,0.07208523068992169,0.3827114314387373
frozenset({'writing_cat_μέτριο'}),frozenset({'race/ethnicity_group E'}),0.491,0.14,0.074,0.15071283095723015,1.0765202211230724,1.0,0.005259999999999987,1.0126139088729016,0.13964848935379354,0.13285457809694792,0.012456780182825738,0.3396421297643293
frozenset({'race/ethnicity_group E'}),frozenset({'writing_cat_μέτριο'}),0.14,0.491,0.074,0.5285714285714285,1.0765202211230722,1.0,0.005259999999999987,1.0796969696969694,0.08265241986172198,0.13285457809694792,0.07381420151557658,0.3396421297643293
"frozenset({'writing_cat_υψηλό', 'gender_female', 'reading_cat_υψηλό'})",frozenset({'math_cat_μέτριο'}),0.136,0.485,0.071,0.5220588235294117,1.0764099454214675,1.0,0.005039999999999989,1.0775384615384613,0.08215962441314537,0.12909090909090906,0.07195888063963436,0.3342252880533656
frozenset({'math_cat_μέτριο'}),"frozenset({'writing_cat_υψηλό', 'gender_female', 'reading_cat_υψηλό'})",0.485,0.136,0.071,0.14639175257731957,1.0764099454214673,1.0,0.005039999999999989,1.0121739130434781,0.137836729112539,0.12909090909090906,0.012027491408934674,0.3342252880533656
"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'race/ethnicity_group C', 'lunch_standard'})",0.281,0.205,0.062,0.22064056939501778,1.0762954604635013,1.0,0.004394999999999996,1.020068493150685,0.09859123334380179,0.14622641509433962,0.01967367219499093,0.26153979689263085
"frozenset({'race/ethnicity_group C', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.205,0.281,0.062,0.3024390243902439,1.0762954604635013,1.0,0.004394999999999996,1.0307342657342657,0.08916615946439432,0.14622641509433962,0.029817836425930274,0.26153979689263085
frozenset({'lunch_standard'}),"frozenset({'math_cat_μέτριο', 'race/ethnicity_group B'})",0.645,0.085,0.059,0.09147286821705426,1.0761513907888736,1.0,0.004174999999999991,1.0071245733788396,0.19933158271663842,0.0879284649776453,0.007074172914813394,0.3927952576379388
"frozenset({'math_cat_μέτριο', 'race/ethnicity_group B'})",frozenset({'lunch_standard'}),0.085,0.645,0.059,0.6941176470588234,1.0761513907888733,1.0,0.004174999999999991,1.1605769230769225,0.07733629711957009,0.0879284649776453,0.13835956917978418,0.3927952576379388
"frozenset({'test preparation course_none', 'lunch_standard'})","frozenset({'reading_cat_μέτριο', 'gender_male'})",0.418,0.229,0.103,0.24641148325358853,1.0760326779632687,1.0,0.007277999999999993,1.023104761904762,0.12140926834150723,0.1893382352941176,0.02258298735873997,0.3480965713211174
"frozenset({'reading_cat_μέτριο', 'gender_male'})","frozenset({'test preparation course_none', 'lunch_standard'})",0.229,0.418,0.103,0.4497816593886462,1.0760326779632685,1.0,0.007277999999999993,1.0577619047619047,0.09164746326168251,0.1893382352941176,0.054607662179804534,0.3480965713211174
"frozenset({'writing_cat_μέτριο', 'gender_male', 'lunch_standard'})",frozenset({'test preparation course_completed'}),0.161,0.358,0.062,0.38509316770186336,1.0756792393906798,1.0,0.004361999999999998,1.0440606060606061,0.08385558845015184,0.13566739606126915,0.04220119579729498,0.2791387626218814
frozenset({'test preparation course_completed'}),"frozenset({'writing_cat_μέτριο', 'gender_male', 'lunch_standard'})",0.358,0.161,0.062,0.17318435754189945,1.0756792393906798,1.0,0.004361999999999998,1.0147364864864865,0.10958697618329812,0.13566739606126915,0.014522476212037484,0.2791387626218814
frozenset({'race/ethnicity_group D'}),"frozenset({'test preparation course_completed', 'math_cat_μέτριο'})",0.262,0.181,0.051,0.1946564885496183,1.0754502129813166,1.0,0.003577999999999998,1.0169573459715637,0.09506349965460434,0.1301020408163265,0.01667458919367316,0.23821222217536164
"frozenset({'test preparation course_completed', 'math_cat_μέτριο'})",frozenset({'race/ethnicity_group D'}),0.181,0.262,0.051,0.281767955801105,1.0754502129813166,1.0,0.003577999999999998,1.027523076923077,0.08566161507337974,0.1301020408163265,0.026785847968976918,0.23821222217536164
frozenset({'lunch_standard'}),"frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'gender_female'})",0.645,0.173,0.12,0.18604651162790697,1.075413362011023,1.0,0.008415000000000006,1.0160285714285713,0.1975352112676058,0.17191977077363896,0.015775709344506637,0.4398440650625084
"frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'gender_female'})",frozenset({'lunch_standard'}),0.173,0.645,0.12,0.6936416184971098,1.0754133620110229,1.0,0.008415000000000006,1.1587735849056602,0.08479443772672315,0.17191977077363896,0.13701864365383043,0.4398440650625084
frozenset({'reading_cat_μέτριο'}),"frozenset({'race/ethnicity_group C', 'lunch_standard'})",0.49,0.205,0.108,0.22040816326530613,1.0751617720258837,1.0,0.007550000000000001,1.0197643979057591,0.13707334785766162,0.1839863713798978,0.019381337440636654,0.3736187157789945
"frozenset({'race/ethnicity_group C', 'lunch_standard'})",frozenset({'reading_cat_μέτριο'}),0.205,0.49,0.108,0.526829268292683,1.0751617720258837,1.0,0.007550000000000001,1.077835051546392,0.08793384579548102,0.1839863713798978,0.07221425155428031,0.3736187157789945
frozenset({'race/ethnicity_group D'}),"frozenset({'gender_male', 'lunch_standard'})",0.262,0.316,0.089,0.3396946564885496,1.074983090153638,1.0,0.006207999999999991,1.0358843930635837,0.09451600133978856,0.18200408997955006,0.03464131065577423,0.3106701130544014
"frozenset({'gender_male', 'lunch_standard'})",frozenset({'race/ethnicity_group D'}),0.316,0.262,0.089,0.28164556962025317,1.074983090153638,1.0,0.006207999999999991,1.0273480176211454,0.10197779091924555,0.18200408997955006,0.026620013035573382,0.3106701130544014
"frozenset({'race/ethnicity_group C', 'gender_female', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.118,0.418,0.053,0.4491525423728814,1.074527613332252,1.0,0.0036760000000000057,1.0565538461538464,0.0786377444059386,0.10973084886128363,0.05352670510804363,0.28797340037304353
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'race/ethnicity_group C', 'gender_female', 'lunch_standard'})",0.418,0.118,0.053,0.12679425837320574,1.074527613332252,1.0,0.0036760000000000057,1.0100712328767123,0.11917266420281415,0.10973084886128363,0.00997081448209268,0.28797340037304353
frozenset({'math_cat_χαμηλό'}),"frozenset({'gender_female', 'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})",0.339,0.151,0.055,0.1622418879056047,1.0744495887788392,1.0,0.003810999999999995,1.013419014084507,0.10482739650667018,0.12643678160919541,0.013241328510724038,0.2632401492508156
"frozenset({'gender_female', 'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})",frozenset({'math_cat_χαμηλό'}),0.151,0.339,0.055,0.3642384105960265,1.0744495887788392,1.0,0.003810999999999995,1.0396979166666669,0.08161473391155359,0.12643678160919541,0.038182164290509066,0.2632401492508156
"frozenset({'math_cat_μέτριο', 'gender_female'})","frozenset({'test preparation course_none', 'lunch_standard'})",0.245,0.418,0.11,0.4489795918367347,1.0741138560687433,1.0,0.007590000000000013,1.0562222222222224,0.09139072847682135,0.19891500904159132,0.053229539238375784,0.3560687432867884
"frozenset({'test preparation course_none', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'gender_female'})",0.418,0.245,0.11,0.2631578947368421,1.0741138560687433,1.0,0.007590000000000013,1.024642857142857,0.11855670103092802,0.19891500904159132,0.024050191704426616,0.3560687432867884
"frozenset({'reading_cat_μέτριο', 'lunch_standard'})",frozenset({'race/ethnicity_group D'}),0.327,0.262,0.092,0.28134556574923547,1.0738380372108223,1.0,0.006325999999999998,1.0269191489361702,0.10217068286064987,0.1851106639839034,0.026213503725251303,0.3162453019585872
frozenset({'race/ethnicity_group D'}),"frozenset({'reading_cat_μέτριο', 'lunch_standard'})",0.262,0.327,0.092,0.3511450381679389,1.0738380372108223,1.0,0.006325999999999998,1.0372117647058823,0.09317190997996934,0.1851106639839034,0.035876728332747236,0.3162453019585872
"frozenset({'parental level of education_some college', 'test preparation course_none'})","frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο'})",0.149,0.325,0.052,0.348993288590604,1.0738255033557047,1.0,0.003574999999999995,1.0368556701030927,0.0807873090481785,0.12322274881516587,0.035545612726820754,0.254496644295302
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'parental level of education_some college', 'test preparation course_none'})",0.325,0.149,0.052,0.15999999999999998,1.0738255033557045,1.0,0.003574999999999995,1.013095238095238,0.10185185185185171,0.12322274881516587,0.012925969447708557,0.254496644295302
"frozenset({'math_cat_χαμηλό', 'gender_female'})","frozenset({'reading_cat_μέτριο', 'test preparation course_none'})",0.205,0.318,0.07,0.34146341463414637,1.073784322748888,1.0,0.004810000000000009,1.0356296296296297,0.0864330637915545,0.1545253863134658,0.03440383377440816,0.2807946003988342
"frozenset({'reading_cat_μέτριο', 'test preparation course_none'})","frozenset({'math_cat_χαμηλό', 'gender_female'})",0.318,0.205,0.07,0.22012578616352202,1.073784322748888,1.0,0.004810000000000009,1.0193951612903225,0.10075408462505256,0.1545253863134658,0.019026146117637775,0.2807946003988342
"frozenset({'reading_cat_μέτριο', 'gender_female'})","frozenset({'math_cat_χαμηλό', 'lunch_free/reduced'})",0.261,0.182,0.051,0.1954022988505747,1.0736390046734874,1.0,0.003497999999999994,1.016657142857143,0.09281222637904943,0.1301020408163265,0.016384228423685452,0.23781103953517746
"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced'})","frozenset({'reading_cat_μέτριο', 'gender_female'})",0.182,0.261,0.051,0.2802197802197802,1.0736390046734874,1.0,0.003497999999999994,1.0267022900763358,0.08384869840356667,0.1301020408163265,0.026007821677645756,0.23781103953517746
"frozenset({'writing_cat_μέτριο', 'gender_female', 'lunch_standard'})",frozenset({'test preparation course_none'}),0.177,0.642,0.122,0.6892655367231638,1.073622331344492,1.0,0.008365999999999998,1.1521090909090907,0.08332171384180227,0.1750358680057389,0.132026638891519,0.43964834468556946
frozenset({'test preparation course_none'}),"frozenset({'writing_cat_μέτριο', 'gender_female', 'lunch_standard'})",0.642,0.177,0.122,0.19003115264797507,1.073622331344492,1.0,0.008365999999999998,1.0160884615384616,0.19154684494917112,0.1750358680057389,0.01583372132196243,0.43964834468556946
frozenset({'gender_male'}),"frozenset({'math_cat_μέτριο', 'race/ethnicity_group D'})",0.482,0.145,0.075,0.15560165975103735,1.0731148948347404,1.0,0.005110000000000003,1.0125552825552826,0.1315315315315316,0.1358695652173913,0.01239960204799691,0.3364215195306911
"frozenset({'math_cat_μέτριο', 'race/ethnicity_group D'})",frozenset({'gender_male'}),0.145,0.482,0.075,0.5172413793103449,1.0731148948347404,1.0,0.005110000000000003,1.0730000000000002,0.07968810916179342,0.1358695652173913,0.06803355079217158,0.3364215195306911
frozenset({'test preparation course_none'}),"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})",0.642,0.09,0.062,0.09657320872274143,1.0730356524749047,1.0,0.0042200000000000015,1.0072758620689655,0.19012434672914044,0.09253731343283583,0.007223306288726849,0.39273104880581516
"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})",frozenset({'test preparation course_none'}),0.09,0.642,0.062,0.6888888888888889,1.0730356524749047,1.0,0.0042200000000000015,1.1507142857142856,0.07479617157036514,0.09253731343283583,0.13097454996896332,0.39273104880581516
"frozenset({'gender_male', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})",0.316,0.18,0.061,0.1930379746835443,1.0724331926863573,1.0,0.004119999999999999,1.016156862745098,0.09874412807976223,0.14022988505747125,0.01589996912627355,0.2659634317862166
"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})","frozenset({'gender_male', 'lunch_standard'})",0.18,0.316,0.061,0.3388888888888889,1.0724331926863573,1.0,0.004119999999999999,1.0346218487394958,0.08236705317872847,0.14022988505747125,0.03346328784925279,0.2659634317862166
"frozenset({'math_cat_μέτριο', 'race/ethnicity_group D'})","frozenset({'test preparation course_none', 'lunch_standard'})",0.145,0.418,0.065,0.4482758620689656,1.0724302920310183,1.0,0.004390000000000012,1.0548750000000002,0.07899235267656342,0.13052208835341367,0.05202038156179655,0.3018891272067316
"frozenset({'test preparation course_none', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'race/ethnicity_group D'})",0.418,0.145,0.065,0.15550239234449761,1.072430292031018,1.0,0.004390000000000012,1.0124362606232293,0.11604546656093076,0.13052208835341367,0.012283499818125876,0.3018891272067316
frozenset({'race/ethnicity_group C'}),"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})",0.319,0.266,0.091,0.2852664576802508,1.0724302920310178,1.0,0.006145999999999985,1.0269561403508773,0.09917542076132359,0.18421052631578946,0.026248579945845722,0.31368586041907276
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})",frozenset({'race/ethnicity_group C'}),0.266,0.319,0.091,0.3421052631578947,1.0724302920310178,1.0,0.006145999999999985,1.03512,0.09201425277719534,0.18421052631578946,0.03392843341834755,0.31368586041907276
"frozenset({'math_cat_μέτριο', 'test preparation course_none'})","frozenset({'gender_male', 'lunch_standard'})",0.304,0.316,0.103,0.3388157894736842,1.0722018654230512,1.0,0.006935999999999998,1.0345074626865671,0.09675259457649814,0.1992263056092843,0.033356417359187394,0.33238257828114587
"frozenset({'gender_male', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'test preparation course_none'})",0.316,0.304,0.103,0.32594936708860756,1.0722018654230512,1.0,0.006935999999999998,1.03256338028169,0.0984500085164367,0.1992263056092843,0.03153644696639018,0.33238257828114587
"frozenset({'gender_male', 'lunch_standard'})","frozenset({'test preparation course_completed', 'writing_cat_μέτριο'})",0.316,0.183,0.062,0.1962025316455696,1.0721449816697792,1.0,0.004172000000000002,1.0164251968503935,0.09837766459158656,0.14187643020594964,0.01615976945602156,0.26750017292660994
"frozenset({'test preparation course_completed', 'writing_cat_μέτριο'})","frozenset({'gender_male', 'lunch_standard'})",0.183,0.316,0.062,0.33879781420765026,1.0721449816697792,1.0,0.004172000000000002,1.034479338842975,0.08236269593714222,0.14187643020594964,0.03333013773048284,0.26750017292660994
"frozenset({'math_cat_μέτριο', 'test preparation course_none', 'race/ethnicity_group D'})",frozenset({'lunch_standard'}),0.094,0.645,0.065,0.6914893617021277,1.0720765297707406,1.0,0.004369999999999999,1.150689655172414,0.07420614705382915,0.09643916913946589,0.13095594845669775,0.39613227775028864
frozenset({'lunch_standard'}),"frozenset({'math_cat_μέτριο', 'test preparation course_none', 'race/ethnicity_group D'})",0.645,0.094,0.065,0.10077519379844961,1.0720765297707406,1.0,0.004369999999999999,1.0075344827586206,0.18938244853737807,0.09643916913946589,0.007478138850385884,0.39613227775028864
frozenset({'lunch_free/reduced'}),"frozenset({'test preparation course_completed', 'gender_female'})",0.355,0.184,0.07,0.1971830985915493,1.0716472749540724,1.0,0.0046800000000000175,1.016421052631579,0.10365448504983427,0.14925373134328362,0.01615575807787906,0.2888089406001225
"frozenset({'test preparation course_completed', 'gender_female'})",frozenset({'lunch_free/reduced'}),0.184,0.355,0.07,0.3804347826086957,1.0716472749540724,1.0,0.0046800000000000175,1.0410526315789475,0.08193277310924399,0.14925373134328362,0.03943377148634991,0.2888089406001225
frozenset({'test preparation course_completed'}),"frozenset({'race/ethnicity_group C', 'gender_female'})",0.358,0.18,0.069,0.19273743016759778,1.0707635009310987,1.0,0.0045600000000000085,1.0157785467128029,0.10293918461330101,0.1471215351812367,0.015533451423899744,0.28803538175046556
"frozenset({'race/ethnicity_group C', 'gender_female'})",frozenset({'test preparation course_completed'}),0.18,0.358,0.069,0.38333333333333336,1.0707635009310987,1.0,0.0045600000000000085,1.041081081081081,0.08059384941675517,0.1471215351812367,0.039460020768432046,0.28803538175046556
"frozenset({'gender_female', 'lunch_standard'})",frozenset({'math_cat_υψηλό'}),0.329,0.176,0.062,0.1884498480243161,1.0707377728654326,1.0,0.004096000000000002,1.0153408239700374,0.09845680496129999,0.1399548532731377,0.015109038864461306,0.2703612876485217
frozenset({'math_cat_υψηλό'}),"frozenset({'gender_female', 'lunch_standard'})",0.176,0.329,0.062,0.3522727272727273,1.0707377728654324,1.0,0.004096000000000002,1.0359298245614037,0.08017538365173822,0.1399548532731377,0.03468364720227612,0.2703612876485217
frozenset({'reading_cat_χαμηλό'}),"frozenset({'test preparation course_none', 'race/ethnicity_group D'})",0.275,0.18,0.053,0.19272727272727272,1.0707070707070707,1.0,0.003499999999999996,1.015765765765766,0.09108653220559522,0.13184079601990048,0.015521064301552104,0.24358585858585857
"frozenset({'test preparation course_none', 'race/ethnicity_group D'})",frozenset({'reading_cat_χαμηλό'}),0.18,0.275,0.053,0.29444444444444445,1.0707070707070707,1.0,0.003499999999999996,1.0275590551181102,0.0805338242061665,0.13184079601990048,0.026819923371647493,0.24358585858585857
frozenset({'test preparation course_none'}),frozenset({'race/ethnicity_group D'}),0.642,0.262,0.18,0.2803738317757009,1.0701291289148889,1.0,0.011795999999999973,1.0255324675324675,0.18305400372439437,0.24861878453038674,0.024896791023984958,0.4836983662695298
frozenset({'race/ethnicity_group D'}),frozenset({'test preparation course_none'}),0.262,0.642,0.18,0.6870229007633587,1.0701291289148889,1.0,0.011795999999999973,1.143853658536585,0.088798554652213,0.24861878453038674,0.12576229263508013,0.4836983662695298
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})",frozenset({'gender_male'}),0.32,0.482,0.165,0.515625,1.0697614107883817,1.0,0.01076000000000002,1.0694193548387096,0.09590017825311961,0.2590266875981162,0.06491312741312744,0.4289743257261411
frozenset({'gender_male'}),"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})",0.482,0.32,0.165,0.34232365145228216,1.0697614107883817,1.0,0.01076000000000002,1.0339432176656151,0.12589212589212612,0.2590266875981162,0.032828899194532586,0.4289743257261411
"frozenset({'gender_male', 'test preparation course_none', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.203,0.281,0.061,0.3004926108374384,1.0693687218414176,1.0,0.003956999999999988,1.0278661971830985,0.08139128288458747,0.1442080378250591,0.02711072439143027,0.2587872306856231
"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'gender_male', 'test preparation course_none', 'lunch_standard'})",0.281,0.203,0.061,0.2170818505338078,1.0693687218414176,1.0,0.003956999999999988,1.0179863636363635,0.09022093526984173,0.1442080378250591,0.01766857030590688,0.2587872306856231
frozenset({'lunch_standard'}),"frozenset({""parental level of education_associate's degree"", 'gender_female'})",0.645,0.116,0.08,0.12403100775193798,1.0692328254477412,1.0,0.00517999999999999,1.0091681415929203,0.18239436619718277,0.11747430249632893,0.009084850398119883,0.4068430900828655
"frozenset({""parental level of education_associate's degree"", 'gender_female'})",frozenset({'lunch_standard'}),0.116,0.645,0.08,0.689655172413793,1.069232825447741,1.0,0.00517999999999999,1.1438888888888885,0.07324660633484148,0.11747430249632893,0.1257892180670226,0.4068430900828655
"frozenset({'parental level of education_some college', 'test preparation course_none'})",frozenset({'math_cat_χαμηλό'}),0.149,0.339,0.054,0.36241610738255037,1.0690740630753697,1.0,0.003488999999999999,1.0367263157894737,0.07592374983679329,0.12442396313364056,0.03542527591913819,0.26085407139039024
frozenset({'math_cat_χαμηλό'}),"frozenset({'parental level of education_some college', 'test preparation course_none'})",0.339,0.149,0.054,0.1592920353982301,1.0690740630753697,1.0,0.003488999999999999,1.0122421052631578,0.09774752059169604,0.12442396313364056,0.012094048646568852,0.26085407139039024
"frozenset({'gender_male', 'test preparation course_none'})","frozenset({'reading_cat_μέτριο', 'math_cat_μέτριο'})",0.308,0.325,0.107,0.3474025974025974,1.068931068931069,1.0,0.0068999999999999895,1.0343283582089553,0.0931878342607097,0.20342205323193915,0.033189033189033136,0.3383166833166833
"frozenset({'reading_cat_μέτριο', 'math_cat_μέτριο'})","frozenset({'gender_male', 'test preparation course_none'})",0.325,0.308,0.107,0.3292307692307692,1.068931068931069,1.0,0.0068999999999999895,1.031651376146789,0.09553478712357202,0.20342205323193915,0.03068030235660292,0.3383166833166833
frozenset({'race/ethnicity_group B'}),"frozenset({'reading_cat_μέτριο', 'gender_female'})",0.19,0.261,0.053,0.2789473684210526,1.068763863682194,1.0,0.0034099999999999964,1.024890510948905,0.0794316328907523,0.13316582914572864,0.02428601951427957,0.24100625126033473
"frozenset({'reading_cat_μέτριο', 'gender_female'})",frozenset({'race/ethnicity_group B'}),0.261,0.19,0.053,0.20306513409961685,1.068763863682194,1.0,0.0034099999999999964,1.0163942307692309,0.08706308882477587,0.13316582914572864,0.016129795184712158,0.24100625126033473
"frozenset({'gender_female', 'test preparation course_none'})","frozenset({'writing_cat_υψηλό', 'reading_cat_υψηλό'})",0.334,0.185,0.066,0.19760479041916168,1.0681340022657388,1.0,0.0042099999999999985,1.0157089552238805,0.09577759577759576,0.1456953642384106,0.015466000514308813,0.27718077358795923
"frozenset({'writing_cat_υψηλό', 'reading_cat_υψηλό'})","frozenset({'gender_female', 'test preparation course_none'})",0.185,0.334,0.066,0.3567567567567568,1.0681340022657388,1.0,0.0042099999999999985,1.035378151260504,0.07826733593604757,0.1456953642384106,0.034169304439574714,0.27718077358795923
"frozenset({""parental level of education_associate's degree"", 'reading_cat_μέτριο'})","frozenset({'test preparation course_none', 'lunch_standard'})",0.112,0.418,0.05,0.44642857142857145,1.0680109364319892,1.0,0.0031840000000000063,1.0513548387096774,0.07171171171171185,0.10416666666666666,0.04884634266077571,0.2830228981544771
"frozenset({'test preparation course_none', 'lunch_standard'})","frozenset({""parental level of education_associate's degree"", 'reading_cat_μέτριο'})",0.418,0.112,0.05,0.11961722488038279,1.0680109364319892,1.0,0.0031840000000000063,1.0086521739130436,0.10941580756013766,0.10416666666666666,0.008577955946377011,0.2830228981544771
"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced'})",frozenset({'race/ethnicity_group C'}),0.182,0.319,0.062,0.34065934065934067,1.0678976196217576,1.0,0.003942000000000001,1.03285,0.07772695007492705,0.14123006833712984,0.03180519920608027,0.2675083537152503
frozenset({'race/ethnicity_group C'}),"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced'})",0.319,0.182,0.062,0.19435736677115986,1.0678976196217576,1.0,0.003942000000000001,1.0153385214007784,0.09336364928236467,0.14123006833712984,0.015106805343716218,0.2675083537152503
"frozenset({'lunch_free/reduced', 'reading_cat_μέτριο'})",frozenset({'math_cat_χαμηλό'}),0.163,0.339,0.059,0.36196319018404904,1.0677380241417376,1.0,0.0037429999999999894,1.0359903846153846,0.07579531417694327,0.13318284424379231,0.034740075921405475,0.2680022440595761
frozenset({'math_cat_χαμηλό'}),"frozenset({'lunch_free/reduced', 'reading_cat_μέτριο'})",0.339,0.163,0.059,0.17404129793510323,1.0677380241417376,1.0,0.0037429999999999894,1.013367857142857,0.09597681991845917,0.13318284424379231,0.01319151485675415,0.2680022440595761
frozenset({'writing_cat_μέτριο'}),frozenset({'lunch_standard'}),0.491,0.645,0.338,0.6883910386965377,1.0672729282116864,1.0,0.02130500000000002,1.1392483660130721,0.12383604003673532,0.4235588972431078,0.12222827801841608,0.6062110232242379
frozenset({'lunch_standard'}),frozenset({'writing_cat_μέτριο'}),0.645,0.491,0.338,0.524031007751938,1.0672729282116864,1.0,0.02130500000000002,1.0693973941368078,0.17755646303858671,0.4235588972431078,0.06489392485645976,0.6062110232242379
"frozenset({'reading_cat_μέτριο', 'math_cat_μέτριο', 'test preparation course_none'})",frozenset({'gender_male'}),0.208,0.482,0.107,0.514423076923077,1.0672677944462179,1.0,0.006744,1.0667722772277228,0.07958085528178986,0.1835334476843911,0.06259281259281274,0.3682073890839451
frozenset({'gender_male'}),"frozenset({'reading_cat_μέτριο', 'math_cat_μέτριο', 'test preparation course_none'})",0.482,0.208,0.107,0.2219917012448133,1.0672677944462179,1.0,0.006744,1.017984,0.12167574784397214,0.1835334476843911,0.017666289450521847,0.3682073890839451
"frozenset({'reading_cat_μέτριο', 'lunch_standard'})","frozenset({'parental level of education_some college', 'test preparation course_none'})",0.327,0.149,0.052,0.15902140672782875,1.0672577632740186,1.0,0.003276999999999995,1.0119163636363635,0.09363927305977812,0.12264150943396226,0.011776036107906876,0.2540073476592164
"frozenset({'parental level of education_some college', 'test preparation course_none'})","frozenset({'reading_cat_μέτριο', 'lunch_standard'})",0.149,0.327,0.052,0.348993288590604,1.0672577632740183,1.0,0.003276999999999995,1.0337835051546391,0.07405315014010655,0.12264150943396226,0.03267947784636554,0.2540073476592164
frozenset({'writing_cat_μέτριο'}),"frozenset({'gender_female', 'test preparation course_none'})",0.491,0.334,0.175,0.3564154786150713,1.067112211422369,1.0,0.011005999999999988,1.0348291139240504,0.12355879876508546,0.2692307692307693,0.0336568747974043,0.44018378721172724
"frozenset({'gender_female', 'test preparation course_none'})",frozenset({'writing_cat_μέτριο'}),0.334,0.491,0.175,0.5239520958083832,1.067112211422369,1.0,0.011005999999999988,1.0692201257861635,0.09443157443157435,0.2692307692307693,0.06473889156853282,0.44018378721172724
"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'test preparation course_none', 'lunch_standard'})",0.139,0.418,0.062,0.44604316546762585,1.0670889126019758,1.0,0.0038979999999999987,1.0506233766233766,0.07302086845753247,0.12525252525252525,0.04818413310588636,0.2971842621596502
"frozenset({'test preparation course_none', 'lunch_standard'})","frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.418,0.139,0.062,0.14832535885167464,1.0670889126019758,1.0,0.0038979999999999987,1.0109494382022473,0.10802571776964855,0.12525252525252525,0.01083084651762442,0.2971842621596502
"frozenset({'gender_female', 'lunch_standard'})",frozenset({'reading_cat_μέτριο'}),0.329,0.49,0.172,0.5227963525835866,1.0669313318032378,1.0,0.010789999999999994,1.0687261146496814,0.09349114476830826,0.2658423493044822,0.06430657369330706,0.43690838037342594
frozenset({'reading_cat_μέτριο'}),"frozenset({'gender_female', 'lunch_standard'})",0.49,0.329,0.172,0.3510204081632653,1.0669313318032378,1.0,0.010789999999999994,1.033930817610063,0.12300501595987226,0.2658423493044822,0.03281729979622247,0.43690838037342594
"frozenset({""parental level of education_associate's degree"", 'reading_cat_μέτριο', 'lunch_standard'})",frozenset({'test preparation course_none'}),0.073,0.642,0.05,0.6849315068493151,1.066871505995818,1.0,0.003134000000000005,1.1362608695652177,0.06761596548004326,0.07518796992481204,0.11992041019361768,0.3814065633935049
frozenset({'test preparation course_none'}),"frozenset({""parental level of education_associate's degree"", 'reading_cat_μέτριο', 'lunch_standard'})",0.642,0.073,0.05,0.07788161993769471,1.066871505995818,1.0,0.003134000000000005,1.005293918918919,0.17508379888268183,0.07518796992481204,0.005266040925237019,0.3814065633935049
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})","frozenset({""parental level of education_associate's degree""})",0.266,0.222,0.063,0.23684210526315788,1.0668563300142246,1.0,0.003947999999999993,1.0194482758620689,0.08537693006357842,0.14823529411764708,0.0190772561223109,0.2603129445234708
"frozenset({""parental level of education_associate's degree""})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})",0.222,0.266,0.063,0.28378378378378377,1.0668563300142246,1.0,0.003947999999999993,1.0248301886792452,0.08054841473864596,0.14823529411764708,0.024228588261285773,0.2603129445234708
frozenset({'parental level of education_some college'}),"frozenset({'math_cat_μέτριο', 'lunch_standard'})",0.226,0.332,0.08,0.35398230088495575,1.066211749653481,1.0,0.004968,1.0340273972602738,0.08023255813953488,0.16736401673640167,0.03290763605532295,0.29747307815332125
"frozenset({'math_cat_μέτριο', 'lunch_standard'})",frozenset({'parental level of education_some college'}),0.332,0.226,0.08,0.24096385542168675,1.066211749653481,1.0,0.004968,1.0197142857142858,0.09296407185628744,0.16736401673640167,0.019333146539646948,0.29747307815332125
frozenset({'parental level of education_some high school'}),frozenset({'race/ethnicity_group D'}),0.179,0.262,0.05,0.2793296089385475,1.0661435455669752,1.0,0.0031020000000000006,1.024046511627907,0.07556638246041414,0.1278772378516624,0.023481854930281185,0.23508465179751803
frozenset({'race/ethnicity_group D'}),frozenset({'parental level of education_some high school'}),0.262,0.179,0.05,0.19083969465648856,1.0661435455669752,1.0,0.0031020000000000006,1.014632075471698,0.08406504065040651,0.1278772378516624,0.01442106535504089,0.23508465179751803
frozenset({'lunch_standard'}),"frozenset({'race/ethnicity_group C', 'math_cat_μέτριο', 'test preparation course_none'})",0.645,0.096,0.066,0.10232558139534884,1.065891472868217,1.0,0.00408,1.0070466321243523,0.1741357234314981,0.09777777777777777,0.006997324552377029,0.39491279069767443
"frozenset({'race/ethnicity_group C', 'math_cat_μέτριο', 'test preparation course_none'})",frozenset({'lunch_standard'}),0.096,0.645,0.066,0.6875,1.065891472868217,1.0,0.00408,1.136,0.06838294448913919,0.09777777777777777,0.11971830985915488,0.39491279069767443
frozenset({'gender_female'}),"frozenset({'parental level of education_high school', 'writing_cat_μέτριο'})",0.518,0.096,0.053,0.1023166023166023,1.0657979407979408,1.0,0.003271999999999997,1.007036559139785,0.12808267439129403,0.09447415329768272,0.006987391943144142,0.3271999678249678
"frozenset({'parental level of education_high school', 'writing_cat_μέτριο'})",frozenset({'gender_female'}),0.096,0.518,0.053,0.5520833333333333,1.0657979407979405,1.0,0.003271999999999997,1.0760930232558137,0.06829186842544659,0.09447415329768272,0.07071230982019346,0.3271999678249678
frozenset({'gender_female'}),"frozenset({'writing_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",0.518,0.221,0.122,0.23552123552123552,1.0657069480598893,1.0,0.007521999999999987,1.0189949494949495,0.12791646826746458,0.19773095623987033,0.01864086716461556,0.3937787173081291
"frozenset({'writing_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",frozenset({'gender_female'}),0.221,0.518,0.122,0.5520361990950227,1.0657069480598893,1.0,0.007521999999999987,1.075979797979798,0.0791472884530397,0.19773095623987033,0.07061452094403038,0.3937787173081291
frozenset({'gender_male'}),"frozenset({'test preparation course_completed', 'writing_cat_μέτριο'})",0.482,0.183,0.094,0.1950207468879668,1.0656871414642994,1.0,0.0057940000000000075,1.0149329896907215,0.11899285303540638,0.1646234676007005,0.014713276484659499,0.35434097453687957
"frozenset({'test preparation course_completed', 'writing_cat_μέτριο'})",frozenset({'gender_male'}),0.183,0.482,0.094,0.5136612021857924,1.0656871414642994,1.0,0.0057940000000000075,1.0651011235955057,0.07544467303836049,0.1646234676007005,0.06112201194168413,0.35434097453687957
"frozenset({'lunch_free/reduced', 'writing_cat_μέτριο'})",frozenset({'race/ethnicity_group C'}),0.153,0.319,0.052,0.33986928104575165,1.0654209437170898,1.0,0.0031929999999999945,1.0316138613861388,0.07249568613204964,0.12380952380952381,0.03064505293061915,0.25143934271723317
frozenset({'race/ethnicity_group C'}),"frozenset({'lunch_free/reduced', 'writing_cat_μέτριο'})",0.319,0.153,0.052,0.16300940438871472,1.0654209437170896,1.0,0.0031929999999999945,1.0119588014981273,0.09016717496893693,0.12380952380952381,0.011817478617136631,0.25143934271723317
"frozenset({""parental level of education_associate's degree"", 'lunch_standard'})",frozenset({'gender_female'}),0.145,0.518,0.08,0.5517241379310346,1.0651045133803756,1.0,0.0048900000000000055,1.0752307692307694,0.07149122807017552,0.13722126929674097,0.06996709114322525,0.3530821461855945
frozenset({'gender_female'}),"frozenset({""parental level of education_associate's degree"", 'lunch_standard'})",0.518,0.145,0.08,0.15444015444015444,1.0651045133803756,1.0,0.0048900000000000055,1.011164383561644,0.1268153526970956,0.13722126929674097,0.011041116304274207,0.3530821461855945
frozenset({'math_cat_μέτριο'}),"frozenset({'test preparation course_completed', 'gender_female'})",0.485,0.184,0.095,0.1958762886597938,1.064545047064097,1.0,0.005760000000000001,1.0147692307692309,0.11773122125702608,0.1655052264808362,0.014554275318374773,0.35609031824294035
"frozenset({'test preparation course_completed', 'gender_female'})",frozenset({'math_cat_μέτριο'}),0.184,0.485,0.095,0.5163043478260869,1.0645450470640967,1.0,0.005760000000000001,1.0647191011235955,0.07430340557275543,0.1655052264808362,0.060785141409877536,0.35609031824294035
"frozenset({'race/ethnicity_group C', 'gender_female'})","frozenset({'writing_cat_μέτριο', 'test preparation course_none'})",0.18,0.308,0.059,0.3277777777777778,1.0642135642135642,1.0,0.0035600000000000007,1.0294214876033057,0.07358412567176521,0.13752913752913754,0.028580603725112404,0.25966810966810966
"frozenset({'writing_cat_μέτριο', 'test preparation course_none'})","frozenset({'race/ethnicity_group C', 'gender_female'})",0.308,0.18,0.059,0.19155844155844154,1.0642135642135642,1.0,0.0035600000000000007,1.01429718875502,0.08719506221220734,0.13752913752913754,0.014095660437123835,0.25966810966810966
frozenset({'gender_female'}),"frozenset({'parental level of education_high school', 'reading_cat_μέτριο'})",0.518,0.098,0.054,0.10424710424710425,1.0637459617051452,1.0,0.003235999999999996,1.0069741379310346,0.12432764714922376,0.09608540925266905,0.006925836194128872,0.32763375620518476
"frozenset({'parental level of education_high school', 'reading_cat_μέτριο'})",frozenset({'gender_female'}),0.098,0.518,0.054,0.5510204081632653,1.0637459617051452,1.0,0.003235999999999996,1.0735454545454544,0.06643672497331025,0.09608540925266905,0.06850707087814364,0.32763375620518476
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})",frozenset({'parental level of education_high school'}),0.283,0.196,0.059,0.20848056537102475,1.0636763539337997,1.0,0.0035320000000000004,1.0157678571428572,0.08349289648488288,0.14047619047619048,0.015523091257493455,0.254750486767145
frozenset({'parental level of education_high school'}),"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})",0.196,0.283,0.059,0.30102040816326525,1.0636763539337997,1.0,0.0035320000000000004,1.0257810218978103,0.0744582173876381,0.14047619047619048,0.025133065778612657,0.254750486767145
"frozenset({'parental level of education_high school', 'lunch_standard'})",frozenset({'math_cat_μέτριο'}),0.126,0.485,0.065,0.5158730158730159,1.0636557028309608,1.0,0.0038900000000000046,1.0637704918032789,0.06847386023587404,0.11904761904761904,0.059947603636924154,0.32494681721485846
frozenset({'math_cat_μέτριο'}),"frozenset({'parental level of education_high school', 'lunch_standard'})",0.485,0.126,0.065,0.13402061855670103,1.0636557028309606,1.0,0.0038900000000000046,1.0092619047619047,0.11620612397311439,0.11904761904761904,0.009176909103776918,0.32494681721485846
frozenset({'parental level of education_some college'}),frozenset({'writing_cat_υψηλό'}),0.226,0.208,0.05,0.22123893805309736,1.0636487406398911,1.0,0.0029920000000000016,1.0170000000000001,0.07731266149870804,0.13020833333333334,0.016715830875122937,0.2308117767188564
frozenset({'writing_cat_υψηλό'}),frozenset({'parental level of education_some college'}),0.208,0.226,0.05,0.24038461538461542,1.0636487406398911,1.0,0.0029920000000000016,1.0189367088607595,0.07555555555555558,0.13020833333333334,0.018584774398727924,0.2308117767188564
"frozenset({""parental level of education_associate's degree""})","frozenset({'reading_cat_μέτριο', 'test preparation course_none'})",0.222,0.318,0.075,0.33783783783783783,1.0623831378548358,1.0,0.004403999999999991,1.0299591836734694,0.07547557840616952,0.16129032258064516,0.029087738765158102,0.2868434472208057
"frozenset({'reading_cat_μέτριο', 'test preparation course_none'})","frozenset({""parental level of education_associate's degree""})",0.318,0.222,0.075,0.23584905660377356,1.0623831378548358,1.0,0.004403999999999991,1.0181234567901234,0.08609970674486787,0.16129032258064516,0.017800843963719226,0.2868434472208057
frozenset({'test preparation course_none'}),"frozenset({'reading_cat_μέτριο', 'lunch_standard'})",0.642,0.327,0.223,0.34735202492211836,1.062238608324521,1.0,0.013065999999999994,1.031183770883055,0.16366460405341074,0.29892761394101874,0.03024075025574792,0.5146546057332304
"frozenset({'reading_cat_μέτριο', 'lunch_standard'})",frozenset({'test preparation course_none'}),0.327,0.642,0.223,0.6819571865443425,1.062238608324521,1.0,0.013065999999999994,1.125634615384615,0.08706081463762413,0.29892761394101874,0.11161225291715765,0.5146546057332304
frozenset({'race/ethnicity_group C'}),"frozenset({'math_cat_μέτριο', 'gender_female'})",0.319,0.245,0.083,0.2601880877742947,1.0619921949971212,1.0,0.004845000000000002,1.0205296610169492,0.08571731861366173,0.17255717255717254,0.020116672548734702,0.29948179898918814
"frozenset({'math_cat_μέτριο', 'gender_female'})",frozenset({'race/ethnicity_group C'}),0.245,0.319,0.083,0.33877551020408164,1.0619921949971212,1.0,0.004845000000000002,1.0299074074074075,0.07731588606079952,0.17255717255717254,0.029038928346669067,0.29948179898918814
frozenset({'parental level of education_some college'}),"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο'})",0.226,0.325,0.078,0.34513274336283184,1.0619469026548671,1.0,0.0045499999999999985,1.0307432432432433,0.07536606373815673,0.16490486257928116,0.02982628646345456,0.2925663716814159
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο'})",frozenset({'parental level of education_some college'}),0.325,0.226,0.078,0.24,1.0619469026548671,1.0,0.0045499999999999985,1.018421052631579,0.08641975308641972,0.16490486257928116,0.018087855297157604,0.2925663716814159
frozenset({'lunch_standard'}),"frozenset({'writing_cat_μέτριο', 'parental level of education_some college', 'test preparation course_none'})",0.645,0.073,0.05,0.07751937984496124,1.0619093129446746,1.0,0.002915000000000008,1.0048991596638657,0.16422535211267653,0.07485029940119761,0.0048752749136583,0.3812254433471382
"frozenset({'writing_cat_μέτριο', 'parental level of education_some college', 'test preparation course_none'})",frozenset({'lunch_standard'}),0.073,0.645,0.05,0.6849315068493151,1.0619093129446746,1.0,0.002915000000000008,1.126739130434783,0.06289104638619218,0.07485029940119761,0.11248311788539472,0.3812254433471382
frozenset({'race/ethnicity_group D'}),"frozenset({'writing_cat_μέτριο', 'lunch_standard'})",0.262,0.338,0.094,0.35877862595419846,1.0614752247165635,1.0,0.0054439999999999905,1.0324047619047618,0.07847546560571975,0.18577075098814225,0.031387652498789186,0.318442567414969
"frozenset({'writing_cat_μέτριο', 'lunch_standard'})",frozenset({'race/ethnicity_group D'}),0.338,0.262,0.094,0.2781065088757396,1.0614752247165633,1.0,0.0054439999999999905,1.022311475409836,0.08748473356045496,0.18577075098814225,0.021824537772004892,0.318442567414969
frozenset({'lunch_standard'}),frozenset({'math_cat_μέτριο'}),0.645,0.485,0.332,0.5147286821705427,1.0612962518980262,1.0,0.019174999999999998,1.0612619808306711,0.16269302562362123,0.41604010025062665,0.05772559644765572,0.5996323823223848
frozenset({'math_cat_μέτριο'}),frozenset({'lunch_standard'}),0.485,0.645,0.332,0.6845360824742268,1.061296251898026,1.0,0.019174999999999998,1.125326797385621,0.1121476196046321,0.41604010025062665,0.11136924640627267,0.5996323823223848
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",frozenset({'gender_female'}),0.191,0.518,0.105,0.5497382198952879,1.0612706947785482,1.0,0.006061999999999998,1.0704883720930232,0.07136382365059743,0.17384105960264898,0.06584692924333592,0.37622046129899533
frozenset({'gender_female'}),"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",0.518,0.191,0.105,0.2027027027027027,1.0612706947785482,1.0,0.006061999999999998,1.0146779661016947,0.11977869986168739,0.17384105960264898,0.014465639929175135,0.37622046129899533
"frozenset({'writing_cat_μέτριο', 'test preparation course_none'})","frozenset({'race/ethnicity_group C', 'lunch_standard'})",0.308,0.205,0.067,0.21753246753246755,1.0611339879632564,1.0,0.0038600000000000023,1.0160165975103737,0.0832542489862825,0.1502242152466368,0.01576411010373278,0.27218086791257523
"frozenset({'race/ethnicity_group C', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'test preparation course_none'})",0.205,0.308,0.067,0.32682926829268294,1.0611339879632564,1.0,0.0038600000000000023,1.0279710144927536,0.0724678494320849,0.1502242152466368,0.027209925278443563,0.27218086791257523
"frozenset({'race/ethnicity_group C', 'test preparation course_none', 'lunch_standard'})",frozenset({'gender_female'}),0.131,0.518,0.072,0.5496183206106869,1.06103922897816,1.0,0.004141999999999993,1.0702033898305083,0.06619997442782241,0.12478336221837086,0.06559817554084424,0.34430722980341294
frozenset({'gender_female'}),"frozenset({'race/ethnicity_group C', 'test preparation course_none', 'lunch_standard'})",0.518,0.131,0.072,0.13899613899613897,1.06103922897816,1.0,0.004141999999999993,1.009286995515695,0.11935223605348066,0.12478336221837086,0.00920154084711043,0.34430722980341294
"frozenset({'race/ethnicity_group C', 'math_cat_μέτριο'})","frozenset({'gender_male', 'test preparation course_none'})",0.153,0.308,0.05,0.32679738562091504,1.0610304727951787,1.0,0.0028760000000000036,1.0279223300970872,0.06791027154663527,0.12165450121654503,0.027163852053345438,0.2445675239792887
"frozenset({'gender_male', 'test preparation course_none'})","frozenset({'race/ethnicity_group C', 'math_cat_μέτριο'})",0.308,0.153,0.05,0.16233766233766234,1.0610304727951787,1.0,0.0028760000000000036,1.0111472868217053,0.0831213872832371,0.12165450121654503,0.011024394731596623,0.2445675239792887
frozenset({'gender_female'}),"frozenset({'race/ethnicity_group C', 'test preparation course_none'})",0.518,0.202,0.111,0.21428571428571427,1.0608203677510606,1.0,0.006363999999999995,1.0156363636363637,0.11894882434301511,0.18226600985221675,0.015395631936985288,0.38189533239038187
"frozenset({'race/ethnicity_group C', 'test preparation course_none'})",frozenset({'gender_female'}),0.202,0.518,0.111,0.5495049504950494,1.0608203677510606,1.0,0.006363999999999995,1.0699340659340657,0.0718462823725981,0.18226600985221675,0.06536296783205274,0.38189533239038187
frozenset({'lunch_standard'}),"frozenset({'reading_cat_μέτριο', 'parental level of education_some college', 'test preparation course_none'})",0.645,0.076,0.052,0.08062015503875969,1.0607915136678907,1.0,0.0029799999999999965,1.0050252951096121,0.16143011917659789,0.07772795216741406,0.005000167790865466,0.38241534067727456
frozenset({'lunch_standard'}),"frozenset({'writing_cat_μέτριο', 'gender_male', 'race/ethnicity_group D'})",0.645,0.076,0.052,0.08062015503875969,1.0607915136678907,1.0,0.0029799999999999965,1.0050252951096121,0.16143011917659789,0.07772795216741406,0.005000167790865466,0.38241534067727456
"frozenset({'writing_cat_μέτριο', 'gender_male', 'race/ethnicity_group D'})",frozenset({'lunch_standard'}),0.076,0.645,0.052,0.6842105263157895,1.0607915136678907,1.0,0.0029799999999999965,1.1241666666666668,0.06202131202131195,0.07772795216741406,0.11045218680504078,0.38241534067727456
"frozenset({'reading_cat_μέτριο', 'parental level of education_some college', 'test preparation course_none'})",frozenset({'lunch_standard'}),0.076,0.645,0.052,0.6842105263157895,1.0607915136678907,1.0,0.0029799999999999965,1.1241666666666668,0.06202131202131195,0.07772795216741406,0.11045218680504078,0.38241534067727456
"frozenset({'parental level of education_some college', 'test preparation course_none', 'lunch_standard'})",frozenset({'writing_cat_μέτριο'}),0.096,0.491,0.05,0.5208333333333334,1.0607603530210457,1.0,0.0028640000000000054,1.0622608695652176,0.0633628318584072,0.09310986964618251,0.058611656843484045,0.31133316361167684
frozenset({'writing_cat_μέτριο'}),"frozenset({'parental level of education_some college', 'test preparation course_none', 'lunch_standard'})",0.491,0.096,0.05,0.10183299389002037,1.0607603530210454,1.0,0.0028640000000000054,1.0064943310657597,0.11253438113948941,0.09310986964618251,0.006452426869491561,0.31133316361167684
"frozenset({'writing_cat_μέτριο', 'test preparation course_none'})",frozenset({'parental level of education_high school'}),0.308,0.196,0.064,0.2077922077922078,1.060164325470448,1.0,0.0036319999999999963,1.0148852459016395,0.08200867052023113,0.14545454545454545,0.014666925114686315,0.26716141001855287
frozenset({'parental level of education_high school'}),"frozenset({'writing_cat_μέτριο', 'test preparation course_none'})",0.196,0.308,0.064,0.32653061224489793,1.0601643254704478,1.0,0.0036319999999999963,1.0275151515151513,0.07058457711442778,0.14545454545454545,0.026778341394361183,0.26716141001855287
"frozenset({'gender_female', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.329,0.281,0.098,0.2978723404255319,1.0600439161050956,1.0,0.0055509999999999865,1.024030303030303,0.0844155844155842,0.19140624999999997,0.023466398366525556,0.3233133944120542
"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'gender_female', 'lunch_standard'})",0.281,0.329,0.098,0.3487544483985765,1.0600439161050956,1.0,0.0055509999999999865,1.0303333333333333,0.07878005165905008,0.19140624999999997,0.029440310579100574,0.3233133944120542
"frozenset({'parental level of education_some high school', 'test preparation course_none'})",frozenset({'gender_female'}),0.102,0.518,0.056,0.5490196078431373,1.0598834128245893,1.0,0.003164,1.0687826086956522,0.06291759465478843,0.09929078014184399,0.06435603286957943,0.3285638579756227
frozenset({'gender_female'}),"frozenset({'parental level of education_some high school', 'test preparation course_none'})",0.518,0.102,0.056,0.1081081081081081,1.0598834128245893,1.0,0.003164,1.0068484848484849,0.11721991701244815,0.09929078014184399,0.0068019021248419895,0.3285638579756227
frozenset({'race/ethnicity_group E'}),"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.14,0.418,0.062,0.44285714285714284,1.0594668489405332,1.0,0.003479999999999997,1.0446153846153847,0.06526631657914474,0.12499999999999999,0.04270986745213549,0.29559125085440874
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})",frozenset({'race/ethnicity_group E'}),0.418,0.14,0.062,0.14832535885167464,1.059466848940533,1.0,0.003479999999999997,1.0097752808988765,0.09644163618224134,0.12499999999999999,0.009680649827528634,0.29559125085440874
frozenset({'parental level of education_some college'}),"frozenset({'math_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",0.226,0.213,0.051,0.22566371681415928,1.0594540695500436,1.0,0.0028619999999999965,1.0163542857142858,0.07250341997264013,0.13144329896907214,0.01609112682866491,0.23255016826623456
"frozenset({'math_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",frozenset({'parental level of education_some college'}),0.213,0.226,0.051,0.23943661971830985,1.0594540695500436,1.0,0.0028619999999999965,1.0176666666666667,0.07130577771133859,0.13144329896907214,0.017359973796265943,0.23255016826623456
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'race/ethnicity_group D'})",frozenset({'lunch_standard'}),0.12,0.645,0.082,0.6833333333333333,1.0594315245478036,1.0,0.004600000000000007,1.1210526315789473,0.06374722838137481,0.12005856515373352,0.10798122065727699,0.4052325581395349
frozenset({'lunch_standard'}),"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'race/ethnicity_group D'})",0.645,0.12,0.082,0.12713178294573643,1.0594315245478036,1.0,0.004600000000000007,1.008170515097691,0.1580212985228446,0.12005856515373352,0.008104298801973223,0.4052325581395349
frozenset({'reading_cat_μέτριο'}),"frozenset({'test preparation course_completed', 'lunch_free/reduced'})",0.49,0.131,0.068,0.13877551020408163,1.059355039725814,1.0,0.003810000000000008,1.0090284360189574,0.10986159169550194,0.12296564195298375,0.00894765270895469,0.32892973983486523
"frozenset({'test preparation course_completed', 'lunch_free/reduced'})",frozenset({'reading_cat_μέτριο'}),0.131,0.49,0.068,0.5190839694656488,1.059355039725814,1.0,0.003810000000000008,1.0604761904761904,0.06447573275570309,0.12296564195298375,0.05702739110911537,0.32892973983486523
frozenset({'race/ethnicity_group D'}),frozenset({'reading_cat_μέτριο'}),0.262,0.49,0.136,0.5190839694656488,1.059355039725814,1.0,0.007620000000000016,1.0604761904761904,0.0759206121472981,0.2207792207792208,0.05702739110911537,0.39831749493690605
frozenset({'reading_cat_μέτριο'}),frozenset({'race/ethnicity_group D'}),0.49,0.262,0.136,0.27755102040816326,1.059355039725814,1.0,0.007620000000000016,1.0215254237288136,0.10986159169550194,0.2207792207792208,0.02107184337149492,0.39831749493690605
"frozenset({'math_cat_μέτριο', 'test preparation course_none'})","frozenset({'race/ethnicity_group C', 'lunch_standard'})",0.304,0.205,0.066,0.21710526315789475,1.0590500641848526,1.0,0.0036800000000000097,1.0154621848739496,0.08011145942180446,0.1489841986455982,0.015226746110559443,0.26952824133504494
"frozenset({'race/ethnicity_group C', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'test preparation course_none'})",0.205,0.304,0.066,0.32195121951219513,1.0590500641848524,1.0,0.0036800000000000097,1.026474820143885,0.07013531541833447,0.1489841986455982,0.02579198205775164,0.26952824133504494
"frozenset({'writing_cat_υψηλό', 'gender_female'})",frozenset({'race/ethnicity_group C'}),0.154,0.319,0.052,0.33766233766233766,1.058502625900745,1.0,0.0028739999999999946,1.0281764705882355,0.06533006001091096,0.12351543942992874,0.027404313747926075,0.2503358710255262
frozenset({'race/ethnicity_group C'}),"frozenset({'writing_cat_υψηλό', 'gender_female'})",0.319,0.154,0.052,0.16300940438871472,1.058502625900745,1.0,0.0028739999999999946,1.0107640449438202,0.08115892917655017,0.12351543942992874,0.010649414171057596,0.2503358710255262
frozenset({'gender_female'}),"frozenset({'reading_cat_μέτριο', 'writing_cat_μέτριο', 'lunch_standard', 'test preparation course_none', 'math_cat_μέτριο'})",0.518,0.135,0.074,0.14285714285714285,1.0582010582010581,1.0,0.00406999999999999,1.0091666666666665,0.11410788381742712,0.12780656303972365,0.009083402146985943,0.3455026455026454
"frozenset({'reading_cat_μέτριο', 'writing_cat_μέτριο', 'lunch_standard', 'test preparation course_none', 'math_cat_μέτριο'})",frozenset({'gender_female'}),0.135,0.518,0.074,0.548148148148148,1.058201058201058,1.0,0.00406999999999999,1.0667213114754095,0.06358381502890159,0.12780656303972365,0.06254802520362664,0.3455026455026454
"frozenset({'parental level of education_some college', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.147,0.418,0.065,0.44217687074829937,1.0578394037040655,1.0,0.0035540000000000085,1.0433414634146343,0.06409955812066026,0.13000000000000003,0.04154101503144224,0.2988396315463985
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'parental level of education_some college', 'lunch_standard'})",0.418,0.147,0.065,0.15550239234449761,1.0578394037040655,1.0,0.0035540000000000085,1.0100679886685553,0.09394660322495395,0.13000000000000003,0.009967634635987835,0.2988396315463985
"frozenset({'math_cat_χαμηλό', 'reading_cat_χαμηλό'})","frozenset({'gender_female', 'test preparation course_none'})",0.218,0.334,0.077,0.3532110091743119,1.057517991539856,1.0,0.004187999999999997,1.0297021276595744,0.06955193144451452,0.16210526315789472,0.02884535912058844,0.29187496566500026
"frozenset({'gender_female', 'test preparation course_none'})","frozenset({'math_cat_χαμηλό', 'reading_cat_χαμηλό'})",0.334,0.218,0.077,0.2305389221556886,1.057517991539856,1.0,0.004187999999999997,1.016295719844358,0.08166608166608162,0.16210526315789472,0.016034427309064715,0.29187496566500026
"frozenset({'reading_cat_μέτριο', 'test preparation course_none'})",frozenset({'parental level of education_some college'}),0.318,0.226,0.076,0.2389937106918239,1.057494295096566,1.0,0.004131999999999997,1.0170743801652893,0.07971909245253891,0.16239316239316237,0.016787739911917177,0.28763844826626594
frozenset({'parental level of education_some college'}),"frozenset({'reading_cat_μέτριο', 'test preparation course_none'})",0.226,0.318,0.076,0.33628318584070793,1.0574942950965658,1.0,0.004131999999999997,1.0275466666666666,0.07024343805249553,0.16239316239316237,0.02680819038226969,0.28763844826626594
frozenset({'lunch_standard'}),"frozenset({'writing_cat_μέτριο', 'race/ethnicity_group B'})",0.645,0.088,0.06,0.09302325581395349,1.0570824524312896,1.0,0.00324,1.0055384615384615,0.15211267605633802,0.08915304606240712,0.005507955936352513,0.38742071881606766
"frozenset({'writing_cat_μέτριο', 'race/ethnicity_group B'})",frozenset({'lunch_standard'}),0.088,0.645,0.06,0.6818181818181819,1.0570824524312896,1.0,0.00324,1.1157142857142859,0.05921052631578947,0.08915304606240712,0.1037131882202306,0.38742071881606766
frozenset({'reading_cat_μέτριο'}),"frozenset({'gender_female', 'test preparation course_none'})",0.49,0.334,0.173,0.3530612244897959,1.0570695344005865,1.0,0.009339999999999987,1.0294637223974763,0.10585968491442807,0.26574500768049153,0.028620457191885676,0.4355126481730416
"frozenset({'gender_female', 'test preparation course_none'})",frozenset({'reading_cat_μέτριο'}),0.334,0.49,0.173,0.5179640718562873,1.0570695344005865,1.0,0.009339999999999987,1.0580124223602483,0.0810637226822197,0.26574500768049153,0.05483151344370069,0.4355126481730416
frozenset({'race/ethnicity_group C'}),"frozenset({'writing_cat_μέτριο', 'gender_female', 'test preparation course_none'})",0.319,0.175,0.059,0.18495297805642633,1.0568741603224363,1.0,0.0031749999999999973,1.0122115384615384,0.07902137932750933,0.13563218390804596,0.012064215825971319,0.26104791759964174
"frozenset({'writing_cat_μέτριο', 'gender_female', 'test preparation course_none'})",frozenset({'race/ethnicity_group C'}),0.175,0.319,0.059,0.33714285714285713,1.056874160322436,1.0,0.0031749999999999973,1.0273706896551726,0.06522855675398043,0.13563218390804596,0.026641493601846,0.26104791759964174
frozenset({'race/ethnicity_group B'}),frozenset({'gender_female'}),0.19,0.518,0.104,0.5473684210526315,1.0566957935378987,1.0,0.005579999999999988,1.0648837209302324,0.0662393162393161,0.17218543046357615,0.06093033413409023,0.37407031091241616
frozenset({'gender_female'}),frozenset({'race/ethnicity_group B'}),0.518,0.19,0.104,0.20077220077220076,1.0566957935378987,1.0,0.005579999999999988,1.0134782608695654,0.1113150335142034,0.17218543046357615,0.01329901329901328,0.37407031091241616
"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})",frozenset({'test preparation course_none'}),0.199,0.642,0.135,0.678391959798995,1.0566852956370638,1.0,0.0072419999999999984,1.11315625,0.06697184075461228,0.19121813031161475,0.10165351899160618,0.44433616681538535
frozenset({'test preparation course_none'}),"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})",0.642,0.199,0.135,0.2102803738317757,1.0566852956370638,1.0,0.0072419999999999984,1.014284023668639,0.149844816883923,0.19121813031161475,0.014082863710082006,0.44433616681538535
"frozenset({'lunch_free/reduced', 'test preparation course_none'})",frozenset({'race/ethnicity_group D'}),0.224,0.262,0.062,0.2767857142857143,1.056434023991276,1.0,0.0033119999999999955,1.0204444444444445,0.06883937479215155,0.14622641509433962,0.020034843205574922,0.25671346782988
frozenset({'race/ethnicity_group D'}),"frozenset({'lunch_free/reduced', 'test preparation course_none'})",0.262,0.224,0.062,0.23664122137404578,1.0564340239912757,1.0,0.0033119999999999955,1.01656,0.07238394964594798,0.14622641509433962,0.01629023372944043,0.25671346782988
"frozenset({'race/ethnicity_group C', 'gender_male', 'lunch_standard'})",frozenset({'test preparation course_none'}),0.087,0.642,0.059,0.67816091954023,1.0563254198445948,1.0,0.003145999999999996,1.112357142857143,0.05840310394118841,0.08805970149253732,0.10100815514030707,0.3850306155333548
frozenset({'test preparation course_none'}),"frozenset({'race/ethnicity_group C', 'gender_male', 'lunch_standard'})",0.642,0.087,0.059,0.09190031152647975,1.0563254198445948,1.0,0.003145999999999996,1.0053962264150944,0.14894422876621513,0.08805970149253732,0.005367263446308601,0.3850306155333548
frozenset({'lunch_standard'}),"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})",0.645,0.091,0.062,0.09612403100775194,1.0563080330522192,1.0,0.0033050000000000024,1.0056689536878216,0.15015901862789652,0.09198813056379823,0.0056369978083079635,0.3887213561632166
"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})",frozenset({'lunch_standard'}),0.091,0.645,0.062,0.6813186813186813,1.0563080330522192,1.0,0.0033050000000000024,1.1139655172413794,0.05864296107030062,0.09198813056379823,0.1023061445596657,0.3887213561632166
frozenset({'test preparation course_none'}),"frozenset({'race/ethnicity_group C', 'math_cat_χαμηλό'})",0.642,0.118,0.08,0.12461059190031153,1.0560219652568774,1.0,0.004243999999999998,1.0075516014234875,0.14818435754189938,0.11764705882352941,0.007495002154548223,0.40128834679761344
"frozenset({'race/ethnicity_group C', 'math_cat_χαμηλό'})",frozenset({'test preparation course_none'}),0.118,0.642,0.08,0.6779661016949153,1.0560219652568774,1.0,0.004243999999999998,1.111684210526316,0.0601473922902494,0.11764705882352941,0.10046397121484726,0.40128834679761344
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({""parental level of education_associate's degree"", 'lunch_standard'})",0.418,0.145,0.064,0.15311004784688997,1.0559313644613102,1.0,0.003390000000000011,1.0095762711864407,0.09101159793814462,0.12825651302605212,0.0094854360782339,0.29724467909585883
"frozenset({""parental level of education_associate's degree"", 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.145,0.418,0.064,0.44137931034482764,1.0559313644613102,1.0,0.003390000000000011,1.041851851851852,0.06195175438596512,0.12825651302605212,0.04017063633131899,0.29724467909585883
frozenset({'parental level of education_high school'}),"frozenset({'reading_cat_μέτριο', 'gender_female'})",0.196,0.261,0.054,0.2755102040816326,1.0555946516537649,1.0,0.0028439999999999924,1.0200281690140844,0.06550580431177427,0.13399503722084366,0.019634917566485274,0.24120337790288526
"frozenset({'reading_cat_μέτριο', 'gender_female'})",frozenset({'parental level of education_high school'}),0.261,0.196,0.054,0.20689655172413793,1.0555946516537649,1.0,0.0028439999999999924,1.0137391304347827,0.07126747857465024,0.13399503722084366,0.013552925030022289,0.24120337790288526
"frozenset({'reading_cat_μέτριο', 'math_cat_μέτριο', 'lunch_standard'})",frozenset({'gender_male'}),0.228,0.482,0.116,0.5087719298245614,1.0555434228725342,1.0,0.006104000000000012,1.0545,0.06816151509737373,0.1952861952861953,0.05168326220957808,0.3747179151197496
frozenset({'gender_male'}),"frozenset({'reading_cat_μέτριο', 'math_cat_μέτριο', 'lunch_standard'})",0.482,0.228,0.116,0.2406639004149378,1.0555434228725342,1.0,0.006104000000000012,1.0166775956284153,0.10158434296365351,0.1952861952861953,0.016404016081525627,0.3747179151197496
"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο'})",frozenset({'parental level of education_some college'}),0.281,0.226,0.067,0.2384341637010676,1.0550184234560513,1.0,0.003493999999999997,1.0163271028037384,0.07253025553733414,0.15227272727272728,0.01606481098329146,0.26744717034610904
frozenset({'parental level of education_some college'}),"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.226,0.281,0.067,0.29646017699115046,1.0550184234560513,1.0,0.003493999999999997,1.0219748427672957,0.06737629681052099,0.15227272727272728,0.02150233239381145,0.26744717034610904
frozenset({'math_cat_μέτριο'}),"frozenset({'gender_female', 'test preparation course_none', 'lunch_standard'})",0.485,0.215,0.11,0.2268041237113402,1.0549029009829778,1.0,0.005725000000000008,1.0152666666666668,0.10105913503971771,0.1864406779661017,0.015037100269223195,0.3692160153440422
"frozenset({'gender_female', 'test preparation course_none', 'lunch_standard'})",frozenset({'math_cat_μέτριο'}),0.215,0.485,0.11,0.5116279069767442,1.0549029009829778,1.0,0.005725000000000008,1.0545238095238096,0.06629994209612053,0.1864406779661017,0.051704673741250914,0.3692160153440422
frozenset({'gender_female'}),"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', ""parental level of education_associate's degree""})",0.518,0.097,0.053,0.1023166023166023,1.0548103331608485,1.0,0.0027539999999999926,1.0059225806451613,0.10780552728411465,0.09430604982206406,0.005887710206647069,0.32435417744696093
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', ""parental level of education_associate's degree""})",frozenset({'gender_female'}),0.097,0.518,0.053,0.5463917525773195,1.0548103331608485,1.0,0.0027539999999999926,1.062590909090909,0.057544035604588324,0.09430604982206406,0.05890405099028946,0.32435417744696093
"frozenset({'gender_male', 'test preparation course_none'})","frozenset({'math_cat_χαμηλό', 'lunch_standard'})",0.308,0.157,0.051,0.16558441558441558,1.054677806270163,1.0,0.0026440000000000005,1.0102879377431906,0.07491782840303754,0.12318840579710144,0.0101831738842415,0.24521258995781287
"frozenset({'math_cat_χαμηλό', 'lunch_standard'})","frozenset({'gender_male', 'test preparation course_none'})",0.157,0.308,0.051,0.3248407643312102,1.054677806270163,1.0,0.0026440000000000005,1.024943396226415,0.06149838345777221,0.12318840579710144,0.02433636464047714,0.24521258995781287
frozenset({'lunch_standard'}),"frozenset({'parental level of education_high school', 'gender_male', 'test preparation course_none'})",0.645,0.075,0.051,0.07906976744186046,1.054263565891473,1.0,0.0026249999999999954,1.004419191919192,0.144987572493786,0.07623318385650224,0.004399748585795099,0.3795348837209302
"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'math_cat_μέτριο'})",frozenset({'lunch_standard'}),0.1,0.645,0.068,0.68,1.054263565891473,1.0,0.003500000000000003,1.1093750000000002,0.05718954248366018,0.10044313146233383,0.09859154929577474,0.39271317829457364
"frozenset({'parental level of education_high school', 'gender_male', 'test preparation course_none'})",frozenset({'lunch_standard'}),0.075,0.645,0.051,0.6799999999999999,1.0542635658914727,1.0,0.0026249999999999954,1.1093749999999998,0.05564387917329084,0.07623318385650224,0.09859154929577443,0.3795348837209302
frozenset({'lunch_standard'}),"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'math_cat_μέτριο'})",0.645,0.1,0.068,0.10542635658914729,1.0542635658914727,1.0,0.003500000000000003,1.0060658578856154,0.14498757249378635,0.10044313146233383,0.006029285099052533,0.39271317829457364
"frozenset({'gender_male', 'reading_cat_χαμηλό'})","frozenset({'test preparation course_none', 'lunch_standard'})",0.177,0.418,0.078,0.44067796610169496,1.054253507420323,1.0,0.004014000000000004,1.0405454545454547,0.06252920833722783,0.15087040618955513,0.038965577494321266,0.31364041845754603
"frozenset({'parental level of education_some high school', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.118,0.418,0.052,0.4406779661016949,1.0542535074203228,1.0,0.0026760000000000048,1.0405454545454547,0.05834641548927274,0.10743801652892561,0.03896557749432117,0.2825399399886465
"frozenset({'test preparation course_none', 'lunch_standard'})","frozenset({'gender_male', 'reading_cat_χαμηλό'})",0.418,0.177,0.078,0.18660287081339713,1.0542535074203228,1.0,0.004014000000000004,1.011805882352941,0.08842188739095963,0.15087040618955513,0.011668129785415725,0.31364041845754603
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'parental level of education_some high school', 'lunch_standard'})",0.418,0.118,0.052,0.12440191387559808,1.0542535074203228,1.0,0.0026760000000000048,1.007311475409836,0.0884218873909597,0.10743801652892561,0.007258405754646359,0.2825399399886465
"frozenset({'writing_cat_μέτριο', 'lunch_standard'})","frozenset({'test preparation course_completed', 'gender_male'})",0.338,0.174,0.062,0.1834319526627219,1.0542066244984016,1.0,0.0031879999999999964,1.0115507246376814,0.07767274144820184,0.13777777777777778,0.011418828889493823,0.2698768958715908
"frozenset({'test preparation course_completed', 'gender_male'})","frozenset({'writing_cat_μέτριο', 'lunch_standard'})",0.174,0.338,0.062,0.3563218390804598,1.0542066244984016,1.0,0.0031879999999999964,1.0284642857142856,0.062251034913692035,0.13777777777777778,0.027676494079244347,0.2698768958715908
"frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο', 'gender_female'})",frozenset({'test preparation course_none'}),0.099,0.642,0.067,0.6767676767676768,1.0541552597627364,1.0,0.0034420000000000006,1.1075625,0.057017907134692805,0.099406528189911,0.09711641555216978,0.3905645237420938
frozenset({'test preparation course_none'}),"frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο', 'gender_female'})",0.642,0.099,0.067,0.1043613707165109,1.0541552597627364,1.0,0.0034420000000000006,1.0059860869565218,0.1435003752188777,0.099406528189911,0.005950466943963259,0.3905645237420938
"frozenset({'lunch_free/reduced', 'reading_cat_μέτριο'})",frozenset({'gender_female'}),0.163,0.518,0.089,0.5460122699386503,1.0540777411943054,1.0,0.004565999999999987,1.0617027027027026,0.06129434980467947,0.1503378378378378,0.058116742611307566,0.35891347087666103
frozenset({'gender_female'}),"frozenset({'lunch_free/reduced', 'reading_cat_μέτριο'})",0.518,0.163,0.089,0.1718146718146718,1.0540777411943054,1.0,0.004565999999999987,1.0106433566433566,0.10643852860273176,0.1503378378378378,0.010531268595784706,0.35891347087666103
"frozenset({'race/ethnicity_group C', 'gender_male'})",frozenset({'lunch_free/reduced'}),0.139,0.355,0.052,0.3741007194244604,1.0538048434491842,1.0,0.0026549999999999976,1.0305172413793102,0.059300455641919006,0.11764705882352941,0.029613518487535503,0.2602897963319485
frozenset({'lunch_free/reduced'}),"frozenset({'race/ethnicity_group C', 'gender_male'})",0.355,0.139,0.052,0.14647887323943662,1.0538048434491842,1.0,0.0026549999999999976,1.0087623762376239,0.07915921288014305,0.11764705882352941,0.00868626392501348,0.2602897963319485
"frozenset({'test preparation course_none', 'lunch_standard'})","frozenset({'writing_cat_χαμηλό', 'gender_male', 'reading_cat_χαμηλό'})",0.418,0.168,0.074,0.17703349282296652,1.053770790612896,1.0,0.0037760000000000016,1.0109767441860464,0.08767530417014956,0.14453125,0.010857563489142434,0.30875484164957845
"frozenset({'writing_cat_χαμηλό', 'gender_male', 'reading_cat_χαμηλό'})","frozenset({'test preparation course_none', 'lunch_standard'})",0.168,0.418,0.074,0.4404761904761904,1.0537707906128957,1.0,0.0037760000000000016,1.0401702127659576,0.06133056133056137,0.14453125,0.03861888397970863,0.30875484164957845
frozenset({'gender_male'}),frozenset({'race/ethnicity_group D'}),0.482,0.262,0.133,0.27593360995850624,1.0531817173988787,1.0,0.006716,1.019243553008596,0.09748308996429295,0.21767594108019642,0.018880230296078906,0.3917835988723829
frozenset({'race/ethnicity_group D'}),frozenset({'gender_male'}),0.262,0.482,0.133,0.5076335877862596,1.0531817173988787,1.0,0.006716,1.052062015503876,0.06842309024594005,0.21767594108019642,0.04948569070706481,0.3917835988723829
"frozenset({'math_cat_χαμηλό', 'gender_female'})",frozenset({'writing_cat_μέτριο'}),0.205,0.491,0.106,0.5170731707317073,1.0531021807163081,1.0,0.0053450000000000025,1.053989898989899,0.06342707962501487,0.17966101694915254,0.05122430399156648,0.36647955888927525
frozenset({'writing_cat_μέτριο'}),"frozenset({'math_cat_χαμηλό', 'gender_female'})",0.491,0.205,0.106,0.21588594704684316,1.0531021807163081,1.0,0.0053450000000000025,1.0138831168831168,0.09906587092708607,0.17966101694915254,0.013693015153261856,0.36647955888927525
frozenset({'gender_male'}),"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})",0.482,0.199,0.101,0.20954356846473032,1.0529827561041725,1.0,0.005082000000000003,1.0133385826771653,0.0971367407010972,0.17413793103448275,0.013163006822384912,0.3585406284534707
"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})",frozenset({'gender_male'}),0.199,0.482,0.101,0.507537688442211,1.0529827561041722,1.0,0.005082000000000003,1.0518571428571428,0.0628175177068269,0.17413793103448275,0.04930055683824527,0.3585406284534707
"frozenset({'race/ethnicity_group C', 'gender_female', 'lunch_standard'})",frozenset({'writing_cat_μέτριο'}),0.118,0.491,0.061,0.516949152542373,1.0528495978459735,1.0,0.0030620000000000022,1.0537192982456143,0.05691238243931456,0.11131386861313868,0.05098065332489775,0.3205927025440989
frozenset({'writing_cat_μέτριο'}),"frozenset({'race/ethnicity_group C', 'gender_female', 'lunch_standard'})",0.491,0.118,0.061,0.12423625254582485,1.0528495978459733,1.0,0.0030620000000000022,1.007120930232558,0.09861831298914626,0.11131386861313868,0.007070581117715255,0.3205927025440989
"frozenset({'gender_male', 'parental level of education_some college'})",frozenset({'test preparation course_none'}),0.108,0.642,0.073,0.6759259259259259,1.0528441213799469,1.0,0.0036640000000000006,1.1046857142857143,0.05626881258062536,0.10782865583456425,0.09476515621767016,0.3948165455174801
frozenset({'test preparation course_none'}),"frozenset({'gender_male', 'parental level of education_some college'})",0.642,0.108,0.073,0.11370716510903425,1.0528441213799469,1.0,0.0036640000000000006,1.006439367311072,0.14020050508915594,0.10782865583456425,0.006398167162594455,0.3948165455174801
"frozenset({'reading_cat_υψηλό', 'lunch_standard'})","frozenset({'gender_female', 'test preparation course_none'})",0.182,0.334,0.064,0.3516483516483517,1.0528393761926695,1.0,0.0032119999999999996,1.0272203389830508,0.06135391198044009,0.1415929203539823,0.026499026499026513,0.2716325590577088
"frozenset({'gender_female', 'test preparation course_none'})","frozenset({'reading_cat_υψηλό', 'lunch_standard'})",0.334,0.182,0.064,0.19161676646706585,1.0528393761926695,1.0,0.0032119999999999996,1.0118962962962963,0.0753566066066066,0.1415929203539823,0.01175643822379689,0.2716325590577088
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})",frozenset({'parental level of education_high school'}),0.32,0.196,0.066,0.20625000000000002,1.052295918367347,1.0,0.003280000000000005,1.0129133858267718,0.07308377896613202,0.14666666666666667,0.012748756218905484,0.2714923469387755
frozenset({'parental level of education_high school'}),"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})",0.196,0.32,0.066,0.336734693877551,1.0522959183673468,1.0,0.003280000000000005,1.0252307692307692,0.06181215136439026,0.14666666666666667,0.024609843937575,0.2714923469387755
"frozenset({'race/ethnicity_group C', 'gender_female'})",frozenset({'writing_cat_μέτριο'}),0.18,0.491,0.093,0.5166666666666667,1.0522742701968772,1.0,0.004619999999999999,1.053103448275862,0.06058221872541304,0.16089965397923872,0.05042567125081872,0.3530380176510523
frozenset({'writing_cat_μέτριο'}),"frozenset({'race/ethnicity_group C', 'gender_female'})",0.491,0.18,0.093,0.1894093686354379,1.0522742701968772,1.0,0.004619999999999999,1.011608040201005,0.09759807338868115,0.16089965397923872,0.01147483979931451,0.3530380176510523
frozenset({'race/ethnicity_group D'}),"frozenset({'writing_cat_υψηλό', 'reading_cat_υψηλό'})",0.262,0.185,0.051,0.1946564885496183,1.0521972354033422,1.0,0.0025299999999999975,1.0119905213270142,0.06721929964397677,0.12878787878787878,0.01184845220812063,0.23516608211264697
"frozenset({'writing_cat_υψηλό', 'reading_cat_υψηλό'})",frozenset({'race/ethnicity_group D'}),0.185,0.262,0.051,0.27567567567567564,1.052197235403342,1.0,0.0025299999999999975,1.0188805970149253,0.060868519186815774,0.12878787878787878,0.01853072584779895,0.23516608211264697
frozenset({'gender_male'}),frozenset({'race/ethnicity_group E'}),0.482,0.14,0.071,0.14730290456431536,1.052163604030824,1.0,0.0035199999999999954,1.0085644768856448,0.09570939148403924,0.12885662431941922,0.008491749493389935,0.3272228808535862
frozenset({'race/ethnicity_group E'}),frozenset({'gender_male'}),0.14,0.482,0.071,0.507142857142857,1.0521636040308238,1.0,0.0035199999999999954,1.051014492753623,0.05764821487061899,0.12885662431941922,0.04853833425261973,0.3272228808535862
"frozenset({'test preparation course_none', 'race/ethnicity_group D'})",frozenset({'writing_cat_χαμηλό'}),0.18,0.301,0.057,0.3166666666666667,1.0520487264673313,1.0,0.002820000000000003,1.0229268292682927,0.06033376123234922,0.13443396226415094,0.02241297091082506,0.25301771871539314
frozenset({'writing_cat_χαμηλό'}),"frozenset({'test preparation course_none', 'race/ethnicity_group D'})",0.301,0.18,0.057,0.18936877076411962,1.0520487264673313,1.0,0.002820000000000003,1.0115573770491804,0.0707778028762895,0.13443396226415094,0.011425330200145885,0.25301771871539314
"frozenset({""parental level of education_associate's degree""})","frozenset({'test preparation course_completed', 'lunch_standard'})",0.222,0.227,0.053,0.23873873873873874,1.051712505456999,1.0,0.002605999999999997,1.0154201183431952,0.06320027162050729,0.13383838383838384,0.015185949209235104,0.23610945747509623
"frozenset({""parental level of education_associate's degree""})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'gender_female'})",0.222,0.227,0.053,0.23873873873873874,1.051712505456999,1.0,0.002605999999999997,1.0154201183431952,0.06320027162050729,0.13383838383838384,0.015185949209235104,0.23610945747509623
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'gender_female'})","frozenset({""parental level of education_associate's degree""})",0.227,0.222,0.053,0.23348017621145373,1.0517125054569987,1.0,0.002605999999999997,1.014977011494253,0.0636090702726451,0.13383838383838384,0.014756010554567768,0.23610945747509623
"frozenset({'test preparation course_completed', 'lunch_standard'})","frozenset({""parental level of education_associate's degree""})",0.227,0.222,0.053,0.23348017621145373,1.0517125054569987,1.0,0.002605999999999997,1.014977011494253,0.0636090702726451,0.13383838383838384,0.014756010554567768,0.23610945747509623
frozenset({'race/ethnicity_group C'}),"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'gender_female'})",0.319,0.155,0.052,0.16300940438871472,1.0516735767013854,1.0,0.0025549999999999948,1.009569288389513,0.07215068338416342,0.12322274881516587,0.009478585075402042,0.2492466376782283
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'gender_female'})",frozenset({'race/ethnicity_group C'}),0.155,0.319,0.052,0.3354838709677419,1.0516735767013852,1.0,0.0025549999999999948,1.0248058252427186,0.058147473827947084,0.12322274881516587,0.024205390554687074,0.2492466376782283
"frozenset({'race/ethnicity_group C', 'lunch_standard'})","frozenset({'gender_female', 'test preparation course_none'})",0.205,0.334,0.072,0.35121951219512193,1.051555425733898,1.0,0.0035299999999999915,1.0265413533834584,0.06167016072676435,0.15417558886509633,0.02585512341609897,0.28339418723528553
"frozenset({'gender_female', 'test preparation course_none'})","frozenset({'race/ethnicity_group C', 'lunch_standard'})",0.334,0.205,0.072,0.21556886227544908,1.051555425733898,1.0,0.0035299999999999915,1.0134732824427481,0.07361528194861512,0.15417558886509633,0.013294166384212694,0.28339418723528553
frozenset({'test preparation course_none'}),"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'race/ethnicity_group D'})",0.642,0.12,0.081,0.12616822429906543,1.0514018691588787,1.0,0.003960000000000005,1.0070588235294118,0.13656114214773452,0.11894273127753303,0.007009345794392544,0.4005841121495327
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'race/ethnicity_group D'})",frozenset({'test preparation course_none'}),0.12,0.642,0.081,0.675,1.0514018691588785,1.0,0.003960000000000005,1.1015384615384616,0.05555555555555563,0.11894273127753303,0.09217877094972075,0.4005841121495327
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})",frozenset({'test preparation course_none'}),0.283,0.642,0.191,0.6749116607773852,1.051264269123653,1.0,0.009314000000000017,1.1012391304347826,0.06801171256033367,0.26021798365122617,0.0919320133446513,0.48620972446968946
frozenset({'test preparation course_none'}),"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})",0.642,0.283,0.191,0.29750778816199375,1.051264269123653,1.0,0.009314000000000017,1.0206518847006654,0.13621340197139456,0.26021798365122617,0.02023401417293413,0.48620972446968946
frozenset({'lunch_standard'}),"frozenset({'math_cat_μέτριο', 'parental level of education_some college'})",0.645,0.118,0.08,0.12403100775193798,1.0511102351859152,1.0,0.0038900000000000046,1.0068849557522124,0.13697183098591567,0.11713030746705709,0.0068378772697709625,0.40099855472342666
"frozenset({'math_cat_μέτριο', 'parental level of education_some college'})",frozenset({'lunch_standard'}),0.118,0.645,0.08,0.6779661016949153,1.0511102351859152,1.0,0.0038900000000000046,1.1023684210526319,0.05513038548752841,0.11713030746705709,0.09286225829553611,0.40099855472342666
frozenset({'gender_male'}),"frozenset({'math_cat_μέτριο', 'test preparation course_none'})",0.482,0.304,0.154,0.3195020746887967,1.0509936667394628,1.0,0.007472000000000006,1.0227804878048778,0.09366695080980804,0.24367088607594936,0.022273095817236667,0.41304051102860884
"frozenset({'math_cat_μέτριο', 'test preparation course_none'})",frozenset({'gender_male'}),0.304,0.482,0.154,0.506578947368421,1.0509936667394628,1.0,0.007472000000000006,1.0498133333333333,0.06971189729810426,0.24367088607594936,0.04744970534444215,0.41304051102860884
frozenset({'lunch_standard'}),"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο', 'gender_male'})",0.645,0.149,0.101,0.15658914728682172,1.0509338744081995,1.0,0.0048950000000000105,1.008998161764706,0.1365221029145171,0.14574314574314573,0.008917916905783458,0.41722074814005516
"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο', 'gender_male'})",frozenset({'lunch_standard'}),0.149,0.645,0.101,0.6778523489932886,1.0509338744081993,1.0,0.0048950000000000105,1.1019791666666667,0.05695105350723099,0.14574314574314573,0.09254182815010872,0.41722074814005516
"frozenset({'gender_female', 'test preparation course_none'})",frozenset({'writing_cat_υψηλό'}),0.334,0.208,0.073,0.21856287425149698,1.0507830492860433,1.0,0.0035279999999999895,1.0135172413793103,0.07256571640133264,0.1556503198294243,0.013336962438758825,0.2847622063565177
frozenset({'writing_cat_υψηλό'}),"frozenset({'gender_female', 'test preparation course_none'})",0.208,0.334,0.073,0.35096153846153844,1.0507830492860433,1.0,0.0035279999999999895,1.0261333333333331,0.06102117061021153,0.1556503198294243,0.025467775467775403,0.2847622063565177
frozenset({'math_cat_μέτριο'}),"frozenset({'test preparation course_none', 'lunch_standard'})",0.485,0.418,0.213,0.43917525773195876,1.0506585113204756,1.0,0.010270000000000001,1.0377573529411765,0.0936232280413875,0.30869565217391304,0.036383604350444625,0.4743723178611947
"frozenset({'test preparation course_none', 'lunch_standard'})",frozenset({'math_cat_μέτριο'}),0.418,0.485,0.213,0.5095693779904307,1.0506585113204756,1.0,0.010270000000000001,1.0500975609756098,0.08284529629091848,0.30869565217391304,0.047707530078506186,0.4743723178611947
"frozenset({'gender_female', 'test preparation course_none'})","frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})",0.334,0.228,0.08,0.23952095808383234,1.0505305179115454,1.0,0.0038479999999999903,1.0151496062992127,0.07222222222222205,0.16597510373443983,0.014923520833979699,0.29519907553314423
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})","frozenset({'gender_female', 'test preparation course_none'})",0.228,0.334,0.08,0.3508771929824561,1.0505305179115452,1.0,0.0038479999999999903,1.0259999999999998,0.06230569948186512,0.16597510373443983,0.025341130604288446,0.29519907553314423
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'race/ethnicity_group C', 'lunch_standard'})",0.418,0.205,0.09,0.215311004784689,1.0502975843155562,1.0,0.004310000000000008,1.013140243902439,0.08228331424207729,0.16885553470919323,0.012969817339231473,0.32716769751429575
"frozenset({'race/ethnicity_group C', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.205,0.418,0.09,0.43902439024390244,1.0502975843155562,1.0,0.004310000000000008,1.0374782608695652,0.06023759608665281,0.16885553470919323,0.036124381862375356,0.32716769751429575
"frozenset({'reading_cat_μέτριο', 'test preparation course_none'})",frozenset({'gender_female'}),0.318,0.518,0.173,0.5440251572327044,1.050241616279352,1.0,0.008275999999999978,1.0570758620689655,0.07014391537979064,0.2609351432880844,0.053994102142540204,0.43900099560476913
frozenset({'gender_female'}),"frozenset({'reading_cat_μέτριο', 'test preparation course_none'})",0.518,0.318,0.173,0.3339768339768339,1.050241616279352,1.0,0.008275999999999978,1.0239884057971012,0.09924927445854195,0.2609351432880844,0.023426442781281403,0.43900099560476913
"frozenset({'race/ethnicity_group C', 'lunch_free/reduced'})",frozenset({'gender_female'}),0.114,0.518,0.062,0.543859649122807,1.0499221025536816,1.0,0.0029479999999999923,1.0566923076923078,0.05366635112502716,0.1087719298245614,0.05365072432117641,0.33177538440696336
frozenset({'gender_female'}),"frozenset({'race/ethnicity_group C', 'lunch_free/reduced'})",0.518,0.114,0.062,0.11969111969111969,1.0499221025536813,1.0,0.0029479999999999923,1.0064649122807017,0.09864810600990472,0.1087719298245614,0.006423385655891291,0.33177538440696336
"frozenset({'gender_female', 'test preparation course_none', 'lunch_standard'})",frozenset({'race/ethnicity_group C'}),0.215,0.319,0.072,0.33488372093023255,1.0497922286214187,1.0,0.0034149999999999875,1.023881118881119,0.06042108987968838,0.1558441558441558,0.023324112966567615,0.28029452504191876
frozenset({'race/ethnicity_group C'}),"frozenset({'gender_female', 'test preparation course_none', 'lunch_standard'})",0.319,0.215,0.072,0.225705329153605,1.0497922286214185,1.0,0.0034149999999999875,1.013825910931174,0.0696483928862781,0.1558441558441558,0.013637361979114653,0.28029452504191876
frozenset({'writing_cat_μέτριο'}),"frozenset({'test preparation course_completed', 'lunch_standard'})",0.491,0.227,0.117,0.23828920570264767,1.0497321837121043,1.0,0.005543000000000006,1.0148208556149734,0.09307675515927,0.19467554076539104,0.014604405824899946,0.3768538539526454
"frozenset({'test preparation course_completed', 'lunch_standard'})",frozenset({'writing_cat_μέτριο'}),0.227,0.491,0.117,0.5154185022026432,1.0497321837121043,1.0,0.005543000000000006,1.0503909090909092,0.061288574872016076,0.19467554076539104,0.047973481734073065,0.3768538539526454
frozenset({'test preparation course_none'}),"frozenset({'race/ethnicity_group C', 'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.642,0.092,0.062,0.09657320872274143,1.0497087904645808,1.0,0.002936000000000001,1.0050620689655172,0.13227608578122188,0.09226190476190477,0.005036573483195409,0.38524312610050115
"frozenset({'race/ethnicity_group C', 'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο'})",frozenset({'test preparation course_none'}),0.092,0.642,0.062,0.6739130434782609,1.0497087904645808,1.0,0.002936000000000001,1.0978666666666665,0.05215290606792669,0.09226190476190477,0.08914257954821467,0.38524312610050115
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})",frozenset({'lunch_standard'}),0.418,0.645,0.283,0.6770334928229664,1.0496643299580874,1.0,0.013389999999999957,1.0991851851851848,0.08129637050259222,0.36282051282051275,0.09023519105060962,0.5578965913727235
frozenset({'lunch_standard'}),"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.645,0.418,0.283,0.43875968992248054,1.0496643299580874,1.0,0.013389999999999957,1.0369889502762433,0.13328024685213716,0.36282051282051275,0.03566957031354048,0.5578965913727235
frozenset({'race/ethnicity_group C'}),"frozenset({'writing_cat_χαμηλό', 'math_cat_χαμηλό'})",0.319,0.23,0.077,0.24137931034482757,1.0494752623688155,1.0,0.0036299999999999943,1.0150000000000001,0.06922592825676514,0.16313559322033896,0.014778325123152674,0.2880809595202398
"frozenset({'writing_cat_χαμηλό', 'math_cat_χαμηλό'})",frozenset({'race/ethnicity_group C'}),0.23,0.319,0.077,0.33478260869565213,1.0494752623688155,1.0,0.0036299999999999943,1.0237254901960784,0.06122448979591827,0.16313559322033896,0.023175636851177857,0.2880809595202398
"frozenset({'reading_cat_μέτριο', 'gender_male'})",frozenset({'lunch_standard'}),0.229,0.645,0.155,0.6768558951965065,1.0493889848007851,1.0,0.007294999999999996,1.0985810810810808,0.061043470984477606,0.21557719054242003,0.08973491604649703,0.45858298635794315
frozenset({'lunch_standard'}),"frozenset({'reading_cat_μέτριο', 'gender_male'})",0.645,0.229,0.155,0.24031007751937983,1.0493889848007851,1.0,0.007294999999999996,1.0148877551020408,0.13257610177192178,0.21557719054242003,0.0146693612443318,0.45858298635794315
"frozenset({'test preparation course_none', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'parental level of education_some college'})",0.418,0.114,0.05,0.11961722488038279,1.0492739024594981,1.0,0.002348000000000003,1.0063804347826089,0.08068728522336778,0.10373443983402489,0.006339982934969283,0.2791068580542265
"frozenset({'writing_cat_μέτριο', 'parental level of education_some college'})","frozenset({'test preparation course_none', 'lunch_standard'})",0.114,0.418,0.05,0.4385964912280702,1.0492739024594981,1.0,0.002348000000000003,1.0366875000000002,0.05300225733634317,0.10373443983402489,0.03538916018568766,0.2791068580542265
"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'test preparation course_none'})",frozenset({'lunch_standard'}),0.099,0.645,0.067,0.6767676767676768,1.0492522120429097,1.0,0.003144999999999995,1.09828125,0.052098000563221544,0.09896602658788774,0.08948641343007543,0.3903218228799624
frozenset({'lunch_standard'}),"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'test preparation course_none'})",0.645,0.099,0.067,0.10387596899224806,1.0492522120429097,1.0,0.003144999999999995,1.0054411764705882,0.13222619297876792,0.09896602658788774,0.005411730291063325,0.3903218228799624
"frozenset({""parental level of education_associate's degree"", 'test preparation course_none', 'lunch_standard'})",frozenset({'gender_female'}),0.092,0.518,0.05,0.5434782608695653,1.0491858317945275,1.0,0.002343999999999999,1.055809523809524,0.051629955947136534,0.0892857142857143,0.052859462384990216,0.3200016786973309
frozenset({'gender_female'}),"frozenset({""parental level of education_associate's degree"", 'test preparation course_none', 'lunch_standard'})",0.518,0.092,0.05,0.09652509652509653,1.0491858317945275,1.0,0.002343999999999999,1.005008547008547,0.0972614107883817,0.0892857142857143,0.0049835864813838416,0.3200016786973309
frozenset({'test preparation course_none'}),"frozenset({'reading_cat_μέτριο', 'math_cat_μέτριο', 'gender_female'})",0.642,0.15,0.101,0.15732087227414332,1.0488058151609556,1.0,0.00470000000000001,1.008687615526802,0.12998506554566097,0.14616497829232997,0.008612790910756854,0.4153271028037384
"frozenset({'reading_cat_μέτριο', 'math_cat_μέτριο', 'gender_female'})",frozenset({'test preparation course_none'}),0.15,0.642,0.101,0.6733333333333335,1.0488058151609556,1.0,0.00470000000000001,1.095918367346939,0.054746651135701915,0.14616497829232997,0.08752327746741183,0.4153271028037384
frozenset({'lunch_standard'}),"frozenset({'parental level of education_high school', 'gender_male'})",0.645,0.102,0.069,0.10697674418604652,1.0487916096671228,1.0,0.0032100000000000045,1.0055729166666667,0.13104715248009816,0.1017699115044248,0.005542031387579655,0.3917236662106704
"frozenset({'parental level of education_high school', 'gender_male'})",frozenset({'lunch_standard'}),0.102,0.645,0.069,0.6764705882352943,1.0487916096671228,1.0,0.0032100000000000045,1.0972727272727276,0.051805945579548825,0.1017699115044248,0.08864954432477254,0.3917236662106704
frozenset({'lunch_standard'}),"frozenset({'reading_cat_μέτριο', 'race/ethnicity_group D'})",0.645,0.136,0.092,0.14263565891472868,1.0487916096671226,1.0,0.004279999999999992,1.0077396021699818,0.13104715248009777,0.13352685050798258,0.007680160780935961,0.4095531235750114
"frozenset({'reading_cat_μέτριο', 'race/ethnicity_group D'})",frozenset({'lunch_standard'}),0.136,0.645,0.092,0.676470588235294,1.0487916096671226,1.0,0.004279999999999992,1.097272727272727,0.05384460547504016,0.13352685050798258,0.08864954432477191,0.4095531235750114
"frozenset({'writing_cat_μέτριο', 'test preparation course_none'})",frozenset({'parental level of education_some college'}),0.308,0.226,0.073,0.237012987012987,1.048730031030916,1.0,0.0033919999999999922,1.0144340425531915,0.06714704252118127,0.15835140997830802,0.014228665391456076,0.28001091828525454
"frozenset({'gender_male', 'test preparation course_none'})",frozenset({'parental level of education_some college'}),0.308,0.226,0.073,0.237012987012987,1.048730031030916,1.0,0.0033919999999999922,1.0144340425531915,0.06714704252118127,0.15835140997830802,0.014228665391456076,0.28001091828525454
frozenset({'parental level of education_some college'}),"frozenset({'gender_male', 'test preparation course_none'})",0.226,0.308,0.073,0.32300884955752207,1.0487300310309158,1.0,0.0033919999999999922,1.0221699346405226,0.06003327315847213,0.15835140997830802,0.021689088955956757,0.28001091828525454
frozenset({'parental level of education_some college'}),"frozenset({'writing_cat_μέτριο', 'test preparation course_none'})",0.226,0.308,0.073,0.32300884955752207,1.0487300310309158,1.0,0.0033919999999999922,1.0221699346405226,0.06003327315847213,0.15835140997830802,0.021689088955956757,0.28001091828525454
"frozenset({'parental level of education_some college', 'test preparation course_none'})","frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})",0.149,0.32,0.05,0.33557046979865773,1.0486577181208054,1.0,0.0023200000000000026,1.0234343434343434,0.05452408930669806,0.11933174224343676,0.022897749703908423,0.24591023489932887
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})","frozenset({'parental level of education_some college', 'test preparation course_none'})",0.32,0.149,0.05,0.15625,1.0486577181208054,1.0,0.0023200000000000026,1.0085925925925925,0.06823529411764714,0.11933174224343676,0.008519388954171571,0.24591023489932887
"frozenset({'writing_cat_μέτριο', 'race/ethnicity_group D'})",frozenset({'lunch_standard'}),0.139,0.645,0.094,0.6762589928057553,1.048463554737605,1.0,0.004344999999999988,1.0965555555555553,0.053685719228012796,0.13623188405797101,0.08805350086128248,0.4109977134571412
frozenset({'lunch_standard'}),"frozenset({'writing_cat_μέτριο', 'race/ethnicity_group D'})",0.645,0.139,0.094,0.14573643410852713,1.048463554737605,1.0,0.004344999999999988,1.0078856624319419,0.13020677255019444,0.13623188405797101,0.00782396528284218,0.4109977134571412
"frozenset({'race/ethnicity_group C', 'gender_female', 'lunch_standard'})",frozenset({'math_cat_μέτριο'}),0.118,0.485,0.06,0.5084745762711864,1.0484011881880133,1.0,0.0027700000000000016,1.0477586206896552,0.05234315948601666,0.11049723756906078,0.04558170149744938,0.316092958238686
frozenset({'math_cat_μέτριο'}),"frozenset({'race/ethnicity_group C', 'gender_female', 'lunch_standard'})",0.485,0.118,0.06,0.12371134020618557,1.0484011881880133,1.0,0.0027700000000000016,1.0065176470588235,0.08964401294498386,0.11049723756906078,0.006475442410641245,0.316092958238686
frozenset({'gender_female'}),"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.518,0.418,0.227,0.43822393822393824,1.0483826273299959,1.0,0.010476000000000013,1.0360000000000003,0.09574643098689394,0.32016925246826516,0.0347490347490348,0.490643069590438
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})",frozenset({'gender_female'}),0.418,0.518,0.227,0.5430622009569378,1.0483826273299957,1.0,0.010476000000000013,1.054848167539267,0.07929515418502212,0.32016925246826516,0.05199626754551408,0.490643069590438
"frozenset({'gender_male', 'parental level of education_some college'})",frozenset({'lunch_standard'}),0.108,0.645,0.073,0.6759259259259259,1.0479471719781797,1.0,0.0033399999999999958,1.0954285714285714,0.05129307697032981,0.10735294117647057,0.08711528429838286,0.39455211024978465
frozenset({'lunch_standard'}),"frozenset({'gender_male', 'parental level of education_some college'})",0.645,0.108,0.073,0.1131782945736434,1.0479471719781797,1.0,0.0033399999999999958,1.0058391608391608,0.12888288635925124,0.10735294117647057,0.005805262974936543,0.39455211024978465
"frozenset({""parental level of education_associate's degree"", 'gender_female', 'test preparation course_none'})",frozenset({'lunch_standard'}),0.074,0.645,0.05,0.6756756756756758,1.047559187094071,1.0,0.002270000000000001,1.0945833333333337,0.049028077753779715,0.07473841554559045,0.08641035401598804,0.3765975277603185
frozenset({'lunch_standard'}),"frozenset({""parental level of education_associate's degree"", 'gender_female', 'test preparation course_none'})",0.645,0.074,0.05,0.07751937984496124,1.0475591870940708,1.0,0.002270000000000001,1.0038151260504202,0.12788732394366206,0.07473841554559045,0.0038006261824635452,0.3765975277603185
"frozenset({""parental level of education_associate's degree""})","frozenset({'gender_female', 'test preparation course_none', 'lunch_standard'})",0.222,0.215,0.05,0.22522522522522523,1.0475591870940708,1.0,0.002270000000000001,1.0131976744186046,0.05835475578406172,0.12919896640826875,0.013025764618121313,0.22889168238005447
"frozenset({'gender_female', 'test preparation course_none', 'lunch_standard'})","frozenset({""parental level of education_associate's degree""})",0.215,0.222,0.05,0.23255813953488375,1.0475591870940708,1.0,0.002270000000000001,1.0137575757575759,0.057834394904458616,0.12919896640826875,0.01357087343815391,0.22889168238005447
"frozenset({'reading_cat_μέτριο', 'math_cat_μέτριο'})","frozenset({'gender_female', 'lunch_standard'})",0.325,0.329,0.112,0.3446153846153846,1.0474631751227494,1.0,0.005074999999999996,1.0238262910798122,0.06712962962962958,0.20664206642066418,0.023271810157056014,0.3425204582651391
"frozenset({'gender_female', 'lunch_standard'})","frozenset({'reading_cat_μέτριο', 'math_cat_μέτριο'})",0.329,0.325,0.112,0.3404255319148936,1.0474631751227494,1.0,0.005074999999999996,1.0233870967741936,0.0675298062593144,0.20664206642066418,0.02285263987391644,0.3425204582651391
"frozenset({'writing_cat_χαμηλό', 'gender_male'})","frozenset({'test preparation course_none', 'lunch_standard'})",0.201,0.418,0.088,0.4378109452736318,1.0473946059177794,1.0,0.003981999999999999,1.0352389380530975,0.056633291614518154,0.16572504708097927,0.03403942486878317,0.32416863053155276
"frozenset({'test preparation course_none', 'lunch_standard'})","frozenset({'writing_cat_χαμηλό', 'gender_male'})",0.418,0.201,0.088,0.21052631578947367,1.0473946059177794,1.0,0.003981999999999999,1.0120666666666667,0.07774914089347078,0.16572504708097927,0.01192279823463537,0.32416863053155276
"frozenset({'race/ethnicity_group C', 'math_cat_μέτριο'})",frozenset({'gender_female'}),0.153,0.518,0.083,0.542483660130719,1.0472657531481062,1.0,0.0037459999999999993,1.0535142857142858,0.053285159528313954,0.141156462585034,0.05079597537493571,0.35135766018118963
frozenset({'gender_female'}),"frozenset({'race/ethnicity_group C', 'math_cat_μέτριο'})",0.518,0.153,0.083,0.16023166023166024,1.0472657531481062,1.0,0.0037459999999999993,1.0086114942528734,0.09363595460680896,0.141156462585034,0.008537969576930635,0.35135766018118963
frozenset({'parental level of education_some college'}),"frozenset({'lunch_free/reduced', 'test preparation course_none'})",0.226,0.224,0.053,0.23451327433628316,1.0469342604298355,1.0,0.002375999999999996,1.0137341040462429,0.05792014041246151,0.13350125944584382,0.013548033938509223,0.235560208596713
"frozenset({'lunch_free/reduced', 'test preparation course_none'})",frozenset({'parental level of education_some college'}),0.224,0.226,0.053,0.23660714285714285,1.0469342604298355,1.0,0.002375999999999996,1.0138947368421052,0.05777086170005826,0.13350125944584382,0.013704318936877057,0.235560208596713
"frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο', 'test preparation course_none'})",frozenset({'lunch_standard'}),0.117,0.645,0.079,0.6752136752136751,1.0468429073080234,1.0,0.0035349999999999965,1.0930263157894735,0.05067591782903503,0.11566617862371888,0.08510894426387361,0.398847147684357
frozenset({'lunch_standard'}),"frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο', 'test preparation course_none'})",0.645,0.117,0.079,0.12248062015503876,1.0468429073080234,1.0,0.0035349999999999965,1.0062455830388692,0.12604742378320544,0.11566617862371888,0.006206817842625992,0.398847147684357
"frozenset({'test preparation course_completed', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'gender_male'})",0.227,0.24,0.057,0.2511013215859031,1.0462555066079295,1.0,0.0025200000000000014,1.0148235294117647,0.057193436372302064,0.13902439024390245,0.014607002086714607,0.24430066079295154
"frozenset({'math_cat_μέτριο', 'gender_male'})","frozenset({'test preparation course_completed', 'lunch_standard'})",0.24,0.227,0.057,0.23750000000000002,1.0462555066079295,1.0,0.0025200000000000014,1.0137704918032788,0.0581717451523546,0.13902439024390245,0.013583441138421744,0.24430066079295154
frozenset({'math_cat_μέτριο'}),"frozenset({'gender_male', 'test preparation course_none', 'lunch_standard'})",0.485,0.203,0.103,0.21237113402061855,1.046163221776446,1.0,0.0045449999999999935,1.0118979057591622,0.08568196814025815,0.17606837606837608,0.011758010063511341,0.35988014829109743
"frozenset({'gender_male', 'test preparation course_none', 'lunch_standard'})",frozenset({'math_cat_μέτριο'}),0.203,0.485,0.103,0.5073891625615763,1.046163221776446,1.0,0.0045449999999999935,1.04545,0.05536538719226218,0.17606837606837608,0.04347410206131321,0.35988014829109743
"frozenset({""parental level of education_associate's degree""})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.222,0.418,0.097,0.43693693693693697,1.0453036768826243,1.0,0.004203999999999999,1.033632,0.05570720589404499,0.17863720073664824,0.03253769233150685,0.33449717660243977
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({""parental level of education_associate's degree""})",0.418,0.222,0.097,0.2320574162679426,1.0453036768826243,1.0,0.004203999999999999,1.013096573208723,0.0744677082226237,0.17863720073664824,0.012927270267278404,0.33449717660243977
"frozenset({'writing_cat_μέτριο', 'gender_female', 'test preparation course_none'})",frozenset({'math_cat_χαμηλό'}),0.175,0.339,0.062,0.3542857142857143,1.0450906026127265,1.0,0.002674999999999997,1.0236725663716815,0.05229716520039095,0.13716814159292035,0.023125135076723587,0.26858828487147074
frozenset({'math_cat_χαμηλό'}),"frozenset({'writing_cat_μέτριο', 'gender_female', 'test preparation course_none'})",0.339,0.175,0.062,0.18289085545722714,1.0450906026127265,1.0,0.002674999999999997,1.0096570397111913,0.06527255868430033,0.13716814159292035,0.009564673281487452,0.26858828487147074
"frozenset({'lunch_free/reduced', 'parental level of education_some college'})",frozenset({'test preparation course_none'}),0.079,0.642,0.053,0.6708860759493671,1.0449938877715998,1.0,0.0022819999999999993,1.0877692307692308,0.04674984123081964,0.07934131736526946,0.08068736298705893,0.3767202965416617
frozenset({'test preparation course_none'}),"frozenset({'lunch_free/reduced', 'parental level of education_some college'})",0.642,0.079,0.053,0.08255451713395638,1.0449938877715998,1.0,0.0022819999999999993,1.003874363327674,0.12026984294297458,0.07934131736526946,0.003859410568899436,0.3767202965416617
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'test preparation course_completed', 'gender_male'})",0.418,0.174,0.076,0.18181818181818182,1.0449320794148382,1.0,0.003268000000000007,1.0095555555555558,0.07388316151202763,0.14728682170542634,0.009465111160026435,0.30929989550679204
"frozenset({'test preparation course_completed', 'gender_male'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.174,0.418,0.076,0.4367816091954023,1.0449320794148382,1.0,0.003268000000000007,1.0333469387755103,0.05205811138014539,0.14728682170542634,0.03227080617766724,0.30929989550679204
"frozenset({'race/ethnicity_group C', 'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο'})",frozenset({'lunch_standard'}),0.092,0.645,0.062,0.6739130434782609,1.0448264239973037,1.0,0.0026599999999999957,1.0886666666666667,0.047250248685519323,0.09185185185185185,0.08144519289650944,0.3850185372430064
frozenset({'lunch_standard'}),"frozenset({'race/ethnicity_group C', 'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.645,0.092,0.062,0.09612403100775194,1.0448264239973037,1.0,0.0026599999999999957,1.0045626072041167,0.12085415720127196,0.09185185185185185,0.004541884369770857,0.3850185372430064
frozenset({'reading_cat_υψηλό'}),"frozenset({'gender_female', 'test preparation course_none'})",0.235,0.334,0.082,0.348936170212766,1.0447190724933113,1.0,0.0035099999999999992,1.0229411764705882,0.05595408895265421,0.16837782340862426,0.02242668200115015,0.2972225761243471
"frozenset({'gender_female', 'test preparation course_none'})",frozenset({'reading_cat_υψηλό'}),0.334,0.235,0.082,0.24550898203592814,1.0447190724933113,1.0,0.0035099999999999992,1.0139285714285715,0.06427158866183255,0.16837782340862426,0.013737231419513921,0.2972225761243471
frozenset({'lunch_standard'}),"frozenset({'gender_male', 'test preparation course_none', 'race/ethnicity_group D'})",0.645,0.095,0.064,0.09922480620155039,1.0444716442268462,1.0,0.0027249999999999983,1.0046901893287437,0.11993838028169006,0.09467455621301776,0.004668294145359541,0.3864545083639331
"frozenset({'gender_male', 'test preparation course_none', 'race/ethnicity_group D'})",frozenset({'lunch_standard'}),0.095,0.645,0.064,0.6736842105263158,1.0444716442268462,1.0,0.0027249999999999983,1.0879032258064516,0.047047651933701626,0.09467455621301776,0.08080059303187538,0.3864545083639331
"frozenset({'gender_male', 'lunch_standard'})",frozenset({'math_cat_μέτριο'}),0.316,0.485,0.16,0.5063291139240507,1.0439775544825787,1.0,0.006739999999999996,1.0432051282051282,0.06158625730994149,0.249609984399376,0.04141575519233141,0.4181130105702727
frozenset({'math_cat_μέτριο'}),"frozenset({'gender_male', 'lunch_standard'})",0.485,0.316,0.16,0.32989690721649484,1.0439775544825787,1.0,0.006739999999999996,1.0207384615384614,0.08179611650485431,0.249609984399376,0.020317115813588943,0.4181130105702727
frozenset({'test preparation course_none'}),"frozenset({'math_cat_μέτριο', 'race/ethnicity_group D', 'lunch_standard'})",0.642,0.097,0.065,0.10124610591900311,1.0437742878247742,1.0,0.0027259999999999993,1.0047244367417678,0.11714654061022774,0.09643916913946589,0.004702221394244861,0.3856745993512541
"frozenset({'math_cat_μέτριο', 'race/ethnicity_group D', 'lunch_standard'})",frozenset({'test preparation course_none'}),0.097,0.642,0.065,0.6701030927835051,1.0437742878247742,1.0,0.0027259999999999993,1.0851874999999997,0.04644347900161852,0.09643916913946589,0.07850025917180194,0.3856745993512541
frozenset({'lunch_standard'}),"frozenset({'race/ethnicity_group C', 'math_cat_μέτριο'})",0.645,0.153,0.103,0.15968992248062014,1.043724983533465,1.0,0.004314999999999999,1.007961254612546,0.11800902502392999,0.14820143884892084,0.007898373648902175,0.41644626842985255
"frozenset({'race/ethnicity_group C', 'math_cat_μέτριο'})",frozenset({'lunch_standard'}),0.153,0.645,0.103,0.6732026143790849,1.043724983533465,1.0,0.004314999999999999,1.0862999999999998,0.04946068935477584,0.14820143884892084,0.07944398416643632,0.41644626842985255
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'parental level of education_some college', 'test preparation course_none'})",0.418,0.149,0.065,0.15550239234449761,1.0436402170771653,1.0,0.002718000000000005,1.007699716713881,0.07184773988897712,0.1294820717131474,0.007640884071090036,0.29587200154137633
"frozenset({'parental level of education_some college', 'test preparation course_none'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.149,0.418,0.065,0.43624161073825507,1.0436402170771653,1.0,0.002718000000000005,1.032357142857143,0.04913676218024053,0.1294820717131474,0.03134297377707059,0.29587200154137633
"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'math_cat_μέτριο'})",frozenset({'test preparation course_none'}),0.1,0.642,0.067,0.67,1.043613707165109,1.0,0.002799999999999997,1.084848484848485,0.04643449419568817,0.09925925925925926,0.07821229050279337,0.38718068535825545
frozenset({'test preparation course_none'}),"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'math_cat_μέτριο'})",0.642,0.1,0.067,0.1043613707165109,1.043613707165109,1.0,0.002799999999999997,1.0048695652173913,0.11673476194446748,0.09925925925925926,0.004845967462789884,0.38718068535825545
"frozenset({'writing_cat_μέτριο', 'gender_male'})","frozenset({'test preparation course_none', 'lunch_standard'})",0.227,0.418,0.099,0.43612334801762115,1.0433572919081846,1.0,0.0041140000000000065,1.0321406250000003,0.053758804082219426,0.1813186813186813,0.031139773226153203,0.33648272664038953
"frozenset({'test preparation course_none', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'gender_male'})",0.418,0.227,0.099,0.2368421052631579,1.0433572919081846,1.0,0.0041140000000000065,1.012896551724138,0.07140129820542201,0.1813186813186813,0.012732348335262485,0.33648272664038953
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none', 'gender_male'})",frozenset({'lunch_standard'}),0.107,0.645,0.072,0.6728971962616822,1.0432514670723756,1.0,0.0029849999999999877,1.0852857142857142,0.04642590518850299,0.10588235294117646,0.07858365144135829,0.3922625516192132
frozenset({'lunch_standard'}),"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none', 'gender_male'})",0.645,0.107,0.072,0.11162790697674417,1.0432514670723754,1.0,0.0029849999999999877,1.0052094240837697,0.11678403755868498,0.10588235294117646,0.005182426625693362,0.3922625516192132
"frozenset({""parental level of education_associate's degree"", 'reading_cat_μέτριο'})",frozenset({'test preparation course_none'}),0.112,0.642,0.075,0.6696428571428571,1.0430574098798397,1.0,0.0030959999999999877,1.0836756756756756,0.0464864864864863,0.11045655375552281,0.07721468475658402,0.39323264352469955
frozenset({'test preparation course_none'}),"frozenset({""parental level of education_associate's degree"", 'reading_cat_μέτριο'})",0.642,0.112,0.075,0.11682242990654206,1.0430574098798397,1.0,0.0030959999999999877,1.0054603174603174,0.11530726256983195,0.11045655375552281,0.005430664309168979,0.39323264352469955
"frozenset({""parental level of education_associate's degree""})","frozenset({'reading_cat_μέτριο', 'gender_male'})",0.222,0.229,0.053,0.23873873873873874,1.0425272434006059,1.0,0.0021619999999999973,1.012792899408284,0.05243245865062806,0.13316582914572864,0.012631308351152701,0.2350898933868366
"frozenset({'reading_cat_μέτριο', 'gender_male'})","frozenset({""parental level of education_associate's degree""})",0.229,0.222,0.053,0.23144104803493448,1.0425272434006057,1.0,0.0021619999999999973,1.0122840909090909,0.05290849913124336,0.13316582914572864,0.012135023181149717,0.2350898933868366
frozenset({'test preparation course_completed'}),frozenset({'math_cat_μέτριο'}),0.358,0.485,0.181,0.505586592178771,1.042446581811899,1.0,0.007370000000000015,1.041638418079096,0.06342403745202334,0.27341389728096677,0.03997396539567187,0.4393912342337154
frozenset({'math_cat_μέτριο'}),frozenset({'test preparation course_completed'}),0.485,0.358,0.181,0.3731958762886598,1.0424465818118989,1.0,0.007370000000000015,1.0242434210526317,0.07906452824116306,0.27341389728096677,0.02366958923467261,0.4393912342337154
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})",frozenset({'race/ethnicity_group C'}),0.418,0.319,0.139,0.3325358851674642,1.0424322419042764,1.0,0.005658000000000024,1.0202795698924734,0.06993992434918074,0.2324414715719064,0.019876483358978236,0.3841362811417258
frozenset({'race/ethnicity_group C'}),"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.319,0.418,0.139,0.4357366771159875,1.0424322419042764,1.0,0.005658000000000024,1.0314333333333336,0.05977244636009278,0.2324414715719064,0.030475390233655548,0.3841362811417258
"frozenset({'reading_cat_μέτριο', 'race/ethnicity_group D'})",frozenset({'test preparation course_none'}),0.136,0.642,0.091,0.6691176470588235,1.0422393256367968,1.0,0.003687999999999983,1.0819555555555553,0.04690679690679669,0.13245997088791847,0.07574761748274712,0.4054310976727139
frozenset({'test preparation course_none'}),"frozenset({'reading_cat_μέτριο', 'race/ethnicity_group D'})",0.642,0.136,0.091,0.14174454828660435,1.0422393256367966,1.0,0.003687999999999983,1.0066932849364791,0.11320523052366577,0.13245997088791847,0.0066487827391253935,0.4054310976727139
"frozenset({'gender_female', 'test preparation course_none'})",frozenset({'race/ethnicity_group C'}),0.334,0.319,0.111,0.3323353293413174,1.0418035402549133,1.0,0.004454,1.0199730941704037,0.06024943862781701,0.2047970479704797,0.019581981411626093,0.3401488558932292
frozenset({'race/ethnicity_group C'}),"frozenset({'gender_female', 'test preparation course_none'})",0.319,0.334,0.111,0.34796238244514105,1.0418035402549133,1.0,0.004454,1.0214134615384614,0.05892235848182984,0.2047970479704797,0.020964538205917465,0.3401488558932292
frozenset({'race/ethnicity_group C'}),"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο'})",0.319,0.325,0.108,0.3385579937304075,1.0417169037858693,1.0,0.0043249999999999955,1.0204976303317537,0.058805134062109035,0.20149253731343283,0.020085916637640745,0.3354328430190499
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο'})",frozenset({'race/ethnicity_group C'}),0.325,0.319,0.108,0.3323076923076923,1.0417169037858693,1.0,0.0043249999999999955,1.019930875576037,0.059327846364883335,0.20149253731343283,0.01954139839602393,0.3354328430190499
frozenset({'writing_cat_μέτριο'}),"frozenset({'race/ethnicity_group C', 'test preparation course_none', 'lunch_standard'})",0.491,0.131,0.067,0.1364562118126273,1.0416504718521167,1.0,0.002679000000000001,1.006318396226415,0.07855613875612118,0.12072072072072074,0.0062787247556125286,0.32395329674600826
"frozenset({'race/ethnicity_group C', 'test preparation course_none', 'lunch_standard'})",frozenset({'writing_cat_μέτριο'}),0.131,0.491,0.067,0.5114503816793893,1.0416504718521167,1.0,0.002679000000000001,1.041859375,0.0460127441045635,0.12072072072072074,0.04017756715007718,0.32395329674600826
"frozenset({'test preparation course_completed', 'math_cat_μέτριο'})","frozenset({'gender_female', 'lunch_standard'})",0.181,0.329,0.062,0.3425414364640884,1.0411593813498128,1.0,0.002451000000000002,1.0205966386554624,0.0482689353657096,0.13839285714285715,0.02018097833694247,0.26549564224420225
"frozenset({'gender_female', 'lunch_standard'})","frozenset({'test preparation course_completed', 'math_cat_μέτριο'})",0.329,0.181,0.062,0.1884498480243161,1.0411593813498128,1.0,0.002451000000000002,1.0091797752808989,0.05891543675784822,0.13839285714285715,0.009096273533963509,0.26549564224420225
frozenset({'writing_cat_μέτριο'}),frozenset({'test preparation course_completed'}),0.491,0.358,0.183,0.3727087576374745,1.0410859151884764,1.0,0.007222000000000006,1.023448051948052,0.07753336124620232,0.2747747747747748,0.02291083744154912,0.4419409709975082
frozenset({'test preparation course_completed'}),frozenset({'writing_cat_μέτριο'}),0.358,0.491,0.183,0.5111731843575419,1.0410859151884764,1.0,0.007222000000000006,1.0412685714285714,0.06147115400983952,0.2747747747747748,0.039632975162164824,0.4419409709975082
"frozenset({'parental level of education_high school', 'test preparation course_none'})",frozenset({'lunch_standard'}),0.14,0.645,0.094,0.6714285714285714,1.0409745293466222,1.0,0.003699999999999995,1.0804347826086955,0.045769421078673864,0.13603473227206944,0.0744466800804827,0.40858250276854924
frozenset({'lunch_standard'}),"frozenset({'parental level of education_high school', 'test preparation course_none'})",0.645,0.14,0.094,0.14573643410852713,1.0409745293466222,1.0,0.003699999999999995,1.006715063520871,0.1108780341624212,0.13603473227206944,0.006670272219217578,0.40858250276854924
frozenset({'reading_cat_μέτριο'}),"frozenset({'parental level of education_some college', 'test preparation course_none'})",0.49,0.149,0.076,0.15510204081632653,1.0409532940693056,1.0,0.0029900000000000065,1.0072222222222222,0.07714138286893721,0.1349911190053286,0.007170435741864321,0.33258457745514314
"frozenset({'parental level of education_some college', 'test preparation course_none'})",frozenset({'reading_cat_μέτριο'}),0.149,0.49,0.076,0.5100671140939598,1.0409532940693056,1.0,0.0029900000000000065,1.0409589041095892,0.046230440967283175,0.1349911190053286,0.039347282537176044,0.33258457745514314
"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο'})","frozenset({'test preparation course_none', 'lunch_standard'})",0.154,0.418,0.067,0.4350649350649351,1.0408252035046295,1.0,0.0026280000000000053,1.0302068965517244,0.04636392505557329,0.1326732673267327,0.029321194269647962,0.297676008202324
"frozenset({'test preparation course_none', 'lunch_standard'})","frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο'})",0.418,0.154,0.067,0.16028708133971292,1.0408252035046295,1.0,0.0026280000000000053,1.0074871794871794,0.06739498384366838,0.1326732673267327,0.007431538226611022,0.297676008202324
frozenset({'test preparation course_completed'}),"frozenset({'race/ethnicity_group C', 'math_cat_μέτριο'})",0.358,0.153,0.057,0.15921787709497207,1.0406397195749808,1.0,0.0022260000000000058,1.0073953488372092,0.06082964420396802,0.12555066079295155,0.007341059144004803,0.2658834483514076
"frozenset({'race/ethnicity_group C', 'math_cat_μέτριο'})",frozenset({'test preparation course_completed'}),0.153,0.358,0.057,0.37254901960784315,1.0406397195749808,1.0,0.0022260000000000058,1.0231875000000001,0.046107003044802204,0.12555066079295155,0.022662024311282183,0.2658834483514076
"frozenset({'race/ethnicity_group C', 'gender_female'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})",0.18,0.283,0.053,0.29444444444444445,1.0404397330192385,1.0,0.0020600000000000063,1.0162204724409452,0.04739990796134391,0.12926829268292683,0.01596156826282354,0.24086179819395367
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})","frozenset({'race/ethnicity_group C', 'gender_female'})",0.283,0.18,0.053,0.1872791519434629,1.0404397330192383,1.0,0.0020600000000000063,1.0089565217391305,0.05420909976053278,0.12926829268292683,0.008877014565198661,0.24086179819395367
frozenset({'math_cat_μέτριο'}),"frozenset({'test preparation course_completed', 'gender_male', 'lunch_standard'})",0.485,0.113,0.057,0.1175257731958763,1.040051090228994,1.0,0.0021950000000000025,1.0051285046728973,0.07477431442684389,0.10536044362292053,0.005102337312149152,0.31097527597846913
"frozenset({'test preparation course_completed', 'gender_male', 'lunch_standard'})",frozenset({'math_cat_μέτριο'}),0.113,0.485,0.057,0.504424778761062,1.0400510902289937,1.0,0.0021950000000000025,1.0391964285714286,0.04341462449811117,0.10536044362292053,0.037718017011770806,0.31097527597846913
frozenset({'reading_cat_μέτριο'}),"frozenset({'gender_female', 'race/ethnicity_group B'})",0.49,0.104,0.053,0.10816326530612244,1.0400313971742543,1.0,0.00204,1.0046681922196796,0.07547169811320756,0.09796672828096119,0.004646501457725944,0.3088893249607535
"frozenset({'gender_female', 'race/ethnicity_group B'})",frozenset({'reading_cat_μέτριο'}),0.104,0.49,0.053,0.5096153846153846,1.0400313971742543,1.0,0.00204,1.04,0.04295822102425876,0.09796672828096119,0.03846153846153841,0.3088893249607535
"frozenset({'gender_female', 'lunch_standard'})",frozenset({'race/ethnicity_group B'}),0.329,0.19,0.065,0.19756838905775076,1.0398336266197408,1.0,0.0024899999999999922,1.0094318181818183,0.0570904505330733,0.14317180616740088,0.00934369019475402,0.2698368261078228
frozenset({'race/ethnicity_group B'}),"frozenset({'gender_female', 'lunch_standard'})",0.19,0.329,0.065,0.34210526315789475,1.0398336266197408,1.0,0.0024899999999999922,1.01992,0.047293447293447144,0.14317180616740088,0.019530943603419865,0.2698368261078228
"frozenset({'writing_cat_μέτριο', 'gender_female'})",frozenset({'lunch_standard'}),0.264,0.645,0.177,0.6704545454545454,1.0394644115574347,1.0,0.006719999999999976,1.0772413793103446,0.0515843773028738,0.24180327868852458,0.0717029449423814,0.4724365750528541
frozenset({'lunch_standard'}),"frozenset({'writing_cat_μέτριο', 'gender_female'})",0.645,0.264,0.177,0.27441860465116275,1.0394644115574345,1.0,0.006719999999999976,1.0143589743589743,0.10694676533778909,0.24180327868852458,0.014155712841253713,0.4724365750528541
frozenset({'writing_cat_μέτριο'}),"frozenset({'parental level of education_some college', 'lunch_standard'})",0.491,0.147,0.075,0.15274949083503056,1.0391121825512284,1.0,0.002823000000000006,1.0067860576923076,0.07394891944990192,0.13321492007104793,0.006740317508828334,0.3314767862338418
"frozenset({'parental level of education_some college', 'lunch_standard'})",frozenset({'writing_cat_μέτριο'}),0.147,0.491,0.075,0.5102040816326531,1.0391121825512282,1.0,0.002823000000000006,1.0392083333333333,0.044126611957796116,0.13321492007104793,0.037729040535664206,0.3314767862338418
frozenset({'lunch_standard'}),"frozenset({""parental level of education_associate's degree"", 'math_cat_μέτριο'})",0.645,0.094,0.063,0.09767441860465116,1.0390895596239484,1.0,0.002369999999999997,1.0040721649484536,0.10596914822266923,0.09319526627218935,0.004055649674007904,0.383943592281049
"frozenset({""parental level of education_associate's degree"", 'math_cat_μέτριο'})",frozenset({'lunch_standard'}),0.094,0.645,0.063,0.6702127659574468,1.0390895596239484,1.0,0.002369999999999997,1.0764516129032258,0.041522127614842794,0.09319526627218935,0.0710218759364699,0.383943592281049
frozenset({'gender_female'}),"frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",0.518,0.223,0.12,0.23166023166023164,1.0388351195526082,1.0,0.00448599999999999,1.0112713567839196,0.07755878284923912,0.1932367149758454,0.01114572929244741,0.3848884117942414
"frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",frozenset({'gender_female'}),0.223,0.518,0.12,0.5381165919282511,1.0388351195526082,1.0,0.00448599999999999,1.0435533980582523,0.048112398112398,0.1932367149758454,0.0417356679009359,0.3848884117942414
"frozenset({'race/ethnicity_group C', 'test preparation course_none', 'lunch_standard'})",frozenset({'math_cat_μέτριο'}),0.131,0.485,0.066,0.5038167938931297,1.0387975131817109,1.0,0.002465000000000009,1.037923076923077,0.04297869372668007,0.12,0.03653746387015482,0.3199496340599669
frozenset({'math_cat_μέτριο'}),"frozenset({'race/ethnicity_group C', 'test preparation course_none', 'lunch_standard'})",0.485,0.131,0.066,0.13608247422680414,1.0387975131817109,1.0,0.002465000000000009,1.0058830548926014,0.07252132980288346,0.12,0.00584864698136264,0.3199496340599669
frozenset({'test preparation course_none'}),"frozenset({'writing_cat_μέτριο', 'parental level of education_some college', 'lunch_standard'})",0.642,0.075,0.05,0.07788161993769471,1.038421599169263,1.0,0.0018500000000000044,1.003125,0.10335195530726282,0.0749625187406297,0.0031152647975078002,0.3722741433021807
"frozenset({'writing_cat_μέτριο', 'parental level of education_some college', 'lunch_standard'})",frozenset({'test preparation course_none'}),0.075,0.642,0.05,0.6666666666666667,1.038421599169263,1.0,0.0018500000000000044,1.0740000000000003,0.04000000000000009,0.0749625187406297,0.06890130353817521,0.3722741433021807
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'lunch_standard'})",frozenset({'test preparation course_none'}),0.225,0.642,0.15,0.6666666666666666,1.0384215991692627,1.0,0.005549999999999999,1.0739999999999998,0.04774193548387096,0.20920502092050208,0.0689013035381749,0.4501557632398754
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})",frozenset({'test preparation course_none'}),0.228,0.642,0.152,0.6666666666666666,1.0384215991692627,1.0,0.00562399999999999,1.0739999999999998,0.04792746113989629,0.2116991643454039,0.0689013035381749,0.4517133956386293
frozenset({'test preparation course_none'}),"frozenset({'parental level of education_high school', 'writing_cat_μέτριο'})",0.642,0.096,0.064,0.09968847352024922,1.0384215991692627,1.0,0.002368000000000002,1.004096885813149,0.10335195530726267,0.09495548961424334,0.0040801698232845315,0.38317757009345793
"frozenset({'parental level of education_high school', 'writing_cat_μέτριο'})",frozenset({'test preparation course_none'}),0.096,0.642,0.064,0.6666666666666666,1.0384215991692627,1.0,0.002368000000000002,1.0739999999999998,0.04092920353982304,0.09495548961424334,0.0689013035381749,0.38317757009345793
frozenset({'test preparation course_none'}),"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'lunch_standard'})",0.642,0.225,0.15,0.2336448598130841,1.0384215991692627,1.0,0.005549999999999999,1.011280487804878,0.10335195530726256,0.20920502092050208,0.011154657823334328,0.4501557632398754
"frozenset({'reading_cat_μέτριο', 'parental level of education_some college'})",frozenset({'test preparation course_none'}),0.114,0.642,0.076,0.6666666666666666,1.0384215991692627,1.0,0.002811999999999995,1.0739999999999998,0.04176072234762973,0.11176470588235293,0.0689013035381749,0.3925233644859813
"frozenset({'parental level of education_high school', 'reading_cat_μέτριο', 'writing_cat_μέτριο'})",frozenset({'test preparation course_none'}),0.081,0.642,0.054,0.6666666666666666,1.0384215991692627,1.0,0.0019979999999999998,1.0739999999999998,0.04026115342763873,0.08071748878923768,0.0689013035381749,0.37538940809968846
frozenset({'test preparation course_none'}),"frozenset({'parental level of education_high school', 'reading_cat_μέτριο', 'writing_cat_μέτριο'})",0.642,0.081,0.054,0.08411214953271028,1.0384215991692627,1.0,0.0019979999999999998,1.0033979591836735,0.10335195530726257,0.08071748878923768,0.003386452157464933,0.37538940809968846
frozenset({'test preparation course_none'}),"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'parental level of education_some college'})",0.642,0.078,0.052,0.08099688473520249,1.0384215991692627,1.0,0.0019239999999999952,1.0032610169491527,0.10335195530726232,0.07784431137724551,0.0032504172832998777,0.37383177570093457
frozenset({'test preparation course_none'}),"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})",0.642,0.228,0.152,0.2367601246105919,1.0384215991692627,1.0,0.00562399999999999,1.0114775510204081,0.10335195530726239,0.2116991643454039,0.011347311671751154,0.4517133956386293
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'parental level of education_some college'})",frozenset({'test preparation course_none'}),0.078,0.642,0.052,0.6666666666666666,1.0384215991692627,1.0,0.0019239999999999952,1.0739999999999998,0.040130151843817685,0.07784431137724551,0.0689013035381749,0.37383177570093457
frozenset({'test preparation course_none'}),"frozenset({'reading_cat_μέτριο', 'parental level of education_some college'})",0.642,0.114,0.076,0.11838006230529595,1.0384215991692627,1.0,0.002811999999999995,1.0049681978798586,0.10335195530726239,0.11176470588235293,0.004943636913426575,0.3925233644859813
"frozenset({""parental level of education_associate's degree""})","frozenset({'writing_cat_μέτριο', 'test preparation course_none'})",0.222,0.308,0.071,0.3198198198198198,1.0383760383760383,1.0,0.0026239999999999875,1.0173774834437084,0.047503530178500085,0.15468409586056642,0.01708066447950835,0.2751696501696501
"frozenset({'writing_cat_μέτριο', 'test preparation course_none'})","frozenset({""parental level of education_associate's degree""})",0.308,0.222,0.071,0.2305194805194805,1.0383760383760383,1.0,0.0026239999999999875,1.011071729957806,0.05340714809085703,0.15468409586056642,0.010950489099589344,0.2751696501696501
frozenset({'math_cat_μέτριο'}),"frozenset({'race/ethnicity_group C', 'gender_male'})",0.485,0.139,0.07,0.1443298969072165,1.0383445820663058,1.0,0.002585000000000004,1.0062289156626507,0.07170596393897374,0.1263537906137184,0.006190356454374561,0.3239635096046874
"frozenset({'race/ethnicity_group C', 'gender_male'})",frozenset({'math_cat_μέτριο'}),0.139,0.485,0.07,0.5035971223021583,1.0383445820663058,1.0,0.002585000000000004,1.037463768115942,0.04289032686245236,0.1263537906137184,0.036110917091569446,0.3239635096046874
frozenset({'reading_cat_μέτριο'}),"frozenset({""parental level of education_associate's degree"", 'gender_female'})",0.49,0.116,0.059,0.12040816326530612,1.0380014074595354,1.0,0.0021599999999999953,1.0050116009280743,0.07178464606181441,0.10786106032906764,0.004986610028626829,0.31451442646023925
"frozenset({""parental level of education_associate's degree"", 'gender_female'})",frozenset({'reading_cat_μέτριο'}),0.116,0.49,0.059,0.5086206896551724,1.0380014074595354,1.0,0.0021599999999999953,1.0378947368421052,0.041414218881816,0.10786106032906764,0.03651115618661252,0.31451442646023925
frozenset({'gender_female'}),frozenset({'writing_cat_μέτριο'}),0.518,0.491,0.264,0.5096525096525096,1.0379888180295511,1.0,0.009662000000000004,1.03803937007874,0.0759304664906325,0.3543624161073826,0.0366454020677989,0.5236653586959086
frozenset({'writing_cat_μέτριο'}),frozenset({'gender_female'}),0.491,0.518,0.264,0.5376782077393075,1.0379888180295511,1.0,0.009662000000000004,1.0425638766519822,0.07190272072393883,0.3543624161073826,0.04082615713549276,0.5236653586959086
frozenset({'reading_cat_μέτριο'}),"frozenset({""parental level of education_bachelor's degree""})",0.49,0.118,0.06,0.12244897959183673,1.0377032168799725,1.0,0.0021800000000000014,1.0050697674418605,0.07124183006535953,0.1094890510948905,0.00504419454856773,0.3154617779315116
frozenset({'reading_cat_μέτριο'}),"frozenset({'parental level of education_some high school', 'lunch_standard'})",0.49,0.118,0.06,0.12244897959183673,1.0377032168799725,1.0,0.0021800000000000014,1.0050697674418605,0.07124183006535953,0.1094890510948905,0.00504419454856773,0.3154617779315116
"frozenset({""parental level of education_bachelor's degree""})",frozenset({'reading_cat_μέτριο'}),0.118,0.49,0.06,0.5084745762711864,1.0377032168799722,1.0,0.0021800000000000014,1.0375862068965518,0.04119425547996979,0.1094890510948905,0.0362246593552675,0.3154617779315116
"frozenset({'parental level of education_some high school', 'lunch_standard'})",frozenset({'reading_cat_μέτριο'}),0.118,0.49,0.06,0.5084745762711864,1.0377032168799722,1.0,0.0021800000000000014,1.0375862068965518,0.04119425547996979,0.1094890510948905,0.0362246593552675,0.3154617779315116
"frozenset({'gender_male', 'lunch_standard'})",frozenset({'writing_cat_μέτριο'}),0.316,0.491,0.161,0.509493670886076,1.0376653174869166,1.0,0.005844000000000016,1.0377032258064518,0.053067451236787765,0.2492260061919505,0.03633334162293911,0.4186979556059708
frozenset({'writing_cat_μέτριο'}),"frozenset({'gender_male', 'lunch_standard'})",0.491,0.316,0.161,0.32790224032586557,1.0376653174869164,1.0,0.005844000000000016,1.017709090909091,0.07131264566986803,0.2492260061919505,0.017400936148926266,0.4186979556059708
frozenset({'lunch_standard'}),"frozenset({'gender_male', 'race/ethnicity_group D'})",0.645,0.133,0.089,0.137984496124031,1.0374774144663983,1.0,0.0032149999999999956,1.0057823741007195,0.10175660705807868,0.12917271407837444,0.0057491304775443884,0.40357871422742897
"frozenset({'gender_male', 'race/ethnicity_group D'})",frozenset({'lunch_standard'}),0.133,0.645,0.089,0.669172932330827,1.0374774144663983,1.0,0.0032149999999999956,1.0730681818181815,0.041665046719282506,0.12917271407837444,0.06809276712909003,0.40357871422742897
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})",frozenset({'parental level of education_some college'}),0.418,0.226,0.098,0.23444976076555027,1.0373883219714612,1.0,0.0035320000000000074,1.0110375,0.0619258012483345,0.1794871794871795,0.010917003573062354,0.3340390396748105
frozenset({'parental level of education_some college'}),"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.226,0.418,0.098,0.4336283185840708,1.0373883219714612,1.0,0.0035320000000000074,1.02759375,0.04656436217898022,0.1794871794871795,0.02685278107228663,0.3340390396748105
"frozenset({'test preparation course_completed', 'reading_cat_μέτριο', 'lunch_standard'})",frozenset({'gender_male'}),0.104,0.482,0.052,0.5,1.037344398340249,1.0,0.0018719999999999987,1.036,0.0401785714285714,0.09737827715355807,0.03474903474903478,0.303941908713693
frozenset({'gender_male'}),"frozenset({'test preparation course_completed', 'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.482,0.152,0.076,0.15767634854771784,1.037344398340249,1.0,0.0027360000000000023,1.0067389162561575,0.06949806949806955,0.13620071684587812,0.0066938072496672656,0.3288381742738589
frozenset({'gender_male'}),"frozenset({'test preparation course_completed', 'reading_cat_μέτριο', 'lunch_standard'})",0.482,0.104,0.052,0.1078838174273859,1.037344398340249,1.0,0.0018719999999999987,1.004353488372093,0.06949806949806946,0.09737827715355807,0.004334617664493195,0.303941908713693
"frozenset({'test preparation course_completed', 'writing_cat_μέτριο', 'reading_cat_μέτριο'})",frozenset({'gender_male'}),0.152,0.482,0.076,0.5,1.037344398340249,1.0,0.0027360000000000023,1.036,0.042452830188679284,0.13620071684587812,0.03474903474903478,0.3288381742738589
frozenset({'lunch_standard'}),"frozenset({'math_cat_μέτριο', 'race/ethnicity_group D'})",0.645,0.145,0.097,0.15038759689922482,1.037155840684309,1.0,0.003475000000000006,1.0063412408759123,0.10091476695222901,0.13997113997113997,0.006301282923069975,0.40967655707030215
"frozenset({'math_cat_μέτριο', 'race/ethnicity_group D'})",frozenset({'lunch_standard'}),0.145,0.645,0.097,0.6689655172413794,1.037155840684309,1.0,0.003475000000000006,1.0723958333333337,0.04190028335443427,0.13997113997113997,0.06750849927149129,0.40967655707030215
"frozenset({'gender_female', 'test preparation course_none'})","frozenset({'lunch_free/reduced', 'writing_cat_μέτριο'})",0.334,0.153,0.053,0.1586826347305389,1.0371414034675746,1.0,0.001897999999999997,1.0067544483985764,0.05377075188395935,0.12211981566820276,0.006709131913269066,0.2525439317443544
"frozenset({'lunch_free/reduced', 'writing_cat_μέτριο'})","frozenset({'gender_female', 'test preparation course_none'})",0.153,0.334,0.053,0.3464052287581699,1.0371414034675746,1.0,0.001897999999999997,1.0189799999999998,0.042280189793054215,0.12211981566820276,0.018626469606861724,0.2525439317443544
"frozenset({'race/ethnicity_group B', 'lunch_standard'})",frozenset({'gender_female'}),0.121,0.518,0.065,0.5371900826446281,1.0370464915919462,1.0,0.0023220000000000046,1.0414642857142857,0.0406405880808612,0.11324041811846688,0.03981344947018272,0.33133635406362677
frozenset({'gender_female'}),"frozenset({'race/ethnicity_group B', 'lunch_standard'})",0.518,0.121,0.065,0.12548262548262548,1.0370464915919462,1.0,0.0023220000000000046,1.0051258278145696,0.07411426747526348,0.11324041811846688,0.005099687693544352,0.33133635406362677
frozenset({'lunch_standard'}),"frozenset({'math_cat_μέτριο', 'test preparation course_none', 'gender_male'})",0.645,0.154,0.103,0.15968992248062014,1.0369475485754556,1.0,0.0036699999999999927,1.006771217712177,0.10036920552440841,0.1479885057471264,0.006725676691040356,0.41426054565589443
"frozenset({'math_cat_μέτριο', 'test preparation course_none', 'gender_male'})",frozenset({'lunch_standard'}),0.154,0.645,0.103,0.6688311688311688,1.0369475485754553,1.0,0.0036699999999999927,1.0719607843137253,0.04211710160894206,0.1479885057471264,0.0671300530455458,0.41426054565589443
frozenset({'parental level of education_some high school'}),frozenset({'math_cat_μέτριο'}),0.179,0.485,0.09,0.5027932960893855,1.0366872084317227,1.0,0.0031850000000000073,1.0357865168539326,0.043104614968196064,0.15679442508710803,0.034550089493952454,0.34418015319933193
frozenset({'math_cat_μέτριο'}),frozenset({'parental level of education_some high school'}),0.485,0.179,0.09,0.18556701030927836,1.0366872084317227,1.0,0.0031850000000000073,1.0080632911392404,0.06871628910463877,0.15679442508710803,0.007998794530180714,0.34418015319933193
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})","frozenset({'race/ethnicity_group C', 'lunch_standard'})",0.32,0.205,0.068,0.21250000000000002,1.0365853658536588,1.0,0.0024000000000000132,1.0095238095238097,0.0519031141868515,0.1487964989059081,0.009433962264150986,0.27210365853658536
"frozenset({'race/ethnicity_group C', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})",0.205,0.32,0.068,0.33170731707317075,1.0365853658536586,1.0,0.0024000000000000132,1.0175182481751825,0.04439511653718115,0.1487964989059081,0.017216642754662857,0.27210365853658536
"frozenset({'race/ethnicity_group C', 'test preparation course_none'})","frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})",0.202,0.32,0.067,0.3316831683168317,1.036509900990099,1.0,0.002360000000000001,1.0174814814814814,0.04414020124939215,0.14725274725274726,0.017181129877693668,0.27052908415841587
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})","frozenset({'race/ethnicity_group C', 'test preparation course_none'})",0.32,0.202,0.067,0.209375,1.0365099009900989,1.0,0.002360000000000001,1.0093280632411068,0.05179982440737491,0.14725274725274726,0.00924185463659147,0.27052908415841587
"frozenset({'lunch_free/reduced', 'race/ethnicity_group D'})",frozenset({'gender_female'}),0.095,0.518,0.051,0.5368421052631579,1.036374720585247,1.0,0.001789999999999993,1.0406818181818183,0.038782363774238826,0.09074733096085409,0.039091504695348306,0.3176488518593782
frozenset({'gender_female'}),"frozenset({'lunch_free/reduced', 'race/ethnicity_group D'})",0.518,0.095,0.051,0.09845559845559845,1.036374720585247,1.0,0.001789999999999993,1.0038329764453962,0.07281750874623681,0.09074733096085409,0.0038183408349154167,0.3176488518593782
frozenset({'test preparation course_none'}),"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'gender_female'})",0.642,0.227,0.151,0.235202492211838,1.0361343269243966,1.0,0.005265999999999993,1.010725050916497,0.0974138887861185,0.21030640668523676,0.01061124477598706,0.45020036504865024
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'gender_female'})",frozenset({'test preparation course_none'}),0.227,0.642,0.151,0.6651982378854625,1.0361343269243963,1.0,0.005265999999999993,1.0692894736842102,0.045115358583997955,0.21030640668523676,0.06479954716609634,0.45020036504865024
"frozenset({'race/ethnicity_group C', 'lunch_standard'})",frozenset({'math_cat_μέτριο'}),0.205,0.485,0.103,0.5024390243902439,1.0359567513200905,1.0,0.0035750000000000087,1.0350490196078432,0.04365878976613554,0.17546848381601363,0.033862183282027024,0.35740507920543124
frozenset({'math_cat_μέτριο'}),"frozenset({'race/ethnicity_group C', 'lunch_standard'})",0.485,0.205,0.103,0.21237113402061855,1.0359567513200905,1.0,0.0035750000000000087,1.0093586387434557,0.0673956075030636,0.17546848381601363,0.009271866692601967,0.35740507920543124
frozenset({'writing_cat_μέτριο'}),"frozenset({""parental level of education_associate's degree"", 'gender_female'})",0.491,0.116,0.059,0.12016293279022403,1.0358873516398623,1.0,0.002043999999999997,1.0047314814814816,0.06806300156504935,0.10766423357664232,0.004709199988941198,0.3143918112226982
"frozenset({""parental level of education_associate's degree"", 'gender_female'})",frozenset({'writing_cat_μέτριο'}),0.116,0.491,0.059,0.5086206896551724,1.0358873516398623,1.0,0.002043999999999997,1.035859649122807,0.039190121941866655,0.10766423357664232,0.03461825079601647,0.3143918112226982
frozenset({'gender_female'}),"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'test preparation course_none'})",0.518,0.205,0.11,0.21235521235521235,1.0358790846595725,1.0,0.003810000000000008,1.0093382352941178,0.07185967559411557,0.17944535073409462,0.00925183944051869,0.37447028910443547
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'test preparation course_none'})",frozenset({'gender_female'}),0.205,0.518,0.11,0.5365853658536586,1.0358790846595725,1.0,0.003810000000000008,1.0401052631578949,0.04356775300171536,0.17944535073409462,0.03855885031879368,0.37447028910443547
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})",frozenset({'parental level of education_high school'}),0.266,0.196,0.054,0.20300751879699247,1.0357526469234308,1.0,0.0018639999999999976,1.0087924528301886,0.04702795438490256,0.1323529411764706,0.008715819399244356,0.23925886143931255
frozenset({'parental level of education_high school'}),"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})",0.196,0.266,0.054,0.2755102040816326,1.0357526469234308,1.0,0.0018639999999999976,1.0131267605633802,0.04293348074442596,0.1323529411764706,0.012956681310126174,0.23925886143931255
"frozenset({'gender_male', 'test preparation course_none', 'lunch_standard'})",frozenset({'reading_cat_μέτριο'}),0.203,0.49,0.103,0.5073891625615763,1.0354880868603598,1.0,0.0035299999999999915,1.0352999999999999,0.04300105979949071,0.17457627118644065,0.03409639717956137,0.35879662209711466
frozenset({'reading_cat_μέτριο'}),"frozenset({'gender_male', 'test preparation course_none', 'lunch_standard'})",0.49,0.203,0.103,0.21020408163265306,1.0354880868603598,1.0,0.0035299999999999915,1.0091214470284235,0.06719969541214528,0.17457627118644065,0.009038998284382739,0.35879662209711466
frozenset({'gender_female'}),"frozenset({'writing_cat_μέτριο', ""parental level of education_associate's degree""})",0.518,0.11,0.059,0.1138996138996139,1.0354510354510353,1.0,0.002019999999999994,1.004400871459695,0.07103171812363718,0.10369068541300527,0.004381588651251565,0.32513162513162513
"frozenset({'writing_cat_μέτριο', ""parental level of education_associate's degree""})",frozenset({'gender_female'}),0.11,0.518,0.059,0.5363636363636364,1.0354510354510353,1.0,0.002019999999999994,1.0396078431372549,0.038468863073700135,0.10369068541300527,0.03809883062995095,0.32513162513162513
"frozenset({'lunch_free/reduced', 'gender_male'})","frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})",0.166,0.32,0.055,0.3313253012048193,1.0353915662650603,1.0,0.0018799999999999997,1.0169369369369368,0.04098539350337911,0.12761020881670534,0.016654854712969534,0.25160015060240964
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})","frozenset({'lunch_free/reduced', 'gender_male'})",0.32,0.166,0.055,0.171875,1.0353915662650601,1.0,0.0018799999999999997,1.0070943396226415,0.05026737967914438,0.12761020881670534,0.007044364508393275,0.25160015060240964
"frozenset({'reading_cat_μέτριο', 'lunch_standard'})",frozenset({'race/ethnicity_group C'}),0.327,0.319,0.108,0.33027522935779813,1.0353455465761696,1.0,0.003686999999999996,1.0168356164383563,0.050726432227175114,0.2007434944237918,0.016556871303668313,0.33441661154410285
frozenset({'race/ethnicity_group C'}),"frozenset({'reading_cat_μέτριο', 'lunch_standard'})",0.319,0.327,0.108,0.3385579937304075,1.0353455465761696,1.0,0.003686999999999996,1.017473933649289,0.050130527002773644,0.2007434944237918,0.017173839123933877,0.33441661154410285
frozenset({'test preparation course_none'}),"frozenset({'reading_cat_μέτριο', 'lunch_standard', 'gender_male'})",0.642,0.155,0.103,0.16043613707165108,1.0350718520751683,1.0,0.003489999999999993,1.0064749536178106,0.09464663448500281,0.14841498559077806,0.00643329830964625,0.4124761330519545
"frozenset({'reading_cat_μέτριο', 'lunch_standard', 'gender_male'})",frozenset({'test preparation course_none'}),0.155,0.642,0.103,0.664516129032258,1.0350718520751683,1.0,0.003489999999999993,1.0671153846153842,0.04009881082323196,0.14841498559077806,0.06289421517390495,0.4124761330519545
frozenset({'parental level of education_high school'}),"frozenset({'reading_cat_μέτριο', 'math_cat_μέτριο', 'writing_cat_μέτριο'})",0.196,0.281,0.057,0.29081632653061223,1.0349335463722855,1.0,0.0019239999999999952,1.013841726618705,0.04198306712053755,0.1357142857142857,0.01365274899946065,0.24683165080979008
"frozenset({'reading_cat_μέτριο', 'math_cat_μέτριο', 'writing_cat_μέτριο'})",frozenset({'parental level of education_high school'}),0.281,0.196,0.057,0.20284697508896796,1.0349335463722855,1.0,0.0019239999999999952,1.0085892857142857,0.046946294805163,0.1357142857142857,0.008516138170358145,0.24683165080979008
frozenset({'lunch_standard'}),frozenset({'reading_cat_μέτριο'}),0.645,0.49,0.327,0.5069767441860465,1.0346464167062175,1.0,0.010950000000000015,1.034433962264151,0.0943274324848173,0.4047029702970297,0.0332877336981305,0.5871618414807784
frozenset({'reading_cat_μέτριο'}),frozenset({'lunch_standard'}),0.49,0.645,0.327,0.6673469387755102,1.0346464167062173,1.0,0.010950000000000015,1.0671779141104294,0.06565929123943165,0.4047029702970297,0.06294912331129633,0.5871618414807784
frozenset({'test preparation course_completed'}),"frozenset({'lunch_free/reduced', 'gender_female'})",0.358,0.189,0.07,0.19553072625698326,1.0345541071798057,1.0,0.0023380000000000067,1.0081180555555553,0.0520249221183802,0.1467505241090147,0.008052683424147047,0.28295054831367683
"frozenset({'lunch_free/reduced', 'gender_female'})",frozenset({'test preparation course_completed'}),0.189,0.358,0.07,0.3703703703703704,1.0345541071798057,1.0,0.0023380000000000067,1.0196470588235296,0.04118372379778064,0.1467505241090147,0.019268489673474175,0.28295054831367683
"frozenset({'math_cat_μέτριο', 'parental level of education_some college'})","frozenset({'test preparation course_none', 'lunch_standard'})",0.118,0.418,0.051,0.4322033898305085,1.0339794015083936,1.0,0.0016760000000000039,1.0250149253731344,0.037259348183718025,0.10515463917525772,0.024404449880598804,0.27710647960424944
"frozenset({'test preparation course_none', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'parental level of education_some college'})",0.418,0.118,0.051,0.12200956937799043,1.0339794015083936,1.0,0.0016760000000000039,1.004566757493188,0.056465197762954104,0.10515463917525772,0.0045459970272000395,0.27710647960424944
frozenset({'lunch_standard'}),"frozenset({""parental level of education_associate's degree"", 'reading_cat_μέτριο', 'test preparation course_none'})",0.645,0.075,0.05,0.07751937984496124,1.03359173126615,1.0,0.0016250000000000014,1.0027310924369748,0.09154929577464797,0.0746268656716418,0.0027236538864445856,0.372093023255814
"frozenset({'math_cat_μέτριο', 'gender_male'})",frozenset({'lunch_standard'}),0.24,0.645,0.16,0.6666666666666667,1.03359173126615,1.0,0.00520000000000001,1.0650000000000002,0.042763157894736926,0.22068965517241382,0.06103286384976542,0.45736434108527135
frozenset({'lunch_standard'}),"frozenset({'math_cat_μέτριο', 'gender_male'})",0.645,0.24,0.16,0.24806201550387597,1.03359173126615,1.0,0.00520000000000001,1.0107216494845361,0.09154929577464807,0.22068965517241382,0.010607915136678914,0.45736434108527135
"frozenset({""parental level of education_associate's degree"", 'reading_cat_μέτριο', 'test preparation course_none'})",frozenset({'lunch_standard'}),0.075,0.645,0.05,0.6666666666666667,1.03359173126615,1.0,0.0016250000000000014,1.0650000000000002,0.035135135135135165,0.0746268656716418,0.06103286384976542,0.372093023255814
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'gender_male'})",frozenset({'lunch_standard'}),0.165,0.645,0.11,0.6666666666666666,1.0335917312661498,1.0,0.003574999999999995,1.0649999999999997,0.0389221556886227,0.15714285714285714,0.061032863849765105,0.41860465116279066
frozenset({'lunch_standard'}),"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'gender_male'})",0.645,0.165,0.11,0.17054263565891473,1.0335917312661498,1.0,0.003574999999999995,1.0066822429906541,0.09154929577464775,0.15714285714285714,0.006637887016664334,0.41860465116279066
"frozenset({'race/ethnicity_group C', 'math_cat_μέτριο', 'reading_cat_μέτριο'})",frozenset({'lunch_standard'}),0.108,0.645,0.072,0.6666666666666666,1.0335917312661498,1.0,0.002339999999999995,1.0649999999999997,0.03643497757847526,0.10572687224669602,0.061032863849765105,0.3891472868217054
frozenset({'lunch_standard'}),"frozenset({'race/ethnicity_group C', 'math_cat_μέτριο', 'reading_cat_μέτριο'})",0.645,0.108,0.072,0.11162790697674417,1.0335917312661498,1.0,0.002339999999999995,1.0040837696335079,0.0915492957746477,0.10572687224669602,0.004067160287829789,0.3891472868217054
"frozenset({'gender_female', 'race/ethnicity_group B'})",frozenset({'test preparation course_none'}),0.104,0.642,0.069,0.6634615384615385,1.0334291876347952,1.0,0.0022320000000000118,1.0637714285714288,0.036102484472049876,0.1019202363367799,0.05994843145681154,0.3854690869877786
frozenset({'test preparation course_none'}),"frozenset({'gender_female', 'race/ethnicity_group B'})",0.642,0.104,0.069,0.1074766355140187,1.0334291876347952,1.0,0.0022320000000000118,1.0038952879581151,0.09035705610881757,0.1019202363367799,0.0038801735647530228,0.3854690869877786
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'parental level of education_some college'})",frozenset({'test preparation course_none'}),0.098,0.642,0.065,0.6632653061224489,1.033123529785746,1.0,0.0020840000000000025,1.063151515151515,0.03554494286201607,0.0962962962962963,0.05940029643142158,0.382255706020726
frozenset({'test preparation course_none'}),"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'parental level of education_some college'})",0.642,0.098,0.065,0.10124610591900311,1.033123529785746,1.0,0.0020840000000000025,1.0036117850953206,0.08955737000429749,0.0962962962962963,0.003598787049892582,0.382255706020726
"frozenset({""parental level of education_associate's degree"", 'test preparation course_none'})",frozenset({'writing_cat_μέτριο'}),0.14,0.491,0.071,0.507142857142857,1.0328775094559206,1.0,0.0022599999999999842,1.0327536231884056,0.03701277432034039,0.12678571428571425,0.03171484703901182,0.32587285423334295
frozenset({'writing_cat_μέτριο'}),"frozenset({""parental level of education_associate's degree"", 'test preparation course_none'})",0.491,0.14,0.071,0.1446028513238289,1.0328775094559206,1.0,0.0022599999999999842,1.0053809523809523,0.06253631810509379,0.12678571428571425,0.0053521527021266274,0.32587285423334295
"frozenset({'test preparation course_completed', 'lunch_standard'})",frozenset({'gender_male'}),0.227,0.482,0.113,0.4977973568281938,1.032774599228618,1.0,0.003586000000000006,1.0314561403508773,0.04105370410651531,0.18959731543624161,0.030496827853656046,0.36611859542654507
frozenset({'gender_male'}),"frozenset({'test preparation course_completed', 'lunch_standard'})",0.482,0.227,0.113,0.2344398340248963,1.032774599228618,1.0,0.003586000000000006,1.0097181571815719,0.06126353913964543,0.18959731543624161,0.009624623576838662,0.36611859542654507
"frozenset({'writing_cat_μέτριο', 'gender_female'})",frozenset({'test preparation course_none'}),0.264,0.642,0.175,0.6628787878787878,1.03252147644671,1.0,0.005511999999999989,1.0619325842696627,0.04279503105590054,0.23939808481532143,0.0583206365329269,0.46773222883035964
frozenset({'test preparation course_none'}),"frozenset({'writing_cat_μέτριο', 'gender_female'})",0.642,0.264,0.175,0.27258566978193144,1.0325214764467099,1.0,0.005511999999999989,1.0118029978586722,0.08798084596967262,0.23939808481532143,0.011665312203711177,0.46773222883035964
"frozenset({'test preparation course_completed', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})",0.227,0.32,0.075,0.33039647577092507,1.032488986784141,1.0,0.002359999999999987,1.0155263157894734,0.04070720137990491,0.15889830508474576,0.01528893495724274,0.2823857378854625
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})","frozenset({'test preparation course_completed', 'lunch_standard'})",0.32,0.227,0.075,0.234375,1.032488986784141,1.0,0.002359999999999987,1.0096326530612245,0.04627450980392132,0.15889830508474576,0.009540750323415255,0.2823857378854625
"frozenset({'reading_cat_μέτριο', 'gender_female'})",frozenset({'test preparation course_none'}),0.261,0.642,0.173,0.6628352490421455,1.032453658944152,1.0,0.005437999999999971,1.0617954545454542,0.042535217877619116,0.236986301369863,0.05819901967079751,0.4661528270132846
frozenset({'test preparation course_none'}),"frozenset({'reading_cat_μέτριο', 'gender_female'})",0.642,0.261,0.173,0.26947040498442365,1.032453658944152,1.0,0.005437999999999971,1.011594882729211,0.08780314528368861,0.236986301369863,0.011461982387582734,0.4661528270132846
"frozenset({""parental level of education_associate's degree"", 'lunch_standard'})","frozenset({'gender_female', 'test preparation course_none'})",0.145,0.334,0.05,0.3448275862068966,1.0324179227751393,1.0,0.001570000000000002,1.0165263157894735,0.03672514619883045,0.11655011655011656,0.01625763694729213,0.24726409250464587
"frozenset({'gender_female', 'test preparation course_none'})","frozenset({""parental level of education_associate's degree"", 'lunch_standard'})",0.334,0.145,0.05,0.1497005988023952,1.0324179227751393,1.0,0.001570000000000002,1.0055281690140845,0.04714714714714721,0.11655011655011656,0.005497776377070417,0.24726409250464587
"frozenset({'race/ethnicity_group C', 'math_cat_μέτριο'})","frozenset({'test preparation course_none', 'lunch_standard'})",0.153,0.418,0.066,0.43137254901960786,1.0319917440660475,1.0,0.002046000000000006,1.0235172413793105,0.03659976387249125,0.13069306930693073,0.02297688834984172,0.2946336429308566
"frozenset({'test preparation course_none', 'lunch_standard'})","frozenset({'race/ethnicity_group C', 'math_cat_μέτριο'})",0.418,0.153,0.066,0.15789473684210528,1.0319917440660475,1.0,0.002046000000000006,1.0058125,0.053264604810996714,0.13069306930693073,0.005778910085130207,0.2946336429308566
frozenset({'parental level of education_some college'}),"frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",0.226,0.223,0.052,0.2300884955752212,1.0317869756736378,1.0,0.0016019999999999923,1.009206896551724,0.039803220035777985,0.1309823677581864,0.009122902928212618,0.23163617603873166
"frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",frozenset({'parental level of education_some college'}),0.223,0.226,0.052,0.23318385650224213,1.0317869756736378,1.0,0.0016019999999999923,1.0093684210526317,0.039649539649539454,0.1309823677581864,0.009281468349150033,0.23163617603873166
"frozenset({""parental level of education_associate's degree""})",frozenset({'test preparation course_completed'}),0.222,0.358,0.082,0.36936936936936937,1.0317580149982386,1.0,0.0025240000000000123,1.0180285714285715,0.039563609003699485,0.16465863453815263,0.017709298083129887,0.2992098243494892
frozenset({'test preparation course_completed'}),"frozenset({""parental level of education_associate's degree""})",0.358,0.222,0.082,0.22905027932960895,1.0317580149982384,1.0,0.0025240000000000123,1.0091449275362319,0.04794468505432741,0.16465863453815263,0.00906205569358476,0.2992098243494892
frozenset({'reading_cat_μέτριο'}),"frozenset({'test preparation course_none', 'race/ethnicity_group D'})",0.49,0.18,0.091,0.18571428571428572,1.0317460317460319,1.0,0.002799999999999997,1.0070175438596491,0.06033182503770732,0.15716753022452504,0.006968641114982594,0.34563492063492063
"frozenset({'test preparation course_none', 'race/ethnicity_group D'})",frozenset({'reading_cat_μέτριο'}),0.18,0.49,0.091,0.5055555555555555,1.0317460317460316,1.0,0.002799999999999997,1.0314606741573034,0.037523452157598454,0.15716753022452504,0.030501089324618716,0.34563492063492063
"frozenset({'test preparation course_completed', 'lunch_free/reduced'})",frozenset({'gender_female'}),0.131,0.518,0.07,0.5343511450381679,1.0315659170621003,1.0,0.002142000000000005,1.0351147540983607,0.03521288837744542,0.12089810017271159,0.03392353742358491,0.3347431400866515
frozenset({'gender_female'}),"frozenset({'test preparation course_completed', 'lunch_free/reduced'})",0.518,0.131,0.07,0.13513513513513514,1.0315659170621003,1.0,0.002142000000000005,1.00478125,0.06348547717842339,0.12089810017271159,0.004758498429384508,0.3347431400866515
"frozenset({'race/ethnicity_group C', 'test preparation course_none'})","frozenset({'math_cat_μέτριο', 'gender_male'})",0.202,0.24,0.05,0.24752475247524752,1.0313531353135315,1.0,0.0015200000000000005,1.01,0.038095238095238106,0.12755102040816327,0.00990099009900991,0.22792904290429045
"frozenset({'math_cat_μέτριο', 'gender_male'})","frozenset({'race/ethnicity_group C', 'test preparation course_none'})",0.24,0.202,0.05,0.20833333333333334,1.0313531353135312,1.0,0.0015200000000000005,1.008,0.04000000000000001,0.12755102040816327,0.00793650793650793,0.22792904290429045
"frozenset({""parental level of education_associate's degree"", 'gender_female'})","frozenset({'test preparation course_none', 'lunch_standard'})",0.116,0.418,0.05,0.4310344827586207,1.0311829731067481,1.0,0.0015119999999999995,1.0229090909090912,0.034208144796380076,0.10330578512396695,0.022396018485602635,0.27532585381950175
"frozenset({'test preparation course_none', 'lunch_standard'})","frozenset({""parental level of education_associate's degree"", 'gender_female'})",0.418,0.116,0.05,0.11961722488038279,1.0311829731067481,1.0,0.0015119999999999995,1.004108695652174,0.05195876288659791,0.10330578512396695,0.004091883348849302,0.27532585381950175
"frozenset({'math_cat_μέτριο', 'lunch_standard'})","frozenset({'parental level of education_some college', 'test preparation course_none'})",0.332,0.149,0.051,0.1536144578313253,1.0309695156464787,1.0,0.0015319999999999986,1.0054519572953737,0.04496888575789594,0.11860465116279069,0.005422394631404586,0.24794816851297807
"frozenset({'parental level of education_some college', 'test preparation course_none'})","frozenset({'math_cat_μέτριο', 'lunch_standard'})",0.149,0.332,0.051,0.3422818791946309,1.0309695156464784,1.0,0.0015319999999999986,1.0156326530612245,0.03529872583581021,0.11860465116279069,0.01539203472250129,0.24794816851297807
"frozenset({'gender_male', 'test preparation course_none'})",frozenset({'math_cat_μέτριο'}),0.308,0.485,0.154,0.5,1.0309278350515465,1.0,0.004620000000000013,1.03,0.04335260115606949,0.24100156494522695,0.029126213592233035,0.40876288659793814
frozenset({'math_cat_μέτριο'}),"frozenset({'gender_male', 'test preparation course_none'})",0.485,0.308,0.154,0.31752577319587627,1.0309278350515463,1.0,0.004620000000000013,1.0139577039274923,0.05825242718446618,0.24100156494522695,0.013765568202133345,0.40876288659793814
frozenset({'lunch_free/reduced'}),frozenset({'test preparation course_completed'}),0.355,0.358,0.131,0.36901408450704226,1.0307655991816824,1.0,0.003910000000000025,1.017455357142857,0.04627492751050387,0.225085910652921,0.01715589487078236,0.3674679361082698
frozenset({'test preparation course_completed'}),frozenset({'lunch_free/reduced'}),0.358,0.355,0.131,0.36592178770949724,1.0307655991816824,1.0,0.003910000000000025,1.0172246696035243,0.046491165489524915,0.225085910652921,0.016933004200770936,0.3674679361082698
"frozenset({'race/ethnicity_group E', 'lunch_standard'})",frozenset({'reading_cat_μέτριο'}),0.099,0.49,0.05,0.5050505050505051,1.0307153164296021,1.0,0.0014899999999999983,1.0304081632653062,0.03307436182019974,0.09276437847866421,0.02951079421667665,0.30354566068851785
frozenset({'reading_cat_μέτριο'}),"frozenset({'race/ethnicity_group E', 'lunch_standard'})",0.49,0.099,0.05,0.10204081632653061,1.0307153164296021,1.0,0.0014899999999999983,1.0033863636363636,0.058431372549019533,0.09276437847866421,0.0033749348796122194,0.30354566068851785
frozenset({'gender_female'}),"frozenset({""parental level of education_bachelor's degree""})",0.518,0.118,0.063,0.12162162162162161,1.0306917086578105,1.0,0.0018760000000000027,1.004123076923077,0.061779621945597135,0.1099476439790576,0.004106146963289819,0.3277599633531837
"frozenset({""parental level of education_bachelor's degree""})",frozenset({'gender_female'}),0.118,0.518,0.063,0.5338983050847458,1.0306917086578105,1.0,0.0018760000000000027,1.0341090909090909,0.03376165280927191,0.1099476439790576,0.03298403544553064,0.3277599633531837
"frozenset({'parental level of education_some college', 'lunch_standard'})",frozenset({'gender_male'}),0.147,0.482,0.073,0.4965986394557823,1.0302876337256894,1.0,0.002145999999999995,1.029,0.03446337664006159,0.13129496402877697,0.02818270165208943,0.32402546080672934
frozenset({'gender_male'}),"frozenset({'parental level of education_some college', 'lunch_standard'})",0.482,0.147,0.073,0.15145228215767634,1.0302876337256894,1.0,0.002145999999999995,1.0052469437652811,0.05675146771037169,0.13129496402877697,0.005219557042996889,0.32402546080672934
"frozenset({'test preparation course_completed', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο'})",0.227,0.325,0.076,0.33480176211453744,1.0301592680447305,1.0,0.002224999999999991,1.0147350993377484,0.03787362974058676,0.15966386554621848,0.014521129058573964,0.28432395798034565
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'test preparation course_completed', 'lunch_standard'})",0.325,0.227,0.076,0.23384615384615384,1.0301592680447305,1.0,0.002224999999999991,1.0089357429718875,0.04337231968810898,0.15966386554621848,0.008856602647029541,0.28432395798034565
frozenset({'race/ethnicity_group D'}),"frozenset({'lunch_free/reduced', 'gender_female'})",0.262,0.189,0.051,0.1946564885496183,1.0299285108445413,1.0,0.0014819999999999972,1.0070236966824644,0.0393750996333492,0.12749999999999997,0.006974708445891875,0.23224887919544407
"frozenset({'lunch_free/reduced', 'gender_female'})",frozenset({'race/ethnicity_group D'}),0.189,0.262,0.051,0.2698412698412698,1.0299285108445413,1.0,0.0014819999999999972,1.0107391304347826,0.03583085515340532,0.12749999999999997,0.010625026885189452,0.23224887919544407
frozenset({'writing_cat_μέτριο'}),"frozenset({'test preparation course_none', 'race/ethnicity_group D'})",0.491,0.18,0.091,0.18533604887983707,1.029644715999095,1.0,0.0026199999999999973,1.00655,0.05656426088646122,0.1568965517241379,0.006507376682728138,0.3454458022176963
"frozenset({'test preparation course_none', 'race/ethnicity_group D'})",frozenset({'writing_cat_μέτριο'}),0.18,0.491,0.091,0.5055555555555555,1.0296447159990947,1.0,0.0026199999999999973,1.029438202247191,0.03511123023318141,0.1568965517241379,0.02859637633704429,0.3454458022176963
frozenset({'test preparation course_none'}),"frozenset({'math_cat_μέτριο', 'parental level of education_some college'})",0.642,0.118,0.078,0.12149532710280374,1.0296214161254555,1.0,0.002243999999999996,1.0039787234042554,0.08036097980232045,0.11436950146627566,0.003962955898870458,0.39125613812767307
"frozenset({'math_cat_μέτριο', 'parental level of education_some college'})",frozenset({'test preparation course_none'}),0.118,0.642,0.078,0.6610169491525424,1.0296214161254553,1.0,0.002243999999999996,1.0561,0.03261817547531828,0.11436950146627566,0.05311996969983903,0.39125613812767307
frozenset({'reading_cat_μέτριο'}),"frozenset({""parental level of education_associate's degree""})",0.49,0.222,0.112,0.2285714285714286,1.0296010296010296,1.0,0.0032200000000000006,1.0085185185185186,0.05637254901960785,0.18666666666666668,0.008446566287183276,0.36653796653796655
"frozenset({""parental level of education_associate's degree""})",frozenset({'reading_cat_μέτριο'}),0.222,0.49,0.112,0.5045045045045045,1.0296010296010296,1.0,0.0032200000000000006,1.0292727272727271,0.03695372750642674,0.18666666666666668,0.028440204910793073,0.36653796653796655
frozenset({'parental level of education_some college'}),frozenset({'reading_cat_μέτριο'}),0.226,0.49,0.114,0.504424778761062,1.0294383240021672,1.0,0.003259999999999999,1.029107142857143,0.036946371095697886,0.18936877076411962,0.028283879923650907,0.3685389199927759
frozenset({'reading_cat_μέτριο'}),frozenset({'parental level of education_some college'}),0.49,0.226,0.114,0.2326530612244898,1.0294383240021672,1.0,0.003259999999999999,1.0086702127659575,0.05607155142758856,0.18936877076411962,0.008595686336550133,0.3685389199927759
frozenset({'gender_male'}),"frozenset({'math_cat_υψηλό', 'reading_cat_υψηλό'})",0.482,0.131,0.065,0.13485477178423239,1.0294257388109342,1.0,0.0018579999999999985,1.0044556354916068,0.05518265518265513,0.11861313868613138,0.004435870867931393,0.3155189889455513
"frozenset({'math_cat_υψηλό', 'reading_cat_υψηλό'})",frozenset({'gender_male'}),0.131,0.482,0.065,0.4961832061068702,1.029425738810934,1.0,0.0018579999999999985,1.0281515151515153,0.0328936885898911,0.11861313868613138,0.027380706770019764,0.3155189889455513
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})","frozenset({'gender_female', 'test preparation course_none'})",0.32,0.334,0.11,0.34375,1.029191616766467,1.0,0.0031199999999999978,1.0148571428571427,0.04171122994652404,0.20220588235294115,0.014639639639639613,0.33654565868263475
"frozenset({'gender_female', 'test preparation course_none'})","frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})",0.334,0.32,0.11,0.32934131736526945,1.029191616766467,1.0,0.0031199999999999978,1.0139285714285713,0.042588042588042566,0.20220588235294115,0.013737231419513886,0.33654565868263475
"frozenset({'writing_cat_μέτριο', ""parental level of education_associate's degree""})",frozenset({'lunch_standard'}),0.11,0.645,0.073,0.6636363636363636,1.0288935870331217,1.0,0.0020499999999999963,1.0554054054054052,0.031553024472833556,0.10703812316715541,0.052496798975672006,0.3884073291050035
frozenset({'lunch_standard'}),"frozenset({'writing_cat_μέτριο', ""parental level of education_associate's degree""})",0.645,0.11,0.073,0.1131782945736434,1.0288935870331217,1.0,0.0020499999999999963,1.003583916083916,0.07910476557977991,0.10703812316715541,0.0035711174984757246,0.3884073291050035
"frozenset({'lunch_free/reduced', 'gender_female'})",frozenset({'race/ethnicity_group C'}),0.189,0.319,0.062,0.328042328042328,1.0283458559320628,1.0,0.0017089999999999952,1.013456692913386,0.033988305954417,0.1390134529147982,0.013278014746443492,0.26119984740674396
frozenset({'race/ethnicity_group C'}),"frozenset({'lunch_free/reduced', 'gender_female'})",0.319,0.189,0.062,0.19435736677115986,1.0283458559320628,1.0,0.0017089999999999952,1.0066498054474708,0.0404765288238358,0.1390134529147982,0.006605877646313022,0.26119984740674396
frozenset({'reading_cat_μέτριο'}),"frozenset({'gender_female', 'race/ethnicity_group D'})",0.49,0.129,0.065,0.1326530612244898,1.0283183040658124,1.0,0.00179,1.0042117647058824,0.05399698340874812,0.11732851985559566,0.004194100142927441,0.31826451510836895
"frozenset({'gender_female', 'race/ethnicity_group D'})",frozenset({'reading_cat_μέτριο'}),0.129,0.49,0.065,0.5038759689922481,1.0283183040658124,1.0,0.00179,1.02796875,0.03161706261591451,0.11732851985559566,0.027207782337741328,0.31826451510836895
frozenset({'lunch_standard'}),"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'parental level of education_some college'})",0.645,0.098,0.065,0.10077519379844961,1.0283183040658124,1.0,0.00179,1.0030862068965518,0.07757313109425786,0.09587020648967552,0.003076711528214642,0.38202024996044925
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'parental level of education_some college'})",frozenset({'lunch_standard'}),0.098,0.645,0.065,0.6632653061224489,1.0283183040658124,1.0,0.00179,1.054242424242424,0.030530445164591503,0.09587020648967552,0.05145156654210964,0.38202024996044925
frozenset({'reading_cat_μέτριο'}),frozenset({'gender_female'}),0.49,0.518,0.261,0.5326530612244899,1.0282877629816407,1.0,0.00718000000000002,1.031353711790393,0.053940350086394855,0.3493975903614458,0.03040054195952249,0.5182570325427469
frozenset({'gender_female'}),frozenset({'reading_cat_μέτριο'}),0.518,0.49,0.261,0.5038610038610039,1.0282877629816405,1.0,0.00718000000000002,1.0279377431906616,0.05707381440676634,0.3493975903614458,0.02717843894314483,0.5182570325427469
frozenset({'lunch_free/reduced'}),frozenset({'gender_female'}),0.355,0.518,0.189,0.5323943661971832,1.0277883517320139,1.0,0.005110000000000003,1.0307831325301207,0.04191788687912722,0.27631578947368424,0.029863830284612345,0.448629615531024
frozenset({'gender_female'}),frozenset({'lunch_free/reduced'}),0.518,0.355,0.189,0.36486486486486486,1.0277883517320137,1.0,0.005110000000000003,1.015531914893617,0.056093437836176464,0.27631578947368424,0.01529436413157345,0.448629615531024
"frozenset({'race/ethnicity_group C', 'gender_male'})",frozenset({'reading_cat_μέτριο'}),0.139,0.49,0.07,0.5035971223021583,1.027749229188078,1.0,0.0018900000000000028,1.027391304347826,0.031358885017421644,0.12522361359570663,0.02666102412187894,0.3232271325796506
frozenset({'reading_cat_μέτριο'}),"frozenset({'race/ethnicity_group C', 'gender_male'})",0.49,0.139,0.07,0.14285714285714288,1.027749229188078,1.0,0.0018900000000000028,1.0045,0.05294117647058831,0.12522361359570663,0.004479840716774523,0.3232271325796506
"frozenset({'reading_cat_μέτριο', 'math_cat_μέτριο', 'gender_male'})",frozenset({'lunch_standard'}),0.175,0.645,0.116,0.6628571428571429,1.027685492801772,1.0,0.0031250000000000167,1.0529661016949154,0.03265412748171387,0.16477272727272727,0.05030181086519128,0.42135105204872647
frozenset({'lunch_standard'}),"frozenset({'reading_cat_μέτριο', 'math_cat_μέτριο', 'gender_male'})",0.645,0.175,0.116,0.17984496124031008,1.027685492801772,1.0,0.0031250000000000167,1.0059073724007561,0.07588635259834911,0.16477272727272727,0.0058726802912849535,0.42135105204872647
frozenset({'lunch_standard'}),"frozenset({'test preparation course_completed', 'math_cat_μέτριο', 'gender_male'})",0.645,0.086,0.057,0.08837209302325581,1.0275824770146025,1.0,0.0015300000000000036,1.0026020408163265,0.07561156412157172,0.08456973293768547,0.0025952877716146822,0.3755813953488372
"frozenset({'test preparation course_completed', 'math_cat_μέτριο', 'gender_male'})",frozenset({'lunch_standard'}),0.086,0.645,0.057,0.6627906976744187,1.0275824770146025,1.0,0.0015300000000000036,1.0527586206896553,0.02936773004721877,0.08456973293768547,0.05011464133639058,0.3755813953488372
frozenset({'reading_cat_μέτριο'}),"frozenset({""parental level of education_associate's degree"", 'lunch_standard'})",0.49,0.145,0.073,0.14897959183673468,1.027445460942998,1.0,0.0019500000000000073,1.0046762589928058,0.0523771152296537,0.12989323843416367,0.004654493376297878,0.32621393384940184
"frozenset({""parental level of education_associate's degree"", 'lunch_standard'})",frozenset({'reading_cat_μέτριο'}),0.145,0.49,0.073,0.503448275862069,1.027445460942998,1.0,0.0019500000000000073,1.0270833333333333,0.031242489786109225,0.12989323843416367,0.026369168356998002,0.32621393384940184
"frozenset({'parental level of education_some college', 'lunch_standard'})",frozenset({'reading_cat_μέτριο'}),0.147,0.49,0.074,0.5034013605442177,1.0273497153963627,1.0,0.0019699999999999995,1.026986301369863,0.031209404011279737,0.1314387211367673,0.026277177537681762,0.3272108843537415
frozenset({'reading_cat_μέτριο'}),"frozenset({'parental level of education_some college', 'lunch_standard'})",0.49,0.147,0.074,0.1510204081632653,1.0273497153963627,1.0,0.0019699999999999995,1.004735576923077,0.052199258081611015,0.1314387211367673,0.004713256932315734,0.3272108843537415
frozenset({'writing_cat_μέτριο'}),frozenset({'parental level of education_some college'}),0.491,0.226,0.114,0.23217922606924646,1.027341708271002,1.0,0.003034000000000009,1.0080477453580903,0.052286905869782664,0.1890547263681592,0.007983496213496708,0.3683020024151542
frozenset({'parental level of education_some college'}),frozenset({'writing_cat_μέτριο'}),0.226,0.491,0.114,0.504424778761062,1.027341708271002,1.0,0.003034000000000009,1.0270892857142857,0.03438505825286741,0.1890547263681592,0.026374810925465543,0.3683020024151542
frozenset({'parental level of education_some college'}),frozenset({'test preparation course_none'}),0.226,0.642,0.149,0.65929203539823,1.0269346345766823,1.0,0.003907999999999995,1.0507532467532466,0.033886547699564666,0.20723226703755215,0.04830177485539103,0.4456896314062801
frozenset({'test preparation course_none'}),frozenset({'parental level of education_some college'}),0.642,0.226,0.149,0.23208722741433022,1.0269346345766823,1.0,0.003907999999999995,1.0079269776876267,0.0732630947471035,0.20723226703755215,0.007864634902235412,0.4456896314062801
frozenset({'lunch_standard'}),"frozenset({'test preparation course_completed', 'parental level of education_some college'})",0.645,0.077,0.051,0.07906976744186046,1.0268800966475384,1.0,0.001334999999999996,1.0022474747474748,0.07373653686826821,0.07600596125186289,0.0022424349315931366,0.3707037148897614
"frozenset({'test preparation course_completed', 'parental level of education_some college'})",frozenset({'lunch_standard'}),0.077,0.645,0.051,0.6623376623376623,1.0268800966475384,1.0,0.001334999999999996,1.0513461538461537,0.028360206487795466,0.07600596125186289,0.04883848545820372,0.3707037148897614
frozenset({'lunch_standard'}),"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο'})",0.645,0.154,0.102,0.15813953488372093,1.0268800966475384,1.0,0.002669999999999992,1.0049171270718231,0.07373653686826821,0.1463414634146341,0.004893067238440816,0.4102385986106916
"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο'})",frozenset({'lunch_standard'}),0.154,0.645,0.102,0.6623376623376623,1.0268800966475384,1.0,0.002669999999999992,1.0513461538461537,0.030941454596022714,0.1463414634146341,0.04883848545820372,0.4102385986106916
"frozenset({'test preparation course_none', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'gender_male'})",0.418,0.24,0.103,0.24641148325358853,1.026714513556619,1.0,0.002680000000000002,1.0085079365079366,0.04470690287925803,0.1855855855855856,0.008436162175774387,0.33778907496012756
"frozenset({'math_cat_μέτριο', 'gender_male'})","frozenset({'test preparation course_none', 'lunch_standard'})",0.24,0.418,0.103,0.42916666666666664,1.0267145135566187,1.0,0.002680000000000002,1.0195620437956205,0.03423607562595812,0.1855855855855856,0.01918671248568154,0.33778907496012756
frozenset({'math_cat_μέτριο'}),frozenset({'gender_male'}),0.485,0.482,0.24,0.4948453608247423,1.0266501261924115,1.0,0.006230000000000013,1.0254285714285716,0.05040453074433668,0.33012379642365886,0.024797993870158883,0.4963853360140309
frozenset({'gender_male'}),frozenset({'math_cat_μέτριο'}),0.482,0.485,0.24,0.4979253112033195,1.0266501261924115,1.0,0.006230000000000013,1.0257438016528926,0.05011261261261272,0.33012379642365886,0.025097691656931112,0.4963853360140309
frozenset({'parental level of education_some high school'}),"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})",0.179,0.283,0.052,0.2905027932960894,1.0265116370886553,1.0,0.001343000000000004,1.0105748031496065,0.03145788438114879,0.12682926829268293,0.010464146856470624,0.23712418816747932
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})",frozenset({'parental level of education_some high school'}),0.283,0.179,0.052,0.18374558303886926,1.0265116370886551,1.0,0.001343000000000004,1.0058138528138527,0.03602081321746604,0.12682926829268293,0.005780247306783517,0.23712418816747932
frozenset({'test preparation course_completed'}),"frozenset({'lunch_free/reduced', 'gender_male'})",0.358,0.166,0.061,0.17039106145251398,1.026452177424783,1.0,0.001571999999999997,1.0052929292929291,0.04014095296460847,0.13174946004319654,0.005265061693661838,0.2689304704852931
"frozenset({'lunch_free/reduced', 'gender_male'})",frozenset({'test preparation course_completed'}),0.166,0.358,0.061,0.36746987951807225,1.0264521774247828,1.0,0.001571999999999997,1.0149714285714284,0.030899870267720193,0.13174946004319654,0.014750591149645274,0.2689304704852931
"frozenset({'gender_female', 'race/ethnicity_group D'})",frozenset({'test preparation course_none'}),0.129,0.642,0.085,0.6589147286821706,1.0263469294114806,1.0,0.0021820000000000034,1.0495909090909092,0.029472546768420385,0.1239067055393586,0.04724784548092343,0.3956567412881258
frozenset({'test preparation course_none'}),"frozenset({'gender_female', 'race/ethnicity_group D'})",0.642,0.129,0.085,0.132398753894081,1.0263469294114806,1.0,0.0021820000000000034,1.0039174147217236,0.07170555372987195,0.1239067055393586,0.0039021284662238897,0.3956567412881258
frozenset({'race/ethnicity_group C'}),"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.319,0.281,0.092,0.2884012539184953,1.0263389819163533,1.0,0.002360999999999988,1.0104008810572687,0.037684351656770546,0.18110236220472437,0.010293816298324446,0.307901694574906
"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο'})",frozenset({'race/ethnicity_group C'}),0.281,0.319,0.092,0.32740213523131667,1.026338981916353,1.0,0.002360999999999988,1.0124920634920636,0.03569268912136404,0.18110236220472437,0.012337937197234456,0.307901694574906
frozenset({'writing_cat_μέτριο'}),"frozenset({'test preparation course_completed', 'lunch_free/reduced'})",0.491,0.131,0.066,0.13441955193482688,1.0261034498841746,1.0,0.001679,1.0039505882352942,0.04997916294576413,0.11870503597122302,0.0039350425026776475,0.3191181729139783
"frozenset({'test preparation course_completed', 'lunch_free/reduced'})",frozenset({'writing_cat_μέτριο'}),0.131,0.491,0.066,0.5038167938931297,1.0261034498841746,1.0,0.001679,1.025830769230769,0.02927433134567772,0.11870503597122302,0.02518034163679711,0.3191181729139783
"frozenset({'lunch_free/reduced', 'test preparation course_none'})",frozenset({'gender_female'}),0.224,0.518,0.119,0.53125,1.0255791505791505,1.0,0.0029679999999999845,1.0282666666666667,0.03214069132807746,0.19101123595505617,0.027489626556016566,0.38048986486486486
frozenset({'gender_female'}),"frozenset({'lunch_free/reduced', 'test preparation course_none'})",0.518,0.224,0.119,0.22972972972972971,1.0255791505791505,1.0,0.0029679999999999845,1.007438596491228,0.051745179399560394,0.19101123595505617,0.007383672332125914,0.38048986486486486
"frozenset({""parental level of education_associate's degree"", 'lunch_standard'})",frozenset({'writing_cat_μέτριο'}),0.145,0.491,0.073,0.503448275862069,1.0253529039960672,1.0,0.001805000000000001,1.0250694444444444,0.028919330289193322,0.12966252220248667,0.024456337646500945,0.32606222347074937
frozenset({'writing_cat_μέτριο'}),"frozenset({""parental level of education_associate's degree"", 'lunch_standard'})",0.491,0.145,0.073,0.14867617107942974,1.0253529039960672,1.0,0.001805000000000001,1.0043181818181819,0.04857765696907719,0.12966252220248667,0.004299615297578651,0.32606222347074937
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})","frozenset({'gender_male', 'test preparation course_none'})",0.228,0.308,0.072,0.3157894736842105,1.0252904989747094,1.0,0.0017759999999999998,1.0113846153846153,0.03195164075993091,0.15517241379310343,0.011256464861575886,0.2747778537252221
"frozenset({'gender_male', 'test preparation course_none'})","frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})",0.308,0.228,0.072,0.23376623376623376,1.0252904989747094,1.0,0.0017759999999999998,1.0075254237288136,0.03564547206165703,0.15517241379310343,0.007469214723100716,0.2747778537252221
"frozenset({'reading_cat_μέτριο', 'lunch_standard'})",frozenset({'parental level of education_some high school'}),0.327,0.179,0.06,0.18348623853211007,1.0250627850955871,1.0,0.001466999999999996,1.0054943820224718,0.03632986627043081,0.13452914798206278,0.005464358748002536,0.25934088462918353
frozenset({'parental level of education_some high school'}),"frozenset({'reading_cat_μέτριο', 'lunch_standard'})",0.179,0.327,0.06,0.33519553072625696,1.025062785095587,1.0,0.001466999999999996,1.012327731092437,0.029780755176613806,0.13452914798206278,0.01217760880573098,0.25934088462918353
"frozenset({'reading_cat_μέτριο', 'test preparation course_none'})","frozenset({'gender_male', 'lunch_standard'})",0.318,0.316,0.103,0.3238993710691824,1.0249980097126024,1.0,0.0025119999999999865,1.0116837209302325,0.035760043276485305,0.19397363465160072,0.011548788112839724,0.324924369078895
"frozenset({'gender_male', 'lunch_standard'})","frozenset({'reading_cat_μέτριο', 'test preparation course_none'})",0.316,0.318,0.103,0.32594936708860756,1.0249980097126024,1.0,0.0025119999999999865,1.0117934272300468,0.0356554817464371,0.19397363465160072,0.0116559634730316,0.324924369078895
"frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced'})",frozenset({'race/ethnicity_group C'}),0.156,0.319,0.051,0.3269230769230769,1.0248372317337835,1.0,0.001235999999999994,1.0117714285714285,0.028714803456927656,0.12028301886792453,0.011634474189540266,0.24339884253677357
frozenset({'race/ethnicity_group C'}),"frozenset({'writing_cat_χαμηλό', 'lunch_free/reduced'})",0.319,0.156,0.051,0.1598746081504702,1.0248372317337833,1.0,0.001235999999999994,1.0046119402985074,0.03558780340329948,0.12028301886792453,0.004590767950794068,0.24339884253677357
frozenset({'test preparation course_completed'}),"frozenset({'reading_cat_μέτριο', 'gender_male'})",0.358,0.229,0.084,0.23463687150837992,1.0246151594252397,1.0,0.002018000000000006,1.0073649635036497,0.03742026405577818,0.16699801192842945,0.007311117390894824,0.3007245492913079
"frozenset({'reading_cat_μέτριο', 'gender_male'})",frozenset({'test preparation course_completed'}),0.229,0.358,0.084,0.36681222707423583,1.0246151594252397,1.0,0.002018000000000006,1.0139172413793103,0.031159286023099343,0.16699801192842945,0.01372621039600599,0.3007245492913079
frozenset({'race/ethnicity_group C'}),"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'test preparation course_none'})",0.319,0.205,0.067,0.21003134796238246,1.0245431607921096,1.0,0.0016050000000000092,1.0063690476190477,0.0351765402064569,0.14660831509846828,0.006328739575323862,0.2684303081275327
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'test preparation course_none'})",frozenset({'race/ethnicity_group C'}),0.205,0.319,0.067,0.32682926829268294,1.0245431607921096,1.0,0.0016050000000000092,1.0116304347826088,0.030132357082512138,0.14660831509846828,0.011496722896744395,0.2684303081275327
frozenset({'race/ethnicity_group C'}),frozenset({'test preparation course_completed'}),0.319,0.358,0.117,0.3667711598746082,1.024500446577118,1.0,0.002798000000000009,1.013851485148515,0.03511678401546253,0.2089285714285714,0.013662242795339863,0.3467934011663544
frozenset({'test preparation course_completed'}),frozenset({'race/ethnicity_group C'}),0.358,0.319,0.117,0.3268156424581006,1.0245004465771177,1.0,0.002798000000000009,1.0116099585062241,0.03725004659584111,0.2089285714285714,0.011476714329075724,0.3467934011663544
frozenset({'race/ethnicity_group C'}),"frozenset({'lunch_free/reduced', 'math_cat_μέτριο'})",0.319,0.153,0.05,0.15673981191222572,1.0244432151125864,1.0,0.0011929999999999996,1.0044349442379181,0.03503671071953009,0.1184834123222749,0.004415362352096483,0.24176859876657036
"frozenset({'lunch_free/reduced', 'math_cat_μέτριο'})",frozenset({'race/ethnicity_group C'}),0.153,0.319,0.05,0.32679738562091504,1.0244432151125864,1.0,0.0011929999999999996,1.0115825242718446,0.028170011806375433,0.1184834123222749,0.011449905463898725,0.24176859876657036
"frozenset({'writing_cat_μέτριο', 'gender_female'})",frozenset({'parental level of education_high school'}),0.264,0.196,0.053,0.20075757575757575,1.024273345701917,1.0,0.0012559999999999932,1.005952606635071,0.03219852337981935,0.13022113022113022,0.0059173827830544005,0.23558286951144092
frozenset({'parental level of education_high school'}),"frozenset({'writing_cat_μέτριο', 'gender_female'})",0.196,0.264,0.053,0.2704081632653061,1.0242733457019169,1.0,0.0012559999999999932,1.0087832167832167,0.02947526518351622,0.13022113022113022,0.008706743566991953,0.23558286951144092
frozenset({'parental level of education_high school'}),frozenset({'race/ethnicity_group C'}),0.196,0.319,0.064,0.32653061224489793,1.023606934936984,1.0,0.0014759999999999912,1.0111818181818182,0.028684701492537143,0.1419068736141907,0.011058167760496222,0.2635787857462734
frozenset({'race/ethnicity_group C'}),frozenset({'parental level of education_high school'}),0.319,0.196,0.064,0.2006269592476489,1.023606934936984,1.0,0.0014759999999999912,1.0057882352941176,0.033865638766519615,0.1419068736141907,0.005754924437374238,0.2635787857462734
"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'lunch_standard'})",frozenset({'test preparation course_none'}),0.102,0.642,0.067,0.6568627450980393,1.0231506932991266,1.0,0.0015160000000000035,1.043314285714286,0.025196955090915192,0.09896602658788774,0.04151604775988634,0.3806120579072751
frozenset({'test preparation course_none'}),"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'lunch_standard'})",0.642,0.102,0.067,0.1043613707165109,1.0231506932991266,1.0,0.0015160000000000035,1.0026365217391304,0.0632035353956476,0.09896602658788774,0.0026295887711702756,0.3806120579072751
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})","frozenset({'gender_male', 'lunch_standard'})",0.266,0.316,0.086,0.3233082706766917,1.0231274388502902,1.0,0.0019439999999999874,1.0107999999999997,0.030796527469741895,0.1733870967741935,0.010684606252473259,0.29773008470543444
"frozenset({'gender_male', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})",0.316,0.266,0.086,0.27215189873417717,1.02312743885029,1.0,0.0019439999999999874,1.0084521739130432,0.033047735618114846,0.1733870967741935,0.008381333425309473,0.29773008470543444
frozenset({'race/ethnicity_group B'}),frozenset({'lunch_free/reduced'}),0.19,0.355,0.069,0.3631578947368421,1.0229799851742032,1.0,0.0015500000000000097,1.012809917355372,0.027733047056718724,0.14495798319327735,0.012647898816809522,0.27876204595997034
frozenset({'lunch_free/reduced'}),frozenset({'race/ethnicity_group B'}),0.355,0.19,0.069,0.1943661971830986,1.0229799851742032,1.0,0.0015500000000000097,1.0054195804195805,0.03482754746657701,0.14495798319327735,0.00539036689271433,0.27876204595997034
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', ""parental level of education_associate's degree""})",frozenset({'lunch_standard'}),0.097,0.645,0.064,0.6597938144329897,1.022936146407736,1.0,0.0014349999999999918,1.0434848484848485,0.024830426356589004,0.09439528023598821,0.04167271671264693,0.37950931031727003
frozenset({'lunch_standard'}),"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', ""parental level of education_associate's degree""})",0.645,0.097,0.064,0.09922480620155039,1.022936146407736,1.0,0.0014349999999999918,1.0024698795180722,0.06316021126760528,0.09439528023598821,0.0024637942431344224,0.37950931031727003
"frozenset({'gender_male', 'test preparation course_none'})","frozenset({'reading_cat_μέτριο', 'lunch_standard'})",0.308,0.327,0.103,0.3344155844155844,1.0226776281822152,1.0,0.0022839999999999944,1.011141463414634,0.0320444469386609,0.19360902255639095,0.011018698983037704,0.3247001469478533
"frozenset({'reading_cat_μέτριο', 'lunch_standard'})","frozenset({'gender_male', 'test preparation course_none'})",0.327,0.308,0.103,0.3149847094801223,1.0226776281822152,1.0,0.0022839999999999944,1.0101964285714284,0.032949119289083714,0.19360902255639095,0.010093510809425303,0.3247001469478533
frozenset({'lunch_standard'}),"frozenset({'test preparation course_completed', 'writing_cat_μέτριο', 'gender_male'})",0.645,0.094,0.062,0.09612403100775194,1.0225960745505525,1.0,0.0013699999999999962,1.0023499142367067,0.06224443434802346,0.0915805022156573,0.0023444050858189144,0.3778492495464291
"frozenset({'test preparation course_completed', 'writing_cat_μέτριο', 'gender_male'})",frozenset({'lunch_standard'}),0.094,0.645,0.062,0.6595744680851063,1.0225960745505525,1.0,0.0013699999999999962,1.0428124999999997,0.02438937548956769,0.0915805022156573,0.04105483967635583,0.3778492495464291
frozenset({'parental level of education_some college'}),"frozenset({'gender_male', 'lunch_standard'})",0.226,0.316,0.073,0.32300884955752207,1.0221799036630446,1.0,0.0015839999999999882,1.0103529411764705,0.0280344058617392,0.1556503198294243,0.010246856078248637,0.2770107538926851
"frozenset({'gender_male', 'lunch_standard'})",frozenset({'parental level of education_some college'}),0.316,0.226,0.073,0.23101265822784808,1.0221799036630446,1.0,0.0015839999999999882,1.0065185185185186,0.03172314347512594,0.1556503198294243,0.006476302619958755,0.2770107538926851
frozenset({'parental level of education_some high school'}),frozenset({'lunch_standard'}),0.179,0.645,0.118,0.659217877094972,1.0220432203022822,1.0,0.0025449999999999917,1.0417213114754098,0.026270154214579077,0.16713881019830026,0.040050358014005694,0.42108180676454027
frozenset({'lunch_standard'}),frozenset({'parental level of education_some high school'}),0.645,0.179,0.118,0.18294573643410852,1.0220432203022822,1.0,0.0025449999999999917,1.004829222011385,0.06075435664836457,0.16713881019830026,0.004806012709023791,0.42108180676454027
"frozenset({'test preparation course_completed', 'lunch_standard'})",frozenset({'race/ethnicity_group C'}),0.227,0.319,0.074,0.32599118942731276,1.0219159543175949,1.0,0.0015869999999999912,1.0103725490196078,0.02774378518233613,0.15677966101694912,0.01026606376991593,0.2789830555287034
frozenset({'race/ethnicity_group C'}),"frozenset({'test preparation course_completed', 'lunch_standard'})",0.319,0.227,0.074,0.23197492163009403,1.0219159543175949,1.0,0.0015869999999999912,1.0064775510204083,0.03149184426717449,0.15677966101694912,0.0064358623933945995,0.2789830555287034
frozenset({'lunch_standard'}),"frozenset({'gender_male', 'test preparation course_none'})",0.645,0.308,0.203,0.3147286821705427,1.02184637068358,1.0,0.0043400000000000105,1.0098190045248867,0.06022340942204968,0.27066666666666667,0.00972352914818306,0.4869097956307259
"frozenset({'gender_male', 'test preparation course_none'})",frozenset({'lunch_standard'}),0.308,0.645,0.203,0.6590909090909092,1.02184637068358,1.0,0.0043400000000000105,1.0413333333333334,0.03089495714570468,0.27066666666666667,0.03969270166453283,0.4869097956307259
frozenset({'lunch_standard'}),"frozenset({'reading_cat_μέτριο', 'gender_female'})",0.645,0.261,0.172,0.26666666666666666,1.0217113665389528,1.0,0.003654999999999964,1.0077272727272726,0.059859154929576885,0.23433242506811988,0.007668019846639585,0.46283524904214557
"frozenset({'reading_cat_μέτριο', 'gender_female'})",frozenset({'lunch_standard'}),0.261,0.645,0.172,0.6590038314176244,1.0217113665389526,1.0,0.003654999999999964,1.0410674157303368,0.028755074424898226,0.23433242506811988,0.039447412444012404,0.46283524904214557
frozenset({'race/ethnicity_group D'}),frozenset({'lunch_free/reduced'}),0.262,0.355,0.095,0.36259541984732824,1.0213955488657134,1.0,0.0019900000000000057,1.0119161676646706,0.028383968050206897,0.18199233716475097,0.01177584472453993,0.3151005268250726
frozenset({'lunch_free/reduced'}),frozenset({'race/ethnicity_group D'}),0.355,0.262,0.095,0.26760563380281693,1.0213955488657134,1.0,0.0019900000000000057,1.0076538461538462,0.032476540187678586,0.18199233716475097,0.00759570975991453,0.3151005268250726
"frozenset({""parental level of education_associate's degree"", 'lunch_standard'})",frozenset({'test preparation course_completed'}),0.145,0.358,0.053,0.36551724137931035,1.0209978809477944,1.0,0.0010900000000000076,1.0118478260869566,0.024053845305086783,0.11777777777777777,0.011709098721667241,0.2567809670583703
frozenset({'test preparation course_completed'}),"frozenset({""parental level of education_associate's degree"", 'lunch_standard'})",0.358,0.145,0.053,0.14804469273743018,1.0209978809477944,1.0,0.0010900000000000076,1.0035737704918033,0.03203432669135389,0.11777777777777777,0.003561044137345249,0.2567809670583703
frozenset({'writing_cat_χαμηλό'}),frozenset({'race/ethnicity_group C'}),0.301,0.319,0.098,0.32558139534883723,1.0206313333819348,1.0,0.0019810000000000105,1.0097586206896554,0.02891886368281233,0.18773946360153257,0.009664310350715457,0.3163957133483998
frozenset({'race/ethnicity_group C'}),frozenset({'writing_cat_χαμηλό'}),0.319,0.301,0.098,0.3072100313479624,1.0206313333819348,1.0,0.0019810000000000105,1.0089638009049775,0.02968323893434041,0.18773946360153257,0.008884165018544185,0.3163957133483998
frozenset({'reading_cat_μέτριο'}),frozenset({'parental level of education_high school'}),0.49,0.196,0.098,0.2,1.0204081632653061,1.0,0.0019600000000000034,1.005,0.039215686274509866,0.16666666666666669,0.004975124378109457,0.35
frozenset({'parental level of education_high school'}),frozenset({'reading_cat_μέτριο'}),0.196,0.49,0.098,0.5,1.0204081632653061,1.0,0.0019600000000000034,1.02,0.024875621890547307,0.16666666666666669,0.01960784313725492,0.35
"frozenset({'reading_cat_μέτριο', 'math_cat_μέτριο'})",frozenset({'parental level of education_high school'}),0.325,0.196,0.065,0.2,1.0204081632653061,1.0,0.0012999999999999956,1.005,0.029629629629629527,0.1425438596491228,0.004975124378109457,0.26581632653061227
"frozenset({""parental level of education_associate's degree"", 'gender_male'})",frozenset({'reading_cat_μέτριο'}),0.106,0.49,0.053,0.5,1.0204081632653061,1.0,0.0010599999999999984,1.02,0.022371364653243814,0.0976058931860037,0.01960784313725492,0.3040816326530612
frozenset({'reading_cat_μέτριο'}),"frozenset({""parental level of education_associate's degree"", 'gender_male'})",0.49,0.106,0.053,0.10816326530612244,1.0204081632653061,1.0,0.0010599999999999984,1.0024256292906177,0.03921568627450975,0.0976058931860037,0.0024197598502488193,0.3040816326530612
frozenset({'parental level of education_high school'}),"frozenset({'reading_cat_μέτριο', 'math_cat_μέτριο'})",0.196,0.325,0.065,0.33163265306122447,1.020408163265306,1.0,0.0012999999999999956,1.0099236641221374,0.024875621890547175,0.1425438596491228,0.009826152683295493,0.26581632653061227
"frozenset({""parental level of education_associate's degree"", 'test preparation course_none'})",frozenset({'gender_female'}),0.14,0.518,0.074,0.5285714285714285,1.020408163265306,1.0,0.0014799999999999813,1.0224242424242422,0.023255813953488077,0.12671232876712327,0.021932424422050734,0.33571428571428563
frozenset({'gender_female'}),"frozenset({""parental level of education_associate's degree"", 'test preparation course_none'})",0.518,0.14,0.074,0.14285714285714285,1.020408163265306,1.0,0.0014799999999999813,1.0033333333333332,0.041493775933609436,0.12671232876712327,0.0033222591362126,0.33571428571428563
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'gender_female', 'race/ethnicity_group D'})",0.418,0.129,0.055,0.13157894736842105,1.0199918400652794,1.0,0.0010780000000000026,1.0029696969696968,0.03367697594501726,0.11178861788617887,0.0029609039821137106,0.2789677682578539
"frozenset({'gender_female', 'race/ethnicity_group D'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.129,0.418,0.055,0.4263565891472868,1.0199918400652794,1.0,0.0010780000000000026,1.0145675675675676,0.022502870264064345,0.11178861788617887,0.014358400596712746,0.2789677682578539
"frozenset({'writing_cat_μέτριο', 'parental level of education_some college'})",frozenset({'lunch_standard'}),0.114,0.645,0.075,0.6578947368421052,1.0199918400652794,1.0,0.001469999999999999,1.0376923076923075,0.022121896162528205,0.10964912280701754,0.03632320237212727,0.3870869033047735
frozenset({'lunch_standard'}),"frozenset({'writing_cat_μέτριο', 'parental level of education_some college'})",0.645,0.114,0.075,0.11627906976744186,1.0199918400652794,1.0,0.001469999999999999,1.002578947368421,0.05521126760563377,0.10964912280701754,0.0025723135072707164,0.3870869033047735
frozenset({'gender_male'}),frozenset({'parental level of education_some high school'}),0.482,0.179,0.088,0.1825726141078838,1.0199587380328705,1.0,0.0017220000000000013,1.0043705583756344,0.037776412776412804,0.15357766143106455,0.004351539717276267,0.33709636291986367
frozenset({'parental level of education_some high school'}),frozenset({'gender_male'}),0.179,0.482,0.088,0.49162011173184356,1.0199587380328705,1.0,0.0017220000000000013,1.0189230769230768,0.02383456981508141,0.15357766143106455,0.018571644269968286,0.33709636291986367
"frozenset({'math_cat_υψηλό', 'reading_cat_υψηλό', 'lunch_standard'})",frozenset({'gender_male'}),0.118,0.482,0.058,0.49152542372881364,1.0197622898938044,1.0,0.001124000000000007,1.0187333333333335,0.021972007193682207,0.10701107011070113,0.018388848897323656,0.30592868696814124
frozenset({'gender_male'}),"frozenset({'math_cat_υψηλό', 'reading_cat_υψηλό', 'lunch_standard'})",0.482,0.118,0.058,0.1203319502074689,1.0197622898938044,1.0,0.001124000000000007,1.0026509433962265,0.037411796032485914,0.10701107011070113,0.002643934475588325,0.30592868696814124
frozenset({'test preparation course_none'}),"frozenset({'writing_cat_μέτριο', 'race/ethnicity_group D'})",0.642,0.139,0.091,0.14174454828660435,1.0197449517021895,1.0,0.0017619999999999858,1.0031978221415607,0.0540855792252436,0.1318840579710145,0.003187628672014329,0.398210403639705
"frozenset({'writing_cat_μέτριο', 'race/ethnicity_group D'})",frozenset({'test preparation course_none'}),0.139,0.642,0.091,0.6546762589928057,1.0197449517021895,1.0,0.0017619999999999858,1.0367083333333331,0.022488545136628582,0.1318840579710145,0.035408544672641526,0.398210403639705
"frozenset({'race/ethnicity_group C', 'gender_male'})",frozenset({'test preparation course_none'}),0.139,0.642,0.091,0.6546762589928057,1.0197449517021895,1.0,0.0017619999999999858,1.0367083333333331,0.022488545136628582,0.1318840579710145,0.035408544672641526,0.398210403639705
frozenset({'test preparation course_none'}),"frozenset({'race/ethnicity_group C', 'gender_male'})",0.642,0.139,0.091,0.14174454828660435,1.0197449517021895,1.0,0.0017619999999999858,1.0031978221415607,0.0540855792252436,0.1318840579710145,0.003187628672014329,0.398210403639705
frozenset({'test preparation course_none'}),"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.642,0.139,0.091,0.14174454828660435,1.0197449517021895,1.0,0.0017619999999999858,1.0031978221415607,0.0540855792252436,0.1318840579710145,0.003187628672014329,0.398210403639705
"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'reading_cat_μέτριο'})",frozenset({'test preparation course_none'}),0.139,0.642,0.091,0.6546762589928057,1.0197449517021895,1.0,0.0017619999999999858,1.0367083333333331,0.022488545136628582,0.1318840579710145,0.035408544672641526,0.398210403639705
frozenset({'race/ethnicity_group C'}),"frozenset({'math_cat_χαμηλό', 'test preparation course_none'})",0.319,0.246,0.08,0.2507836990595611,1.0194459311364272,1.0,0.0015259999999999996,1.0063849372384936,0.028010279001468418,0.16494845360824745,0.006344428460956377,0.2879934755460407
"frozenset({'math_cat_χαμηλό', 'test preparation course_none'})",frozenset({'race/ethnicity_group C'}),0.246,0.319,0.08,0.3252032520325203,1.0194459311364272,1.0,0.0015259999999999996,1.0091927710843374,0.025298408488063653,0.16494845360824745,0.00910903382161573,0.2879934755460407
"frozenset({'test preparation course_completed', 'math_cat_μέτριο'})",frozenset({'lunch_standard'}),0.181,0.645,0.119,0.6574585635359116,1.0193156023812582,1.0,0.002254999999999993,1.0363709677419355,0.023137460112249958,0.1683168316831683,0.03509454517158189,0.42097734378345963
frozenset({'lunch_standard'}),"frozenset({'test preparation course_completed', 'math_cat_μέτριο'})",0.645,0.181,0.119,0.18449612403100774,1.0193156023812582,1.0,0.002254999999999993,1.004287072243346,0.053379098118120326,0.1683168316831683,0.004268771710632163,0.42097734378345963
"frozenset({'test preparation course_completed', 'gender_male'})",frozenset({'math_cat_μέτριο'}),0.174,0.485,0.086,0.4942528735632184,1.019078089821069,1.0,0.0016100000000000003,1.0182954545454546,0.0226645644461963,0.150087260034904,0.01796674478294837,0.3357862305960422
frozenset({'math_cat_μέτριο'}),"frozenset({'test preparation course_completed', 'gender_male'})",0.485,0.174,0.086,0.17731958762886596,1.0190780898210687,1.0,0.0016100000000000003,1.0040350877192983,0.03635132083991873,0.150087260034904,0.004018871221387375,0.3357862305960422
"frozenset({'gender_male', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'test preparation course_none'})",0.316,0.205,0.066,0.2088607594936709,1.018832973139858,1.0,0.0012200000000000127,1.00488,0.02702463228779047,0.14505494505494507,0.004856301249900504,0.26540598950293304
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'test preparation course_none'})","frozenset({'gender_male', 'lunch_standard'})",0.205,0.316,0.066,0.32195121951219513,1.018832973139858,1.0,0.0012200000000000127,1.0087769784172662,0.023251381741948022,0.14505494505494507,0.008700613321922701,0.26540598950293304
frozenset({'lunch_standard'}),"frozenset({""parental level of education_associate's degree"", 'test preparation course_none'})",0.645,0.14,0.092,0.14263565891472868,1.0188261351052048,1.0,0.0016999999999999932,1.0030741410488244,0.0520514390691976,0.13275613275613274,0.0030647196682891476,0.3998892580287928
"frozenset({""parental level of education_associate's degree"", 'test preparation course_none'})",frozenset({'lunch_standard'}),0.14,0.645,0.092,0.657142857142857,1.0188261351052046,1.0,0.0016999999999999932,1.0354166666666662,0.021486349848331563,0.13275613275613274,0.03420523138832961,0.3998892580287928
"frozenset({""parental level of education_associate's degree""})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})",0.222,0.283,0.064,0.2882882882882883,1.0186865310540223,1.0,0.0011740000000000084,1.0074303797468356,0.02357808483290505,0.14512471655328799,0.0073755764132333495,0.2572183490911406
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})","frozenset({""parental level of education_associate's degree""})",0.283,0.222,0.064,0.22614840989399296,1.0186865310540223,1.0,0.0011740000000000084,1.0053607305936074,0.025584030683403248,0.14512471655328799,0.00533214639330714,0.2572183490911406
frozenset({'test preparation course_none'}),"frozenset({'writing_cat_μέτριο', 'lunch_standard'})",0.642,0.338,0.221,0.3442367601246106,1.0184519530313922,1.0,0.00400399999999998,1.0095106888361043,0.050607952678277765,0.29117259552042163,0.009421087801526515,0.4990414569853822
"frozenset({'writing_cat_μέτριο', 'lunch_standard'})",frozenset({'test preparation course_none'}),0.338,0.642,0.221,0.6538461538461539,1.0184519530313922,1.0,0.00400399999999998,1.0342222222222222,0.02736804691665172,0.29117259552042163,0.033089815212720225,0.4990414569853822
"frozenset({""parental level of education_associate's degree""})","frozenset({'reading_cat_μέτριο', 'gender_female'})",0.222,0.261,0.059,0.26576576576576577,1.0182596389492942,1.0,0.0010579999999999964,1.0064907975460122,0.023049104614177956,0.1391509433962264,0.006448938789940137,0.24590970280625452
"frozenset({'reading_cat_μέτριο', 'gender_female'})","frozenset({""parental level of education_associate's degree""})",0.261,0.222,0.059,0.2260536398467433,1.0182596389492942,1.0,0.0010579999999999964,1.0052376237623764,0.0242654984977408,0.1391509433962264,0.005210333993243296,0.24590970280625452
frozenset({'gender_male'}),"frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο', 'math_cat_μέτριο'})",0.482,0.108,0.053,0.10995850622406639,1.0181343168895036,1.0,0.0009440000000000004,1.0022004662004662,0.03438478910177025,0.09869646182495345,0.0021956347803434857,0.30034962348240357
"frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο', 'math_cat_μέτριο'})",frozenset({'gender_male'}),0.108,0.482,0.053,0.49074074074074076,1.0181343168895036,1.0,0.0009440000000000004,1.0171636363636363,0.019967848379727565,0.09869646182495345,0.016874016874016934,0.30034962348240357
frozenset({'lunch_standard'}),"frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο', 'gender_female'})",0.645,0.099,0.065,0.10077519379844961,1.01793125048939,1.0,0.0011449999999999932,1.0019741379310345,0.0496208017334775,0.09572901325478644,0.0019702483889562777,0.37867042518205307
"frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο', 'gender_female'})",frozenset({'lunch_standard'}),0.099,0.645,0.065,0.6565656565656566,1.01793125048939,1.0,0.0011449999999999932,1.0336764705882353,0.019550926321181477,0.09572901325478644,0.03257931426945509,0.37867042518205307
"frozenset({'gender_female', 'lunch_standard'})",frozenset({'test preparation course_none'}),0.329,0.642,0.215,0.6534954407294833,1.0179056709181982,1.0,0.00378199999999998,1.0331754385964913,0.02621564482029584,0.28439153439153436,0.03211016963542814,0.49419320323078525
frozenset({'test preparation course_none'}),"frozenset({'gender_female', 'lunch_standard'})",0.642,0.329,0.215,0.3348909657320872,1.0179056709181982,1.0,0.00378199999999998,1.008857142857143,0.0491360270235154,0.28439153439153436,0.008779382611158262,0.49419320323078525
frozenset({'race/ethnicity_group C'}),"frozenset({'math_cat_μέτριο', 'test preparation course_none', 'gender_male'})",0.319,0.154,0.05,0.15673981191222572,1.0177909864430241,1.0,0.0008739999999999998,1.0032490706319703,0.02566813509544786,0.1182033096926714,0.0032385483596048706,0.2407075682937752
"frozenset({'math_cat_μέτριο', 'test preparation course_none', 'gender_male'})",frozenset({'race/ethnicity_group C'}),0.154,0.319,0.05,0.3246753246753247,1.0177909864430241,1.0,0.0008739999999999998,1.0084038461538463,0.020661938534278952,0.1182033096926714,0.00833381009592462,0.2407075682937752
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'gender_female'})",frozenset({'lunch_standard'}),0.227,0.645,0.149,0.6563876651982379,1.017655294880989,1.0,0.002584999999999976,1.0331410256410256,0.022443717061565904,0.20608575380359612,0.032077930135881304,0.4436977085681112
frozenset({'lunch_standard'}),"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'gender_female'})",0.645,0.227,0.149,0.23100775193798448,1.017655294880989,1.0,0.002584999999999976,1.0052116935483872,0.04887040362983224,0.20608575380359612,0.005184672623524543,0.4436977085681112
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",frozenset({'race/ethnicity_group C'}),0.191,0.319,0.062,0.32460732984293195,1.0175778364982193,1.0,0.0010709999999999956,1.0083023255813954,0.021352526017783716,0.13839285714285715,0.008233964527065991,0.2594823483070459
frozenset({'race/ethnicity_group C'}),"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",0.319,0.191,0.062,0.19435736677115986,1.017577836498219,1.0,0.0010709999999999956,1.0041673151750972,0.025365922978541885,0.13839285714285715,0.004150020730729121,0.2594823483070459
"frozenset({'gender_female', 'test preparation course_none'})","frozenset({'math_cat_χαμηλό', 'reading_cat_χαμηλό', 'writing_cat_χαμηλό'})",0.334,0.206,0.07,0.2095808383233533,1.0173827103075403,1.0,0.0011960000000000026,1.004530303030303,0.02565422565422571,0.14893617021276595,0.004509871943769923,0.2746933317830359
"frozenset({'math_cat_χαμηλό', 'reading_cat_χαμηλό', 'writing_cat_χαμηλό'})","frozenset({'gender_female', 'test preparation course_none'})",0.206,0.334,0.07,0.3398058252427185,1.0173827103075403,1.0,0.0011960000000000026,1.0087941176470587,0.021518531845987808,0.14893617021276595,0.008717455319397147,0.2746933317830359
"frozenset({'parental level of education_some college', 'lunch_standard'})",frozenset({'test preparation course_none'}),0.147,0.642,0.096,0.653061224489796,1.0172293216351962,1.0,0.0016260000000000024,1.0318823529411767,0.019856389214536956,0.13852813852813853,0.030897275111161908,0.4012969673850849
frozenset({'test preparation course_none'}),"frozenset({'parental level of education_some college', 'lunch_standard'})",0.642,0.147,0.096,0.14953271028037382,1.0172293216351962,1.0,0.0016260000000000024,1.002978021978022,0.04731145251396655,0.13852813852813853,0.002969179695631687,0.4012969673850849
"frozenset({'gender_male', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'test preparation course_none'})",0.316,0.308,0.099,0.31329113924050633,1.0171790235081375,1.0,0.0016720000000000068,1.0077050691244238,0.02469135802469146,0.18857142857142858,0.007646154971829968,0.3173598553345389
"frozenset({'writing_cat_μέτριο', 'test preparation course_none'})","frozenset({'gender_male', 'lunch_standard'})",0.308,0.316,0.099,0.32142857142857145,1.0171790235081375,1.0,0.0016720000000000068,1.0079999999999998,0.024405908798972482,0.18857142857142858,0.007936507936507967,0.3173598553345389
frozenset({'gender_female'}),"frozenset({""parental level of education_associate's degree"", 'reading_cat_μέτριο'})",0.518,0.112,0.059,0.1138996138996139,1.0169608383894098,1.0,0.0009839999999999918,1.0021437908496733,0.034601589422603274,0.1033274956217163,0.002139204841907536,0.3203426640926641
"frozenset({""parental level of education_associate's degree"", 'reading_cat_μέτριο'})",frozenset({'gender_female'}),0.112,0.518,0.059,0.5267857142857143,1.0169608383894098,1.0,0.0009839999999999918,1.018566037735849,0.018781493357764387,0.1033274956217163,0.018227622999407233,0.3203426640926641
"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'gender_female'})",frozenset({'lunch_standard'}),0.093,0.645,0.061,0.6559139784946236,1.0169208968908894,1.0,0.001014999999999995,1.0317187499999998,0.018345473277061743,0.09010339734121121,0.030743601393305946,0.3752438109527382
frozenset({'lunch_standard'}),"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'gender_female'})",0.645,0.093,0.061,0.09457364341085271,1.0169208968908894,1.0,0.001014999999999995,1.0017380136986302,0.04687139228815494,0.09010339734121121,0.0017349982479081725,0.3752438109527382
frozenset({'reading_cat_υψηλό'}),frozenset({'parental level of education_some college'}),0.235,0.226,0.054,0.2297872340425532,1.0167576727546601,1.0,0.0008900000000000019,1.0049171270718231,0.02154442023723074,0.1326781326781327,0.004893067238440805,0.23436264356994915
frozenset({'parental level of education_some college'}),frozenset({'reading_cat_υψηλό'}),0.226,0.235,0.054,0.23893805309734512,1.0167576727546601,1.0,0.0008900000000000019,1.0051744186046512,0.021293903722844336,0.1326781326781327,0.00514778182659494,0.23436264356994915
"frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο'})",frozenset({'lunch_free/reduced'}),0.169,0.355,0.061,0.36094674556213013,1.0167513959496624,1.0,0.001004999999999999,1.0093055555555555,0.01982600461620404,0.13174946004319654,0.009219760561442095,0.26638886573881154
frozenset({'lunch_free/reduced'}),"frozenset({'race/ethnicity_group C', 'reading_cat_μέτριο'})",0.355,0.169,0.061,0.17183098591549295,1.0167513959496624,1.0,0.001004999999999999,1.0034183673469388,0.025543271063667532,0.13174946004319654,0.0034067219199674366,0.26638886573881154
"frozenset({'lunch_free/reduced', 'race/ethnicity_group D'})",frozenset({'test preparation course_none'}),0.095,0.642,0.062,0.6526315789473685,1.0165600918183308,1.0,0.001009999999999997,1.0306060606060607,0.018000356442701784,0.09185185185185185,0.029697147897677197,0.37460239383505495
frozenset({'test preparation course_none'}),"frozenset({'lunch_free/reduced', 'race/ethnicity_group D'})",0.642,0.095,0.062,0.09657320872274143,1.0165600918183308,1.0,0.001009999999999997,1.001741379310345,0.04550369435934389,0.09185185185185185,0.0017383521798247832,0.37460239383505495
"frozenset({'parental level of education_some college', 'test preparation course_none'})",frozenset({'gender_male'}),0.149,0.482,0.073,0.4899328859060403,1.0164582695146065,1.0,0.0011820000000000025,1.0155526315789476,0.019026769473463975,0.13082437275985662,0.015314451556062352,0.3206925840318583
frozenset({'gender_male'}),"frozenset({'parental level of education_some college', 'test preparation course_none'})",0.482,0.149,0.073,0.15145228215767634,1.0164582695146063,1.0,0.0011820000000000025,1.0028899755501222,0.03125826413497653,0.13082437275985662,0.002881647658844118,0.3206925840318583
frozenset({'gender_male'}),frozenset({'lunch_standard'}),0.482,0.645,0.316,0.6556016597510373,1.016436681784554,1.0,0.005110000000000003,1.0307831325301204,0.031217926787547062,0.3896424167694205,0.029863830284612196,0.5727620701855962
frozenset({'lunch_standard'}),frozenset({'gender_male'}),0.645,0.482,0.316,0.48992248062015503,1.016436681784554,1.0,0.005110000000000003,1.0155319148936168,0.04555179176323768,0.3896424167694205,0.015294364131573451,0.5727620701855962
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})",frozenset({'gender_female'}),0.283,0.518,0.149,0.5265017667844524,1.0164126771900626,1.0,0.0024059999999999915,1.0179552238805971,0.02252113111117343,0.2285276073619632,0.01763852029969369,0.40707327721462
frozenset({'gender_female'}),"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})",0.518,0.283,0.149,0.2876447876447876,1.0164126771900623,1.0,0.0024059999999999915,1.0065203252032522,0.03350135063633061,0.2285276073619632,0.006478085975993934,0.40707327721462
"frozenset({'race/ethnicity_group C', 'gender_female'})",frozenset({'lunch_standard'}),0.18,0.645,0.118,0.6555555555555556,1.016365202411714,1.0,0.001899999999999999,1.0306451612903225,0.019636213311285643,0.1669024045261669,0.029733959311424057,0.419250645994832
frozenset({'lunch_standard'}),"frozenset({'race/ethnicity_group C', 'gender_female'})",0.645,0.18,0.118,0.18294573643410852,1.016365202411714,1.0,0.001899999999999999,1.0036053130929792,0.04535688708522318,0.1669024045261669,0.003592361505010404,0.419250645994832
"frozenset({'test preparation course_none', 'race/ethnicity_group D'})",frozenset({'lunch_standard'}),0.18,0.645,0.118,0.6555555555555556,1.016365202411714,1.0,0.001899999999999999,1.0306451612903225,0.019636213311285643,0.1669024045261669,0.029733959311424057,0.419250645994832
frozenset({'lunch_standard'}),"frozenset({'test preparation course_none', 'race/ethnicity_group D'})",0.645,0.18,0.118,0.18294573643410852,1.016365202411714,1.0,0.001899999999999999,1.0036053130929792,0.04535688708522318,0.1669024045261669,0.003592361505010404,0.419250645994832
frozenset({'parental level of education_some college'}),"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})",0.226,0.283,0.065,0.28761061946902655,1.0162919415866662,1.0,0.0010420000000000013,1.006472049689441,0.020711588153448642,0.1463963963963964,0.006430431616494529,0.25864629913380655
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'lunch_standard'})",frozenset({'parental level of education_some college'}),0.283,0.226,0.065,0.2296819787985866,1.0162919415866662,1.0,0.0010420000000000013,1.0047798165137616,0.022358116081965477,0.1463963963963964,0.004757078551145462,0.25864629913380655
frozenset({'parental level of education_some college'}),"frozenset({'test preparation course_none', 'lunch_standard'})",0.226,0.418,0.096,0.4247787610619469,1.0162171317271458,1.0,0.0015320000000000056,1.0117846153846155,0.02061800172265296,0.1751824817518248,0.011647355776541088,0.3272219164161409
"frozenset({'test preparation course_none', 'lunch_standard'})",frozenset({'parental level of education_some college'}),0.418,0.226,0.096,0.22966507177033493,1.0162171317271458,1.0,0.0015320000000000056,1.0047577639751553,0.027419816723940532,0.1751824817518248,0.004735234845393962,0.3272219164161409
frozenset({'gender_female'}),"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",0.518,0.152,0.08,0.15444015444015444,1.016053647632595,1.0,0.0012640000000000012,1.0028858447488584,0.0327800829875519,0.13559322033898302,0.0028775406133896707,0.3403779719569193
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",frozenset({'gender_female'}),0.152,0.518,0.08,0.5263157894736842,1.016053647632595,1.0,0.0012640000000000012,1.0175555555555555,0.018632075471698133,0.13559322033898302,0.01725267525660615,0.3403779719569193
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})","frozenset({'gender_male', 'test preparation course_none', 'lunch_standard'})",0.32,0.203,0.066,0.20625000000000002,1.0160098522167489,1.0,0.0010399999999999993,1.0040944881889764,0.023172905525846686,0.14442013129102846,0.004077791718946052,0.2656865763546798
"frozenset({'gender_male', 'test preparation course_none', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})",0.203,0.32,0.066,0.3251231527093596,1.0160098522167487,1.0,0.0010399999999999993,1.0075912408759125,0.019771111364586886,0.14442013129102846,0.007534048101999399,0.2656865763546798
frozenset({'lunch_free/reduced'}),"frozenset({'test preparation course_completed', 'writing_cat_μέτριο'})",0.355,0.183,0.066,0.1859154929577465,1.0159316555068116,1.0,0.0010350000000000081,1.0035813148788926,0.02431289640591985,0.13983050847457626,0.0035685348320030758,0.2732856153313323
"frozenset({'test preparation course_completed', 'writing_cat_μέτριο'})",frozenset({'lunch_free/reduced'}),0.183,0.355,0.066,0.36065573770491804,1.0159316555068114,1.0,0.0010350000000000081,1.0088461538461537,0.019194391899410412,0.13983050847457626,0.008768585589020252,0.2732856153313323
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})",frozenset({'test preparation course_completed'}),0.418,0.358,0.152,0.36363636363636365,1.0157440325038092,1.0,0.002355999999999997,1.008857142857143,0.026632302405498243,0.24358974358974358,0.008779382611158352,0.3941086846114779
frozenset({'test preparation course_completed'}),"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.358,0.418,0.152,0.4245810055865922,1.0157440325038092,1.0,0.002355999999999997,1.0114368932038835,0.024143302180685326,0.24358974358974358,0.0113075697364127,0.3941086846114779
"frozenset({'race/ethnicity_group C', 'gender_male'})","frozenset({'test preparation course_none', 'lunch_standard'})",0.139,0.418,0.059,0.4244604316546762,1.0154555781212349,1.0,0.000897999999999996,1.011225,0.017677513336876633,0.11847389558232932,0.011100398032089674,0.28280437850676393
"frozenset({'test preparation course_none', 'lunch_standard'})","frozenset({'race/ethnicity_group C', 'gender_male'})",0.418,0.139,0.059,0.14114832535885166,1.0154555781212349,1.0,0.000897999999999996,1.0025013927576603,0.026151785194245325,0.11847389558232932,0.002495151404008884,0.28280437850676393
"frozenset({'reading_cat_μέτριο', 'lunch_standard'})",frozenset({'gender_female'}),0.327,0.518,0.172,0.5259938837920489,1.0154322080927585,1.0,0.0026139999999999775,1.0168645161290322,0.02258198279138859,0.2555720653789004,0.016584821145329682,0.42902010791919043
frozenset({'gender_female'}),"frozenset({'reading_cat_μέτριο', 'lunch_standard'})",0.518,0.327,0.172,0.332046332046332,1.0154322080927582,1.0,0.0026139999999999775,1.0075549132947978,0.0315304448518766,0.2555720653789004,0.007498264556213938,0.42902010791919043
frozenset({'test preparation course_none'}),frozenset({'race/ethnicity_group A'}),0.642,0.089,0.058,0.09034267912772585,1.0150862823339983,1.0,0.0008620000000000017,1.0014760273972603,0.04151415912155662,0.08618127786032691,0.0014738519514005018,0.37101403619307644
"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο', 'race/ethnicity_group D'})",frozenset({'test preparation course_none'}),0.089,0.642,0.058,0.651685393258427,1.0150862823339983,1.0,0.0008620000000000017,1.0278064516129033,0.016314016427571097,0.08618127786032691,0.02705417111292455,0.37101403619307644
frozenset({'test preparation course_none'}),"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο', 'race/ethnicity_group D'})",0.642,0.089,0.058,0.09034267912772585,1.0150862823339983,1.0,0.0008620000000000017,1.0014760273972603,0.04151415912155662,0.08618127786032691,0.0014738519514005018,0.37101403619307644
frozenset({'race/ethnicity_group A'}),frozenset({'test preparation course_none'}),0.089,0.642,0.058,0.651685393258427,1.0150862823339983,1.0,0.0008620000000000017,1.0278064516129033,0.016314016427571097,0.08618127786032691,0.02705417111292455,0.37101403619307644
"frozenset({'test preparation course_completed', 'gender_female'})","frozenset({'math_cat_μέτριο', 'lunch_standard'})",0.184,0.332,0.062,0.33695652173913043,1.0149292823467784,1.0,0.0009119999999999961,1.0074754098360654,0.018026565464895557,0.13656387665198239,0.007419942723249125,0.26185175484546885
"frozenset({'math_cat_μέτριο', 'lunch_standard'})","frozenset({'test preparation course_completed', 'gender_female'})",0.332,0.184,0.062,0.18674698795180722,1.0149292823467784,1.0,0.0009119999999999961,1.003377777777778,0.02202047517867482,0.13656387665198239,0.0033664068036853216,0.26185175484546885
"frozenset({'gender_male', 'test preparation course_none'})",frozenset({'math_cat_υψηλό'}),0.308,0.176,0.055,0.17857142857142858,1.0146103896103897,1.0,0.000792000000000001,1.0031304347826089,0.020809248554913323,0.12820512820512822,0.003120665742024982,0.2455357142857143
frozenset({'math_cat_υψηλό'}),"frozenset({'gender_male', 'test preparation course_none'})",0.176,0.308,0.055,0.3125,1.0146103896103895,1.0,0.000792000000000001,1.0065454545454544,0.017475728155339827,0.12820512820512822,0.006502890173410411,0.2455357142857143
"frozenset({'gender_male', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.316,0.418,0.134,0.42405063291139244,1.0144751983526135,1.0,0.0019120000000000248,1.0105054945054948,0.02086060923452938,0.22333333333333336,0.010396276480055774,0.37231239779540914
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'gender_male', 'lunch_standard'})",0.418,0.316,0.134,0.32057416267942584,1.0144751983526135,1.0,0.0019120000000000248,1.0067323943661972,0.024516592296251017,0.22333333333333336,0.006687372338341874,0.37231239779540914
frozenset({'gender_male'}),"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'lunch_standard'})",0.482,0.225,0.11,0.2282157676348548,1.0142923005993547,1.0,0.0015499999999999958,1.0041666666666669,0.027202527202527128,0.18425460636515914,0.004149377593361017,0.35855232826187183
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο', 'lunch_standard'})",frozenset({'gender_male'}),0.225,0.482,0.11,0.4888888888888889,1.0142923005993545,1.0,0.0015499999999999958,1.0134782608695654,0.018181818181818132,0.18425460636515914,0.013299013299013299,0.35855232826187183
frozenset({'math_cat_μέτριο'}),"frozenset({'gender_female', 'reading_cat_υψηλό', 'lunch_standard'})",0.485,0.12,0.059,0.12164948453608247,1.013745704467354,1.0,0.0008000000000000021,1.0018779342723005,0.026328780648346293,0.10805860805860805,0.001874414245548269,0.30665807560137454
"frozenset({'gender_female', 'reading_cat_υψηλό', 'lunch_standard'})",frozenset({'math_cat_μέτριο'}),0.12,0.485,0.059,0.49166666666666664,1.013745704467354,1.0,0.0008000000000000021,1.0131147540983607,0.015408320493066296,0.10805860805860805,0.012944983818770201,0.30665807560137454
"frozenset({'math_cat_μέτριο', 'parental level of education_some college', 'test preparation course_none'})",frozenset({'lunch_standard'}),0.078,0.645,0.051,0.6538461538461539,1.0137149672033392,1.0,0.0006899999999999962,1.0255555555555556,0.014673982391221049,0.07589285714285715,0.02491874322860236,0.36645796064400715
frozenset({'lunch_standard'}),"frozenset({'math_cat_μέτριο', 'parental level of education_some college', 'test preparation course_none'})",0.645,0.078,0.051,0.07906976744186046,1.0137149672033392,1.0,0.0006899999999999962,1.0011616161616161,0.038111019055509315,0.07589285714285715,0.0011602683751198092,0.36645796064400715
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({""parental level of education_bachelor's degree""})",0.418,0.118,0.05,0.11961722488038279,1.0137052955964643,1.0,0.0006760000000000099,1.0018369565217393,0.023230240549828515,0.10288065843621398,0.0018335882997537356,0.27167301921985243
"frozenset({""parental level of education_bachelor's degree""})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.118,0.418,0.05,0.42372881355932207,1.0137052955964643,1.0,0.0006760000000000099,1.0099411764705883,0.015328798185941269,0.10288065843621398,0.00984332226687644,0.27167301921985243
"frozenset({'gender_male', 'test preparation course_none', 'lunch_standard'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.203,0.418,0.086,0.42364532019704426,1.0135055507106323,1.0,0.0011459999999999942,1.0097948717948717,0.016719675527413765,0.16074766355140185,0.009699862881519378,0.3146934734956513
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'gender_male', 'test preparation course_none', 'lunch_standard'})",0.418,0.203,0.086,0.20574162679425836,1.0135055507106323,1.0,0.0011459999999999942,1.0034518072289156,0.022896187964516788,0.16074766355140185,0.003439933242482247,0.3146934734956513
"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'reading_cat_μέτριο', 'gender_female'})",frozenset({'test preparation course_none'}),0.083,0.642,0.054,0.6506024096385542,1.0133993919603648,1.0,0.0007139999999999924,1.0246206896551724,0.014418999151823427,0.0804769001490313,0.02402907720266533,0.36735727958563225
frozenset({'test preparation course_none'}),"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'reading_cat_μέτριο', 'gender_female'})",0.642,0.083,0.054,0.08411214953271028,1.0133993919603648,1.0,0.0007139999999999924,1.0012142857142858,0.03693358162631867,0.0804769001490313,0.001212813012770198,0.36735727958563225
frozenset({'writing_cat_μέτριο'}),"frozenset({'race/ethnicity_group C', 'lunch_standard'})",0.491,0.205,0.102,0.20773930753564154,1.0133624757836173,1.0,0.001344999999999999,1.0034575835475579,0.025906236757964465,0.1717171717171717,0.0034456698561528996,0.3526501415726988
"frozenset({'race/ethnicity_group C', 'lunch_standard'})",frozenset({'writing_cat_μέτριο'}),0.205,0.491,0.102,0.4975609756097561,1.0133624757836173,1.0,0.001344999999999999,1.0130582524271845,0.016586508817363413,0.1717171717171717,0.012889932435670144,0.3526501415726988
frozenset({'gender_female'}),"frozenset({'test preparation course_completed', 'math_cat_μέτριο'})",0.518,0.181,0.095,0.1833976833976834,1.0132468696004608,1.0,0.001242000000000007,1.002936170212766,0.027123826162917822,0.1572847682119205,0.002927574356145785,0.354129780925361
"frozenset({'test preparation course_completed', 'math_cat_μέτριο'})",frozenset({'gender_female'}),0.181,0.518,0.095,0.5248618784530387,1.0132468696004606,1.0,0.001242000000000007,1.0144418604651162,0.015962984384037107,0.1572847682119205,0.014236262350702593,0.354129780925361
"frozenset({'test preparation course_completed', 'reading_cat_μέτριο'})",frozenset({'gender_male'}),0.172,0.482,0.084,0.4883720930232559,1.013220110006755,1.0,0.0010960000000000136,1.0124545454545457,0.015757994018863776,0.1473684210526316,0.012301337882733436,0.3313229759722089
frozenset({'gender_male'}),"frozenset({'test preparation course_completed', 'reading_cat_μέτριο'})",0.482,0.172,0.084,0.17427385892116184,1.013220110006755,1.0,0.0010960000000000136,1.0027537688442212,0.025188453759882642,0.1473684210526316,0.002746206426523984,0.3313229759722089
frozenset({'math_cat_μέτριο'}),"frozenset({'writing_cat_υψηλό', 'gender_female', 'lunch_standard'})",0.485,0.116,0.057,0.1175257731958763,1.0131532172058302,1.0,0.0007399999999999976,1.0017289719626168,0.025208652699710357,0.1047794117647059,0.0017259877781406058,0.3044525417703519
"frozenset({'writing_cat_υψηλό', 'gender_female', 'lunch_standard'})",frozenset({'math_cat_μέτριο'}),0.116,0.485,0.057,0.49137931034482757,1.01315321720583,1.0,0.0007399999999999976,1.012542372881356,0.014686036357862935,0.1047794117647059,0.012387010378305984,0.3044525417703519
frozenset({'lunch_standard'}),"frozenset({""parental level of education_associate's degree""})",0.645,0.222,0.145,0.22480620155038758,1.0126405475242684,1.0,0.0018099999999999783,1.00362,0.035162700339970446,0.20083102493074792,0.0036069428668220782,0.43897967735177035
"frozenset({""parental level of education_associate's degree""})",frozenset({'lunch_standard'}),0.222,0.645,0.145,0.6531531531531531,1.0126405475242684,1.0,0.0018099999999999783,1.0235064935064935,0.01604467689034641,0.20083102493074792,0.02296662860043134,0.43897967735177035
"frozenset({""parental level of education_associate's degree""})","frozenset({'writing_cat_μέτριο', 'gender_male'})",0.222,0.227,0.051,0.22972972972972971,1.012025241100131,1.0,0.0006059999999999954,1.0035438596491228,0.015272947225162445,0.1281407035175879,0.0035313450578650808,0.22719966662697938
"frozenset({'writing_cat_μέτριο', 'gender_male'})","frozenset({""parental level of education_associate's degree""})",0.227,0.222,0.051,0.22466960352422904,1.012025241100131,1.0,0.0006059999999999954,1.0034431818181817,0.015371737310706832,0.1281407035175879,0.003431366997723704,0.22719966662697938
"frozenset({'test preparation course_completed', 'math_cat_μέτριο', 'gender_female'})",frozenset({'lunch_standard'}),0.095,0.645,0.062,0.6526315789473685,1.0118319053447573,1.0,0.0007249999999999965,1.021969696969697,0.012921047941543334,0.09144542772861358,0.021497405485544886,0.3743778049775602
frozenset({'lunch_standard'}),"frozenset({'test preparation course_completed', 'math_cat_μέτριο', 'gender_female'})",0.645,0.095,0.062,0.09612403100775194,1.0118319053447573,1.0,0.0007249999999999965,1.0012435677530016,0.03293957292139921,0.09144542772861358,0.0012420232129855642,0.3743778049775602
"frozenset({'math_cat_μέτριο', 'gender_male'})","frozenset({'writing_cat_χαμηλό', 'test preparation course_none'})",0.24,0.243,0.059,0.24583333333333332,1.0116598079561043,1.0,0.00068,1.0037569060773481,0.015165031222123105,0.1391509433962264,0.00374284456186701,0.2443158436213992
"frozenset({'writing_cat_χαμηλό', 'test preparation course_none'})","frozenset({'math_cat_μέτριο', 'gender_male'})",0.243,0.24,0.059,0.24279835390946503,1.0116598079561043,1.0,0.00068,1.003695652173913,0.015225130421153976,0.1391509433962264,0.0036820446177171506,0.2443158436213992
frozenset({'test preparation course_none'}),"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', ""parental level of education_associate's degree""})",0.642,0.097,0.063,0.09813084112149532,1.0116581558917044,1.0,0.0007259999999999975,1.0012538860103626,0.03218941207768013,0.09319526627218935,0.0012523157491642538,0.37380768860198477
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', ""parental level of education_associate's degree""})",frozenset({'test preparation course_none'}),0.097,0.642,0.063,0.6494845360824743,1.0116581558917044,1.0,0.0007259999999999975,1.0213529411764706,0.012761693824816704,0.09319526627218935,0.020906525370039765,0.37380768860198477
frozenset({'lunch_standard'}),"frozenset({'test preparation course_completed', 'writing_cat_μέτριο', 'math_cat_μέτριο'})",0.645,0.115,0.075,0.11627906976744186,1.0111223458038423,1.0,0.0008249999999999924,1.0014473684210525,0.030985915492957462,0.1094890510948905,0.0014452765733806259,0.38422649140546
"frozenset({'test preparation course_completed', 'writing_cat_μέτριο', 'math_cat_μέτριο'})",frozenset({'lunch_standard'}),0.115,0.645,0.075,0.6521739130434782,1.011122345803842,1.0,0.0008249999999999924,1.0206249999999997,0.01242937853107333,0.1094890510948905,0.020208205756276458,0.38422649140546
frozenset({'gender_female'}),"frozenset({'writing_cat_μέτριο', 'lunch_standard'})",0.518,0.338,0.177,0.3416988416988417,1.0109433186356263,1.0,0.0019159999999999733,1.0056187683284457,0.022458213188925307,0.26067746686303384,0.0055873741674345315,0.432683740376048
"frozenset({'writing_cat_μέτριο', 'lunch_standard'})",frozenset({'gender_female'}),0.338,0.518,0.177,0.5236686390532543,1.010943318635626,1.0,0.0019159999999999733,1.0119006211180122,0.016351750388311173,0.26067746686303384,0.011760661936212283,0.432683740376048
frozenset({'reading_cat_μέτριο'}),frozenset({'test preparation course_none'}),0.49,0.642,0.318,0.6489795918367347,1.010871638374976,1.0,0.0034199999999999786,1.0198837209302325,0.021087680355160798,0.3906633906633906,0.01949606658305779,0.5721533473202365
frozenset({'test preparation course_none'}),frozenset({'reading_cat_μέτριο'}),0.642,0.49,0.318,0.4953271028037383,1.010871638374976,1.0,0.0034199999999999786,1.0105555555555554,0.030041108885843597,0.3906633906633906,0.010445299615173188,0.5721533473202365
"frozenset({'math_cat_μέτριο', 'test preparation course_none'})","frozenset({'lunch_free/reduced', 'gender_male'})",0.304,0.166,0.051,0.16776315789473684,1.0106214331008243,1.0,0.0005359999999999948,1.0021185770750987,0.015100292990759379,0.12171837708830549,0.002114098195128091,0.23749603677869371
"frozenset({'lunch_free/reduced', 'gender_male'})","frozenset({'math_cat_μέτριο', 'test preparation course_none'})",0.166,0.304,0.051,0.3072289156626506,1.0106214331008243,1.0,0.0005359999999999948,1.0046608695652175,0.012601683359194876,0.12171837708830549,0.004639246641739368,0.23749603677869371
"frozenset({""parental level of education_associate's degree"", 'reading_cat_μέτριο'})",frozenset({'lunch_standard'}),0.112,0.645,0.073,0.6517857142857142,1.0105204872646731,1.0,0.0007599999999999968,1.0194871794871792,0.011724052819943183,0.10672514619883039,0.01911468812877232,0.3824820044296788
frozenset({'lunch_standard'}),"frozenset({""parental level of education_associate's degree"", 'reading_cat_μέτριο'})",0.645,0.112,0.073,0.1131782945736434,1.0105204872646731,1.0,0.0007599999999999968,1.0013286713286713,0.029326644800308582,0.10672514619883039,0.0013269083036524696,0.3824820044296788
"frozenset({""parental level of education_associate's degree""})","frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",0.222,0.223,0.05,0.22522522522522523,1.0099785884539247,1.0,0.0004940000000000014,1.002872093023256,0.012699228791773813,0.12658227848101267,0.002863867728732593,0.22472023593099827
"frozenset({'reading_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})","frozenset({""parental level of education_associate's degree""})",0.223,0.222,0.05,0.2242152466367713,1.0099785884539247,1.0,0.0004940000000000014,1.0028554913294798,0.01271557271557275,0.12658227848101267,0.0028473607156443525,0.22472023593099827
"frozenset({'race/ethnicity_group B', 'lunch_standard'})",frozenset({'writing_cat_μέτριο'}),0.121,0.491,0.06,0.49586776859504134,1.0099139889919375,1.0,0.0005889999999999992,1.009655737704918,0.011167993932499039,0.10869565217391303,0.009563396060984958,0.3090336806315329
frozenset({'writing_cat_μέτριο'}),"frozenset({'race/ethnicity_group B', 'lunch_standard'})",0.491,0.121,0.06,0.12219959266802444,1.0099139889919375,1.0,0.0005889999999999992,1.0013665893271462,0.01928618205631956,0.10869565217391303,0.0013647243094703533,0.3090336806315329
"frozenset({'math_cat_μέτριο', 'race/ethnicity_group D'})",frozenset({'test preparation course_none'}),0.145,0.642,0.094,0.6482758620689656,1.0097754860887314,1.0,0.000910000000000008,1.0178431372549022,0.011322632823192834,0.13564213564213562,0.01753034097476418,0.3973466537759158
frozenset({'test preparation course_none'}),"frozenset({'math_cat_μέτριο', 'race/ethnicity_group D'})",0.642,0.145,0.094,0.14641744548286603,1.0097754860887314,1.0,0.000910000000000008,1.0016605839416057,0.027041483418519197,0.13564213564213562,0.00165783097411233,0.3973466537759158
frozenset({'race/ethnicity_group C'}),frozenset({'writing_cat_υψηλό'}),0.319,0.208,0.067,0.21003134796238246,1.0097660959729926,1.0,0.0006480000000000097,1.0025714285714287,0.014202117167466841,0.14565217391304347,0.0025648332858364494,0.26607336628888356
frozenset({'writing_cat_υψηλό'}),frozenset({'race/ethnicity_group C'}),0.208,0.319,0.067,0.32211538461538464,1.0097660959729926,1.0,0.0006480000000000097,1.004595744680851,0.012211668928087019,0.14565217391304347,0.0045747204337512936,0.26607336628888356
"frozenset({'gender_male', 'race/ethnicity_group B'})",frozenset({'lunch_standard'}),0.086,0.645,0.056,0.6511627906976745,1.0095547142599604,1.0,0.0005300000000000027,1.0176666666666667,0.010354798374492082,0.08296296296296297,0.01735997379626605,0.3689922480620155
frozenset({'lunch_standard'}),"frozenset({'gender_male', 'race/ethnicity_group B'})",0.645,0.086,0.056,0.08682170542635659,1.0095547142599604,1.0,0.0005300000000000027,1.000899830220713,0.026659959758551444,0.08296296296296297,0.0008990212542194691,0.3689922480620155
frozenset({'test preparation course_none'}),frozenset({'lunch_standard'}),0.642,0.645,0.418,0.6510903426791277,1.0094423917505855,1.0,0.003909999999999969,1.017455357142857,0.026128678730854355,0.4810126582278481,0.017155894870782167,0.6495761790915018
frozenset({'lunch_standard'}),frozenset({'test preparation course_none'}),0.645,0.642,0.418,0.6480620155038759,1.0094423917505855,1.0,0.003909999999999969,1.0172246696035239,0.026349484466608054,0.4810126582278481,0.016933004200770565,0.6495761790915018
"frozenset({""parental level of education_associate's degree""})",frozenset({'writing_cat_μέτριο'}),0.222,0.491,0.11,0.4954954954954955,1.0091557953065082,1.0,0.0009979999999999989,1.0089107142857143,0.011661603178312677,0.1824212271973466,0.00883201472592435,0.3597640410267702
frozenset({'writing_cat_μέτριο'}),"frozenset({""parental level of education_associate's degree""})",0.491,0.222,0.11,0.2240325865580448,1.0091557953065082,1.0,0.0009979999999999989,1.0026194225721785,0.017824611537774582,0.1824212271973466,0.0026125791234509095,0.3597640410267702
frozenset({'gender_female'}),"frozenset({""parental level of education_associate's degree""})",0.518,0.222,0.116,0.22393822393822393,1.0087307384604682,1.0,0.0010040000000000049,1.002497512437811,0.01795678924023474,0.1858974358974359,0.00249129040902819,0.3732303732303732
"frozenset({""parental level of education_associate's degree""})",frozenset({'gender_female'}),0.222,0.518,0.116,0.5225225225225225,1.0087307384604682,1.0,0.0010040000000000049,1.0094716981132075,0.011124900274798387,0.1858974358974359,0.009382826810212653,0.3732303732303732
"frozenset({'math_cat_χαμηλό', 'gender_female'})","frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})",0.205,0.266,0.055,0.2682926829268293,1.0086191087474785,1.0,0.0004699999999999982,1.0031333333333334,0.01074899942824467,0.13221153846153846,0.003123546221838243,0.23752980011003116
"frozenset({'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})","frozenset({'math_cat_χαμηλό', 'gender_female'})",0.266,0.205,0.055,0.20676691729323307,1.0086191087474785,1.0,0.0004699999999999982,1.0022274881516589,0.011642308645033397,0.13221153846153846,0.0022225374757648846,0.23752980011003116
frozenset({'lunch_standard'}),frozenset({'parental level of education_some college'}),0.645,0.226,0.147,0.22790697674418603,1.0084379501955134,1.0,0.0012299999999999811,1.0024698795180722,0.023569991376832064,0.20303867403314918,0.002463794243134397,0.43917472731014606
frozenset({'parental level of education_some college'}),frozenset({'lunch_standard'}),0.226,0.645,0.147,0.6504424778761061,1.0084379501955132,1.0,0.0012299999999999811,1.015569620253164,0.010810525760691709,0.20303867403314918,0.015330923594665008,0.43917472731014606
frozenset({'test preparation course_completed'}),frozenset({'gender_male'}),0.358,0.482,0.174,0.4860335195530726,1.0083682978279516,1.0,0.0014440000000000008,1.0078478260869566,0.012926558527589794,0.26126126126126126,0.007786717283923993,0.4235146850877396
frozenset({'gender_male'}),frozenset({'test preparation course_completed'}),0.482,0.358,0.174,0.36099585062240663,1.0083682978279516,1.0,0.0014440000000000008,1.0046883116883119,0.016020947055429825,0.26126126126126126,0.004666433991287614,0.4235146850877396
frozenset({'test preparation course_completed'}),"frozenset({'race/ethnicity_group C', 'lunch_standard'})",0.358,0.205,0.074,0.20670391061452514,1.0083117590952446,1.0,0.0006099999999999994,1.0021478873239438,0.01283994274648479,0.15132924335378325,0.002143283791855542,0.2838397601853113
"frozenset({'race/ethnicity_group C', 'lunch_standard'})",frozenset({'test preparation course_completed'}),0.205,0.358,0.074,0.36097560975609755,1.0083117590952446,1.0,0.0006099999999999994,1.0046564885496183,0.01036885942546319,0.15132924335378325,0.004634906162145737,0.2838397601853113
frozenset({'race/ethnicity_group E'}),"frozenset({'test preparation course_none', 'lunch_standard'})",0.14,0.418,0.059,0.4214285714285714,1.0082023239917977,1.0,0.0004799999999999943,1.005925925925926,0.009459992116673125,0.11823647294589176,0.005891016200294487,0.28128844839371153
"frozenset({'test preparation course_none', 'lunch_standard'})",frozenset({'race/ethnicity_group E'}),0.418,0.14,0.059,0.14114832535885166,1.0082023239917974,1.0,0.0004799999999999943,1.0013370473537604,0.013978682509173345,0.11823647294589176,0.0013352620451763354,0.28128844839371153
frozenset({'gender_female'}),"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})",0.518,0.18,0.094,0.18146718146718147,1.008151008151008,1.0,0.0007599999999999968,1.0017924528301887,0.016774079632735868,0.1556291390728477,0.001789245691684722,0.35184470184470185
"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο', 'test preparation course_none'})",frozenset({'gender_female'}),0.18,0.518,0.094,0.5222222222222223,1.008151008151008,1.0,0.0007599999999999968,1.0088372093023257,0.009859885832900839,0.1556291390728477,0.008759797141539913,0.35184470184470185
frozenset({'gender_female'}),frozenset({'parental level of education_some college'}),0.518,0.226,0.118,0.2277992277992278,1.0079611849523353,1.0,0.0009319999999999884,1.00233,0.01638652507208644,0.18849840255591052,0.0023245837199325345,0.3749615608022687
frozenset({'parental level of education_some college'}),frozenset({'gender_female'}),0.226,0.518,0.118,0.5221238938053097,1.007961184952335,1.0,0.0009319999999999884,1.0086296296296295,0.010204528533263132,0.18849840255591052,0.00855579627657602,0.3749615608022687
"frozenset({'writing_cat_μέτριο', 'test preparation course_none'})",frozenset({'race/ethnicity_group C'}),0.308,0.319,0.099,0.32142857142857145,1.0076130765785938,1.0,0.0007479999999999987,1.0035789473684211,0.010918432883750783,0.1875,0.0035661841829242973,0.3158866995073892
frozenset({'race/ethnicity_group C'}),"frozenset({'writing_cat_μέτριο', 'test preparation course_none'})",0.319,0.308,0.099,0.3103448275862069,1.0076130765785938,1.0,0.0007479999999999987,1.0033999999999998,0.011094795235764378,0.1875,0.0033884791708192344,0.3158866995073892
frozenset({'gender_male'}),"frozenset({'test preparation course_none', 'lunch_standard'})",0.482,0.418,0.203,0.42116182572614114,1.007564176378328,1.0,0.0015240000000000253,1.0054623655913981,0.014493029271354634,0.2912482065997131,0.005432690251101639,0.4534038793702476
"frozenset({'test preparation course_none', 'lunch_standard'})",frozenset({'gender_male'}),0.418,0.482,0.203,0.4856459330143541,1.007564176378328,1.0,0.0015240000000000253,1.0070883720930233,0.012899294093748624,0.2912482065997131,0.007038480722691349,0.4534038793702476
"frozenset({'gender_male', 'test preparation course_none'})","frozenset({'math_cat_μέτριο', 'lunch_standard'})",0.308,0.332,0.103,0.3344155844155844,1.007275856673447,1.0,0.0007439999999999947,1.0036292682926828,0.010438296200684588,0.1918063314711359,0.0036161443347071426,0.322328274135503
"frozenset({'math_cat_μέτριο', 'lunch_standard'})","frozenset({'gender_male', 'test preparation course_none'})",0.332,0.308,0.103,0.31024096385542166,1.007275856673447,1.0,0.0007439999999999947,1.0032489082969431,0.010813324806697208,0.1918063314711359,0.003238387074308761,0.322328274135503
"frozenset({'math_cat_χαμηλό', 'test preparation course_none'})","frozenset({""parental level of education_associate's degree""})",0.246,0.222,0.055,0.22357723577235772,1.007104665641251,1.0,0.00038799999999999946,1.0020314136125656,0.009356161080298998,0.13317191283292978,0.002027295337220714,0.23566249176005272
"frozenset({""parental level of education_associate's degree""})","frozenset({'math_cat_χαμηλό', 'test preparation course_none'})",0.222,0.246,0.055,0.24774774774774774,1.007104665641251,1.0,0.00038799999999999946,1.0023233532934133,0.009067539144659954,0.13317191283292978,0.0023179678352092128,0.23566249176005272
"frozenset({'test preparation course_completed', 'math_cat_μέτριο', 'reading_cat_μέτριο'})",frozenset({'lunch_standard'}),0.117,0.645,0.076,0.6495726495726495,1.0070893791824023,1.0,0.0005349999999999938,1.0130487804878046,0.007972223877928024,0.1107871720116618,0.012880703021547797,0.38370105346849526
frozenset({'lunch_standard'}),"frozenset({'test preparation course_completed', 'math_cat_μέτριο', 'reading_cat_μέτριο'})",0.645,0.117,0.076,0.11782945736434108,1.0070893791824023,1.0,0.0005349999999999938,1.0009402460456942,0.019829503335804074,0.1107871720116618,0.0009393628135233033,0.38370105346849526
"frozenset({'math_cat_μέτριο', 'gender_female', 'lunch_standard'})",frozenset({'test preparation course_completed'}),0.172,0.358,0.062,0.3604651162790698,1.0068857996622063,1.0,0.0004240000000000077,1.0038545454545453,0.008259311204612896,0.13247863247863248,0.0038397449829747853,0.26682473691048464
frozenset({'test preparation course_completed'}),"frozenset({'math_cat_μέτριο', 'gender_female', 'lunch_standard'})",0.358,0.172,0.062,0.17318435754189945,1.0068857996622063,1.0,0.0004240000000000077,1.0014324324324324,0.010652195759220372,0.13247863247863248,0.0014303835047095007,0.26682473691048464
"frozenset({'test preparation course_completed', 'gender_male'})",frozenset({'lunch_standard'}),0.174,0.645,0.113,0.6494252873563219,1.0068609106299564,1.0,0.0007700000000000068,1.0126229508196722,0.008249587520624041,0.16005665722379606,0.012465598186822144,0.4123095429029672
frozenset({'lunch_standard'}),"frozenset({'test preparation course_completed', 'gender_male'})",0.645,0.174,0.113,0.17519379844961241,1.0068609106299564,1.0,0.0007700000000000068,1.0014473684210528,0.019194814907142137,0.16005665722379606,0.0014452765733806616,0.4123095429029672
"frozenset({'gender_female', 'test preparation course_none'})",frozenset({'parental level of education_some college'}),0.334,0.226,0.076,0.2275449101796407,1.0068358857506226,1.0,0.0005159999999999887,1.002,0.010194404931246815,0.15702479338842973,0.0019960079840319143,0.28191404801017433
frozenset({'parental level of education_some college'}),"frozenset({'gender_female', 'test preparation course_none'})",0.226,0.334,0.076,0.33628318584070793,1.0068358857506226,1.0,0.0005159999999999887,1.0034399999999999,0.00877192982456121,0.15702479338842973,0.0034282069680298998,0.28191404801017433
"frozenset({""parental level of education_associate's degree""})","frozenset({'writing_cat_μέτριο', 'gender_female'})",0.222,0.264,0.059,0.26576576576576577,1.0066885066885067,1.0,0.0003919999999999965,1.00240490797546,0.008539932900527134,0.13817330210772832,0.002399138268703477,0.2446253071253071
"frozenset({'writing_cat_μέτριο', 'gender_female'})","frozenset({""parental level of education_associate's degree""})",0.264,0.222,0.059,0.22348484848484845,1.0066885066885065,1.0,0.0003919999999999965,1.0019121951219512,0.009027266028002868,0.13817330210772832,0.0019085456103450514,0.2446253071253071
frozenset({'race/ethnicity_group C'}),frozenset({'lunch_free/reduced'}),0.319,0.355,0.114,0.35736677115987464,1.0066669610137315,1.0,0.0007550000000000057,1.0036829268292684,0.009725120436922039,0.2035714285714286,0.003669412650968463,0.3392467658616275
frozenset({'lunch_free/reduced'}),frozenset({'race/ethnicity_group C'}),0.355,0.319,0.114,0.3211267605633803,1.0066669610137313,1.0,0.0007550000000000057,1.0031327800829877,0.010267917856657224,0.2035714285714286,0.003122996421997492,0.3392467658616275
"frozenset({'reading_cat_μέτριο', 'parental level of education_some college'})",frozenset({'lunch_standard'}),0.114,0.645,0.074,0.6491228070175438,1.0063919488644089,1.0,0.0004699999999999982,1.0117499999999997,0.0071685681166493535,0.10802919708029196,0.011613540894489438,0.3819257445940432
frozenset({'lunch_standard'}),"frozenset({'reading_cat_μέτριο', 'parental level of education_some college'})",0.645,0.114,0.074,0.11472868217054262,1.0063919488644089,1.0,0.0004699999999999982,1.0008231173380036,0.017891130567186837,0.10802919708029196,0.0008224403730729301,0.3819257445940432
frozenset({'parental level of education_high school'}),frozenset({'lunch_free/reduced'}),0.196,0.355,0.07,0.35714285714285715,1.006036217303823,1.0,0.0004200000000000037,1.0033333333333334,0.007462686567164243,0.14553014553014557,0.003322259136212664,0.27716297786720323
frozenset({'lunch_free/reduced'}),frozenset({'parental level of education_high school'}),0.355,0.196,0.07,0.1971830985915493,1.0060362173038229,1.0,0.0004200000000000037,1.0014736842105263,0.00930232558139543,0.14553014553014557,0.0014715156611309738,0.27716297786720323
"frozenset({'test preparation course_completed', 'math_cat_μέτριο', 'lunch_standard'})",frozenset({'gender_female'}),0.119,0.518,0.062,0.5210084033613446,1.005807728496804,1.0,0.0003579999999999972,1.006280701754386,0.00655413569623956,0.10782608695652175,0.006241500749677507,0.3203497615262321
frozenset({'gender_female'}),"frozenset({'test preparation course_completed', 'math_cat_μέτριο', 'lunch_standard'})",0.518,0.119,0.062,0.11969111969111969,1.005807728496804,1.0,0.0003579999999999972,1.0007850877192983,0.011979654664703429,0.10782608695652175,0.0007844718400904576,0.3203497615262321
"frozenset({'race/ethnicity_group C', 'gender_female', 'test preparation course_none'})",frozenset({'lunch_standard'}),0.111,0.645,0.072,0.6486486486486486,1.0056568196103077,1.0,0.0004049999999999887,1.0103846153846152,0.006327334083239419,0.10526315789473682,0.010277883517319876,0.3801382778126964
frozenset({'lunch_standard'}),"frozenset({'race/ethnicity_group C', 'gender_female', 'test preparation course_none'})",0.645,0.111,0.072,0.11162790697674417,1.0056568196103077,1.0,0.0004049999999999887,1.0007068062827225,0.01584507042253477,0.10526315789473682,0.0007063070604546335,0.3801382778126964
"frozenset({'reading_cat_μέτριο', 'lunch_standard'})","frozenset({""parental level of education_associate's degree""})",0.327,0.222,0.073,0.22324159021406725,1.0055927487120147,1.0,0.0004059999999999897,1.0015984251968504,0.00826395815099004,0.1533613445378151,0.0015958743111404252,0.276035209521448
"frozenset({""parental level of education_associate's degree""})","frozenset({'reading_cat_μέτριο', 'lunch_standard'})",0.222,0.327,0.073,0.3288288288288288,1.0055927487120147,1.0,0.0004059999999999897,1.002724832214765,0.007148642462231745,0.1533613445378151,0.002717427680280517,0.276035209521448
frozenset({'test preparation course_completed'}),"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο'})",0.358,0.325,0.117,0.3268156424581006,1.005586592178771,1.0,0.0006500000000000117,1.0026970954356846,0.008653513326410678,0.2067137809187279,0.0026898406786675017,0.3434078212290503
"frozenset({'math_cat_μέτριο', 'reading_cat_μέτριο'})",frozenset({'test preparation course_completed'}),0.325,0.358,0.117,0.36,1.005586592178771,1.0,0.0006500000000000117,1.003125,0.008230452674897267,0.2067137809187279,0.0031152647975077907,0.3434078212290503
frozenset({'lunch_standard'}),"frozenset({'race/ethnicity_group C', 'test preparation course_none'})",0.645,0.202,0.131,0.20310077519379846,1.005449382147517,1.0,0.0007099999999999884,1.0013813229571984,0.015267175572518835,0.18296089385474862,0.0013794175360882803,0.4258078133394735
"frozenset({'race/ethnicity_group C', 'test preparation course_none'})",frozenset({'lunch_standard'}),0.202,0.645,0.131,0.6485148514851485,1.005449382147517,1.0,0.0007099999999999884,1.01,0.006791788631884945,0.18296089385474862,0.00990099009900991,0.4258078133394735
"frozenset({'writing_cat_μέτριο', ""parental level of education_associate's degree""})",frozenset({'test preparation course_none'}),0.11,0.642,0.071,0.6454545454545454,1.005380911922968,1.0,0.00037999999999999146,1.0097435897435896,0.0060136097483777734,0.1042584434654919,0.00964956830878597,0.3780232228830359
frozenset({'test preparation course_none'}),"frozenset({'writing_cat_μέτριο', ""parental level of education_associate's degree""})",0.642,0.11,0.071,0.11059190031152646,1.0053809119229677,1.0,0.00037999999999999146,1.0006654991243433,0.014950035407978264,0.1042584434654919,0.0006650565298050154,0.3780232228830359
frozenset({'math_cat_μέτριο'}),"frozenset({'race/ethnicity_group B', 'lunch_standard'})",0.485,0.121,0.059,0.12164948453608247,1.0053676407940701,1.0,0.0003150000000000028,1.0007394366197184,0.010366957380286417,0.10786106032906764,0.0007388902572041817,0.3046263951606032
"frozenset({'race/ethnicity_group B', 'lunch_standard'})",frozenset({'math_cat_μέτριο'}),0.121,0.485,0.059,0.48760330578512395,1.00536764079407,1.0,0.0003150000000000028,1.0050806451612901,0.006073928385492042,0.10786106032906764,0.005054962689561099,0.3046263951606032
"frozenset({'race/ethnicity_group C', 'gender_male', 'test preparation course_none'})",frozenset({'lunch_standard'}),0.091,0.645,0.059,0.6483516483516484,1.0051963540335633,1.0,0.0003049999999999997,1.00953125,0.005687009378903987,0.08714918759231904,0.009441262962389752,0.3699122582843513
frozenset({'lunch_standard'}),"frozenset({'race/ethnicity_group C', 'gender_male', 'test preparation course_none'})",0.645,0.091,0.059,0.09147286821705426,1.0051963540335633,1.0,0.0003049999999999997,1.0005204778156997,0.014561947958940068,0.08714918759231904,0.0005202070594656378,0.3699122582843513
frozenset({'gender_female'}),"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced', 'test preparation course_none', 'writing_cat_χαμηλό'})",0.518,0.098,0.051,0.09845559845559845,1.0046489638326372,1.0,0.00023599999999999316,1.0005053533190578,0.009600520706207517,0.09026548672566372,0.0005050980660736689,0.30943188086045226
"frozenset({'math_cat_χαμηλό', 'lunch_free/reduced', 'test preparation course_none', 'writing_cat_χαμηλό'})",frozenset({'gender_female'}),0.098,0.518,0.051,0.520408163265306,1.004648963832637,1.0,0.00023599999999999316,1.0050212765957445,0.005130211729924637,0.09026548672566372,0.004996189347107921,0.30943188086045226
"frozenset({'race/ethnicity_group C', 'test preparation course_completed'})",frozenset({'math_cat_μέτριο'}),0.117,0.485,0.057,0.48717948717948717,1.0044937879989426,1.0,0.0002549999999999983,1.00425,0.005066460034571105,0.10458715596330277,0.004232013940751811,0.3023526301876817
frozenset({'math_cat_μέτριο'}),"frozenset({'race/ethnicity_group C', 'test preparation course_completed'})",0.485,0.117,0.057,0.1175257731958763,1.0044937879989426,1.0,0.0002549999999999983,1.0005957943925234,0.008686765457332593,0.10458715596330277,0.0005954396329289859,0.3023526301876817
"frozenset({'math_cat_υψηλό', 'reading_cat_υψηλό'})","frozenset({'test preparation course_none', 'lunch_standard'})",0.131,0.418,0.055,0.4198473282442748,1.004419445560466,1.0,0.00024199999999999916,1.0031842105263158,0.005063291139240489,0.11133603238866398,0.0031741035124996627,0.27571313780634793
"frozenset({'test preparation course_none', 'lunch_standard'})","frozenset({'math_cat_υψηλό', 'reading_cat_υψηλό'})",0.418,0.131,0.055,0.13157894736842105,1.004419445560466,1.0,0.00024199999999999916,1.0006666666666666,0.007560137457044647,0.11133603238866398,0.0006662225183211047,0.27571313780634793
frozenset({'gender_female'}),frozenset({'test preparation course_none'}),0.518,0.642,0.334,0.6447876447876448,1.004342125837453,1.0,0.0014440000000000008,1.0078478260869566,0.008969612641936051,0.4043583535108959,0.007786717283924068,0.5825184329857227
frozenset({'test preparation course_none'}),frozenset({'gender_female'}),0.642,0.518,0.334,0.5202492211838007,1.004342125837453,1.0,0.0014440000000000008,1.0046883116883119,0.012076405847522838,0.4043583535108959,0.004666433991287687,0.5825184329857227
"frozenset({'race/ethnicity_group B', 'lunch_standard'})",frozenset({'test preparation course_none'}),0.121,0.642,0.078,0.6446280991735538,1.0040936124198656,1.0,0.00031799999999999884,1.0073953488372094,0.004638137743939775,0.11386861313868613,0.007341059144004965,0.38306171313817877
frozenset({'test preparation course_none'}),"frozenset({'race/ethnicity_group B', 'lunch_standard'})",0.642,0.121,0.078,0.12149532710280374,1.0040936124198656,1.0,0.00031799999999999884,1.0005638297872341,0.011388053287494588,0.11386861313868613,0.0005635120623478288,0.38306171313817877
"frozenset({'race/ethnicity_group C', 'gender_female'})","frozenset({'math_cat_μέτριο', 'lunch_standard'})",0.18,0.332,0.06,0.3333333333333333,1.004016064257028,1.0,0.00023999999999999716,1.0019999999999998,0.004878048780487747,0.1327433628318584,0.0019960079840318826,0.2570281124497992
"frozenset({'math_cat_μέτριο', 'lunch_standard'})","frozenset({'race/ethnicity_group C', 'gender_female'})",0.332,0.18,0.06,0.18072289156626503,1.004016064257028,1.0,0.00023999999999999716,1.0008823529411766,0.005988023952095738,0.1327433628318584,0.000881575080811022,0.2570281124497992
"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο'})",frozenset({'test preparation course_completed'}),0.281,0.358,0.101,0.3594306049822064,1.0039961033022526,1.0,0.0004019999999999996,1.0022333333333335,0.005535741334912345,0.18773234200743494,0.002228356670103472,0.3207767550050697
frozenset({'test preparation course_completed'}),"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.358,0.281,0.101,0.28212290502793297,1.0039961033022524,1.0,0.0004019999999999996,1.0015642023346305,0.00619968538909965,0.18773234200743494,0.0015617594268886525,0.3207767550050697
"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'math_cat_μέτριο'})",frozenset({'gender_female'}),0.1,0.518,0.052,0.5199999999999999,1.0038610038610036,1.0,0.00019999999999999185,1.0041666666666664,0.0042735042735041,0.09187279151943463,0.0041493775933607695,0.31019305019305016
frozenset({'gender_female'}),"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'math_cat_μέτριο'})",0.518,0.1,0.052,0.10038610038610038,1.0038610038610036,1.0,0.00019999999999999185,1.0004291845493563,0.007979572294924667,0.09187279151943463,0.0004290004290004158,0.31019305019305016
"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'reading_cat_μέτριο'})",frozenset({'lunch_standard'}),0.139,0.645,0.09,0.6474820143884892,1.003848084323239,1.0,0.0003449999999999842,1.0070408163265305,0.004452187379016444,0.1296829971181556,0.006991589826730035,0.3935084490547097
frozenset({'lunch_standard'}),"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.645,0.139,0.09,0.13953488372093023,1.003848084323239,1.0,0.0003449999999999842,1.0006216216216217,0.010798122065727205,0.1296829971181556,0.0006212354482348643,0.3935084490547097
frozenset({'test preparation course_completed'}),"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})",0.358,0.32,0.115,0.32122905027932963,1.0038407821229052,1.0,0.00044000000000000983,1.0018106995884772,0.005959637003928075,0.20426287744227356,0.0018074268813670948,0.3403020251396648
"frozenset({'writing_cat_μέτριο', 'math_cat_μέτριο'})",frozenset({'test preparation course_completed'}),0.32,0.358,0.115,0.359375,1.0038407821229052,1.0,0.00044000000000000983,1.0021463414634146,0.005626598465473272,0.20426287744227356,0.002141744548286628,0.3403020251396648
"frozenset({'gender_female', 'test preparation course_none'})",frozenset({'lunch_free/reduced'}),0.334,0.355,0.119,0.3562874251497006,1.0036265497174666,1.0,0.0004299999999999998,1.002,0.005425593660887777,0.20877192982456136,0.0019960079840319286,0.3457493463776672
frozenset({'lunch_free/reduced'}),"frozenset({'gender_female', 'test preparation course_none'})",0.355,0.334,0.119,0.3352112676056338,1.0036265497174663,1.0,0.0004299999999999998,1.001822033898305,0.005602240896358541,0.20877192982456136,0.0018187201285792512,0.3457493463776672
"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'test preparation course_completed', 'lunch_standard'})",0.281,0.227,0.064,0.2277580071174377,1.0033392384028093,1.0,0.00021299999999999097,1.0009815668202764,0.0046288247566062014,0.14414414414414414,0.0009806042916399786,0.2548481665543135
"frozenset({'test preparation course_completed', 'lunch_standard'})","frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.227,0.281,0.064,0.2819383259911894,1.0033392384028093,1.0,0.00021299999999999097,1.0013067484662577,0.004305465717981706,0.14414414414414414,0.0013050431031841035,0.2548481665543135
frozenset({'gender_male'}),"frozenset({'math_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",0.482,0.213,0.103,0.21369294605809128,1.0032532678783628,1.0,0.00033400000000000096,1.0008812664907651,0.006260074221239288,0.17398648648648649,0.0008804905439533438,0.3486305105877311
"frozenset({'math_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",frozenset({'gender_male'}),0.213,0.482,0.103,0.4835680751173709,1.0032532678783628,1.0,0.00033400000000000096,1.0030363636363637,0.004120353807626368,0.17398648648648649,0.0030271720412565497,0.3486305105877311
frozenset({'test preparation course_none'}),"frozenset({'gender_female', 'parental level of education_some college'})",0.642,0.118,0.076,0.11838006230529595,1.0032208669940335,1.0,0.00024399999999999422,1.0004310954063604,0.008967950602763682,0.1111111111111111,0.0004309096431926935,0.38122392945773276
"frozenset({'gender_female', 'parental level of education_some college'})",frozenset({'test preparation course_none'}),0.118,0.642,0.076,0.6440677966101696,1.0032208669940335,1.0,0.00024399999999999422,1.005809523809524,0.0036400525122328776,0.1111111111111111,0.005775968184831108,0.38122392945773276
"frozenset({'math_cat_μέτριο', 'lunch_standard', 'gender_male'})",frozenset({'test preparation course_none'}),0.16,0.642,0.103,0.6437499999999999,1.0027258566978192,1.0,0.0002799999999999886,1.004912280701754,0.0032362459546924254,0.1473533619456366,0.004888268156424353,0.4020930685358255
frozenset({'test preparation course_none'}),"frozenset({'math_cat_μέτριο', 'lunch_standard', 'gender_male'})",0.642,0.16,0.103,0.16043613707165108,1.0027258566978192,1.0,0.0002799999999999886,1.0005194805194804,0.0075934262624068076,0.1473533619456366,0.0005192107995846156,0.4020930685358255
"frozenset({'race/ethnicity_group C', 'gender_female'})","frozenset({'writing_cat_μέτριο', 'lunch_standard'})",0.18,0.338,0.061,0.3388888888888889,1.0026298487836949,1.0,0.00015999999999999348,1.001344537815126,0.0031987205117951513,0.1334792122538293,0.001342732460557226,0.259681130834977
"frozenset({'writing_cat_μέτριο', 'lunch_standard'})","frozenset({'race/ethnicity_group C', 'gender_female'})",0.338,0.18,0.061,0.18047337278106507,1.0026298487836949,1.0,0.00015999999999999348,1.00057761732852,0.003962161359021186,0.1334792122538293,0.0005772838793476504,0.259681130834977
frozenset({'lunch_standard'}),"frozenset({'test preparation course_completed', ""parental level of education_associate's degree""})",0.645,0.082,0.053,0.08217054263565891,1.0020797882397428,1.0,0.00010999999999999205,1.0001858108108108,0.005846399149614247,0.07863501483679526,0.00018577629156744024,0.3642560030251465
"frozenset({'test preparation course_completed', ""parental level of education_associate's degree""})",frozenset({'lunch_standard'}),0.082,0.645,0.053,0.646341463414634,1.0020797882397428,1.0,0.00010999999999999205,1.0037931034482757,0.0022608624162451605,0.07863501483679526,0.003778770182067709,0.3642560030251465
frozenset({'lunch_free/reduced'}),"frozenset({'parental level of education_some college', 'test preparation course_none'})",0.355,0.149,0.053,0.14929577464788732,1.0019850647509216,1.0,0.00010500000000000093,1.0003476821192052,0.0030715225976305667,0.11751662971175166,0.00034756127836348507,0.25250023631723223
"frozenset({'parental level of education_some college', 'test preparation course_none'})",frozenset({'lunch_free/reduced'}),0.149,0.355,0.053,0.35570469798657717,1.0019850647509216,1.0,0.00010500000000000093,1.0010937500000001,0.002328004789038444,0.11751662971175166,0.0010925550179491242,0.25250023631723223
"frozenset({'gender_female', 'test preparation course_none'})","frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο'})",0.334,0.281,0.094,0.281437125748503,1.001555607645918,1.0,0.00014599999999997948,1.0006083333333333,0.0023321193533956215,0.1804222648752399,0.0006079634888775584,0.3079783493511198
"frozenset({'math_cat_μέτριο', 'writing_cat_μέτριο', 'reading_cat_μέτριο'})","frozenset({'gender_female', 'test preparation course_none'})",0.281,0.334,0.094,0.33451957295373663,1.001555607645918,1.0,0.00014599999999997948,1.0007807486631015,0.002160210694522231,0.1804222648752399,0.0007801395701750953,0.3079783493511198
frozenset({'math_cat_μέτριο'}),"frozenset({'lunch_free/reduced', 'gender_male', 'test preparation course_none'})",0.485,0.105,0.051,0.10515463917525773,1.0014727540500736,1.0,7.499999999999868e-05,1.0001728110599077,0.002855511136493382,0.09461966604823749,0.0001727812014052927,0.29543446244477173
"frozenset({'lunch_free/reduced', 'gender_male', 'test preparation course_none'})",frozenset({'math_cat_μέτριο'}),0.105,0.485,0.051,0.4857142857142857,1.0014727540500736,1.0,7.499999999999868e-05,1.001388888888889,0.0016431153466973092,0.09461966604823749,0.0013869625520111122,0.29543446244477173
"frozenset({'gender_female', 'race/ethnicity_group D'})","frozenset({'test preparation course_none', 'lunch_standard'})",0.129,0.418,0.054,0.41860465116279066,1.0014465338822744,1.0,7.800000000000168e-05,1.0010400000000002,0.0016583747927031867,0.10953346855983774,0.0010389195236953288,0.27389562701680203
"frozenset({'test preparation course_none', 'lunch_standard'})","frozenset({'gender_female', 'race/ethnicity_group D'})",0.418,0.129,0.054,0.1291866028708134,1.0014465338822744,1.0,7.800000000000168e-05,1.0002142857142857,0.0024818633066056276,0.10953346855983774,0.0002142398057559145,0.27389562701680203
"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο'})",frozenset({'test preparation course_none'}),0.154,0.642,0.099,0.6428571428571429,1.0013351134846462,1.0,0.0001320000000000071,1.0024000000000002,0.0015760441292357037,0.14203730272596843,0.0023942537909019262,0.3985313751668892
frozenset({'test preparation course_none'}),"frozenset({'race/ethnicity_group C', 'writing_cat_μέτριο'})",0.642,0.154,0.099,0.15420560747663553,1.0013351134846462,1.0,0.0001320000000000071,1.000243093922652,0.0037243947858475,0.14203730272596843,0.00024303484235878623,0.3985313751668892
"frozenset({'reading_cat_μέτριο', 'lunch_standard'})",frozenset({'parental level of education_some college'}),0.327,0.226,0.074,0.2262996941896024,1.0013260804849664,1.0,9.799999999998699e-05,1.0003873517786561,0.0019677924581339504,0.1544885177453027,0.0003872017953519462,0.2768666612540932
frozenset({'parental level of education_some college'}),"frozenset({'reading_cat_μέτριο', 'lunch_standard'})",0.226,0.327,0.074,0.327433628318584,1.0013260804849664,1.0,9.799999999998699e-05,1.0006447368421052,0.0017110133389200883,0.1544885177453027,0.0006443214243447425,0.2768666612540932
"frozenset({'math_cat_μέτριο', 'lunch_standard'})",frozenset({'test preparation course_completed'}),0.332,0.358,0.119,0.358433734939759,1.0012115501110588,1.0,0.00014399999999999136,1.0006760563380281,0.0018115030443313965,0.2084063047285464,0.0006755995946402053,0.34541798478831526
frozenset({'test preparation course_completed'}),"frozenset({'math_cat_μέτριο', 'lunch_standard'})",0.358,0.332,0.119,0.3324022346368715,1.0012115501110588,1.0,0.00014399999999999136,1.000602510460251,0.0018848660959709857,0.2084063047285464,0.0006021476599872774,0.34541798478831526
frozenset({'parental level of education_some college'}),"frozenset({'writing_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",0.226,0.221,0.05,0.22123893805309736,1.001081167661074,1.0,5.3999999999998494e-05,1.0003068181818182,0.0013953488372092633,0.12594458438287154,0.00030672407329570466,0.22374164097225002
"frozenset({'writing_cat_μέτριο', 'test preparation course_none', 'lunch_standard'})",frozenset({'parental level of education_some college'}),0.221,0.226,0.05,0.22624434389140272,1.001081167661074,1.0,5.3999999999998494e-05,1.0003157894736843,0.0013863928112964952,0.12594458438287154,0.00031568978217404344,0.22374164097225002
"frozenset({'gender_male', 'lunch_standard'})",frozenset({'reading_cat_μέτριο'}),0.316,0.49,0.155,0.49050632911392406,1.001033324722294,1.0,0.00015999999999999348,1.0009937888198757,0.0015091492171287822,0.23809523809523808,0.0009928021841648321,0.40341642986308446
frozenset({'reading_cat_μέτριο'}),"frozenset({'gender_male', 'lunch_standard'})",0.49,0.316,0.155,0.3163265306122449,1.001033324722294,1.0,0.00015999999999999348,1.0004776119402985,0.0020240354206197786,0.23809523809523808,0.00047738393603057456,0.40341642986308446
frozenset({'test preparation course_completed'}),"frozenset({'math_cat_μέτριο', 'gender_male'})",0.358,0.24,0.086,0.24022346368715083,1.000931098696462,1.0,7.999999999999674e-05,1.0002941176470588,0.001448960370933796,0.16796874999999997,0.00029403116730374116,0.29927839851024207
"frozenset({'math_cat_μέτριο', 'gender_male'})",frozenset({'test preparation course_completed'}),0.24,0.358,0.086,0.35833333333333334,1.000931098696462,1.0,7.999999999999674e-05,1.0005194805194806,0.0012239902080782854,0.16796874999999997,0.0005192107995846606,0.29927839851024207
"frozenset({'gender_male', 'lunch_standard'})",frozenset({'test preparation course_none'}),0.316,0.642,0.203,0.6424050632911393,1.0006309397058244,1.0,0.000128000000000017,1.001132743362832,0.0009218448419901549,0.2688741721854305,0.0011314617070930675,0.47930222011908996
frozenset({'test preparation course_none'}),"frozenset({'gender_male', 'lunch_standard'})",0.642,0.316,0.203,0.31619937694704053,1.0006309397058244,1.0,0.000128000000000017,1.0002915717539864,0.001761290145031469,0.2688741721854305,0.0002914867646791381,0.47930222011908996
"frozenset({'reading_cat_μέτριο', 'gender_male'})","frozenset({'test preparation course_completed', 'lunch_standard'})",0.229,0.227,0.052,0.22707423580786024,1.0003270299905738,1.0,1.6999999999996185e-05,1.00009604519774,0.0004240247430907958,0.1287128712871287,9.603597394596246e-05,0.22807456283785082
"frozenset({'test preparation course_completed', 'lunch_standard'})","frozenset({'reading_cat_μέτριο', 'gender_male'})",0.227,0.229,0.052,0.22907488986784139,1.0003270299905738,1.0,1.6999999999996185e-05,1.0000971428571428,0.00042292765449288943,0.1287128712871287,9.713342132474286e-05,0.22807456283785082
frozenset({'test preparation course_none'}),"frozenset({'math_cat_χαμηλό', 'reading_cat_μέτριο', 'gender_female'})",0.642,0.109,0.07,0.1090342679127726,1.0003143845208495,1.0,2.2000000000008124e-05,1.0000384615384617,0.0008778930566643305,0.10279001468428782,3.846005922850517e-05,0.375618051387579
"frozenset({'math_cat_χαμηλό', 'reading_cat_μέτριο', 'gender_female'})",frozenset({'test preparation course_none'}),0.109,0.642,0.07,0.6422018348623854,1.0003143845208495,1.0,2.2000000000008124e-05,1.0005641025641026,0.0003527336860671496,0.10279001468428782,0.0005637845318026304,0.375618051387579
frozenset({'test preparation course_none'}),frozenset({'race/ethnicity_group B'}),0.642,0.19,0.122,0.19003115264797507,1.0001639613051319,1.0,1.9999999999992246e-05,1.0000384615384617,0.00045791739170235936,0.17183098591549292,3.846005922848025e-05,0.4160682079029349
frozenset({'race/ethnicity_group B'}),frozenset({'test preparation course_none'}),0.19,0.642,0.122,0.6421052631578947,1.0001639613051319,1.0,1.9999999999992246e-05,1.0002941176470588,0.00020238818053017855,0.17183098591549292,0.0002940311673036855,0.4160682079029349
"frozenset({'math_cat_μέτριο', 'lunch_standard'})",frozenset({'gender_female'}),0.332,0.518,0.172,0.5180722891566264,1.0001395543564215,1.0,2.399999999996849e-05,1.0001499999999997,0.00020888455646818422,0.2536873156342182,0.00014997750337423588,0.42505931060147917
frozenset({'gender_female'}),"frozenset({'math_cat_μέτριο', 'lunch_standard'})",0.518,0.332,0.172,0.332046332046332,1.0001395543564215,1.0,2.399999999996849e-05,1.0000693641618497,0.0002894914600015499,0.2536873156342182,6.935935079637137e-05,0.42505931060147917
